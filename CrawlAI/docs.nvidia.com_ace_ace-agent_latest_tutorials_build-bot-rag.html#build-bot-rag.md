[ACE](https://docs.nvidia.com/ace/overview/latest/index.html)Workflows
  * [Animation Pipeline](https://docs.nvidia.com/ace/animation-pipeline/latest/index.html)
  * [Tokkio](https://docs.nvidia.com/ace/tokkio/latest/index.html)
  * [Gaming Avatar](https://docs.nvidia.com/ace/gaming-avatar/latest/index.html)

Microservices
  * [Animation Graph](https://docs.nvidia.com/ace/animation-graph-microservice/latest/index.html)
  * [Omniverse Renderer](https://docs.nvidia.com/ace/omniverse-renderer-microservice/latest/index.html)
  * [Audio2Face-3D](https://docs.nvidia.com/ace/audio2face-3d-microservice/latest/index.html)
  * [ACE Agent](https://docs.nvidia.com/ace/ace-agent/latest/index.html)
  * [Riva ASR](https://docs.nvidia.com/ace/riva-asr-microservice/latest/index.html)
  * [Riva TTS](https://docs.nvidia.com/ace/riva-tts-microservice/latest/index.html)
  * [Riva Translation](https://docs.nvidia.com/ace/riva-translation-microservice/latest/index.html)
  * [Audio2Face-2D](https://docs.nvidia.com/ace/audio2face-2d-microservice/latest/index.html)
  * [Voice Font](https://docs.nvidia.com/ace/voice-font-microservice/latest/index.html)

Microservices (Early Access)
  * [Audio2Face-3D Authoring](https://docs.nvidia.com/ace/audio2face-3d-authoring-microservice/latest/index.html)
  * [Unreal Renderer](https://docs.nvidia.com/ace/unreal-renderer-microservice/latest/index.html)

Interfaces
  * [Animation Data Format](https://docs.nvidia.com/ace/animation-data-format/latest/index.html)
  * [Colang](https://docs.nvidia.com/ace/colang-language/latest/index.html)
  * [UMIM](https://docs.nvidia.com/ace/umim/latest/index.html)

Containers
  * [Resource Downloader](https://docs.nvidia.com/ace/resource-downloader-container/latest/index.html)

Tools
  * [Avatar Customization](https://docs.nvidia.com/ace/avatar-customization/latest/index.html)
  * [Unified Cloud Services Tools (UCS Tools)](https://docs.nvidia.com/ucf/)
  * [Maya-ACE](https://docs.nvidia.com/ace/maya-ace/latest/index.html)
  * [ACE Unreal Plugin](https://docs.nvidia.com/ace/ace-unreal-plugin/latest/index.html)


[Skip to main content](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#main-content)
Back to top
`Ctrl`+`K`
[ ![ACE Agent - Home](https://docs.nvidia.com/ace/ace-agent/latest/_static/nvidia-logo-horiz-rgb-blk-for-screen.svg) ![ACE Agent - Home](https://docs.nvidia.com/ace/ace-agent/latest/_static/nvidia-logo-horiz-rgb-wht-for-screen.svg) ACE Agent ](https://docs.nvidia.com/ace/ace-agent/latest/index.html)
Search `Ctrl`+`K`
Search `Ctrl`+`K`
[ ![ACE Agent - Home](https://docs.nvidia.com/ace/ace-agent/latest/_static/nvidia-logo-horiz-rgb-blk-for-screen.svg) ![ACE Agent - Home](https://docs.nvidia.com/ace/ace-agent/latest/_static/nvidia-logo-horiz-rgb-wht-for-screen.svg) ACE Agent ](https://docs.nvidia.com/ace/ace-agent/latest/index.html)
Table of Contents
  * [Getting Started](https://docs.nvidia.com/ace/ace-agent/latest/getting-started.html)
    * [Overview](https://docs.nvidia.com/ace/ace-agent/latest/overview.html)
    * [Quick Start Guide](https://docs.nvidia.com/ace/ace-agent/latest/quick-start-guide.html)
    * [Release Notes](https://docs.nvidia.com/ace/ace-agent/latest/release-notes.html)
  * [Architecture](https://docs.nvidia.com/ace/ace-agent/latest/architecture/index.html)
    * [Architecture Introduction](https://docs.nvidia.com/ace/ace-agent/latest/architecture/arch-intro.html)
  * [Deployment](https://docs.nvidia.com/ace/ace-agent/latest/deployment/index.html)
    * [Interface Introduction](https://docs.nvidia.com/ace/ace-agent/latest/deployment/interface-intro.html)
    * [Docker Environment](https://docs.nvidia.com/ace/ace-agent/latest/deployment/docker-environment.html)
    * [Kubernetes Environment](https://docs.nvidia.com/ace/ace-agent/latest/deployment/kubernetes-environment.html)
    * [Python Environment](https://docs.nvidia.com/ace/ace-agent/latest/deployment/python-environment.html)
    * [Sample Clients](https://docs.nvidia.com/ace/ace-agent/latest/deployment/sample-clients.html)
  * [Tutorials](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/index.html)
    * [Building a Bot using Colang 2.0 and Event Interface](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-colang.html)
    * [Building LangChain-Based Bots](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-langchain-bot.html)
    * [Building a Low Latency Speech-To-Speech RAG Bot](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html)
    * [Customizing a Bot](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/customize-a-bot.html)
  * [Sample Bots](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/index.html)
    * [Sample Bots Introduction](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/sample-bots-intro.html)
    * [Chitchat Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/chitchat-bot.html)
    * [LLM Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/llm-bot.html)
    * [RAG Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/rag-bot.html)
    * [Stock Market Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/stock-market-bot.html)
    * [Gaming Non-Playing Character (NPC) Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/gaming-npc-bot.html)
    * [DuckDuckGo LangChain Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/duckduckgo-langchain-bot.html)
    * [Spanish Weather Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/spanish-weather-bot.html)
    * [Food Ordering Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/food-ordering-bot.html)
    * [Plan and Execute (LangGraph) Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/plan-execute-langgraph-bot.html)
    * [Colang 1.0 Sample Bots](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/colang1-sample-bots.html)
  * [Configuration Guide](https://docs.nvidia.com/ace/ace-agent/latest/config/index.html)
    * [Bot Configurations Introduction](https://docs.nvidia.com/ace/ace-agent/latest/config/bot-configurations.html)
    * [General Configurations](https://docs.nvidia.com/ace/ace-agent/latest/config/general-configurations.html)
    * [Slots Configurations](https://docs.nvidia.com/ace/ace-agent/latest/config/slots-configurations.html)
    * [Plugin Configurations](https://docs.nvidia.com/ace/ace-agent/latest/config/plugin-configurations.html)
    * [Model Configurations](https://docs.nvidia.com/ace/ace-agent/latest/config/model-configurations.html)
    * [Chat Engine Configurations](https://docs.nvidia.com/ace/ace-agent/latest/config/chat-engine-configurations.html)
    * [Speech Configurations](https://docs.nvidia.com/ace/ace-agent/latest/config/speech-configurations.html)
  * [User Guide](https://docs.nvidia.com/ace/ace-agent/latest/user/index.html)
    * [Using Colang](https://docs.nvidia.com/ace/ace-agent/latest/user/colang.html)
    * [Integration with LangChain and LlamaIndex](https://docs.nvidia.com/ace/ace-agent/latest/user/integrate-langchan-llmaindex.html)
    * [NLP Server](https://docs.nvidia.com/ace/ace-agent/latest/user/nlp-server.html)
    * [Speech AI](https://docs.nvidia.com/ace/ace-agent/latest/user/speech-ai.html)
    * [Training Models](https://docs.nvidia.com/ace/ace-agent/latest/user/training-models.html)
    * [Plugin Server](https://docs.nvidia.com/ace/ace-agent/latest/user/plugin-server.html)
    * [Streaming in ACE Agent](https://docs.nvidia.com/ace/ace-agent/latest/user/streaming.html)
  * [API Guide](https://docs.nvidia.com/ace/ace-agent/latest/api/index.html)
    * [API Introduction](https://docs.nvidia.com/ace/ace-agent/latest/api/api-intro.html)
    * [HTTP Interface](https://docs.nvidia.com/ace/ace-agent/latest/api/http-interface.html)
    * [gRPC Interface](https://docs.nvidia.com/ace/ace-agent/latest/api/grpc-interface.html)
    * [Plugin Server](https://docs.nvidia.com/ace/ace-agent/latest/api/plugin-server-api.html)
    * [NLP Server](https://docs.nvidia.com/ace/ace-agent/latest/api/nlp-server-api.html)
  * [Best Practices](https://docs.nvidia.com/ace/ace-agent/latest/best-practices/index.html)
    * [Best Practices Introduction](https://docs.nvidia.com/ace/ace-agent/latest/best-practices/best-practices-intro.html)
    * [Planning your Bot](https://docs.nvidia.com/ace/ace-agent/latest/best-practices/planning-bot.html)
    * [Logging and Debugging Issues](https://docs.nvidia.com/ace/ace-agent/latest/best-practices/log-debug-issues.html)
    * [Security Considerations](https://docs.nvidia.com/ace/ace-agent/latest/best-practices/security.html)
  * [Reference](https://docs.nvidia.com/ace/ace-agent/latest/reference/index.html)
    * [Basic Concepts](https://docs.nvidia.com/ace/ace-agent/latest/reference/basic-concepts.html)
    * [Support Matrix](https://docs.nvidia.com/ace/ace-agent/latest/reference/support-matrix.html)
    * [Migrating from ACE Agent 4.0.0 to ACE Agent 4.1.0](https://docs.nvidia.com/ace/ace-agent/latest/reference/migrating.html)
    * [Troubleshooting](https://docs.nvidia.com/ace/ace-agent/latest/reference/troubleshooting.html)
    * [End User License Agreement](https://docs.nvidia.com/ace/ace-agent/latest/reference/eula.html)
    * [Notices](https://docs.nvidia.com/ace/ace-agent/latest/reference/notices.html)


4.1
[4.1](https://docs.nvidia.com/ace/ace-agent/4.1/tutorials/build-bot-rag.html)[4.0](https://docs.nvidia.com/ace/ace-agent/4.0/tutorials/build-bot-rag.html)
  * [ ](https://docs.nvidia.com/ace/ace-agent/latest/index.html)
  * [Tutorials](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/index.html)
  * Building a...


# Building a Low Latency Speech-To-Speech RAG Bot[#](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#building-a-low-latency-speech-to-speech-rag-bot "Link to this heading")
In this section, we will build a replica of the [RAG Sample Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/rag-bot.html#rag-bot) packaged as part of this release. The RAG sample bot utilizes the [NVIDIA RAG Examples pipeline](https://github.com/NVIDIA/GenerativeAIExamples/tree/main) to answer questions based on the uploaded documents. The RAG sample bot showcases the following ACE Agent features:
  * Integrating a RAG example from [NVIDIA’s Generative AI Examples](https://github.com/NVIDIA/GenerativeAIExamples)
  * Low latency using ASR 2 pass End of Utterance (EOU)
  * Always-on Barge-In support
  * Handling conversation history with plugins
  * Support deployment using [Event Architecture](https://docs.nvidia.com/ace/ace-agent/latest/architecture/arch-intro.html#ace-agent-event-arch)


Similar to the [tutorial for LangChain-based Bots](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-langchain-bot.html#build-langchain-bot), we will use a Plugin to interact with the RAG server. This plugin can be connected to the Chat Controller or the Chat Engine. Then, we will utilize Colang to add an always-on barge-in support, improve end-to-end speech latency for bot responses, and add an option for long pause handling. Similar features can be added in the LangChain agents or custom RAG pipelines in similar ways to improve the user experience.
The minimal file structure of the RAG bot will looks like this:
> ```
samples
└── rag_tutorial_bot
  └── plugin
    └── rag.py
    └── schemas.py
  └── colang
    └── main.co
    └── speech.co
  └── plugin_config.yaml
  └── bot_config.yaml
  └── speech_config.yaml
  └── model_config.yaml

```

Create an empty directory called `samples/rag_tutorial_bot` and perform the following steps for updating bot configurations.
## Creating the RAG Plugin[#](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#creating-the-rag-plugin "Link to this heading")
In this section, let’s build the custom RAG plugin and test it as a standalone component.
  1. Deploy the RAG server by following the instructions in the [RAG Sample Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/rag-bot.html#rag-bot) or the [NVIDIA Generative AI examples repository](https://github.com/NVIDIA/GenerativeAIExamples/tree/main).
  2. Define the input and output API schemas used by ACE Agent for communication with the Plugin server and Chat Controller. Copy `samples/rag_bot/plugin/schemas.py` to `samples/rag_tutorial_bot/plugin/schemas.py`.
> Let’s understand the APIs and their schemas a little. There are two APIs used by the Chat Controller microservice: `/chat` and `/event`.
>>     1. Events such as pipeline creation and deletion are sent by the Chat Controller to the `/event` endpoint. In most cases, you will not need to modify the default `/event` endpoint that we will define in the next step.
>>     2. User questions are sent by the Chat Controller to the `/chat` endpoint, along with the `UserId` and an optional `QueryId`. The response schema for this API contains a `Response` attribute, which contains the details of the response. In this tutorial, we only need to manage two sub-fields: `Response.Text` (which contains the chunk that we are streaming) and `Response.IsFinal` (which indicates whether the stream is complete).
  3. Create the actual custom Plugin with the RAG Server API call and the `/chat` and `/event` APIs. Copy `samples/rag_bot/plugin/rag.py` to `samples/rag_tutorial_bot/plugin/rag.py`.
>     1. The `rag_stream` function takes in the query and additional request parameters and performs API calls to the RAG server, and returns the streaming response generator which formats the rag response into the `ChatResponse` schema, and pushes the chunk into the response stream. If you are using a custom RAG solution or if you want to modify the way you call the RAG, you will only need to make changes in the `generator` method.
>> ```
async def rag_stream(
  question: Optional[str] = "",
  chat_history: Optional[List] = [],
  num_tokens: Optional[int] = MAX_TOKENS,
) -> int:
"""
  Call the RAG chain server and return the streaming response.
  """
  request_json = {
    "messages": chat_history + [{"role": "user", "content": question}],
   "use_knowledge_base": True,
   "temperature": TEMPERATURE,
    "top_p": TOP_P,
    "max_tokens": num_tokens,
    "seed": 42,
    "bad": [],
   "stop": STOP_WORDS,
   "stream": True,
 }
 # Method that forwards the stream to the Chat controller
 async def generator():
    full_response = ""
    if question:
      async with aiohttp.ClientSession() as session:
        async with session.post(GENERATION_URL, json=request_json) as resp:
          async for chunk, _ in resp.content.iter_chunks():
            try:
              chunk = chunk.decode("utf-8")
              chunk = chunk.strip("\n")
              try:
                if len(chunk) > 6:
                  parsed = json.loads(chunk[6:])
                  message = parsed["choices"][0]["message"]["content"]
                else:
                  logger.debug(f"Received empty RAG response chunk '{chunk}'.")
                  message = ""
              except Exception as e:
                logger.warning(f"Parsing RAG response chunk '{chunk}' failed. {e}")
                message = ""
              if not message:
                continue
              full_response += message
              json_chunk = ChatResponse()
              json_chunk.Response.Text = message
              json_chunk.Response.CleanedText = message
              json_chunk = json.dumps(json_chunk.dict())
              yield json_chunk
            except Exception as e:
              yield f"Internal error in RAG stream: {e}"
              break
    logger.info(f"Full RAG response for query `{question}` : {full_response}")
    json_chunk = ChatResponse()
    json_chunk.Response.IsFinal = True
    json_chunk = json.dumps(json_chunk.dict())
    yield json_chunk
  return StreamingResponse(generator(), media_type="text/event-stream")

```

>     2. The `/chat` endpoint invokes the `rag_stream` with user query and chat history if available and forward the streaming RAG response.
>> ```
@router.post(
  "/chat",
  status_code=status.HTTP_200_OK,
)
async def chat(
  request: Annotated[
    ChatRequest,
    Body(
      description="Chat Engine Request JSON. All the fields populated as part of this JSON is also available as part of request JSON."
    ),
  ],
  response: Response,
) -> StreamingResponse:
"""
  This endpoint can be used to provide response to query driven user request.
  """
  req = request.dict(exclude_none=True)
  logger.info(f"Received request JSON at /chat_stream endpoint for RAG : {json.dumps(req,indent=4)}")
  try:
    chat_history = []
    if "Metadata" in req:
      chat_history = req["Metadata"].get("ChatHistory", [])
    resp = await rag_stream(question=req["Query"], chat_history=chat_history)
    return resp
  except Exception as e:
    response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
    return {"StatusMessage": str(e)}

```

>     3. The `/event` endpoint is only invoked if the chat controller is directly connected via the [Plugin Server Architecture](https://docs.nvidia.com/ace/ace-agent/latest/architecture/arch-intro.html#plugin-serv-arch) and it is optional for our tutorial.
  4. Register this plugin. Add the following to `plugin_config.yaml`.
> ```
config:
workers:1
timeout:30
plugins:
-name:rag
path:./plugin/rag.py

```

  5. Deploy the Plugin server for testing. Run the bot using the [Docker Environment](https://docs.nvidia.com/ace/ace-agent/latest/deployment/docker-environment.html#docker-environment).
> ```
exportBOT_PATH=./samples/rag_tutorial_bot
sourcedeploy/docker/docker_init.sh
dockercompose-fdeploy/docker/docker-compose.ymlup--buildplugin-server-d

```

  6. Ingest documents as required for your use case by visiting `http://<your-ip>:8090/kb`. We can test the endpoint by asking queries relevant to your documents using the following `curl` command.
> ```
curl-X'POST'\
'http://localhost:9002/rag/chat'\
-H'accept: application/json'\
-H'Content-Type: application/json'\
-d'{
  "Query": "Who is the president of the United States?",
  "UserId": "user1"
}'

```

  7. After you are done testing the plugin, stop the server.
> ```
dockercompose-fdeploy/docker/docker-compose.ymldown

```



## Connecting Chat Controller to the RAG Plugin[#](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#connecting-chat-controller-to-the-rag-plugin "Link to this heading")
[![LangChain UCS App](https://docs.nvidia.com/ace/ace-agent/latest/_images/langchain-ucs-app.png) ](https://docs.nvidia.com/ace/ace-agent/latest/_images/langchain-ucs-app.png)
  1. Copy the `model_config.yaml` and `speech_config.yaml` files from `samples/chitchat_bot`. They represent the common settings for a speech pipeline.
  2. Update the `server` URL in the `dialog_manager` component in the `speech_config.yaml` file to point to the pre-built RAG Plugin we defined in the previous step.
> ```
dialog_manager:
DialogManager:
server:"http://localhost:9002/rag"
use_streaming:true

```

> With this change, the Chat Controller will directly call the `/chat` and `/event` endpoints of the Plugin server.
  3. Deploy the bot.
>     1. Set the environment variables required for the `docker-compose.yml` file.
>> ```
exportBOT_PATH=./samples/rag_tutorial_bot/
sourcedeploy/docker/docker_init.sh

```

>     2. For [Plugin Server Architecture](https://docs.nvidia.com/ace/ace-agent/latest/architecture/arch-intro.html#plugin-serv-arch) based bots, we need to use the `speech_lite` pipeline configuration for the Chat Controller microservice. Update the `PIPELINE` variable in `deploy/docker/docker_init.sh` or override it by setting the `PIPELINE` environment variable manually.
>> ```
exportPIPELINE=speech_lite

```

>     3. Deploy the Riva ASR and TTS speech models.
>> ```
dockercompose-fdeploy/docker/docker-compose.ymlupmodel-utils-speech

```

>     4. Deploy the Plugin server with RAG plugin.
>> ```
dockercompose-fdeploy/docker/docker-compose.ymlup--buildplugin-server-d

```

>     5. Deploy the Chat Controller microservice with the gRPC interface.
>> ```
dockercompose-fdeploy/docker/docker-compose.ymlupchat-controller-d

```

  4. Interact with the bot using the [Speech WebUI application](https://docs.nvidia.com/ace/ace-agent/latest/deployment/sample-clients.html#speech-web-sample-app).
> ```
dockercompose-fdeploy/docker/docker-compose.ymlupbot-web-ui-clientbot-web-ui-speech-d

```

> Notice that we are not deploying the Chat Engine container at all.
> You can interact with the bot using your browser at `http://<YOUR_IP_ADDRESS>:7006`.


Here’s an example snippet:
[![Speech Based Sample App](https://docs.nvidia.com/ace/ace-agent/latest/_images/speech-based-sample-app.png) ](https://docs.nvidia.com/ace/ace-agent/latest/_images/speech-based-sample-app.png)
## Connecting the Plugin to the Chat Engine[#](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#connecting-the-plugin-to-the-chat-engine "Link to this heading")
You can add guardrails to the bot or add any custom logic in Colang by creating the configurations needed for the Chat Engine and connecting the Plugin server to the Chat Engine using the [Event Interface Architecture](https://docs.nvidia.com/ace/ace-agent/latest/architecture/arch-intro.html#ace-agent-event-arch) or [Chat Engine Server Architecture](https://docs.nvidia.com/ace/ace-agent/latest/architecture/arch-intro.html#chat-engine-serv-arch).
[![Speech Bot Web App UCS App](https://docs.nvidia.com/ace/ace-agent/latest/_images/speech-bot-webapp-ucs-app.png) ](https://docs.nvidia.com/ace/ace-agent/latest/_images/speech-bot-webapp-ucs-app.png)
### Creating the Bot and Colang Configurations[#](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#creating-the-bot-and-colang-configurations "Link to this heading")
`bot_config.yaml` is the configuration entry point for any bot. Let’s create this file and add a few important configuration parameters.
  1. Give the bot a name. In `bot_config.yaml`, you need to add a unique name for the bot. Let’s name the bot `nvidia_rag_bot`. We will use Colang2.0-beta syntax for this tutorial.
> ```
bot:nvidia_rag_bot
colang_version:"2.x"
storage:
name:cache
configs:
use_stateful_guardrails:True

```

  2. All the intelligence in our bot will be present in the RAG server. Since our Colang configs are only meant to route the queries to the plugin, let’s keep the model section empty.
> ```
models:[]

```

  3. Create a folder name `colang` and a Colang file called `main.co`, which will contain all the Colang logic. Let’s update `main.co` to route all queries to the RAG plugin.
> ```
importcore
flowtechnicalhelper
# Helper flows for notifying errors
activatenotificationofundefinedflowstart"I have encountered some technical issue!"
activatenotificationofcolangerrors"I have encountered some technical issue!"
flowgenerateragresponse$transcript
# Invoke /chat endpoint from plugin
$started=awaitInvokeStreamingChatAction(question=$transcript,endpoint="rag/chat",chat_history=True)
if$started
# Get first sentence from RAG response
$response=awaitStreamingResponseChatAction(endpoint="rag/chat")
while$response
botsay$response
# Check for next sentence
$response=awaitStreamingResponseChatAction(endpoint="rag/chat")
flowrag
# Wait for user queries
usersaidsomethingas$ref
# Generate RAG response when user query received
generateragresponse$ref.transcript
flowmain
activatetechnicalhelper
activaterag

```

> The above flow routes all user utterances to a POST endpoint called `/rag/chat` in a plugin server. It passes the user’s question as well as a `session_id` to the endpoint as request parameters.
> Note
> If you want to add more complicated logic in Colang, you must update `main.co` and possibly the bot config file according to your use case. Refer to [Using Colang](https://docs.nvidia.com/ace/ace-agent/latest/user/colang.html#colang) for more information.


### Testing the Bot[#](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#testing-the-bot "Link to this heading")
  1. Run the bot in gRPC Interface using the [Docker Environment](https://docs.nvidia.com/ace/ace-agent/latest/deployment/docker-environment.html#docker-grpc-interface).
  2. Set the environment variables required for the `docker-compose.yml` file.
> ```
exportBOT_PATH=./samples/rag_tutorial_bot/
sourcedeploy/docker/docker_init.sh

```

  3. For [Event Interface Architecture](https://docs.nvidia.com/ace/ace-agent/latest/architecture/arch-intro.html#ace-agent-event-arch) based bots, we need to use the `speech_umim` pipeline configuration for the Chat Controller microservice. Update the `PIPELINE` variable in `deploy/docker/docker_init.sh` or override it by setting the `PIPELINE` environment variable manually.
> ```
exportPIPELINE=speech_umim

```

  4. Deploy the Riva ASR and TTS speech models.
> ```
dockercompose-fdeploy/docker/docker-compose.ymlupmodel-utils-speech

```

  5. Deploy the ACE Agent microservices. Deploy the Chat Controller, Chat Engine, Redis, Plugin server, and NLP server microservices. The NLP server will not have any models deployed for this bot.
> ```
dockercompose-fdeploy/docker/docker-compose.ymlup--buildspeech-event-bot-d

```

  6. Interact with the bot using the [Sample WebUI application](https://docs.nvidia.com/ace/ace-agent/latest/deployment/sample-clients.html#speech-web-sample-app) using your browser at `http://<YOUR_IP_ADDRESS>:7006`.


## Improving the User Experience for Speech-To-Speech Conversations[#](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#improving-the-user-experience-for-speech-to-speech-conversations "Link to this heading")
As humans, when we are talking, we take a pause during conversation, start thinking before others stop speaking and even interrupt during conversation. Most agents/bots today wait for the user to finish a query before processing the user query. Detecting the end of user speech is also subjective, as some people might take long pauses between words.
Riva Automatic Speech Recognition (ASR) pipeline waits, by default, 800 ms of silence in user audio for marking the end of the user speech. Therefore, user perceived latency will always be 800 ms + agent processing latency (RAG latency in this tutorial) at minimum. Apart from this, we will have ASR processing latency, Text-to-Speech Generation Latency, Network latency, and so on.
[Riva ASR pipeline](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-overview.html) has multiple components. It might not be optimal to run a full pipeline for every audio chunk. The ASR pipeline will return a greedy decoded partial transcript for every user audio chunk after 800 ms. The final generated transcript will include beam LM decoding, punctuation and custom vocabulary. You can optionally configure it to return an interim transcript after some silence value less than 800 ms (240 ms, for example) with the same processing as the final transcript.
In this section, we will focus on the following user experience improvements:
  * Reduce 560 ms latency by using interim transcript with 240 ms silence as EOU instead of 800 ms for triggering RAG API
  * Always on Barge In support, basically stop the bot response when user interrupts
  * Option to configure the end of user speech threshold (currently 800 ms) for long pauses in user audio. Refer to [Customizing ASR Recognition for Long Pause Handling](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/customize-a-bot.html#cust-asr-long-pause) for more information.


### Updating Bot Configurations[#](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#updating-bot-configurations "Link to this heading")
  1. Copy `speech.co` from RAG sample bot directory `samples/rag_bot/`. The `speech.co` implements Colang flows for utilizing partial/interim transcripts and supporting Always on Barge In. Follow the diagram below to understand the flows with interim transcript mode.

[![Interim Transcript with Barge In](https://docs.nvidia.com/ace/ace-agent/latest/_images/Interim-Transcript-with-Barge-In.png) ](https://docs.nvidia.com/ace/ace-agent/latest/_images/Interim-Transcript-with-Barge-In.png)
Note
We will send an early trigger to the RAG server and might need to retrigger if the user takes more than 240 ms pause between words. On average, you might do two extra RAG calls for each user query which will require extra compute/cost for deploying on scale. If your use case has external API calls or any action which is non reversible or costly, you shouldn’t use the two pass EOU approach. You might need extra handling to ensure consequences for triggering pipeline multiple times for single user query.
  1. Update `main.co` to utilize flows from `speech.co` Colang file. The flow, `handle user transcript with interruption` listens to every partial, interim, and final transcript and decides to interrupt the bot response, setting the new active transcript and ignoring the spurious transcript. The flow, `user partially said something` returns when a new active transcript is set.
> ```
importcore
flowtechnicalhelper
activatenotificationofundefinedflowstart"I have encountered some technical issue!"
activatenotificationofcolangerrors"I have encountered some technical issue!"
# activate interim transcript mode and add list of flows to stop during user interruption / Barge In
activatehandleusertranscriptwithinterruption$mode="interim"$stop_flows_list=["_bot_say","generate rag response"]
flowgenerateragresponse$transcript
$started=awaitInvokeStreamingChatAction(question=$transcript,endpoint="rag/chat",chat_history=True)
if$started
$response=awaitStreamingResponseChatAction(endpoint="rag/chat")
log"response from RAG: {$response}"
while$response
botsay$response
$response=awaitStreamingResponseChatAction(endpoint="rag/chat")
log"response from RAG: {$response}"
flowrag
# wait for new active transcript
userpartiallysaidsomethingas$ref
generateragresponse$ref.transcript
flowmain
activatetechnicalhelper
activaterag

```

> We have added `generate rag response` in `stop_flows_list` to interrupt active RAG calls. Make sure any flows added in `stop_flows_list` are not using the activate keyword. In this example, we set the interim transcript mode, but you can even decide to work with partial transcripts, or only use final transcript.
  2. Riva ASR might detect a few spurious transcripts and that might interrupt the bot response unnecessarily. Create and update `actions.py` with spurious filter action.
> ```
importlogging
fromnemoguardrails.actions.actionsimportaction
logger=logging.getLogger("nemoguardrails")
# Transcript filtering for spurious transcript and filler words. Along with this any transcript less than 3 chars is removed
FILTER_WORDS=[
"yeah",
"okay",
"right",
"yes",
"yum",
"and",
"one",
"all",
"when",
"thank",
"but",
"next",
"what",
"i see",
"the",
"hmm",
"mmm",
"so that",
"why",
"that",
"well",
]
INCLUDE_WORDS=["hi"]
@action(name="IsSpuriousAction")
asyncdefis_spurious(query):
"""
  Filter transcript less than 3 chars or in FILTER_WORDS list to avoid spurious transcript and filler words.
  """
ifquery.strip().lower()inFILTER_WORDSor(len(query)<3andquery.strip().lower()notinINCLUDE_WORDS):
returnTrue
else:
returnFalse

```

  3. Update the `speech_config.yaml` parameters to send an interim transcript with 240 ms user silence. Refer to the [Customizing ASR Recognition for Long Pause Handling](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/customize-a-bot.html#cust-asr-long-pause) section for more information. Update the `riva_asr` component config as shown below.
> ```
riva_asr:
RivaASR:
server:"localhost:50051"
# Update interim and final transcript silence threshold
endpointing_stop_history:800# Second pass End of User Speech
endpointing_stop_history_eou:240# First pass End of User Speech

```

  4. With a higher value for `endpointing_stop_history`, we can avoid breaking user query into multiple queries for long pauses in user audio. It is recommended to use a value of 800 ms at minimum to avoid ASR transcript quality issues.


### Testing the Bot[#](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#id1 "Link to this heading")
  1. Run the bot in gRPC Interface using the [Docker Environment](https://docs.nvidia.com/ace/ace-agent/latest/deployment/docker-environment.html#docker-grpc-interface).
  2. Set the environment variables required for the `docker-compose.yml` file.
> ```
exportBOT_PATH=./samples/rag_tutorial_bot/
sourcedeploy/docker/docker_init.sh

```

  3. For [Event Interface Architecture](https://docs.nvidia.com/ace/ace-agent/latest/architecture/arch-intro.html#ace-agent-event-arch) based bots, we need to use the `speech_umim` pipeline configuration for the Chat Controller microservice. Update the `PIPELINE` variable in `deploy/docker/docker_init.sh` or override it by setting the `PIPELINE` environment variable manually.
> ```
exportPIPELINE=speech_umim

```

  4. Deploy the Riva ASR and TTS speech models.
> ```
dockercompose-fdeploy/docker/docker-compose.ymlupmodel-utils-speech

```

  5. Deploy the ACE Agent microservices. Deploy the Chat Controller, Chat Engine, Redis, Plugin server, and NLP server microservices. The NLP server will not have any models deployed for this bot.
> ```
dockercompose-fdeploy/docker/docker-compose.ymlup--buildspeech-event-bot-d

```

  6. Interact with the bot using the [Sample WebUI application](https://docs.nvidia.com/ace/ace-agent/latest/deployment/sample-clients.html#speech-web-sample-app) using your browser at `http://<YOUR_IP_ADDRESS>:7006`. You should be able to observe on average 500-600 ms improvements in latency and can barge-in anytime to stop the current bot response.


[ previous Building LangChain-Based Bots ](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-langchain-bot.html "previous page") [ next Customizing a Bot ](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/customize-a-bot.html "next page")
On this page 
  * [Creating the RAG Plugin](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#creating-the-rag-plugin)
  * [Connecting Chat Controller to the RAG Plugin](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#connecting-chat-controller-to-the-rag-plugin)
  * [Connecting the Plugin to the Chat Engine](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#connecting-the-plugin-to-the-chat-engine)
    * [Creating the Bot and Colang Configurations](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#creating-the-bot-and-colang-configurations)
    * [Testing the Bot](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#testing-the-bot)
  * [Improving the User Experience for Speech-To-Speech Conversations](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#improving-the-user-experience-for-speech-to-speech-conversations)
    * [Updating Bot Configurations](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#updating-bot-configurations)
    * [Testing the Bot](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#id1)


[ ![NVIDIA](https://docs.nvidia.com/ace/ace-agent/latest/_static/nvidia-logo-horiz-rgb-1c-blk-for-screen.svg) ![NVIDIA](https://docs.nvidia.com/ace/ace-agent/latest/_static/nvidia-logo-horiz-rgb-1c-wht-for-screen.svg) ](https://www.nvidia.com)
[Privacy Policy](https://www.nvidia.com/en-us/about-nvidia/privacy-policy/) | [Manage My Privacy](https://www.nvidia.com/en-us/about-nvidia/privacy-center/) | [Do Not Sell or Share My Data](https://www.nvidia.com/en-us/preferences/start/) | [Terms of Service](https://www.nvidia.com/en-us/about-nvidia/terms-of-service/) | [Accessibility](https://www.nvidia.com/en-us/about-nvidia/accessibility/) | [Corporate Policies](https://www.nvidia.com/en-us/about-nvidia/company-policies/) | [Product Security](https://www.nvidia.com/en-us/product-security/) | [Contact](https://www.nvidia.com/en-us/contact/)
Copyright © 2023-2025, NVIDIA Corporation. 
Last updated on Feb 27, 2025. 
