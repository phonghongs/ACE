This file is a merged representation of the entire codebase, combined into a single document by Repomix. The content has been processed where security check has been disabled.

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information

## Additional Info

# Directory Structure
```
cline_docs/
  activeContext.md
  productContext.md
  systemPatterns.md
  techContext.md
CrawlAI/
  bot.py
microservices/
  ace_agent/
    4.1/
      clients/
        chat_client/
          chat_client.py
          requirements.txt
        event_client/
          event_client.py
          requirements.txt
          style.css
        speech_client/
          requirements.txt
          setup.sh
          speech_client.py
      deploy/
        docker/
          dockerfiles/
            chat_engine.Dockerfile
            nlp_server.Dockerfile
            plugin_server.Dockerfile
          .env
          docker_init.sh
          docker-compose.yml
        ucs_apps/
          chat_bot/
            food_ordering_bot/
              app-params.yaml
              app.yaml
              README.md
            llm_bot/
              app-params.yaml
              app.yaml
              README.md
            rag_bot/
              app-params.yaml
              app.yaml
              README.md
            stock_bot/
              app-params.yaml
              app.yaml
              README.md
          speech_bot/
            ddg_langchain_bot/
              app-params.yaml
              app.yaml
              README.md
            food_ordering_bot/
              app-params.yaml
              app.yaml
              README.md
            llm_bot/
              app-params.yaml
              app.yaml
              README.md
            npc_bots/
              app-params.yaml
              app.yaml
              README.md
            rag_bot/
              app-params.yaml
              app.yaml
              README.md
            stock_bot/
              app-params.yaml
              app.yaml
              README.md
      openapi/
        chat_engine_openapi.json
        nlp_server_openapi.json
        plugin_server_openapi.json
      proto/
        ace_agent.proto
      samples/
        chitchat_bot/
          colang/
            main.co
          chitchat_bot_config.yml
          model_config.yaml
          README.md
          speech_config.yaml
        colang_1.0/
          chitchat_bot/
            colang/
              bot.co
              fallback.co
              flows.co
              user.co
            chitchat_bot_config.yml
            model_config.yaml
            README.md
            speech_config.yaml
          npc_bots/
            elara/
              bot_config.yaml
              flows.co
            jin/
              bot_config.yaml
              flows.co
            plugins/
              prompt_former.py
            model_config.yaml
            plugin_config.yaml
            README.md
            speech_config.yaml
          rag_bot/
            plugins/
              rag.py
              schemas.py
            flows.co
            model_config.yaml
            plugin_config.yaml
            rag_bot_config.yml
            README.md
            speech_config.yaml
          spanish_bot_nmt/
            colang/
              date.co
              general.co
              open_domain.co
              weather.co
            model_config.yaml
            plugin_config.yaml
            README.md
            spanish_bot_nmt.yml
          stock_bot/
            colang/
              contextual_query.co
              fallback.co
              general.co
              off_topic.co
              profanity_rail.co
              stock_faq.co
              stock_price.co
            kb/
              stock_faq.md
            plugins/
              yahoo_fin.py
            cmudict_ipa.txt
            model_config.yaml
            plugin_config.yaml
            README.md
            speech_config.yaml
            stock_bot_config.yml
        ddg_langchain_bot/
          plugins/
            langchain_agent.py
            requirements_dev.txt
            schemas.py
          model_config.yaml
          plugin_config.yaml
          README.md
          speech_config.yaml
        event_interface_tutorial_bot/
          step_0/
            bot_config.yml
            main.co
          step_1/
            bot_config.yml
            main.co
          step_2/
            bot_config.yml
            main.co
          step_3/
            bot_config.yml
            main.co
          step_4/
            bot_config.yml
            main.co
          step_5/
            bot_config.yml
            main.co
          step_6/
            actions.py
            bot_config.yml
            main.co
          step_7/
            plugin/
              yahoo_fin.py
            bot_config.yml
            main.co
            plugin_config.yaml
          README.md
          speech_config.yaml
        food_ordering_bot/
          colang/
            bug_fix.co
            cart.co
            general.co
            main.co
            order.co
            query_menu.co
          plugin/
            __init__.py
            all_gtc_items_with_diff_sizes_v3.json
            cart_manager.py
            data_format.py
            menu_api.py
            order_food.py
            requirements.txt
          food_ordering_bot_config.yaml
          model_config.yaml
          plugin_config.yaml
          README.md
          slots.yaml
          speech_config.yaml
        langgraph_plan_and_execute/
          plugin/
            plan_and_execute.py
            requirements_dev.txt
            schemas.py
          langgraph_bot_config.yaml
          main.co
          model_config.yaml
          plugin_config.yaml
          README.md
          speech_config.yaml
        llm_bot/
          colang/
            main.co
            speech.co
          actions.py
          docker-compose-nim-ms.yaml
          llm_bot_config.yml
          model_config.yaml
          speech_config.yaml
        npc_bots/
          elara/
            colang/
              bug_fix.co
              elara.co
              speech.co
            actions.py
            bot_config.yaml
            model_config.yaml
            plugin_config.yaml
            speech_config.yaml
          jin/
            colang/
              bug_fix.co
              jin.co
              speech.co
            actions.py
            bot_config.yaml
            model_config.yaml
            plugin_config.yaml
            speech_config.yaml
          plugins/
            prompt_former.py
            requirements.txt
            schemas.py
            slm_responder.py
          README.md
        rag_bot/
          colang/
            main.co
            speech.co
          plugins/
            rag.py
            schemas.py
            utils.py
          RAG/
            basic_rag/
              docker-compose.yaml
              prompt.yaml
            slm_rag/
              docker-compose.yaml
              prompt.yaml
            docker-compose-nim-ms.yaml
            docker-compose-vectordb.yaml
          actions.py
          model_config.yaml
          plugin_config.yaml
          rag_bot_config.yml
          README.md
          speech_config.yaml
        spanish_bot/
          colang/
            bug_fix.co
            date.co
            general.co
            main.co
            off_topic.co
            open_domain.co
            weather.co
          plugin_config.yaml
          README.md
          spanish_bot_config.yaml
        spanish_bot_nmt/
          colang/
            bug_fix.co
            date.co
            general.co
            main.co
            off_topic.co
            open_domain.co
            weather.co
          model_config.yaml
          plugin_config.yaml
          README.md
          spanish_nmt_bot_config.yaml
        stock_bot/
          colang/
            bug_fix.co
            general.co
            main.co
            off_topic.co
            profanity_rail.co
            stock_faq.co
            stock_price.co
          kb/
            stock_faq.md
          plugins/
            yahoo_fin.py
          cmudict_ipa.txt
          model_config.yaml
          plugin_config.yaml
          README.md
          speech_config.yaml
          stock_bot_config.yml
        training/
          pipeline_intent_slot.yaml
          pipeline_ner.yaml
          pipeline_text_classification.yaml
      webui/
        client/
          public/
            env.js
          src/
            components/
              App/
                index.css
                index.tsx
              AppHeader/
                index.css
                index.tsx
              BotFace/
                index.css
                index.tsx
              ConversationHistory/
                index.css
                index.tsx
              ConversationMessage/
                index.css
                index.tsx
              Loading/
                index.css
                index.tsx
              TextInput/
                index.css
                index.tsx
              ToastNotices/
                index.css
                index.tsx
              Toggle/
                index.css
                index.tsx
              UserSpeechInput/
                index.css
                index.tsx
              UserTextInput/
                index.css
                index.tsx
            utils/
              audio-contexts.ts
              linear-pcm-processor.worklet.js
              useAudioPlayer.ts
              useMicrophone.ts
              useRealTimeVolume.ts
              useRequestAnimationFrame.ts
              useRerender.ts
              useServerState.ts
              useToastNotices.ts
            index.css
            main.tsx
            vite-env.d.ts
          .eslintrc.cjs
          .yarnrc.yml
          entrypoint.js
          index.html
          package.json
          tsconfig.json
          tsconfig.node.json
          vite.config.ts
        server/
          chat-session/
            clients/
              GRPCClient.ts
              HTTPClient.ts
              RedisClient.ts
            tasks/
              AbstractTask.ts
              ChatSessionTask.ts
              GRPCSpeechTask.test.ts
              GRPCSpeechTask.ts
              GRPCSpeechTranscriptionTask.test.ts
              GRPCSpeechTranscriptionTask.ts
              GRPCTextTask.ts
              HTTPChatTask.ts
              UMIMTask.ts
              WebSocketTask.test.ts
              WebSocketTask.ts
            logger.ts
          data/
            emojis-all.json
          emoji-finder/
            index.test.ts
            index.ts
          grpc/
            gen/
              ace_agent_connect.d.ts
              ace_agent_connect.js
              ace_agent_pb.d.ts
              ace_agent_pb.js
            ace_agent.proto
            buf.gen.yaml
          umim/
            umim.ts
          utils/
            sleep.ts
            waitAbortSignal.ts
          .yarnrc.yml
          config.ts
          index.ts
          package.json
          tsconfig.json
        shared/
          package.json
          types.ts
        docker-compose.yml
        README.md
      README.md
tools/
  avatar_configurator/
    1.0/
      README.md
  ucs_tools/
    README.md
workflows/
  animation_pipeline/
    1.0/
      deploy/
        ucs_apps/
          animation_pipeline_params.yaml
          animation_pipeline_values.yaml
          animation_pipeline.yaml
      README.md
  tokkio/
    4.1/
      3-stream/
        config/
          lp_grpc_in_udp_out_anim_tuning.json
        tokkio-app-params.yaml
        tokkio-app.yaml
      llm-rag-ov/
        1-stream/
          config/
            lp_grpc_in_udp_out_anim_tuning.json
          tokkio-app-params.yaml
          tokkio-app.yaml
        3-stream/
          config/
            lp_grpc_in_udp_out_anim_tuning.json
          tokkio-app-params.yaml
          tokkio-app.yaml
        6-stream/
          config/
            lp_grpc_in_udp_out_anim_tuning.json
          tokkio-app-params.yaml
          tokkio-app.yaml
      scripts/
        one-click/
          baremetal/
            config-files/
              helm-release/
                rproxy-override-values.yml
                tokkio-app-audio-video-app.yml
                tokkio-fluent-bit-override-values.yml
                tokkio-logging-es-override-values.yml
              k8s-manifest/
                es-replicas-update-deployment.yml
                logging-es-replicas-update-configmap.yml
                namespace.yml
              k8s-secret/
                docker-config-json.yml
                ngc-api-key-secret.yml
                nvidia-api-key-secret.yml
                openai-key-secret.yml
            config-template-examples/
              llm-ov-3d-coturn-1x-stream/
                config-template.yml
                my-config.env
            iac-ref/
              templates/
                ansible-hosts.tpl
              backend.tf
              locals.tf
              outputs.tf
              variables.tf
              versions.tf
            playbooks/
              scripts/
                coturn-setup-bm.sh
                coturn-setup.sh
                install-tokkio-ui-aws.sh
                install-tokkio-ui-bm.sh
                install-tokkio-ui-gcp.sh
                install-tokkio-ui.sh
                mount-data-disk-aws.sh
                mount-data-disk-gcp.sh
                mount-data-disk.sh
              tasks/
                helm-release-prep-values.yml
              add-sysctl-config.yml
              app-pre-requisites.yml
              authorize-ssh-keys.yml
              bootstrap-cns-kubeconfig.yml
              check-all-pods-in-namespace-up.yml
              check-inventory.yml
              cns-install.yml
              cns-prepare.yml
              cns-validate.yml
              copy-task-config.yml
              docker-compose.yml
              docker-login.yml
              empty_var_check.yml
              get-network-interface.yml
              helm-release.yml
              helm-task-pre-requisites.yml
              install-tokkio-ui-aws.yml
              install-tokkio-ui-bm.yml
              install-tokkio-ui-gcp.yml
              install-tokkio-ui.yml
              k8s-label.yml
              k8s-manifest.yml
              k8s-patch.yml
              k8s-secret-check-empty-dockerconfig.yml
              k8s-secret-check-empty-opaque.yml
              k8s-secret-check-empty.yml
              k8s-secret-dockerconfig.yml
              k8s-secret-old.yml
              k8s-secret-opaque.yml
              k8s-secret.yml
              k8s-taint.yml
              k8s-task-pre-requisites.yml
              mount-data-disk.yml
              nvidia-container-toolkit-install.yml
              nvidia-docker-install.yml
              nvidia-driver-install.yml
              output-jinja2-expression-to-file.yml
              process-user-config.yml
              setup-coturn-bm.yml
              setup-coturn.yml
              sleep.yml
              write-to-file.yml
            ansible-requirements.yml
            app-tasks.yml
            config-template.yml
            envbuild.sh
            infra-tasks.yml
            install-pre-requisites.sh
            platform-tasks.yml
            setup-cns-access.sh
      README.md
.gitattributes
.gitignore
LICENSE
README.md
```

# Files

## File: cline_docs/activeContext.md
````markdown
# Active Context

## Current Task
Creating new speech-to-speech bot using:
- Whisper OpenAI API for ASR
- ElevenLabs for TTS
- OpenAI for LLM

## Completed Work
- Documented system architecture
- Defined technical requirements
- Outlined product goals

## Next Steps
1. Examine existing Docker deployment configuration
2. Review sample bot implementations
3. Plan API integration points
4. Modify speech client implementation

## Open Questions
- Should we maintain compatibility with existing Riva implementation?
- What audio formats need to be supported?
- What are the rate limits for each API?
````

## File: cline_docs/productContext.md
````markdown
# Product Context

## Project Purpose
Create a speech-to-speech conversational AI bot using:
- Whisper OpenAI API for speech recognition
- ElevenLabs for text-to-speech
- OpenAI for language model

## Target Use Cases
- Customer service voice assistants
- Interactive voice response systems
- Virtual companions
- Language learning applications

## Expected User Experience
- Natural voice conversations
- Low latency responses
- Human-like speech synthesis
- Context-aware interactions

## Key Performance Metrics
- Speech recognition accuracy
- Response generation time
- Speech synthesis quality
- Conversation coherence
````

## File: cline_docs/systemPatterns.md
````markdown
# System Patterns

## ACE Agent 4.1 Architecture
- Microservices-based architecture
- Core components:
  - Chat Controller (orchestrates pipeline)
  - Chat Engine (conversation flow)
  - NLP Server (NLP tasks)
  - Plugin Server (business logic)
  - Web App (frontend)

## Current Integration Points
- Uses NVIDIA Riva for ASR/TTS by default
- Supports custom LLM integration
- Conversation flows defined in Colang
- Supports RAG workflows

## New Integration Requirements
- Replace ASR with Whisper OpenAI API
- Replace TTS with ElevenLabs API
- Configure OpenAI LLM
- Maintain existing chat controller/engine
- Keep Docker/Kubernetes deployment
````

## File: cline_docs/techContext.md
````markdown
# Technical Context

## Current Technology Stack
- Docker/Kubernetes deployment
- Python microservices
- NVIDIA Riva for ASR/TTS
- NVIDIA NeMo Guardrails for conversation flow
- Colang for dialog management

## New Technology Requirements
- Whisper OpenAI API for ASR
- ElevenLabs API for TTS
- OpenAI API for LLM
- Python client libraries for all APIs

## Development Setup
- Docker Desktop required
- Kubernetes cluster (local or cloud)
- API keys for:
  - OpenAI (Whisper + LLM)
  - ElevenLabs
- Python 3.8+ environment

## Configuration Needs
- API endpoint URLs
- Authentication keys
- Rate limiting considerations
- Audio format compatibility
````

## File: CrawlAI/bot.py
````python
# Import necessary libraries
from crawl4ai import Crawl4AI
import time

# List of URLs to crawl
urls_to_crawl = [
    'https://example.com',
    'https://example2.com',
    'https://example3.com',
]

# Define a function to crawl the list of URLs
def crawl_urls(url_list):
    # Initialize the Crawl4AI object
    crawler = Crawl4AI()

    # Iterate over the list of URLs
    for url in url_list:
        print(f"Crawling URL: {url}")
        
        # Crawl the URL (You can add your logic for extracting the data here)
        data = crawler.crawl(url)  # Assuming the method `crawl` is used to get the data from the URL
        
        # Do something with the data (e.g., print it, store it, etc.)
        print(f"Data crawled from {url}: {data[:100]}...")  # Print first 100 characters of the data for demonstration

        # Pause for a moment between requests to avoid overloading the server
        time.sleep(2)  # Adjust the sleep time based on your needs

# Call the function with the list of URLs
crawl_urls(urls_to_crawl)
````

## File: microservices/ace_agent/4.1/clients/chat_client/chat_client.py
````python
"""
 copyright(c) 2022-24 NVIDIA Corporation.All rights reserved.

 NVIDIA Corporation and its licensors retain all intellectual property
 and proprietary rights in and to this software, related documentation
 and any modifications thereto.Any use, reproduction, disclosure or
 distribution of this software and related documentation without an express
 license agreement from NVIDIA Corporation is strictly prohibited.
"""

"""
This sample python application showcases how any developer can communicate with the REST-based API's exposed by the Chat Engine.
"""

import json
import argparse
import uuid
import aiohttp
import asyncio

parser = argparse.ArgumentParser(description="Application to demonstrate interactions with Agent Server")
parser.add_argument("--host", default="localhost", help="hostname to be used by the server")
parser.add_argument("--port", default=9000, help="port to be used by the server")
parser.add_argument("--timeout", default=15, help="Maximum time to wait for response")

args = parser.parse_args()

user_id = str(uuid.uuid4())


async def check_status():
    """
    Send a request to the isReady endpoint at the specified host and port.
    Input: None
    Output: True if server is active, else False
    """

    # Check if server is active and ready to process queries
    try:
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(args.timeout)) as session:
            async with session.get(f"http://{args.host}:{args.port}/isReady") as resp:
                if resp.ok:
                    print("Server is active and ready to process queries!")
                else:
                    print("Server is not ready to process queries. Please ensure that server is running.")
                    return False
    except:
        print("Could not reach server. Exiting.")
        return False

    return True


async def chat():
    """
    Call the /chat endpoint of Chat Engine and handle the response (whether streaming or non-streaming).
    """
    query = input("[YOU] ").strip()
    if not query:
        return

    payload = {"UserId": user_id, "Query": query}

    # Post a request to ACE Agent server hosted at args.host:args.port.
    # The payload is a dict containing information relavent to ACE Agent.
    # The fields in the current payload are mandatory, but some other information can also be passed.
    # Refer to Nvidia ACE Agent documentation to find out the valid data fields you can send.
    try:
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(args.timeout)) as session:
            async with session.post(f"http://{args.host}:{args.port}/chat", json=payload) as response:
                response.raise_for_status()

                # In case of a streaming response, print each chunk as it is received
                if response.headers.get("Transfer-Encoding") == "chunked":
                    print("[BOT] ", end="", flush=True)
                    async for chunk, _ in response.content.iter_chunks():
                        if not chunk:
                            break

                        chunk = chunk.decode("utf-8")
                        parsed_chunk = json.loads(chunk)
                        if parsed_chunk["Response"]["IsFinal"]:
                            print("")
                            return
                        else:
                            print(parsed_chunk["Response"]["Text"], end="", flush=True)

                # In case of a JSON response, return the value directly
                else:
                    response_data = await response.json()
                    print(f"[BOT] {response_data['Response']['CleanedText']}")

    except KeyboardInterrupt:
        print("Force interrupting Chat Engine Sample App")
        return

    except Exception as e:
        return f"Ran into an error while querying the bot: {e}"


async def main():
    """
    Run the Chat Engine sample app functionality.
    Output: None
    """

    server_up = await check_status()
    if not server_up:
        exit()

    while True:
        try:
            await chat()
        except KeyboardInterrupt:
            return


if __name__ == "__main__":
    asyncio.run(main())
````

## File: microservices/ace_agent/4.1/clients/chat_client/requirements.txt
````
aiohttp==3.9.5
````

## File: microservices/ace_agent/4.1/clients/event_client/event_client.py
````python
# SPDX-FileCopyrightText: Copyright (c) 2022-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
#
# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
# property and proprietary rights in and to this material, related
# documentation and any modifications thereto. Any use, reproduction,
# disclosure or distribution of this material and related documentation
# without an express license agreement from NVIDIA CORPORATION or
# its affiliates is strictly prohibited.


# ACE SIMULATOR
# ACE Simulator is a simple command line tool that simulates a basic interactive system that can be used
# with ACE Agent event interface
########################################################################################################################

import asyncio
import json
import logging
import uuid
from abc import ABC, abstractmethod
from collections import deque, namedtuple
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Deque, Dict, List, Optional, Tuple, Type, Union
from uuid import uuid4

import redis.asyncio as redis
import typer
from rich import print
from textual.app import App, ComposeResult
from textual.containers import Container, Horizontal, Vertical
from textual.reactive import reactive
from textual.widget import Widget
from textual.widgets import (
    Button,
    Footer,
    Header,
    Input,
    Label,
    Markdown,
    RadioButton,
    Static,
    TextLog,
)
from textual.worker import Worker, WorkerState
from transitions import Machine
from typing_extensions import Annotated

logging.basicConfig(filename="umim_compliance_errors.txt", level=logging.WARNING)
logger = logging.getLogger("ace_sim")

cli = typer.Typer()
stream_id = "1"
channel_id = "umim_events_1"
create_pipeline = False
app_in_active_mode = False
redis_host = "localhost"
redis_port = 6379
ui_sidebar_open = False

SYSTEM_EVENTS_STREAM = "ace_agent_system_events"

MOTION_DURATION = 20

EVENT_FIELDS_TO_HIDE = {
    "uid",
    "action_info_modality",
    "action_info_modality_policy",
    "user_id",
    "bot_id",
    "source_uid",
    "tags",
    "event_created_at",
    "action_started_at",
    "action_finished_at",
    "action_updated_at",
    "type",
}


########################################################################################################################
# UTILITIES
########################################################################################################################


class EventProvider(ABC):
    @abstractmethod
    async def receive_events(self, timeout_ms: Optional[int] = 500) -> List[str]:
        """Receive incoming events. Returns when it received one or more events"""
        raise NotImplementedError

    @abstractmethod
    async def send_event(self, channel_id: str, event_data: Union[str, Dict[str, Any]]) -> None:
        """Publishes the event"""
        raise NotImplementedError


class RedisEventProvider(EventProvider):
    def __init__(
        self,
        redis_host: str,
        redis_port: int,
        channels: List[str],
        discard_existing_events: bool = True,
    ):
        super().__init__()
        self.redis: redis.Redis = redis.Redis(host=redis_host, port=redis_port)
        self._channel_state: Dict[str, str] = dict(
            map(lambda c: (c, "$" if discard_existing_events else "0"), channels)
        )

    async def receive_events(self, timeout_ms: Optional[int] = 500) -> List[str]:
        """Receive incoming events. Returns when it received one or more events"""
        if timeout_ms is not None and timeout_ms < 100:
            logger.warning(f"Redis timeout resolution is about 100ms, but a timeout of {timeout_ms}ms was given.")

        result = await self.redis.xread(streams=self._channel_state, block=timeout_ms)
        event_list: List[str] = []

        for channel in result:
            channel_id = str(channel[0].decode())
            for event_id, value in channel[1]:
                for key in value.keys():
                    event_data = value[key].decode()
                    event_list.append(event_data)

                self._channel_state[channel_id] = event_id.decode()

        return event_list

    async def send_event(self, channel_id: str, event_data: Union[str, Dict[str, Any]]) -> None:
        """Publishes the event"""

        if isinstance(event_data, dict):
            event_data = json.dumps(event_data)

        await self.redis.xadd(channel_id, {"event": event_data.encode()})


def event_provider_factory(
    provider_name: str,
    host: str,
    port: int,
    channels=List[str],
    discard_existing_events: bool = True,
) -> EventProvider:
    providers = ["redis"]
    if provider_name == "redis":
        return RedisEventProvider(host, port, channels, discard_existing_events)
    else:
        raise Exception(f"Event provider {provider_name} does not exist. Available providers { ','.join(providers)}")


def new_uuid() -> str:
    """Helper to create a new UID."""

    return str(uuid.uuid4())


# Very basic event validation - will be replaced by validation based on pydantic models
Property = namedtuple("Property", ["name", "type"])
Validator = namedtuple("Validator", ["description", "function"])


def _has_property(e: Dict[str, Any], p: Property) -> bool:
    return p.name in e and type(e[p.name]) == p.type


_event_validators = [
    Validator("Events need to provide 'type'", lambda e: "type" in e),
    Validator("Events need to provide 'uid'", lambda e: _has_property(e, Property("uid", str))),
    Validator(
        "Events need to provide 'event_created_at' of type 'str'",
        lambda e: _has_property(e, Property("event_created_at", str)),
    ),
    Validator(
        "Events need to provide 'source_uid' of type 'str'",
        lambda e: _has_property(e, Property("source_uid", str)),
    ),
    Validator(
        "***Action events need to provide an 'action_uid' of type 'str'",
        lambda e: "Action" not in e["type"] or _has_property(e, Property("action_uid", str)),
    ),
    Validator(
        "***ActionFinished events require 'action_finished_at' field of type 'str'",
        lambda e: "ActionFinished" not in e["type"] or _has_property(e, Property("action_finished_at", str)),
    ),
    Validator(
        "***ActionFinished events require 'is_success' field of type 'bool'",
        lambda e: "ActionFinished" not in e["type"] or _has_property(e, Property("is_success", bool)),
    ),
    Validator(
        "Unsuccessful ***ActionFinished events need to provide 'failure_reason'.",
        lambda e: "ActionFinished" not in e["type"] or (e["is_success"] or "failure_reason" in e),
    ),
    Validator(
        "***StartUtteranceBotAction events need to provide 'script' of type 'str'",
        lambda e: e["type"] != "StartUtteranceBotAction" or _has_property(e, Property("script", str)),
    ),
    Validator(
        "***UtteranceBotActionScriptUpdated events need to provide 'interim_script' of type 'str'",
        lambda e: e["type"] != "UtteranceBotActionScriptUpdated " or _has_property(e, Property("interim_script", str)),
    ),
    Validator(
        "***UtteranceBotActionFinished events need to provide 'final_script' of type 'str'",
        lambda e: e["type"] != "UtteranceBotActionFinished" or _has_property(e, Property("final_script", str)),
    ),
    Validator(
        "***UtteranceUserActionTranscriptUpdated events need to provide 'interim_transcript' of type 'str'",
        lambda e: e["type"] != "UtteranceUserActionTranscriptUpdated"
        or _has_property(e, Property("interim_transcript", str)),
    ),
    Validator(
        "***UtteranceUserActionFinished events need to provide 'final_transcript' of type 'str'",
        lambda e: e["type"] != "UtteranceUserActionFinished" or _has_property(e, Property("final_transcript", str)),
    ),
]


_action_to_modality_info: Dict[str, Tuple[str, str]] = {
    "UtteranceBotAction": ("bot_speech", "replace"),
    "UtteranceUserAction": ("user_speech", "replace"),
    "VisualChoiceSceneAction": ("information", "override"),
    "VisualInformationSceneAction": ("information", "override"),
    "VisualFormSceneAction": ("information", "override"),
    "GestureBotAction": ("bot_gesture", "override"),
    "TimerBotAction": ("time", "parallel"),
}


def _add_modality_info(event_dict: Dict[str, Any]) -> None:
    """Add modality related information to the action event"""
    for action_name, modality_info in _action_to_modality_info.items():
        modality_name, modality_policy = modality_info
        if action_name in event_dict["type"]:
            event_dict["action_info_modality"] = modality_name
            event_dict["action_info_modality_policy"] = modality_policy


def _update_action_properties(event_dict: Dict[str, Any]) -> None:
    """Update action related even properties and ensure UMIM compliance (very basic)"""

    if "Started" in event_dict["type"]:
        event_dict["action_started_at"] = datetime.now(timezone.utc).isoformat()
    elif "Start" in event_dict["type"]:
        event_dict["action_uid"] = new_uuid()
    elif "Finished" in event_dict["type"]:
        event_dict["action_finished_at"] = datetime.now(timezone.utc).isoformat()
        if event_dict["is_success"] and "failure_reason" in event_dict:
            del event_dict["failure_reason"]


def ensure_valid_event(event: Dict[str, Any]) -> None:
    """Performs basic event validation and throws an AssertionError if any of the validators fail."""
    for validator in _event_validators:
        assert validator.function(event), validator.description


def is_valid_event(event: Dict[str, Any]) -> bool:
    """Performs a basic event validation and returns True if the event conforms."""
    for validator in _event_validators:
        if not validator.function(event):
            return False
    return True


def new_event(event_type: str, **payload) -> Dict[str, Any]:
    """Helper to create a generic event structure."""

    event: Dict[str, Any] = {
        "type": event_type,
        "uid": new_uuid(),
        "event_created_at": datetime.now(timezone.utc).isoformat(),
        "source_uid": "umim_tui_app",
    }

    event = {**event, **payload}

    if "Action" in event_type:
        _add_modality_info(event)
        _update_action_properties(event)

    ensure_valid_event(event)
    return event


def read_isoformat(timestamp: str) -> datetime:
    """
    ISO 8601 has multiple legal ways to indicate UTC timezone. 'Z' or '+00:00'. However the Python
    datetime.fromisoformat only accepts the latter.
    This function provides a more flexible wrapper to accept all valid IOS 8601 formats
    """
    normalized = timestamp.replace("Z", "+00:00")
    return datetime.fromisoformat(normalized)


########################################################################################################################
# EVENT PARSING and PRINTING
########################################################################################################################
def _fix_event(event: Dict[str, Any]) -> None:
    """
    Try to make the event UMIM compatible
    """
    if "created_at" in event:
        event["event_created_at"] = event["created_at"]
        del event["created_at"]


def _type_to_sign(event_type: str) -> str:
    if "Started" in event_type:
        return ":rocket: "
    elif "Start" in event_type:
        return ":play_button:  "
    elif "Stop" in event_type:
        return ":stop_button:  "
    elif "Finished" in event_type:
        return ":stop_sign: "
    elif "Updated" in event_type:
        return ":counterclockwise_arrows_button: "
    elif "Change" in event_type:
        return ":pencil: "
    else:
        return ":frog: "


def _timestamp(show_time: bool, timestamp: Optional[str] = None) -> str:
    if show_time:
        if timestamp:
            date = read_isoformat(timestamp)
            date_str = date.strftime("%m/%d,%H:%M:%S.%f")
            return f"[blue]{date_str}[/blue] "
        else:
            return "                      "
    else:
        return ""


def _event_to_short_str(event: dict, show_time: bool = True) -> str:
    action_info = ""
    if "action_uid" in event:
        action_info = f", id={event['action_uid'][0:4]}.."

    if event["type"] == "StartTimerBotAction":
        param_str = f'"{event["timer_name"]}", duration={event["duration"]}{action_info}'
    elif event["type"] == "TimerBotActionStarted":
        param_str = f'{read_isoformat(event["action_started_at"]).strftime("%H:%M:%S")}{action_info}'
    elif event["type"] == "ChangeTimerBotAction":
        param_str = f"duration={event['duration']}{action_info}"
    elif event["type"] == "StopTimerBotAction":
        param_str = f"{action_info}"
    elif event["type"] == "TimerBotActionFinished":
        if not event["is_success"]:
            status = '"success"' if event["is_success"] else '"failure"'
            reason = f', reason="{event["failure_reason"]}"' if not event["is_success"] else ""
            param_str = f"{status}{reason}{action_info}"
        else:
            if event["was_stopped"]:
                param_str = f"was_stopped=True{action_info}"
            else:
                param_str = f'{read_isoformat(event["action_finished_at"]).strftime("%H:%M:%S")}{action_info}'
    elif event["type"] == "UtteranceUserActionFinished":
        param_str = f'"{event["final_transcript"]}"{action_info}'
    elif event["type"] == "UtteranceUserActionTranscriptUpdated":
        param_str = f'"{event["interim_transcript"]}"{action_info}'
    elif event["type"] == "UtteranceUserActionStarted":
        param_str = action_info

    elif event["type"] == "StartUtteranceBotAction":
        param_str = f'"{event["script"]}"{action_info}'
    elif event["type"] == "UtteranceBotActionStarted":
        param_str = action_info
    elif event["type"] == "StopUtteranceBotAction":
        param_str = action_info
    elif event["type"] == "UtteranceBotActionFinished":
        param_str = f'"{event["final_script"]}"{action_info}'

    elif event["type"] == "StartGestureBotAction":
        param_str = f'"{event["gesture"]}"{action_info}'
    elif event["type"] == "GestureBotActionStarted":
        param_str = action_info
    elif event["type"] == "StopGestureBotAction":
        param_str = action_info
    elif event["type"] == "GestureBotActionFinished":
        status = '"success"' if event["is_success"] else '"failure"'
        was_stopped = f", was_stopped={'True' if event['was_stopped'] else 'False'}"
        reason = f', reason="{event["failure_reason"]}"' if not event["is_success"] else ""
        param_str = f"{status}{reason}{was_stopped}{action_info}"

    elif event["type"] == "StartPostureBotAction":
        param_str = f'"{event["posture"]}"{action_info}'
    elif event["type"] == "PostureBotActionStarted":
        param_str = action_info
    elif event["type"] == "StopPostureBotAction":
        param_str = action_info
    elif event["type"] == "PostureBotActionFinished":
        status = '"success"' if event["is_success"] else '"failure"'
        reason = f', reason="{event["failure_reason"]}"' if not event["is_success"] else ""
        param_str = f"{status}{reason}{action_info}"

    elif event["type"] == "AttentionUserActionStarted":
        param_str = f"level={event['attention_level']}{action_info}"
    elif event["type"] == "AttentionUserActionUpdated":
        param_str = f"level={event['attention_level']}{action_info}"
    elif event["type"] == "AttentionUserActionFinished":
        status = '"success"' if event["is_success"] else '"failure"'
        reason = f', reason="{event["failure_reason"]}"' if not event["is_success"] else ""
        param_str = f"{status}{reason}{action_info}"

    elif event["type"] == "PresenceUserActionStarted":
        param_str = action_info
    elif event["type"] == "PresenceUserActionFinished":
        status = '"success"' if event["is_success"] else '"failure"'
        reason = f', reason="{event["failure_reason"]}"' if not event["is_success"] else ""
        param_str = f"{status}{reason}{action_info}"

    elif event["type"] == "StartVisualInformationSceneAction":
        param_str = f'"{event["title"]}", summary={event.get("summary","")}{action_info}'
    elif event["type"] == "VisualInformationSceneActionStarted":
        param_str = action_info
    elif event["type"] == "StopVisualInformationSceneAction":
        param_str = action_info
    elif event["type"] == "VisualInformationSceneActionConfirmationUpdated":
        param_str = f"{event['confirmation_status']}{action_info}"
    elif event["type"] == "VisualInformationSceneActionFinished":
        status = '"success"' if event["is_success"] else '"failure"'
        reason = f', reason="{event["failure_reason"]}"' if not event["is_success"] else ""
        param_str = f"{status}{reason}{action_info}"

    elif event["type"] == "StartVisualChoiceSceneAction":
        param_str = f'"{event["prompt"]}"{action_info}'
    elif event["type"] == "VisualChoiceSceneActionStarted":
        param_str = action_info
    elif event["type"] == "VisualChoiceSceneActionConfirmationUpdated":
        param_str = f"{event['confirmation_status']}{action_info}"
    elif event["type"] == "VisualChoiceSceneActionChoiceUpdated":
        param_str = f"{event['current_choice']}{action_info}"
    elif event["type"] == "StopVisualChoiceSceneAction":
        param_str = action_info
    elif event["type"] == "VisualChoiceSceneActionFinished":
        status = '"success"' if event["is_success"] else '"failure"'
        reason = f', reason="{event["failure_reason"]}"' if not event["is_success"] else ""
        param_str = f"{status}{reason}{action_info}"

    elif event["type"] == "StartVisualFormSceneAction":
        param_str = f'"{event["prompt"]}"{action_info}'
    elif event["type"] == "VisualFormSceneActionStarted":
        param_str = action_info
    elif event["type"] == "VisualFormSceneActionConfirmationUpdated":
        param_str = f"{event['confirmation_status']}{action_info}"
    elif event["type"] == "VisualFormSceneActionInputUpdated":
        inputs = ",".join([f'{input["id"]}="{input["value"]}"' for input in event["interim_inputs"]])
        param_str = f"{inputs}{action_info}"
    elif event["type"] == "StopVisualFormSceneAction":
        param_str = action_info
    elif event["type"] == "VisualFormSceneActionFinished":
        status = '"success"' if event["is_success"] else '"failure"'
        reason = f', reason="{event["failure_reason"]}"' if not event["is_success"] else ""
        param_str = f"{status}{reason}{action_info}"

    elif event["type"] == "UserIntent":
        param_str = f'"{event["intent"]}"'
    elif event["type"] == "BotIntent":
        param_str = f'"{event["intent"]}"'
    else:
        param_str = ",".join([f"{key}={value}" for key, value in event.items()])

    return (
        f"{_timestamp(show_time, event['event_created_at'])}{_type_to_sign(event['type'])}{event['type']}({param_str})"
    )


def pretty_event(event_data: Dict[str, Any], show_time: bool = True, strict: bool = True) -> str:
    try:
        if not strict:
            _fix_event(event_data)

        if event_data["type"] == "Error":
            return f"[red]ERROR[/red] {event_data['reason']}"

        if event_data["type"] == "Listen":
            return f"[blue][Legacy Event][/blue] Listen()"

        if "InternalSystemAction" in event_data["type"]:
            return f"[blue][Internal][/blue] {event_data['type']}()"

        return _event_to_short_str(event_data, show_time)

    except Exception as ex:
        logging.error(f"[Malformed] " + str(event_data) + f"\n[Error] {str(ex)}\n\n")
        return ":ogre: [Malformed] " + str(event_data)


def try_parse_event(data):
    if isinstance(data, str):
        data = json.loads(data)
    return data


def get_action_name(event_type: str) -> str:
    action_name = (
        event_type.replace("Started", "")
        .replace("Start", "")
        .replace("Finished", "")
        .replace("Updated", "")
        .replace("Stop", "")
        .replace("Change", "")
    )

    # For Updated action there might still be a part after Action (e.g. UtteranceBotActionScriptUpdated)
    keyword = "Action"
    index = action_name.find(keyword)
    if index != -1:
        cleaned_action_name = action_name[: index + len(keyword)]
    else:
        cleaned_action_name = action_name

    return cleaned_action_name


########################################################################################################################
# ACTION HANDLERS
########################################################################################################################


@dataclass
class InternalEvent:
    type: str
    data: Any = None


class ActionHandler(object):
    states = ["init", "running", "background", "finished"]
    triggers: List[str]
    action_name: str
    tui_element_id: str

    def __init__(self, active_mode: bool, app: App) -> None:
        self.active_mode = active_mode
        self.app: App = app
        self.action_state: Dict[str, Any] = {}
        self.task_done = False
        self.was_stopped = False

        transitions = [
            {"trigger": "start", "source": "init", "dest": "running", "before": ["update_action_state", "on_start"]},
            {
                "trigger": "started",
                "source": "init",
                "dest": "running",
                "before": ["update_action_state", "on_started_from_init"],
            },
            {
                "trigger": "started",
                "source": "running",
                "dest": "running",
                "before": ["update_action_state", "on_started_from_running"],
            },
            {
                "trigger": "change",
                "source": "running",
                "dest": "running",
                "before": ["update_action_state", "on_change"],
            },
            {
                "trigger": "promote",
                "source": "running",
                "dest": "running",
                "before": ["update_action_state", "on_promote_when_running"],
            },
            {"trigger": "tick", "source": "running", "dest": "running", "before": ["update_action_state", "on_tick"]},
            {
                "trigger": "demote",
                "source": "running",
                "dest": "background",
                "before": ["update_action_state", "on_demote"],
            },
            {
                "trigger": "promote",
                "source": "background",
                "dest": "running",
                "before": ["update_action_state", "on_promote"],
            },
            {
                "trigger": "started",
                "source": "background",
                "dest": "background",
                "before": ["update_action_state"],
            },
            {
                "trigger": "stop",
                "source": "running",
                "dest": "finished",
                "before": ["update_action_state", "on_stop_from_running"],
            },
            {
                "trigger": "stop",
                "source": "background",
                "dest": "finished",
                "before": ["update_action_state", "on_stop_from_background"],
            },
            {
                "trigger": "finished",
                "source": "running",
                "dest": "finished",
                "before": ["update_action_state", "on_finished_from_running"],
            },
            {
                "trigger": "done",
                "source": "running",
                "dest": "finished",
                "before": ["update_action_state", "on_done"],
            },
            {
                "trigger": "finished",
                "source": "finished",
                "dest": "finished",
                "before": ["update_action_state", "on_finished_from_finished"],
            },
            {
                "trigger": "started",
                "source": "finished",
                "dest": "finished",
                "before": ["update_action_state"],
            },
        ]

        self.machine = Machine(model=self, states=ActionHandler.states, initial="init", transitions=transitions)

    def update_action_state(self, event: Union[InternalEvent, dict]) -> None:
        if isinstance(event, dict):
            assert "action_uid" not in self.action_state or event["action_uid"] == self.action_state["action_uid"]
            self.action_state.update(event)

    def send_event(self, event: Union[InternalEvent, dict]) -> None:
        if not app_in_active_mode:
            return
        self.app.run_worker(self.app.send_events(event), exclusive=False)

    def on_started_from_running(self, event: Union[InternalEvent, dict]) -> None:
        pass

    def on_tick(self, event: Union[InternalEvent, dict]) -> None:
        pass

    def on_finished_from_finished(self, event: Union[InternalEvent, dict]):
        pass

    def get_string_for_tui_element_on_start(self) -> str:
        return ""

    def on_promote_when_running(self, event: Union[InternalEvent, dict]) -> None:
        self.send_event({"type": "Error", "reason": f"Event promote received for running action {self.action_uid}"})

    def send_action_started_event(self) -> None:
        pass

    def send_action_finished_event(self) -> None:
        pass

    def _update_ui_element(self) -> None:
        self.app.query_one(self.tui_element_id, expect_type=Static).add_class("active").update(
            self.get_string_for_tui_element_on_start()
        )

    def on_start(self, event: Union[InternalEvent, dict]) -> None:
        if self.active_mode:
            self.send_action_started_event()
        self._update_ui_element()

    def on_promote(self, event: Union[InternalEvent, dict]) -> None:
        self._update_ui_element()

    def on_demote(self, event: Union[InternalEvent, dict]):
        pass

    def on_started_from_init(self, event: Union[InternalEvent, dict]) -> None:
        self._update_ui_element()

    def on_finished_from_running(self, event: Union[InternalEvent, dict]):
        self.app.query_one(self.tui_element_id, expect_type=Static).remove_class("active").update("")

    def on_done(self, event: Union[InternalEvent, dict]):
        if self.active_mode:
            self.send_action_finished_event()
        self.app.query_one(self.tui_element_id, expect_type=Static).remove_class("active").update("")

    def _handle_generic_stop_behavior(self):
        self.was_stopped = True
        if self.active_mode:
            self.send_action_finished_event()

        self.app.query_one(self.tui_element_id, expect_type=Static).remove_class("active").update("[Stopped]")

    def on_stop_from_running(self, event: Union[InternalEvent, dict]):
        self._handle_generic_stop_behavior()

    def on_stop_from_background(self, event: Union[InternalEvent, dict]):
        self._handle_generic_stop_behavior()

    @property
    def action_uid(self) -> str:
        return self.action_state["action_uid"]


class TimerActionHandler(ActionHandler):
    triggers = [
        "StartTimerBotAction",
        "TimerBotActionStarted",
        "ChangeTimerBotAction",
        "StopTimerBotAction",
        "TimerBotActionFinished",
    ]
    action_name = "TimerBotAction"
    tui_element_id = "#bot-timer"

    def __init__(self, active_mode: bool, app: App) -> None:
        super().__init__(active_mode, app)
        self.timer_duration: timedelta = timedelta(seconds=11)
        self.timer_start: datetime = datetime.now(timezone.utc)

    def get_string_for_tui_element_on_start(self) -> str:
        return f"Timer: {self.timer_duration.total_seconds()} sec"

    def on_start(self, event: Union[InternalEvent, dict]) -> None:
        if isinstance(event, dict) and event["type"] == "StartTimerBotAction":
            self.timer_duration = timedelta(seconds=event["duration"])
            self.timer_start = read_isoformat(event["event_created_at"])

        super().on_start(event)

    def on_change(self, event: Union[InternalEvent, dict]) -> None:
        assert isinstance(event, dict) and event["type"] == "ChangeTimerBotAction"
        self.timer_duration = event["duration"]

        self._update_ui_element()

    def on_tick(self, event: Union[InternalEvent, dict]) -> None:
        if self.active_mode:
            if datetime.now(timezone.utc) - self.timer_start > self.timer_duration:
                self.task_done = True
            else:
                difference = self.timer_duration - (datetime.now(timezone.utc) - self.timer_start)
                self.app.query_one("#bot-timer", expect_type=Static).update(
                    f"Timer: {difference.total_seconds():.2f} sec"
                )

    def send_action_started_event(self) -> None:
        action_started = new_event("TimerBotActionStarted", action_uid=self.action_uid)
        self.send_event(action_started)

    def send_action_finished_event(self) -> None:
        action_finished = new_event(
            "TimerBotActionFinished",
            action_uid=self.action_uid,
            was_stopped=self.was_stopped,
            is_success=True,
        )
        self.send_event(action_finished)


class PresenceUserActionHandler(ActionHandler):
    triggers = [
        "PresenceUserActionStarted",
        "PresenceUserActionFinished",
    ]
    action_name = "PresenceUserAction"
    tui_element_id = "#presence-user"

    def __init__(self, active_mode: bool, app: App) -> None:
        super().__init__(active_mode, app)

    def get_string_for_tui_element_on_start(self) -> str:
        return "User present."

    def send_action_started_event(self) -> None:
        pass

    def send_action_finished_event(self) -> None:
        pass


_gesture_stack = deque([])


class GestureActionHandler(ActionHandler):
    triggers = ["StartGestureBotAction", "GestureBotActionStarted", "StopGestureBotAction", "GestureBotActionFinished"]
    action_name = "GestureBotAction"
    tui_element_id = "#bot-gesture"

    def __init__(self, active_mode: bool, app: App) -> None:
        super().__init__(active_mode, app)
        self.progress = 0

    @property
    def gesture(self) -> str:
        return self.action_state.get("gesture", "unknown")

    def get_string_for_tui_element_on_start(self) -> str:
        return f"Gesture: {self.gesture}"

    def on_start(self, event: Union[InternalEvent, dict]) -> None:
        super().on_start(event)
        _gesture_stack.append(self.app.current_animation)
        self.app.current_animation = self.gesture

    def on_started_from_running(self, event: Union[InternalEvent, dict]) -> None:
        _gesture_stack.append(self.app.current_animation)
        self.app.current_animation = self.gesture

    def on_enter_finished(self, event: Union[InternalEvent, dict]) -> None:
        previous_gesture = _gesture_stack.pop()
        self.app.current_animation = previous_gesture

    def on_tick(self, event: Union[InternalEvent, dict]) -> None:
        if self.active_mode:
            if self.progress < MOTION_DURATION:
                self.progress += 1
            else:
                self.task_done = True

    def send_action_started_event(self) -> None:
        action_started = new_event("GestureBotActionStarted", action_uid=self.action_uid)
        self.send_event(action_started)

    def send_action_finished_event(self) -> None:
        action_finished = new_event(
            "GestureBotActionFinished",
            action_uid=self.action_uid,
            was_stopped=self.was_stopped,
            is_success=True,
        )
        self.send_event(action_finished)


class FacialGestureActionHandler(ActionHandler):
    triggers = [
        "StartFacialGestureBotAction",
        "FacialGestureBotActionStarted",
        "StopFacialGestureBotAction",
        "FacialGestureBotActionFinished",
    ]
    action_name = "FacialGestureBotAction"
    tui_element_id = "#bot-facial-gesture"

    def __init__(self, active_mode: bool, app: App) -> None:
        super().__init__(active_mode, app)
        self.progress = 0

    @property
    def gesture(self) -> str:
        return self.action_state.get("facial_gesture", "unknown")

    def get_string_for_tui_element_on_start(self) -> str:
        return f"Face: {self.gesture}"

    def on_start(self, event: Union[InternalEvent, dict]) -> None:
        super().on_start(event)
        self.app.current_animation = self.gesture

    def on_started_from_running(self, event: Union[InternalEvent, dict]) -> None:
        self.app.current_animation = self.gesture

    def on_tick(self, event: Union[InternalEvent, dict]) -> None:
        if self.active_mode:
            if self.progress < MOTION_DURATION:
                self.progress += 1
            else:
                self.task_done = True

    def send_action_started_event(self) -> None:
        action_started = new_event("FacialGestureBotActionStarted", action_uid=self.action_uid)
        self.send_event(action_started)

    def send_action_finished_event(self) -> None:
        action_finished = new_event(
            "FacialGestureBotActionStarted", action_uid=self.action_uid, was_stopped=self.was_stopped
        )
        self.send_event(action_finished)


class MotionEffectActionHandler(ActionHandler):
    triggers = [
        "StartMotionEffectCameraAction",
        "MotionEffectCameraActionStarted",
        "StopMotionEffectCameraAction",
        "MotionEffectCameraActionFinished",
    ]
    action_name = "MotionEffectCameraAction"
    tui_element_id = "#camera-motion-effect"

    def __init__(self, active_mode: bool, app: App) -> None:
        super().__init__(active_mode, app)
        self.progress = 0

    @property
    def effect(self) -> str:
        return self.action_state.get("effect", "unknown")

    def get_string_for_tui_element_on_start(self) -> str:
        return f"Camera Effect: {self.effect}"

    def on_tick(self, event: Union[InternalEvent, dict]) -> None:
        if self.active_mode:
            if self.progress < MOTION_DURATION:
                self.progress += 1
            else:
                self.task_done = True

    def send_action_started_event(self) -> None:
        action_started = new_event("MotionEffectCameraActionStarted", action_uid=self.action_uid)
        self.send_event(action_started)

    def send_action_finished_event(self) -> None:
        action_finished = new_event(
            "MotionEffectCameraActionFinished",
            action_uid=self.action_uid,
            was_stopped=self.was_stopped,
            is_success=True,
        )
        self.send_event(action_finished)


class OverrideActionHandler(ActionHandler):
    action_stack: Deque[ActionHandler] = deque()

    def __init__(self, active_mode: bool, app: App) -> None:
        super().__init__(active_mode, app)
        self.stopped_from_running = False

    def on_start(self, event: Union[InternalEvent, dict]) -> None:
        super().on_start(event)

        if len(self.action_stack) > 0:
            for action in reversed(self.action_stack):
                if action.state != "finished":
                    self.action_stack[-1].demote(InternalEvent("overridden"))
                    break
        self.action_stack.append(self)

    def on_stop_from_background(self, event: Union[InternalEvent, dict]):
        super().on_stop_from_background(event)

        self._remove_from_stack()

    def on_stop_from_running(self, event: Union[InternalEvent, dict]):
        super().on_stop_from_running(event)

        self._remove_from_stack()
        self.stopped_from_running = True

    def _remove_from_stack(self) -> None:
        action_to_remove: Optional[ActionHandler] = None
        for action in reversed(self.action_stack):
            if action.action_uid == self.action_uid:
                action_to_remove = action
                break

        if action_to_remove:
            self.action_stack.remove(action_to_remove)

    def on_enter_finished(self, event: Union[InternalEvent, dict]) -> None:
        if self.stopped_from_running:
            if len(self.action_stack) > 0:
                for action in reversed(self.action_stack):
                    if action.state != "finished":
                        self.action_stack[-1].promote(InternalEvent("overriding action finished"))
                        break
            else:
                self._reset_default_state()

    def _reset_default_state(self) -> None:
        pass


class PostureActionHandler(OverrideActionHandler):
    triggers = ["StartPostureBotAction", "PostureBotActionStarted", "StopPostureBotAction", "PostureBotActionFinished"]
    action_name = "PostureBotAction"
    tui_element_id = "#bot-posture"
    # Posture has its own action stack
    action_stack: Deque[ActionHandler] = deque()

    def __init__(self, active_mode: bool, app: App) -> None:
        super().__init__(active_mode, app)
        self.progress = 0

    @property
    def posture(self) -> str:
        return self.action_state.get("posture", "unknown")

    def on_start(self, event: Union[InternalEvent, dict]) -> None:
        super().on_start(event)
        self._change_animation()

    def on_promote(self, event: Union[InternalEvent, dict]) -> None:
        super().on_promote(event)
        self._change_animation()

    def get_string_for_tui_element_on_start(self) -> str:
        return f"Posture: {self.posture}"

    def send_action_started_event(self) -> None:
        action_started = new_event("PostureBotActionStarted", action_uid=self.action_uid)
        self.send_event(action_started)

    def send_action_finished_event(self) -> None:
        action_finished = new_event(
            "PostureBotActionFinished",
            action_uid=self.action_uid,
            was_stopped=self.was_stopped,
            is_success=True,
        )
        self.send_event(action_finished)

    def _change_animation(self) -> None:
        self.app.current_animation = self.posture

    def _reset_default_state(self) -> None:
        self.app.current_animation = "idle"


class PositionActionHandler(OverrideActionHandler):
    triggers = [
        "StartPositionBotAction",
        "PositionBotActionStarted",
        "StopPositionBotAction",
        "PositionBotActionFinished",
    ]
    action_name = "PositionBotAction"
    tui_element_id = "#bot-position"
    # Position has its own action stack
    action_stack: Deque[ActionHandler] = deque()

    def __init__(self, active_mode: bool, app: App) -> None:
        super().__init__(active_mode, app)

    @property
    def position(self) -> str:
        return self.action_state.get("position", "unknown")

    def get_string_for_tui_element_on_start(self) -> str:
        return f"Position: {self.position}"

    def send_action_started_event(self) -> None:
        action_started = new_event("PositionBotActionStarted", action_uid=self.action_uid)
        self.send_event(action_started)

    def send_action_finished_event(self) -> None:
        action_finished = new_event(
            "PositionBotActionFinished",
            action_uid=self.action_uid,
            was_stopped=self.was_stopped,
            is_success=True,
        )
        self.send_event(action_finished)


class CameraShotActionHandler(OverrideActionHandler):
    triggers = [
        "StartShotCameraAction",
        "ShotCameraActionStarted",
        "StopShotCameraAction",
        "ShotCameraActionFinished",
    ]
    action_name = "ShotCameraAction"
    tui_element_id = "#camera-shot"
    # Position has its own action stack
    action_stack: Deque[ActionHandler] = deque()

    def __init__(self, active_mode: bool, app: App) -> None:
        super().__init__(active_mode, app)

    @property
    def position(self) -> str:
        return self.action_state.get("shot", "unknown")

    @property
    def start_transition(self) -> str:
        return self.action_state.get("start_transition", "unknown")

    def get_string_for_tui_element_on_start(self) -> str:
        return f"Shot: {self.position} Transition: {self.start_transition}"

    def send_action_started_event(self) -> None:
        action_started = new_event("ShotCameraActionStarted", action_uid=self.action_uid)
        self.send_event(action_started)

    def send_action_finished_event(self) -> None:
        action_finished = new_event(
            "ShotCameraActionFinished",
            action_uid=self.action_uid,
            was_stopped=self.was_stopped,
            is_success=True,
        )
        self.send_event(action_finished)


class UtteranceBotActionHandler(ActionHandler):
    triggers = [
        "StartUtteranceBotAction",
        "UtteranceBotActionStarted",
        "StopUtteranceBotAction",
        "UtteranceBotActionFinished",
    ]
    action_name = "UtteranceBotAction"
    tui_element_id = "#bot-utterance"

    @property
    def script(self) -> str:
        return self.action_state.get("script", "unknown")

    def __init__(self, active_mode: bool, app: App) -> None:
        super().__init__(active_mode, app)
        self.progress = 0

    def get_string_for_tui_element_on_start(self) -> str:
        return self.script if not self.active_mode else ""

    def on_tick(self, event: Union[InternalEvent, dict]) -> None:
        if self.active_mode:
            if self.progress < len(self.script):
                self.progress += 4
                self.app.bell()
                self.app.query_one("#bot-utterance", expect_type=Static).update(self.script[: self.progress])
            else:
                self.task_done = True

    def send_action_started_event(self) -> None:
        action_started = new_event("UtteranceBotActionStarted", action_uid=self.action_uid)
        self.send_event(action_started)

    def send_action_finished_event(self) -> None:
        action_finished = new_event(
            "UtteranceBotActionFinished",
            action_uid=self.action_uid,
            is_success=True,
            final_script=self.script,
            was_stopped=self.was_stopped,
        )
        self.send_event(action_finished)


class VisualActionHandler(OverrideActionHandler):
    tui_element_id = "#scene-ui"
    # All UI actions share one action stack
    action_stack: Deque[ActionHandler] = deque()

    def on_start(self, event: Union[InternalEvent, dict]) -> None:
        super().on_start(event)
        self._show_screen()

    def on_enter_finished(self, event: Union[InternalEvent, dict]) -> None:
        self._hide_screen()
        super().on_enter_finished(event)

    def on_promote(self, event: Union[InternalEvent, dict]) -> None:
        super().on_promote(event)
        self._show_screen()

    def _show_screen(self) -> None:
        choices = self.app.query("UserChoice")
        choices.remove()
        buttons = self.app.query("ConfirmationButtons")
        buttons.remove()
        forms = self.app.query("InputForm")
        forms.remove()
        self.app.query_one("#ui-view", expect_type=Markdown).update(self._generate_ui_content())
        global ui_sidebar_open
        if not ui_sidebar_open:
            self.app.action_toggle_sidebar()
            ui_sidebar_open = True

    def _hide_screen(self) -> None:
        global ui_sidebar_open
        if ui_sidebar_open:
            self.app.action_toggle_sidebar()
            ui_sidebar_open = False

    def _reset_default_state(self) -> None:
        choices = self.app.query("UserChoice")
        choices.remove()
        buttons = self.app.query("ConfirmationButtons")
        buttons.remove()
        forms = self.app.query("InputForm")
        forms.remove()
        self.app.query_one("#ui-view", expect_type=Markdown).update("")


class VisualInformationActionHandler(VisualActionHandler):
    triggers = [
        "StartVisualInformationSceneAction",
        "VisualInformationSceneActionStarted",
        "StopVisualInformationSceneAction",
        "VisualInformationSceneActionFinished",
        "VisualInformationSceneActionConfirmationUpdated",
    ]
    action_name = "VisualInformationSceneAction"

    def get_string_for_tui_element_on_start(self) -> str:
        return f"Information: {self.action_state.get('title', 'unknown')}"

    def _show_screen(self) -> None:
        super()._show_screen()
        new_buttons = ConfirmationButtons(self, "VisualInformationSceneAction")
        self.app.query_one("#ui-controls-view").mount(new_buttons)

    def _generate_ui_content(self) -> str:
        prompts = self.action_state.get("support_prompts", []) or []
        support_prompts = " | ".join([p for p in prompts])

        content = "Content:\n"

        for i, c in enumerate(self.action_state.get("content", [])):
            content += "- "
            if "text" in c and c["text"]:
                content += f"{c['text']} "
            if "image" in c and c["image"]:
                content += f"  `[{c['image']}]`"

            content += "\n"

        return f"""
# {self.action_state['title']}

*Hints : {support_prompts or "None"}*

{content}
"""

    def send_action_started_event(self) -> None:
        action_started = new_event("VisualInformationSceneActionStarted", action_uid=self.action_uid)
        self.send_event(action_started)

    def send_action_finished_event(self) -> None:
        action_finished = new_event(
            "VisualInformationSceneActionFinished",
            action_uid=self.action_uid,
            is_success=True,
        )
        self.send_event(action_finished)


class VisualChoiceActionHandler(VisualActionHandler):
    triggers = [
        "StartVisualChoiceSceneAction",
        "VisualChoiceSceneActionStarted",
        "StopVisualChoiceSceneAction",
        "VisualChoiceSceneActionFinished",
    ]
    action_name = "VisualChoiceSceneAction"

    def get_string_for_tui_element_on_start(self) -> str:
        return f"Choice: {self.action_state.get('prompt', 'unknown')}"

    def _show_screen(self) -> None:
        super()._show_screen()
        options = self.action_state.get("options", [])

        max_text_length = max([len(o.get("text", "")) for o in options]) + 1
        max_image_length = max([len(o.get("image", "")) for o in options]) + 1

        choices = []
        for option in self.action_state.get("options", []):
            option_id = option.get("id", "")
            option_text = option.get("text", "")
            option_image = option.get("image", "")
            text = f"{option_text:<{max_text_length}}:city_sunset: {option_image:<{max_image_length}}([italic]{option_id}[/])"
            choices.append((option_id, text))

        new_choice = UserChoice(self, choices)
        new_buttons = ConfirmationButtons(self, "VisualChoiceSceneAction")

        self.app.query_one("#ui-controls-view").mount(new_choice, new_buttons)

    def _generate_ui_content(self) -> str:
        prompts = self.action_state.get("support_prompts", []) or []
        support_prompts = " | ".join([p for p in prompts])

        return f"""
# {self.action_state['prompt']}

*Hints : {support_prompts or "None"}*
"""

    def send_action_started_event(self) -> None:
        action_started = new_event("VisualChoiceSceneActionStarted", action_uid=self.action_uid)
        self.send_event(action_started)

    def send_action_finished_event(self) -> None:
        action_finished = new_event(
            "VisualChoiceSceneActionFinished",
            action_uid=self.action_uid,
            is_success=True,
        )
        self.send_event(action_finished)


class VisualFormActionHandler(VisualActionHandler):
    triggers = [
        "StartVisualFormSceneAction",
        "VisualFormSceneActionStarted",
        "StopVisualFormSceneAction",
        "VisualFormSceneActionFinished",
        "VisualFormSceneActionInputUpdated",
        "VisualFormSceneActionConfirmationUpdated",
    ]
    action_name = "VisualFormSceneAction"

    def get_string_for_tui_element_on_start(self) -> str:
        return f"Form: {self.action_state.get('prompt', 'unknown')}"

    @property
    def inputs(self) -> List[Dict[str, str]]:
        inputs = self.action_state.get("inputs", [])
        return [{"id": i["id"], "value": i.get("value", ""), "description": i.get("description", "")} for i in inputs]

    def _show_screen(self) -> None:
        super()._show_screen()
        new_buttons = ConfirmationButtons(self, "VisualFormSceneAction")
        new_form = InputForm(self, self.inputs)
        self.app.query_one("#ui-controls-view").mount(new_form, new_buttons)

    def _generate_ui_content(self) -> str:
        prompts = self.action_state.get("support_prompts", []) or []
        support_prompts = " | ".join([p for p in prompts])

        # content = ""
        # for input in self.action_state.get("inputs", []):
        #     input_id = input.get("id", "")
        #     input_value = input.get("value", "")
        #     input_description = input.get("description", "")
        #     content += f"\n- Input `{input_id}` : *{input_description}*  (Value: {input_value})"

        return f"""
# {self.action_state['prompt']}

*Hints : {support_prompts or "None"}*
"""

    def send_action_started_event(self) -> None:
        action_started = new_event("VisualFormSceneActionStarted", action_uid=self.action_uid)
        self.send_event(action_started)

    def send_action_finished_event(self) -> None:
        action_finished = new_event(
            "VisualFormSceneActionFinished",
            action_uid=self.action_uid,
            is_success=True,
        )
        self.send_event(action_finished)


########################################################################################################################
# Textual UI
########################################################################################################################


@dataclass
class UserUtteranceState:
    in_progress: bool
    interim_transcript: str
    action_uid: str


class Title(Static):
    pass


class UserChoice(Widget):
    def __init__(self, action_handler: ActionHandler, options: List[Tuple[str, str]]) -> None:
        self.action_handler = action_handler
        self.options = options
        self.choices = set()
        super().__init__()

    def toggle_choice(self, id: str) -> None:
        if id not in self.choices:
            self.choices.add(id)
        else:
            self.choices.remove(id)

    def compose(self) -> ComposeResult:
        for id, text in self.options:
            yield RadioButton(text, id=id)

    def on_radio_button_changed(self, event: RadioButton.Changed) -> None:
        self.toggle_choice(event.radio_button.id)
        e = new_event(
            "VisualChoiceSceneActionChoiceUpdated",
            action_uid=self.action_handler.action_uid,
            current_choice=list(self.choices),
        )
        self.action_handler.send_event(e)


class InputForm(Static):
    def __init__(self, action_handler: ActionHandler, inputs: List[Dict[str, str]]) -> None:
        self.action_handler = action_handler
        self.inputs = {i["id"]: i for i in inputs}
        super().__init__()

    def compose(self) -> ComposeResult:
        for input in self.inputs.values():
            yield Label(input["description"], id=f"label-{input['id']}")
            yield Input(id=input["id"], value=input["value"])

    def on_input_changed(self, event: Input.Changed) -> None:
        if event.input.id in self.inputs:
            self.inputs[event.input.id]["value"] = event.value
            e = new_event(
                "VisualFormSceneActionInputUpdated",
                action_uid=self.action_handler.action_uid,
                interim_inputs=list(self.inputs.values()),
            )

            self.action_handler.send_event(e)


class ConfirmationButtons(Static):
    def __init__(
        self,
        action_handler: ActionHandler,
        action_type: str,
    ) -> None:
        self.action_handler = action_handler
        self.action_type = action_type
        super().__init__()

    def on_button_pressed(self, event: Button.Pressed) -> None:
        """Event handler called when a button is pressed."""

        button_id = event.button.id
        if button_id == "confirm-button":
            e = new_event(
                f"{self.action_type}ConfirmationUpdated",
                action_uid=self.action_handler.action_uid,
                confirmation_status="confirm",
            )
            self.action_handler.send_event(e)
        elif button_id == "deny-button":
            e = new_event(
                f"{self.action_type}ConfirmationUpdated",
                action_uid=self.action_handler.action_uid,
                confirmation_status="cancel",
            )
            self.action_handler.send_event(e)

    def compose(self) -> ComposeResult:
        yield Button("OK", variant="primary", classes="confirm-button", id="confirm-button")
        yield Button("Cancel", classes="deny-button", id="deny-button")


class UIView(Static):
    def compose(self) -> ComposeResult:
        yield Markdown("", id="ui-view")


class UIControlsView(Container):
    pass


class Sidebar(Container):
    def compose(self) -> ComposeResult:
        yield UIView()
        yield UIControlsView(id="ui-controls-view")


class UmimTuiApp(App):
    """A Textual app to manage stopwatches."""

    TITLE = "ACE CLI Simulator"
    SUB_TITLE = "Welcome"

    BINDINGS = [
        ("ctrl+t", "toggle_timestamps", "Toggle timestamps"),
        ("ctrl+s", "save_interaction", "Save events"),
        ("ctrl+c", "exit_app", "Exit"),
        ("ctrl+b", "toggle_sidebar", "Toggle scene UI"),
    ]
    CSS_PATH = "style.css"

    show_sidebar = reactive(False)

    event_worker: Optional[Worker] = None
    interaction_history: List[Dict[str, Any]] = []

    utterance_to_process: Optional[Dict[str, Any]] = None
    motion_to_process: Optional[Dict[str, Any]] = None

    user_utterance = UserUtteranceState(in_progress=False, interim_transcript="", action_uid="")

    show_system_events = False
    show_timestamps = False

    def __init__(self, event_log_path: Optional[Path] = None):
        self.SUB_TITLE = f"Connected to stream {stream_id}"
        super().__init__()

        self.running_actions: Dict[str, ActionHandler] = {}
        self.latest_action_id_per_action: Dict[str, List[str]] = {}
        self.trigger_to_handler: Dict[str, Type[ActionHandler]] = {}
        self.event_log_path = event_log_path
        self.event_log_index = 0
        self.event_log: List[Dict[str, Any]] = []
        self.current_animation = "idle"

        for handler_cls in [
            GestureActionHandler,
            FacialGestureActionHandler,
            PostureActionHandler,
            VisualChoiceActionHandler,
            VisualFormActionHandler,
            VisualInformationActionHandler,
            UtteranceBotActionHandler,
            PositionActionHandler,
            CameraShotActionHandler,
            TimerActionHandler,
            MotionEffectActionHandler,
            PresenceUserActionHandler,
        ]:
            self._register_action_handler(handler_cls)

    def _register_action_handler(self, handler_cls: Type[ActionHandler]) -> None:
        for trigger in handler_cls.triggers:
            self.trigger_to_handler[trigger] = handler_cls

    def _load_event_log(self) -> None:
        with open(self.event_log_path, "r", encoding="utf-8") as f:
            self.event_log = json.loads(f.read())
            self.event_log_index = -1

    @property
    def chat_log(self) -> TextLog:
        """Get the Markdown widget."""
        return self.query_one(TextLog)

    @property
    def input_prompt(self) -> Input:
        """Get the prompt widget."""
        return self.query_one("#input-prompt", expect_type=Input)

    def on_mount(self) -> None:
        self.channel = channel_id
        self.event_client = event_provider_factory(
            "redis",
            redis_host,
            redis_port,
            [self.channel],
            discard_existing_events=False,
        )

        if create_pipeline:
            self.run_worker(self.event_client.redis.delete(self.channel), exclusive=False)  # type: ignore
            self.run_worker(self.send_pipeline_acquired(), exclusive=False)

        self.update_timer = self.set_interval(1 / 10, self.update_time, pause=False)
        self.action_task = self.set_interval(1 / 10, self.process_actions, pause=False)

    def on_input_changed(self, event: Input.Changed) -> None:
        """Called as the user types."""

        # Don't start the UserUtterance until we know if the user wants to create a event or if he is actually
        # entering an utterance
        if (
            len(event.value) < 2
            or event.value.startswith("/")
            or event.value.startswith("{")
            or not app_in_active_mode
        ):
            return

        if not self.user_utterance.in_progress:
            self.user_utterance.in_progress = True
            action_started = new_event("UtteranceUserActionStarted", action_uid=new_uuid())
            self.user_utterance.interim_transcript = ""
            self.user_utterance.action_uid = action_started["action_uid"]
            self.run_worker(self.send_events(action_started), exclusive=False)

        if abs(self.user_utterance.interim_transcript.count(" ") - event.value.count(" ")) > 1:
            self.user_utterance.interim_transcript = event.value
            action_updated = new_event(
                "UtteranceUserActionTranscriptUpdated",
                action_uid=self.user_utterance.action_uid,
                interim_transcript=self.user_utterance.interim_transcript,
                stability=0.1,
            )

            self.run_worker(self.send_events(action_updated), exclusive=False)

    async def process_actions(self) -> None:
        handlers_to_remove = []
        for handler in self.running_actions.values():
            if handler.state == "finished":
                handlers_to_remove.append(handler.action_uid)
            else:
                if handler.state == "running":
                    if not handler.task_done:
                        handler.tick(InternalEvent("tick"))
                    else:
                        handler.done(InternalEvent("done"))

        for id in handlers_to_remove:
            handler = self.running_actions[id]
            self.latest_action_id_per_action[handler.action_name].remove(id)
            del self.running_actions[id]

    async def update_time(self) -> None:
        if self.event_worker and (
            self.event_worker.state == WorkerState.RUNNING or self.event_worker.state == WorkerState.PENDING
        ):
            return

        self.event_worker = self.run_worker(self.process_events(), group="event_receivers", exclusive=False)

    async def process_events(self) -> None:
        events = await self.event_client.receive_events(200)
        for event_str in events:
            event = json.loads(event_str)
            self.add_event(event)

    async def send_events(self, event_data: Union[str, dict]) -> None:
        if isinstance(event_data, dict):
            if event_data["action_uid"] == "LATEST":
                action_name = self.trigger_to_handler[event_data["type"]].action_name
                if (
                    action_name in self.latest_action_id_per_action
                    and len(self.latest_action_id_per_action[action_name]) > 0
                ):
                    event_data["action_uid"] = self.latest_action_id_per_action[action_name][-1]
                else:
                    self.add_event({"type": "Error", "reason": f"LATEST used but no running {action_name}"})

        await self.event_client.send_event(self.channel, event_data)

    async def send_pipeline_acquired(self) -> None:
        session_user_id = new_uuid()
        await self.event_client.send_event(
            SYSTEM_EVENTS_STREAM, new_event("PipelineAcquired", stream_uid=stream_id, user_uid=session_user_id)
        )

    async def send_pipeline_released(self) -> None:
        await self.event_client.send_event(SYSTEM_EVENTS_STREAM, new_event("PipelineReleased", stream_uid=stream_id))

    def compose(self) -> ComposeResult:
        """Create child widgets for the app."""
        yield Container(
            Sidebar(classes="-hidden", id="ui-container"),
            Header(show_clock=True),
            Horizontal(
                Container(
                    Vertical(
                        Container(
                            Vertical(
                                Container(
                                    Static("\n:speech_balloon:  [yellow]Chat:[/yellow]"),
                                    Static(
                                        "...",
                                        classes="fill-width modality",
                                        id="bot-utterance",
                                    ),
                                    classes="fill-width",
                                ),
                                Container(
                                    Static("\n:wave:  [yellow]Motion:[/yellow]"),
                                    Static(
                                        "...",
                                        classes="fill-width modality-slim",
                                        id="bot-gesture",
                                    ),
                                    Static(
                                        "...",
                                        classes="fill-width modality-slim",
                                        id="bot-facial-gesture",
                                    ),
                                    Static(
                                        "...",
                                        classes="fill-width modality-slim",
                                        id="bot-posture",
                                    ),
                                    Static(
                                        "...",
                                        classes="fill-width modality-slim",
                                        id="bot-position",
                                    ),
                                ),
                            ),
                            classes="avatar-area",
                        ),
                        Container(
                            Horizontal(
                                Container(
                                    Static("\n:tv:  [yellow]UI/Camera:[/yellow]"),
                                    Static(
                                        "...",
                                        classes="modality-slim fill-width",
                                        id="scene-ui",
                                    ),
                                    Static(
                                        "...",
                                        classes="modality-slim fill-width",
                                        id="camera-shot",
                                    ),
                                    Static(
                                        "...",
                                        classes="modality-slim fill-width",
                                        id="camera-motion-effect",
                                    ),
                                    classes="fill-width",
                                ),
                                Container(
                                    Static("\n:watch:  [yellow]Utils:[/yellow]"),
                                    Static(
                                        "...",
                                        classes="modality-slim fill-width",
                                        id="bot-timer",
                                    ),
                                    Static(
                                        "...",
                                        classes="fill-width modality-slim",
                                        id="presence-user",
                                    ),
                                    classes="third",
                                ),
                            ),
                        ),
                    ),
                    Label("Prompt:"),
                    Input(id="input-prompt"),
                    classes="fill-width",
                ),
                Container(
                    Static("[bold]Interaction History[/bold]"),
                    TextLog(highlight=True, markup=True, wrap=True),
                    classes="fill-width log",
                ),
                classes="fill-width",
            ),
        )
        yield Footer()

    def action_toggle_timestamps(self) -> None:
        """An action to toggle showing timestamps."""
        self.show_timestamps = not self.show_timestamps

        self.chat_log.clear()
        for event in self.interaction_history:
            self._show_event(event)

    def action_toggle_sidebar(self) -> None:
        sidebar = self.query_one(Sidebar)

        self.set_focus(self.query_one("#input-prompt"))
        if sidebar.has_class("-hidden"):
            sidebar.remove_class("-hidden")
        else:
            if sidebar.query("*:focus"):
                self.screen.set_focus(self.query_one("#input-prompt"))
            sidebar.add_class("-hidden")

    def _show_event(self, event: Dict[str, Any]) -> None:
        if self.show_system_events or "is_system_action" not in event or not event["is_system_action"]:
            entry = pretty_event(event, show_time=self.show_timestamps)
            self.chat_log.write(entry)
            # self.update_ui(event)

    def action_save_interaction(self) -> None:
        """An action to toggle showing all events ."""
        if len(self.interaction_history) > 0:
            with open(f"interaction_{channel_id}.json", "w", encoding="utf-8") as f:
                f.write(json.dumps(self.interaction_history, indent=4))

    def action_exit_app(self) -> None:
        """Exit the app"""
        if create_pipeline:
            asyncio.ensure_future(self.send_pipeline_released())
        self.exit()

    def on_input_submitted(self, event: Input.Submitted) -> None:
        self.send_event(event.value)
        self.input_prompt.value = ""

    def add_event(self, event_dict: Dict[str, Any]) -> None:
        self._show_event(event_dict)
        self.interaction_history.append(event_dict)

        # Is it a UMIM event?
        if "uid" in event_dict:
            try:
                event = try_parse_event(event_dict)

                if not isinstance(event, dict) or "action_uid" not in event:
                    return

                action_uid = event["action_uid"]
                handler = None

                if action_uid in self.running_actions:
                    handler = self.running_actions[action_uid]
                elif ("Started" in event["type"] or "Start" in event["type"]) and event[
                    "type"
                ] in self.trigger_to_handler:
                    handler = self.trigger_to_handler[event["type"]](app_in_active_mode, self)
                    self.running_actions[action_uid] = handler
                    self.latest_action_id_per_action.setdefault(handler.action_name, []).append(action_uid)

                if handler and event["type"] in handler.triggers:
                    if "Started" in event["type"]:
                        handler.started(event)
                    elif "Start" in event["type"]:
                        handler.start(event)
                    elif "Change" in event["type"]:
                        handler.change(event)
                    elif "Stop" in event["type"]:
                        handler.stop(event)
                    elif "Finished" in event["type"]:
                        handler.finished(event)
            except Exception as e:
                self.add_event(
                    {
                        "type": "Error",
                        "reason": f"{str(type(e).__name__)}: {str(e)} when running handler {type(handler).__name__ or 'unknown'}",
                    }
                )

    def send_event(self, event_description: str) -> None:
        if event_description[:5] == "/bot ":
            event = new_event("StartUtteranceBotAction", script=event_description[5:])
            self.run_worker(self.send_events(event), exclusive=False)
        elif event_description.startswith("{"):
            try:
                event_json = json.loads(event_description)
                event_type = event_json["type"]
                del event_json["type"]
                self.run_worker(
                    self.send_events(json.dumps(new_event(event_type=event_type, **event_json))), exclusive=False
                )
            except Exception as e:
                self.add_event({"type": "Error", "reason": str(e)})
        else:
            action_finished = new_event(
                "UtteranceUserActionFinished",
                action_uid=self.user_utterance.action_uid or new_uuid(),
                final_transcript=event_description,
                is_success=True,
            )
            self.user_utterance = UserUtteranceState(in_progress=False, interim_transcript="", action_uid="")
            self.run_worker(self.send_events(action_finished), exclusive=False)


async def list_all_active_streams(redis_host, redis_port) -> None:
    event_client = event_provider_factory(
        "redis",
        redis_host,
        redis_port,
        [SYSTEM_EVENTS_STREAM],
        discard_existing_events=False,
    )

    streams = {}
    events = await event_client.receive_events()
    for event_str in events:
        event = json.loads(event_str)
        if event["type"].strip() == "PipelineAcquired":
            streams[event["stream_uid"]] = ("ACTIVE", event["event_created_at"])
        elif event["type"].strip() == "PipelineReleased":
            streams[event["stream_uid"]] = ("DONE", event["event_created_at"])

    active_streams = [(id, status) for id, status in streams.items() if status[0] != "DONE"]
    completed_streams = [(id, status) for id, status in streams.items() if status[0] == "DONE"]
    print(f"[green]Active streams:[/green]")
    for id, status in active_streams:
        time_difference = datetime.now(timezone.utc) - read_isoformat(status[1])
        print(f"Stream {id} is running since: {status[1]} (for {time_difference})")

    print("\n[red]The following streams are completed:[/red]")
    for id, status in completed_streams:
        time_difference = datetime.now(timezone.utc) - read_isoformat(status[1])
        print(f"Stream {id} is done. Closed since {status[1]} (for {time_difference})")


@cli.command()
def main(
    stream: Annotated[
        Optional[str], typer.Option(help="Stream ID to use. Will generated random stream ID if not set.")
    ] = None,
    create: bool = True,
    active_mode: bool = True,
    list_streams: bool = False,
    event_provider_host: Optional[str] = None,
    event_provider_port: Optional[int] = None,
    event_log: Optional[Path] = typer.Option(None),
) -> None:
    global channel_id
    global create_pipeline
    global stream_id
    global app_in_active_mode
    global redis_port
    global redis_host

    stream_id = stream or new_uuid()
    channel_id = f"umim_events_{stream_id}"
    create_pipeline = create
    app_in_active_mode = active_mode

    if event_provider_port:
        redis_port = event_provider_port
    if event_provider_host:
        redis_host = event_provider_host

    if list_streams:
        asyncio.run(list_all_active_streams(redis_host, redis_port))
        return

    app = UmimTuiApp(event_log)
    app.run()


if __name__ == "__main__":
    cli()
````

## File: microservices/ace_agent/4.1/clients/event_client/requirements.txt
````
textual==0.29.0
typer==0.9.0
redis==4.6.0
transitions==0.9.0
typing_extensions==4.7.1
````

## File: microservices/ace_agent/4.1/clients/event_client/style.css
````css
.box {
    height: 100%;
    width: 1fr;
    /* border: solid green; */
}

.border{
    border: solid lightgray; 
}

.fill-width {
    width: 1fr;
    /* border: solid lightgray;  */
}

.fill-width-large {
    width: 2fr;
    /* border: solid green; */
}

.third{
    width: 30%;
}

.twothirds{
    width: 70%;
}


.robert {
    width: 100%;
    layout: horizontal;
    background: $boost;
    height: 10;
    min-width: 50;

}

.face{
    width: 40;
    height: 40;
}

.info{
    dock: right;
}


.face{
    dock: left;
    row-span: 4;
    column-span: 4;
}

.speech{
    width: 65%;
}

.modality{
    padding: 1;
    margin-left: 0;
    margin-right: 2;
    background: darkslategray;
}

.modality-slim{
    margin-left: 0;
    margin-right: 2;
    background: darkslategray;
}

.next-step{
    padding: 1;
    margin-left: 0;
    margin-right: 2;
    background: darksalmon;
}

.active{
    background: yellow;
}

.log{
    padding: 1;
    background: darkgray;
}

Sidebar {
    width: 50%;
    background: $panel;
    transition: offset 400ms in_out_cubic;
    layer: overlay;
    dock: right;

}

UIView{

}

UserChoice{
    height: auto;
    width: 1fr;
}

ConfirmationButtons
{
    padding-top: 2;
    layout: horizontal;
    height: auto;
    width: 1fr;
}

UIControlsView{
    margin-left: 4;
    margin-right: 4;

}


Sidebar:focus-within {
    offset: 0 0 !important;
}

Sidebar.-hidden {
    offset-x: 100%;
}

Sidebar Title {
    background: $boost;
    color: $secondary;
    padding: 2 4;
    border-right: vkey $background;
    dock: top;
    text-align: center;
    text-style: bold;
}

.confirm-button{
    dock: right
}

.deny-button{
}
````

## File: microservices/ace_agent/4.1/clients/speech_client/requirements.txt
````
grpcio-tools
````

## File: microservices/ace_agent/4.1/clients/speech_client/setup.sh
````bash
#!/bin/bash

# Input directory containing .proto files, default to ./proto/ if not provided
INPUT_DIR=${1:-"./../../proto/"}
# Output directory, default to current directory if not provided
OUTPUT_DIR=${2:-"./"}

# Check if protoc is installed
if ! command -v python3 -m grpc_tools.protoc &> /dev/null
then
  echo "Error: protoc is not installed. Please install it first."
  exit 1
fi

# Check if the input directory exists
if [ ! -d "$INPUT_DIR" ]; then
  echo "Error: Directory $INPUT_DIR not found."
  exit 1
fi

# Compile .proto file(s) in the input directory
echo "Compiling .proto files in $INPUT_DIR..."
python3 -m grpc_tools.protoc --proto_path=${INPUT_DIR} --python_out=${OUTPUT_DIR} --grpc_python_out=${OUTPUT_DIR} ${INPUT_DIR}*.proto
if [ $? -ne 0 ]; then
  echo "Failed to compile .proto files in $INPUT_DIR."
  exit 1
fi

echo "Compilation complete. Python files are located in ${OUTPUT_DIR}"
````

## File: microservices/ace_agent/4.1/clients/speech_client/speech_client.py
````python
# SPDX-FileCopyrightText: Copyright (c) 2022-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
#
# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
# property and proprietary rights in and to this material, related
# documentation and any modifications thereto. Any use, reproduction,
# disclosure or distribution of this material and related documentation
# without an express license agreement from NVIDIA CORPORATION or
# its affiliates is strictly prohibited.

# First we will import all the necessary dependencies
import argparse
import wave
import os
import grpc
import time
import uuid
import json

# We need to import python lib files generated from the proto
import ace_agent_pb2
import ace_agent_pb2_grpc


if __name__ == "__main__":
    # Parse arguments
    help_msg = "Speech Client App"
    parser = argparse.ArgumentParser(description=help_msg)
    parser.add_argument("--audio_file_path", type=str, help="ASR input audio wav file path")
    parser.add_argument("--tts_output_path", default="tts_output.wav", type=str, help="TTS output audio wav file path")
    parser.add_argument("--server", default="0.0.0.0:50055", type=str, help="GRPC Server URL")
    args = parser.parse_args()

    # First lets validate the audio file path received in args
    input_audio_file_path = args.audio_file_path
    if not input_audio_file_path or not os.path.exists(input_audio_file_path):
        print("ASR audio input file path is not valid")
        exit(0)

    # We will now create a grpc client stub using the Chat Controller Grpc server address
    channel = grpc.insecure_channel(args.server)
    stub = ace_agent_pb2_grpc.AceAgentGrpcStub(channel)

    # After creating the stub, we will now try to acquire a pipeline on Chat Controller server
    # We need to send a unique streamID in the request,  we will use stream-1 for now
    streamId = str(uuid.uuid4())
    print(f"Using streamId {streamId}")
    pipeline_request = ace_agent_pb2.PipelineRequest(stream_id=streamId)
    status_response = stub.CreatePipeline(pipeline_request)
    time.sleep(5)
    # If we get a status = PIPELINE_AVAILABLE, it means the pipeline has been allocated successfully else we exit
    if status_response.status == ace_agent_pb2.PIPELINE_AVAILABLE:
        print("Pipeline created successfully...")
    else:
        print(f"Could not create pipeline due, status response: {status_response}")

    # Now we will read audio data from a wav file and send audio query to Chat Controller server
    with wave.open(input_audio_file_path, "rb") as audio_file:
        # We need to read audio data in chunks of frames, lets read the data in chunks of 1600 frames
        def generator(audio_file):
            CHUNK = 1600
            sample_rate = audio_file.getframerate()
            channels = audio_file.getnchannels()
            frame_size = audio_file.getsampwidth()
            audio_content = None
            config = ace_agent_pb2.StreamingRecognitionConfig(
                encoding=ace_agent_pb2.LINEAR_PCM, sample_rate_hertz=sample_rate, audio_channel_count=channels
            )

            yield ace_agent_pb2.SendAudioRequest(streaming_config=config, stream_id=streamId)

            while True:
                audio_content = audio_file.readframes(CHUNK)

                if len(audio_content) <= 0:
                    break
                else:
                    yield ace_agent_pb2.SendAudioRequest(audio_content=audio_content)

                # We add a delay to the audio streaming to simulate a real time audio streaming
                bytes_read = len(audio_content)
                if bytes_read > 0:
                    time.sleep(0.05)

            # Simulate a silence of 1 second at the end of the audio to indicate end of speech
            # We'll split the silence into 10 chunks for easier processing
            number_of_chunks = 10
            silence_duration = 1
            for _ in range(number_of_chunks):
                empty_buffers = bytes((sample_rate * frame_size * silence_duration) // number_of_chunks)
                time.sleep(len(empty_buffers) / (sample_rate * frame_size))
                yield ace_agent_pb2.SendAudioRequest(audio_content=empty_buffers, stream_id=streamId)

        request_status = stub.SendAudio(generator(audio_file))
        print(request_status)

    # Now we will receive the TTS audio data back from Chat Controller server and write it to a wav file
    output_audio_file_path = args.tts_output_path
    receive_audio_request = ace_agent_pb2.ReceiveAudioRequest(stream_id=streamId)

    first_chunk = True
    config_set = False

    try:
        with wave.open(output_audio_file_path, "w") as output_audio_file:
            responses = stub.ReceiveAudio(receive_audio_request, timeout=10)
            for response in responses:
                # Here we check if this is the first chunk of the incoming audio data,
                # if yes, then we use this chunk to get the information about the audio buffer
                # in order to configure the wav writer
                if first_chunk:
                    output_audio_file.setnchannels(response.audio_channel_count)
                    output_audio_file.setsampwidth(response.frame_size)
                    output_audio_file.setframerate(response.sample_rate_hertz)
                    first_chunk = False
                    config_set = True
                if config_set:
                    data = response.audio_content
                    output_audio_file.writeframesraw(data)

            output_audio_file.close()
    except Exception as e:
        print(f"Exception occurred in getting TTS data")

    # Now we will call the Server side streaming Stream Speech Results API for streaming all the metadata from Chat Controller
    # We need to send a unique request_id as well along with the stream_id in this request header.

    stream_speech_results_request = ace_agent_pb2.StreamingSpeechResultsRequest(
        stream_id=streamId, request_id=str(uuid.uuid4())
    )
    streaming_responses = stub.StreamSpeechResults(stream_speech_results_request, timeout=10)

    # All new responses received on the grpc stream will now be printed in this loop.
    # The messages contain ASR transcripts, Chat Engine Response, Pipeline states and TTS latency etc.
    print("ASR Transcripts:")
    try:
        for response in streaming_responses:
            if response.message_type == ace_agent_pb2.ASR_RESPONSE:
                if not response.asr_result.results.is_final:
                    print(f"[PARTIAL] : {response.asr_result.results.alternatives[0].transcript}")
                else:
                    print(f"[FINAL] : {response.asr_result.results.alternatives[0].transcript}")
            elif response.message_type == ace_agent_pb2.CHAT_ENGINE_RESPONSE:
                chat_engine_response = json.loads(response.chat_engine_response.result)["Response"]["Text"]
                print(f"Bot Response: {chat_engine_response}")
            # The TTS response is the final response we are expecting after ASR and Chat Engine response.
            # Hence we will break the loop here
            elif response.message_type == ace_agent_pb2.TTS_RESPONSE:
                break
    except Exception as e:
        print(f"Response loop terminated due to an exception")
# Once done with all the requests, We will now free up the acquired pipeline on the Chat Controller server
pipeline_request = ace_agent_pb2.PipelineRequest(stream_id=streamId)
status_response = stub.FreePipeline(pipeline_request)
# If we get a status_response.response_msg saying Pipeline is released, it means the pipeline has been released successfully
print(status_response)
````

## File: microservices/ace_agent/4.1/deploy/docker/dockerfiles/chat_engine.Dockerfile
````dockerfile
ARG BASE_IMAGE

FROM $BASE_IMAGE

ARG HOST_UID
ARG HOST_GID

RUN echo "Creating docker image for Chat Engine.."

# Create specified user with home directory
RUN mkdir -p /home/ace-agent
RUN groupadd ace-agent --gid $HOST_GID \
    && useradd ace-agent --uid $HOST_UID --gid $HOST_GID -d /home/ace-agent \
    && chown -R $HOST_UID:$HOST_GID /home/ace-agent

##############################
# Install custom dependencies 
##############################

WORKDIR /home/ace-agent
USER $HOST_UID:$HOST_GID
````

## File: microservices/ace_agent/4.1/deploy/docker/dockerfiles/nlp_server.Dockerfile
````dockerfile
ARG BASE_IMAGE

FROM $BASE_IMAGE

ARG HOST_UID
ARG HOST_GID

RUN echo "Creating docker image for NLP Server.."

# Create specified user with home directory
RUN mkdir -p /home/ace-agent
RUN groupadd ace-agent --gid $HOST_GID \
    && useradd ace-agent --uid $HOST_UID --gid $HOST_GID -d /home/ace-agent \
    && chown -R $HOST_UID:$HOST_GID /home/ace-agent

##############################
# Install custom dependencies 
##############################

WORKDIR /home/ace-agent
USER $HOST_UID:$HOST_GID
````

## File: microservices/ace_agent/4.1/deploy/docker/dockerfiles/plugin_server.Dockerfile
````dockerfile
ARG BASE_IMAGE

FROM $BASE_IMAGE

ARG HOST_UID
ARG HOST_GID



# Create specified user with home directory
RUN mkdir -p /home/ace-agent
RUN groupadd ace-agent --gid $HOST_GID \
    && useradd ace-agent --uid $HOST_UID --gid $HOST_GID -d /home/ace-agent \
    && chown -R $HOST_UID:$HOST_GID /home/ace-agent

##############################
# Install custom dependencies 
##############################

WORKDIR /home/ace-agent
USER $HOST_UID:$HOST_GID
````

## File: microservices/ace_agent/4.1/deploy/docker/.env
````
# Environment variables
OPENAI_API_KEY=${OPENAI_API_KEY}

# NGC CLI variables
NGC_CLI_API_KEY=${NGC_CLI_API_KEY}
NGC_CLI_ORG=nvidia
NGC_CLI_TEAM=${NGC_CLI_TEAM}
NGC_ORGANIZATION_ID=${NGC_ORGANIZATION_ID}

# API Keys
NVIDIA_API_KEY=${NVIDIA_API_KEY}
````

## File: microservices/ace_agent/4.1/deploy/docker/docker_init.sh
````bash
export PIPELINE=speech_umim
export TAG=4.1.0
export CLI_CMD="aceagent chat cli -c $PWD/$BOT_PATH --log-path ${PWD}/log"
export DOCKER_REGISTRY=nvcr.io/nvidia/ace
export DOCKER_GROUP=$(stat -c %g /var/run/docker.sock)
export HOST_UID=$(id -u)
export HOST_GID=$(id -g)
export DOCKER_USER=$(id -u):$(id -g)
mkdir -p $PWD/speech_logs
````

## File: microservices/ace_agent/4.1/deploy/docker/docker-compose.yml
````yaml
services:
  model-utils: &model-utils
    container_name: model-utils
    image: ${DOCKER_REGISTRY}/model-utils:${TAG}
    env_file: .env
    environment:
      - BUILDER_ROOT=${PWD}
      - CACHE_PATH=${PWD}/.cache
      - TRANSFORMERS_CACHE=${PWD}/.cache
    volumes:
      - ${HOME}/.docker:/.docker
      - ${PWD}:${PWD}
      - /var/run/docker.sock:/var/run/docker.sock # Used for running sibling containers
    network_mode: 'host'
    tty: true
    user: ${DOCKER_USER}
    group_add:
      - $DOCKER_GROUP
    command: deploy --model_config_path ${PWD}/${BOT_PATH}/model_config.yaml --model_repository_path ./model_repository --gpus 1

  model-utils-speech:
    <<: *model-utils
    container_name: model-utils-speech
    command: deploy --model_config_path ${PWD}/${BOT_PATH}/model_config.yaml --model_repository_path ./model_repository --gpus 1 --speech

  nlp-server:
    container_name: nlp-server
    image: nlp-server:${TAG}
    build:
      context: .
      dockerfile: dockerfiles/nlp_server.Dockerfile
      args:
        BASE_IMAGE: ${DOCKER_REGISTRY}/nlp-server:${TAG}
        HOST_UID: ${HOST_UID}
        HOST_GID: ${HOST_GID}
    env_file: .env
    volumes:
      - ${PWD}:${PWD}
    network_mode: 'host'
    tty: true
    user: ${DOCKER_USER}
    shm_size: 2G
    command: aceagent nlp-server deploy --timeout 200 --workers 1 --config ${PWD}/${BOT_PATH}/model_config.yaml
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
  plugin-server:
    container_name: plugin-server
    image: plugin-server:${TAG}
    build:
      context: .
      dockerfile: dockerfiles/plugin_server.Dockerfile
      args:
        BASE_IMAGE: ${DOCKER_REGISTRY}/plugin-server:${TAG}
        HOST_UID: ${HOST_UID}
        HOST_GID: ${HOST_GID}
    volumes:
      - ${PWD}:${PWD}
    network_mode: 'host'
    env_file: .env
    tty: true
    user: ${DOCKER_USER}
    command: aceagent plugin-server deploy -c ${PWD}/${BOT_PATH}/plugin_config.yaml --log-path ${PWD}/log

  chat-engine: &chat-engine
    container_name: chat-engine
    image: chat-engine:${TAG}
    build:
      context: .
      dockerfile: dockerfiles/chat_engine.Dockerfile
      args:
        BASE_IMAGE: ${DOCKER_REGISTRY}/chat-engine:${TAG}
        HOST_UID: ${HOST_UID}
        HOST_GID: ${HOST_GID}
    env_file: .env
    volumes:
      - ${PWD}:${PWD}
    network_mode: 'host'
    stdin_open: true
    tty: true
    user: ${DOCKER_USER}
    command: aceagent chat server -c ${PWD}/${BOT_PATH} --log-path ${PWD}/log

  redis:
    container_name: redis-server
    image: redis
    network_mode: 'host'

  chat-controller:
    container_name: chat-controller
    image: ${DOCKER_REGISTRY}/chat-controller:${TAG}
    env_file: .env
    environment:
      - NVIDIA_DISABLE_REQUIRE=1
    volumes:
      - ${PWD}/${BOT_PATH}:/workspace/config
      - ${PWD}/speech_logs:/workspace/log
    network_mode: 'host'
    tty: true
    user: ${DOCKER_USER}
    command: bash run_pipeline.sh --pipeline ${PIPELINE} --speech_config /workspace/config/speech_config.yaml

  chat-bot: &chat-bot
    <<: *chat-engine
    container_name: chat-engine-server
    depends_on:
      - nlp-server
      - plugin-server
      - bot-web-ui-server

  chat-bot-cli:
    <<: *chat-bot
    container_name: chat-engine-cli
    command: bash

  event-bot: &event-bot
    <<: *chat-engine
    container_name: chat-engine-event
    command: aceagent chat event -c ${PWD}/${BOT_PATH} --log-path ${PWD}/log
    depends_on:
      - nlp-server
      - plugin-server
      - redis
      - bot-web-ui-event

  bot-web-ui-client:
    container_name: bot-web-ui-client
    image: ${DOCKER_REGISTRY}/ace-agent-ui-client:${TAG}
    network_mode: host

    # Uncomment to serve the client with SSL (https://):
    # Note: if SSL is enabled for the client, SSL must also be enabled for the server (see below)
    # command: --ssl -C /app/cert.pem -K /app/cert.key
    # volumes:
    #   - <path_to_cert.pem>:/app/cert.pem
    #   - <path_to_cert.key>:/app/cert.key

  bot-web-ui-server: &bot-web-ui-server-base
    container_name: bot-web-ui-server
    image: ${DOCKER_REGISTRY}/ace-agent-ui-server:${TAG}
    network_mode: host
    depends_on:
      - bot-web-ui-client
    command: --ace-agent-text-chat-interface server

    # Uncomment to use the websocket server with SSL enabled (wss://):
    # Note: if SSL is enabled for the server, SSL must also be enabled for the client (see above)
    # environment:
    #   SSL_CERT_PATH: /app/cert.pem
    #   SSL_KEY_PATH: /app/cert.key
    # volumes:
    #   - <path_to_cert.pem>:/app/cert.pem
    #   - <path_to_cert.key>:/app/cert.key

  bot-web-ui-event:
    <<: *bot-web-ui-server-base
    container_name: bot-web-ui-event
    command: --ace-agent-text-chat-interface event

  bot-web-ui-speech:
    <<: *bot-web-ui-server-base
    container_name: bot-web-ui-speech
    command: --ace-agent-text-chat-interface grpc --speech

  bot-web-ui-speech-event:
    <<: *bot-web-ui-server-base
    container_name: bot-web-ui-speech-event
    command: --ace-agent-text-chat-interface event --speech

  speech-bot:
    <<: *chat-bot
    container_name: chat-engine-server-speech
    depends_on:
      - nlp-server
      - plugin-server
      - chat-controller
      - bot-web-ui-speech

  speech-event-bot:
    <<: *event-bot
    container_name: chat-engine-event-speech
    depends_on:
      - nlp-server
      - plugin-server
      - chat-controller
      - redis
      - bot-web-ui-speech-event
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/chat_bot/food_ordering_bot/app-params.yaml
````yaml
chat-engine:
  botConfigName: food_ordering_bot_config.yaml
  logLevel: "DEBUG"
  interface: "event"
  # gunicorn configs
  gunicornWorkers: "1"
  gunicornTimeout: "150"

nlp-server:
  ucfVisibleGpus: [0]
  modelConfigPath: "model_config.yaml"
  customModelDir: ""
  # gunicorn configs
  gunicornWorkers: "4"
  gunicornTimeout: "120"

plugin-server:
  pluginConfigPath: "plugin_config.yaml"

webapp:
  chatInterface: "event"
  speechFlags: ""
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/chat_bot/food_ordering_bot/app.yaml
````yaml
specVersion: 2.5.0

version: 4.1.0

name: ucf-app-chat-bot

description: ACE Agent Food Ordering Bot

dependencies:
- ucf.svc.ace-agent.chat-engine:4.1.0
- ucf.svc.ace-agent.plugin-server:4.1.0
- ucf.svc.ace-agent.nlp-server:4.1.0
- ucf.svc.ace-agent.web-app:4.1.0
- ucf.svc.core.redis-timeseries:0.0.20

secrets:
  k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY:
    k8sSecret:
      secretName: ngc-api-key-secret
      key: NGC_CLI_API_KEY
  k8sSecret/openai-key-secret/OPENAI_API_KEY:
    k8sSecret:
      secretName: openai-key-secret
      key: OPENAI_API_KEY

components:
- name: nlp-server
  type: ucf.svc.ace-agent.nlp-server
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  files:
    config_dir: ../../../../samples/food_ordering_bot
- name: chat-engine
  type: ucf.svc.ace-agent.chat-engine
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
    openai-key-secret: k8sSecret/openai-key-secret/OPENAI_API_KEY
  files:
    config_dir: ../../../../samples/food_ordering_bot
- name: plugin-server
  type: ucf.svc.ace-agent.plugin-server
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  files:
    config_dir: ../../../../samples/food_ordering_bot
- name: webapp
  type: ucf.svc.ace-agent.web-app
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
- name: redis-timeseries
  type: ucf.svc.core.redis-timeseries
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret

connections:
  chat-engine/plugin-server: plugin-server/http-api
  chat-engine/nlp-server: nlp-server/api-server
  chat-engine/redis: redis-timeseries/redis
  webapp/redis: redis-timeseries/redis
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/chat_bot/food_ordering_bot/README.md
````markdown
# Food Ordering Sample Bot
The food ordering bot is a virtual assistant that can help you with placing your food order. The sample bot is capable of listing menu items, managing order cart and listing bill amount. For more details on the sample bot refer [the ACE Agent documentation](https://docs.nvidia.com/ace/latest/modules/ace_agent/sample-bots/food-ordering-bot.html).


This directory contains sample UCS application for building helm chart for text to text conversation using the food ordering bot. In following sections, we will add steps for deploying the sample bot using UCS application.

## Prerequisites
Before you start using NVIDIA ACE Agent, its assumed that you meet the following prerequisites. 
- You have access and are logged into [NVIDIA GPU Cloud (NGC)](https://ngc.nvidia.com/). For more details about NGC, refer to [the NGC documentation](https://docs.nvidia.com/ngc/index.html).
- You have installed [UCS tools](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Introduction.html) along with prerequisite setups such as Helm, Kubernetes, GPU Operator, and so on. Refer to UCS tools [developer system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Requirements.html) and [deployment system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Prerequisites.html) prerequisite sections for detailed instructions. 
- You have access to an NVIDIA Volta, NVIDIA Turing, NVIDIA Ampere, NVIDIA Ada Lovelace, or an NVIDIA Hopper Architecture-based GPU. The current version of ACE Agent is only supported on NVIDIA data centers.
- Install the Local Path Provisioner by running the following command if not already done:

    ```
    curl https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.23/deploy/local-path-storage.yaml | sed 's/^  name: local-path$/  name: mdx-local-path/g' | microk8s kubectl apply -f -
    ```

## Deployment

1. Setup the mandatory Kubernetes secrets required for deployment. Setup `ngc-docker-reg-secret` for downloading docker images and `ngc-api-key-secret` for downloading models/resources from NGC. 

    ```
    export NGC_CLI_API_KEY=...

    kubectl create secret docker-registry ngc-docker-reg-secret --docker-server=nvcr.io --docker-username='$oauthtoken' --docker-password="${NGC_CLI_API_KEY}"

    kubectl create secret generic ngc-api-key-secret --from-literal=NGC_CLI_API_KEY="${NGC_CLI_API_KEY}"
    ```

2. The Food Ordering sample bot uses OpenAI gpt-3.5-turbo-instruct as the main model. Configure the OpenAI API key. To create Kubernetes secret, run:

    ```
    export OPENAI_API_KEY=...

    kubectl create secret generic openai-key-secret --from-literal=OPENAI_API_KEY=${OPENAI_API_KEY}
    ```

3. Generate the Helm Chart using UCS tools.
    ```
    ucf_app_builder_cli app build app.yaml app-params.yaml
    ```

4. Deploy the generated Helm Chart.
    ```
    helm install ace-agent ucf-app-chat-bot-4.1.0/
    ```

5. Wait for all pods to be ready.
    ```
    watch kubectl get pods
    ```

5. Try out the deployed bot using a sample web frontend application. Get the nodeport for `ace-agent-webapp-deployment-service` using `kubectl get svc` and interact with the bot using the URL `http://<workstation IP>:<Webapp_NodePort>`. 


6. Stop the deployment and remove the persistent volumes.
    ```
    helm uninstall ace-agent

    kubectl delete pvc --all
    ```
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/chat_bot/llm_bot/app-params.yaml
````yaml
chat-engine:
  botConfigName: llm_bot_config.yml
  logLevel: "DEBUG"
  interface: "event"
  # gunicorn configs
  gunicornWorkers: "1"
  gunicornTimeout: "150"

webapp:
  chatInterface: "event"
  speechFlags: ""
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/chat_bot/llm_bot/app.yaml
````yaml
specVersion: 2.5.0

version: 4.1.0

name: ucf-app-chat-bot

description: ACE Agent LLM Bot UCS Application

dependencies:
- ucf.svc.ace-agent.chat-engine:4.1.0
- ucf.svc.ace-agent.web-app:4.1.0
- ucf.svc.core.redis-timeseries:0.0.20

secrets:
  k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY:
    k8sSecret:
      secretName: ngc-api-key-secret
      key: NGC_CLI_API_KEY
  k8sSecret/nvidia-api-key-secret/NVIDIA_API_KEY:
    k8sSecret:
      secretName: nvidia-api-key-secret
      key: NVIDIA_API_KEY

components:
- name: chat-engine
  type: ucf.svc.ace-agent.chat-engine
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
    nvidia-api-key-secret: k8sSecret/nvidia-api-key-secret/NVIDIA_API_KEY
  files:
    config_dir: ../../../../samples/llm_bot
- name: webapp
  type: ucf.svc.ace-agent.web-app
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
- name: redis-timeseries
  type: ucf.svc.core.redis-timeseries
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret

connections:
  webapp/redis: redis-timeseries/redis
  chat-engine/redis: redis-timeseries/redis
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/chat_bot/llm_bot/README.md
````markdown
# Large Language Model (LLM) Bot
This is an example chatbot that showcases bot usecase with Large Language Model (LLM). 


This directory contains sample UCS application for building helm chart for text to text conversation using the LLMs. In following sections, we will add steps for deploying the sample bot using the UCS application.

## Prerequisites
Before you start using NVIDIA ACE Agent, its assumed that you meet the following prerequisites. 
- You have access and are logged into [NVIDIA GPU Cloud (NGC)](https://ngc.nvidia.com/). For more details about NGC, refer to [the NGC documentation](https://docs.nvidia.com/ngc/index.html).
- You have installed [UCS tools](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Introduction.html) along with prerequisite setups such as Helm, Kubernetes, GPU Operator, and so on. Refer to UCS tools [developer system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Requirements.html) and [deployment system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Prerequisites.html) prerequisite sections for detailed instructions. 
- You have access to an NVIDIA Volta, NVIDIA Turing, NVIDIA Ampere, NVIDIA Ada Lovelace, or an NVIDIA Hopper Architecture-based GPU. The current version of ACE Agent is only supported on NVIDIA data centers.
- Install the Local Path Provisioner by running the following command if not already done:

    ```
    curl https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.23/deploy/local-path-storage.yaml | sed 's/^  name: local-path$/  name: mdx-local-path/g' | microk8s kubectl apply -f -
    ```

## Deployment

1. Setup the mandatory Kubernetes secrets required for deployment. Setup `ngc-docker-reg-secret` for downloading docker images and `ngc-api-key-secret` for downloading models/resources from NGC. 

    ```
    export NGC_CLI_API_KEY=...

    kubectl create secret docker-registry ngc-docker-reg-secret --docker-server=nvcr.io --docker-username='$oauthtoken' --docker-password="${NGC_CLI_API_KEY}"

    kubectl create secret generic ngc-api-key-secret --from-literal=NGC_CLI_API_KEY="${NGC_CLI_API_KEY}"
    ```

2.  The sample bot uses Meta's llama3-8b-instruct as the main model. Configure the NVIDIA API key to use hosted NIM model, you can use dummy value otherwise. To create Kubernetes secret, run:

    ```
    export NVIDIA_API_KEY=...

    kubectl create secret generic nvidia-api-key-secret-key-secret --from-literal=NVIDIA_API_KEY=${NVIDIA_API_KEY}
    ```

3. Generate the Helm Chart using UCS tools.
    ```
    ucf_app_builder_cli app build app.yaml app-params.yaml
    ```

4. Add the LLM url under `./ucf-app-chat-bot-4.1.0/charts/ace-agent-chat-engine/files/config_dir/actions.py`
    ```
    BASE_URL = "http://0.0.0.0:8010/v1" # Set to "https://integrate.api.nvidia.com/v1" for using hosted NIM models and provide API key using NVIDIA_API_KEY env variable
    ```

5. Deploy the generated Helm Chart.
    ```
    helm install ace-agent ucf-app-chat-bot-4.1.0/
    ```

6. Wait for all pods to be ready.
    ```
    watch kubectl get pods
    ```

7. Try out the deployed bot using a sample web frontend application. Get the nodeport for `ace-agent-webapp-deployment-service` using `kubectl get svc` and interact with the bot using the URL `http://<workstation IP>:<Webapp_NodePort>`. 


8. Stop the deployment and remove the persistent volumes.
    ```
    helm uninstall ace-agent

    kubectl delete pvc --all
    ```
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/chat_bot/rag_bot/app-params.yaml
````yaml
chat-engine:
  botConfigName: rag_bot_config.yml
  interface: "event"

plugin-server:
  pluginConfigPath: "plugin_config.yaml"
  pluginConfig:
    plugins:
      rag:
        parameters:
          RAG_SERVER_URL: "http://{UPDATE_RAG_HOST_IP}:{UPDATE_RAG_PORT}"

webapp:
  chatInterface: "event"
  speechFlags: ""
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/chat_bot/rag_bot/app.yaml
````yaml
specVersion: 2.5.0

version: 4.1.0

name: ucf-app-chat-bot

description: ACE Agent RAG Bot App

dependencies:
- ucf.svc.ace-agent.chat-engine:4.1.0
- ucf.svc.ace-agent.plugin-server:4.1.0
- ucf.svc.ace-agent.web-app:4.1.0
- ucf.svc.core.redis-timeseries:0.0.20

secrets:
  k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY:
    k8sSecret:
      secretName: ngc-api-key-secret
      key: NGC_CLI_API_KEY

components:
- name: chat-engine
  type: ucf.svc.ace-agent.chat-engine
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  files:
    config_dir: ../../../../samples/rag_bot
- name: plugin-server
  type: ucf.svc.ace-agent.plugin-server
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  files:
    config_dir: ../../../../samples/rag_bot

- name: webapp
  type: ucf.svc.ace-agent.web-app
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret

- name: redis-timeseries
  type: ucf.svc.core.redis-timeseries
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret

connections:
  chat-engine/plugin-server: plugin-server/http-api
  chat-engine/redis: redis-timeseries/redis
  webapp/redis: redis-timeseries/redis
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/chat_bot/rag_bot/README.md
````markdown
# Retrieval Augmented Generation Bot
This is an example chatbot that showcases Retrieval Augmented Generation (RAG) usecases using [NVIDIA Generative AI Examples](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RetrievalAugmentedGeneration).  For more details on the sample bot refer [the ACE Agent documentation](https://docs.nvidia.com/ace/latest/modules/ace_agent/sample-bots/rag-bot.html).


This directory contains sample UCS application for building helm chart for text to text conversation using the Retrieval Augmented Generation. In following sections, we will add steps for deploying the sample bot using the UCS application.

## Prerequisites
Before you start using NVIDIA ACE Agent, its assumed that you meet the following prerequisites. 
- You have access and are logged into [NVIDIA GPU Cloud (NGC)](https://ngc.nvidia.com/). For more details about NGC, refer to [the NGC documentation](https://docs.nvidia.com/ngc/index.html).
- You have installed [UCS tools](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Introduction.html) along with prerequisite setups such as Helm, Kubernetes, GPU Operator, and so on. Refer to UCS tools [developer system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Requirements.html) and [deployment system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Prerequisites.html) prerequisite sections for detailed instructions. 
- You have access to an NVIDIA Volta, NVIDIA Turing, NVIDIA Ampere, NVIDIA Ada Lovelace, or an NVIDIA Hopper Architecture-based GPU. The current version of ACE Agent is only supported on NVIDIA data centers.
- Install the Local Path Provisioner by running the following command if not already done:

    ```
    curl https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.23/deploy/local-path-storage.yaml | sed 's/^  name: local-path$/  name: mdx-local-path/g' | microk8s kubectl apply -f -
    ```

## Deployment

1. Setup the mandatory Kubernetes secrets required for deployment. Setup `ngc-docker-reg-secret` for downloading docker images and `ngc-api-key-secret` for downloading models/resources from NGC. 

    ```
    export NGC_CLI_API_KEY=...

    kubectl create secret docker-registry ngc-docker-reg-secret --docker-server=nvcr.io --docker-username='$oauthtoken' --docker-password="${NGC_CLI_API_KEY}"

    kubectl create secret generic ngc-api-key-secret --from-literal=NGC_CLI_API_KEY="${NGC_CLI_API_KEY}"
    ```

2. Deploy one of the RAG examples by following the instructions in [the GenerativeAIExamples repository](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RetrievalAugmentedGeneration/examples). A good example to start with is [the NVIDIA API Catalog](https://nvidia.github.io/GenerativeAIExamples/latest/api-catalog.html) example. You can also deploy RAG Server in Kubernetes using [NVIDIA Enterprise RAG LLM Operator](https://docs.nvidia.com/ai-enterprise/rag-llm-operator/24.3.0/index.html).

3. Update `app-params.yaml` with the deployed RAG Chain Server IP and Port details via `RAG_SERVER URL` parameter. 
4. Generate the Helm Chart using UCS tools.
    ```
    ucf_app_builder_cli app build app.yaml app-params.yaml
    ```

5. Deploy the generated Helm Chart.
    ```
    helm install ace-agent ucf-app-chat-bot-4.1.0/
    ```

6. Wait for all pods to be ready.
    ```
    watch kubectl get pods
    ```

7. Try out the deployed bot using a sample web frontend application. Get the nodeport for `ace-agent-webapp-deployment-service` using `kubectl get svc` and interact with the bot using the URL `http://<workstation IP>:<Webapp_NodePort>`. 


8. Stop the deployment and remove the persistent volumes.
    ```
    helm uninstall ace-agent

    kubectl delete pvc --all
    ```
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/chat_bot/stock_bot/app-params.yaml
````yaml
chat-engine:
  botConfigName: stock_bot_config.yml
  interface: "event"
  logLevel: "DEBUG"
  # gunicorn configs
  gunicornWorkers: "1"
  gunicornTimeout: "150"

plugin-server:
  pluginConfigPath: "plugin_config.yaml"
  pluginConfig:
    config:
      workers: 4
      timeout: 120

webapp:
  chatInterface: "event"
  speechFlags: ""
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/chat_bot/stock_bot/app.yaml
````yaml
specVersion: 2.5.0

version: 4.1.0

name: ucf-app-chat-bot

description: ACE Agent Stock Bot UCS Application

dependencies:
- ucf.svc.ace-agent.chat-engine:4.1.0
- ucf.svc.ace-agent.plugin-server:4.1.0
- ucf.svc.ace-agent.web-app:4.1.0
- ucf.svc.core.redis-timeseries:0.0.20

secrets:
  k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY:
    k8sSecret:
      secretName: ngc-api-key-secret
      key: NGC_CLI_API_KEY
  k8sSecret/openai-key-secret/OPENAI_API_KEY:
    k8sSecret:
      secretName: openai-key-secret
      key: OPENAI_API_KEY

components:
- name: chat-engine
  type: ucf.svc.ace-agent.chat-engine
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
    openai-key-secret: k8sSecret/openai-key-secret/OPENAI_API_KEY
  files:
    config_dir: ../../../../samples/stock_bot
- name: plugin-server
  type: ucf.svc.ace-agent.plugin-server
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  files:
    config_dir: ../../../../samples/stock_bot
- name: webapp
  type: ucf.svc.ace-agent.web-app
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
- name: redis-timeseries
  type: ucf.svc.core.redis-timeseries
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret

connections:
  chat-engine/plugin-server: plugin-server/http-api
  chat-engine/redis: redis-timeseries/redis
  webapp/redis: redis-timeseries/redis
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/chat_bot/stock_bot/README.md
````markdown
# Stock Sample Bot
This is an example chatbot for retrieving live stock prices for the stock of a specified company or organization. This bot also provides answers to queries related to the stock market and stocks. Here, [the Yahoo Finance API](https://pypi.org/project/yfinance/) is used for getting the stock prices of a stock. This bot only answers the questions related to stock prices and the stock market and does not have the ability to answer any off-topic queries. For more details on the sample bot refer [the ACE Agent documentation](https://docs.nvidia.com/ace/latest/modules/ace_agent/sample-bots/stock-market-bot.html).


This directory contains sample UCS application for building helm chart for text to text conversation using the stock bot. In following sections, we will add steps for deploying the sample bot using the UCS application.

## Prerequisites
Before you start using NVIDIA ACE Agent, its assumed that you meet the following prerequisites. 
- You have access and are logged into [NVIDIA GPU Cloud (NGC)](https://ngc.nvidia.com/). For more details about NGC, refer to [the NGC documentation](https://docs.nvidia.com/ngc/index.html).
- You have installed [UCS tools](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Introduction.html) along with prerequisite setups such as Helm, Kubernetes, GPU Operator, and so on. Refer to UCS tools [developer system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Requirements.html) and [deployment system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Prerequisites.html) prerequisite sections for detailed instructions. 
- You have access to an NVIDIA Volta, NVIDIA Turing, NVIDIA Ampere, NVIDIA Ada Lovelace, or an NVIDIA Hopper Architecture-based GPU. The current version of ACE Agent is only supported on NVIDIA data centers.
- Install the Local Path Provisioner by running the following command if not already done:

    ```
    curl https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.23/deploy/local-path-storage.yaml | sed 's/^  name: local-path$/  name: mdx-local-path/g' | microk8s kubectl apply -f -
    ```

## Deployment

1. Setup the mandatory Kubernetes secrets required for deployment. Setup `ngc-docker-reg-secret` for downloading docker images and `ngc-api-key-secret` for downloading models/resources from NGC. 

    ```
    export NGC_CLI_API_KEY=...

    kubectl create secret docker-registry ngc-docker-reg-secret --docker-server=nvcr.io --docker-username='$oauthtoken' --docker-password="${NGC_CLI_API_KEY}"

    kubectl create secret generic ngc-api-key-secret --from-literal=NGC_CLI_API_KEY="${NGC_CLI_API_KEY}"
    ```

2. The Stock bot uses OpenAI gpt-3.5-turbo-instruct as the main model. Configure the OpenAI API key. To create Kubernetes secret, run:

    ```
    export OPENAI_API_KEY=...

    kubectl create secret generic openai-key-secret --from-literal=OPENAI_API_KEY=${OPENAI_API_KEY}
    ```

3. Generate the Helm Chart using UCS tools.
    ```
    ucf_app_builder_cli app build app.yaml app-params.yaml
    ```

4. Deploy the generated Helm Chart.
    ```
    helm install ace-agent ucf-app-chat-bot-4.1.0/
    ```

5. Wait for all pods to be ready.
    ```
    watch kubectl get pods
    ```

5. Try out the deployed bot using a sample web frontend application. Get the nodeport for `ace-agent-webapp-deployment-service` using `kubectl get svc` and interact with the bot using the URL `http://<workstation IP>:<Webapp_NodePort>`. 


6. Stop the deployment and remove the persistent volumes.
    ```
    helm uninstall ace-agent

    kubectl delete pvc --all
    ```
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/ddg_langchain_bot/app-params.yaml
````yaml
chat-controller:
  pipeline: speech_lite
  speechConfigPath: "speech_config.yaml"
  chatEndpointPrefix: "langchain"
  pipelineParams: # can be extended for all pipeline params in speech_config.yaml
    grpc_server:
      nvidia::rrt::BotRuntimeGrpc:
        virtual_assistant_num_instances: 30

plugin-server:
  pluginConfigPath: "plugin_config.yaml"
  pluginConfig: # can be extended for all params in plugin_config.yaml
    config:
      workers: 4
      timeout: 120
  applicationSpecs:
    deployment:
      containers:
        container:
          image: # change image path with custom built docker image
            repository: <CUSTOM_DOCKER_IMAGE_PATH>
            tag: <VERSION>

riva-speech:
  riva:
    visibleGpus: "0"
  modelRepoGenerator:
    clearAllRMIRSAndModels: false
    #> description: Flag to clear artifacts and models before downloading and deploying
    ngcModelConfigs:
      triton0:
        models:
        #> description: List of NGC models for deployment
        - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0 #english
        - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
  persistentVolumeClaim:
    keepPVC: True

webapp:
  chatInterface: "grpc"
  speechFlags: "--speech"
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/ddg_langchain_bot/app.yaml
````yaml
specVersion: 2.5.0

version: 4.1.0

name: ucf-app-speech-bot

description: ACE Agent DuckDuckGo LangChain Agent

dependencies:
- ucf.svc.riva.speech-skills:2.17.0
- ucf.svc.ace-agent.plugin-server:4.1.0
- ucf.svc.ace-agent.chat-controller:4.1.0
- ucf.svc.ace-agent.web-app:4.1.0

secrets:
  k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY:
    k8sSecret:
      secretName: ngc-api-key-secret
      key: NGC_CLI_API_KEY
  k8sSecret/openai-key-secret/OPENAI_API_KEY:
    k8sSecret:
      secretName: openai-key-secret
      key: OPENAI_API_KEY

components:
- name: riva-speech
  type: ucf.svc.riva.speech-skills
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY

- name: plugin-server
  type: ucf.svc.ace-agent.plugin-server
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
    openai-key-secret: k8sSecret/openai-key-secret/OPENAI_API_KEY
  files:
    config_dir: ../../../../samples/ddg_langchain_bot

- name: chat-controller
  type: ucf.svc.ace-agent.chat-controller
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  files:
    config_dir: ../../../../samples/ddg_langchain_bot

- name: webapp
  type: ucf.svc.ace-agent.web-app
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret

connections:
  chat-controller/riva: riva-speech/riva-speech-api
  chat-controller/chat-api: plugin-server/http-api
  webapp/chat-controller: chat-controller/grpc-api
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/ddg_langchain_bot/README.md
````markdown
# DuckDuckGo LangChain Sample Bot
This is an example chatbot that showcases a [LangChain](https://www.langchain.com/) agent that uses conversation history and [the DuckDuckGo tool](https://python.langchain.com/v0.1/docs/integrations/tools/ddg/) to answer questions. It first rephrases the question based on the conversation history, poses the rephrased question to DuckDuckGo, and generates a final answer based on the DuckDuckGo output. It relies on a custom plugin endpoint which streams the response from the agent. It uses an [OpenAI](https://openai.com/) chat model for rephrasing and final response formation. For more details on the sample bot refer [the ACE Agent documentation](https://docs.nvidia.com/ace/latest/modules/ace_agent/sample-bots/duckduckgo-langchain-bot.html).

In following sections, we will add steps for deploying the sample bot using the UCS application.

## Prerequisites
Before you start using NVIDIA ACE Agent, its assumed that you meet the following prerequisites. 
- You have access and are logged into [NVIDIA GPU Cloud (NGC)](https://ngc.nvidia.com/). For more details about NGC, refer to [the NGC documentation](https://docs.nvidia.com/ngc/index.html).
- You have installed [UCS tools](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Introduction.html) along with prerequisite setups such as Helm, Kubernetes, GPU Operator, and so on. Refer to UCS tools [developer system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Requirements.html) and [deployment system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Prerequisites.html) prerequisite sections for detailed instructions. 
- You have access to an NVIDIA Volta, NVIDIA Turing, NVIDIA Ampere, NVIDIA Ada Lovelace, or an NVIDIA Hopper Architecture-based GPU. The current version of ACE Agent is only supported on NVIDIA data centers.
- Install the Local Path Provisioner by running the following command if not already done:

    ```
    curl https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.23/deploy/local-path-storage.yaml | sed 's/^  name: local-path$/  name: mdx-local-path/g' | microk8s kubectl apply -f -
    ```

## Deployment

1. Setup the mandatory Kubernetes secrets required for deployment. Setup `ngc-docker-reg-secret` for downloading docker images and `ngc-api-key-secret` for downloading models/resources from NGC. 

    ```
    export NGC_CLI_API_KEY=...

    kubectl create secret docker-registry ngc-docker-reg-secret --docker-server=nvcr.io --docker-username='$oauthtoken' --docker-password="${NGC_CLI_API_KEY}"

    kubectl create secret generic ngc-api-key-secret --from-literal=NGC_CLI_API_KEY="${NGC_CLI_API_KEY}"
    ```

2. The sample bot uses OpenAI `gpt-3.5-turbo-instruct` as the main model. Configure the OpenAI API key. To create Kubernetes secret, run:

    ```
    export OPENAI_API_KEY=...

    kubectl create secret generic openai-key-secret --from-literal=OPENAI_API_KEY=${OPENAI_API_KEY}
    ```

3. The LangChain plugin requires additional packages to be installed in the Plugin server container. 
    - For building the plugin server custom container, change directory to ACE Agent microservices root directory.
    
    - Change directory to Build a custom Dockerfile by copying the requirements from `samples/ddg_langchain_bot/plugins/requirements_dev.txt` into `deploy/docker/dockerfiles/plugin_server.Dockerfile`.

        ```
            ##############################
            # Install custom dependencies 
            ##############################
            RUN pip3 install \
                langchain==0.1.1 \
                langchain-community==0.0.13 \
                langchain-core==0.1.12 \
                duckduckgo-search==5.3.1b1
        ```
    
        > Note: If you see a crash in the Plugin server or an issue with fetching a response from DuckDuckGo, try using a more recent duckduckgo-search version.
        
    - Build the container and push to the NGC Docker registry.
        ```
        # Set required environment variables for docker-compose.yaml
        source deploy/docker/docker_init.sh

        # Build custom plugin server docker image
        docker compose -f deploy/docker/docker-compose.yml build plugin-server

        # Retag docker image and push to NGC docker registry
        docker tag docker.io/library/plugin-server:4.1.0 <CUSTOM_DOCKER_IMAGE_PATH>:<VERSION>

        docker push <CUSTOM_DOCKER_IMAGE_PATH>:<VERSION>
        ```
        If you want to use a different Docker registry, update `imagePullSecrets` in `langchain-app/app.yaml`.


4. Override the Plugin server image using `langchain-app/params.yaml`.
    ```
    plugin-server:
        pluginConfigPath: "plugin_config.yaml"
        applicationSpecs:
            deployment:
            containers:
                container:
                image:
                    repository: <CUSTOM_DOCKER_IMAGE_PATH>
                    tag: <VERSION>
    ```

4. Generate the Helm Chart using UCS tools.
    ```
    ucf_app_builder_cli app build app.yaml app-params.yaml
    ```

5. Deploy the generated Helm Chart.
    ```
    helm install ace-agent ucf-app-speech-bot-4.1.0/
    ```

6. Wait for all pods to be ready.
    ```
    watch kubectl get pods
    ```

7. Try out the deployed bot using a sample web frontend application. Get the nodeport for `ace-agent-webapp-deployment-service` using `kubectl get svc` and interact with the bot using the URL `http://<workstation IP>:<Webapp_NodePort>`. 

    > Note: For accessing the mic on the browser, we need to either convert http to https endpoint by adding SSL validation or update your chrome://flags/ or edge://flags/ to allow  http://<Node_IP>:<Webapp_NodePort> as a secure endpoint. 

8. Stop the deployment and remove the persistent volumes.
    ```
    helm uninstall ace-agent

    kubectl delete pvc --all
    ```
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/food_ordering_bot/app-params.yaml
````yaml
chat-controller:
  pipeline: speech_umim
  speechConfigPath: "speech_config.yaml"

chat-engine:
  botConfigName: food_ordering_bot_config.yaml
  interface: event

riva-speech:
  riva:
    visibleGpus: "0"
  modelRepoGenerator:
    clearAllRMIRSAndModels: false
    #> description: Flag to clear artifacts and models before downloading and deploying
    ngcModelConfigs:
      triton0:
        models:
        #> description: List of NGC models for deployment
        - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0 #english
        - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
  persistentVolumeClaim:
    keepPVC: True

nlp-server:
  ucfVisibleGpus: [0]
  modelConfigPath: "model_config.yaml"

plugin-server:
  pluginConfigPath: "plugin_config.yaml"

webapp:
  chatInterface: "event"
  speechFlags: "--speech"
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/food_ordering_bot/app.yaml
````yaml
specVersion: 2.5.0

version: 4.1.0

name: ucf-app-speech-bot

description: ACE Agent Food Ordering Bot

dependencies:
- ucf.svc.riva.speech-skills:2.17.0
- ucf.svc.ace-agent.chat-engine:4.1.0
- ucf.svc.ace-agent.chat-controller:4.1.0
- ucf.svc.ace-agent.plugin-server:4.1.0
- ucf.svc.ace-agent.web-app:4.1.0
- ucf.svc.ace-agent.nlp-server:4.1.0
- ucf.svc.core.redis-timeseries:0.0.20

secrets:
  k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY:
    k8sSecret:
      secretName: ngc-api-key-secret
      key: NGC_CLI_API_KEY
  k8sSecret/openai-key-secret/OPENAI_API_KEY:
    k8sSecret:
      secretName: openai-key-secret
      key: OPENAI_API_KEY

components:
- name: webapp
  type: ucf.svc.ace-agent.web-app
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
- name: redis-timeseries
  type: ucf.svc.core.redis-timeseries
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
- name: riva-speech
  type: ucf.svc.riva.speech-skills
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
- name: nlp-server
  type: ucf.svc.ace-agent.nlp-server
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  files:
    config_dir: ../../../../samples/food_ordering_bot
- name: chat-engine
  type: ucf.svc.ace-agent.chat-engine
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
    openai-key-secret: k8sSecret/openai-key-secret/OPENAI_API_KEY
  files:
    config_dir: ../../../../samples/food_ordering_bot
- name: chat-controller
  type: ucf.svc.ace-agent.chat-controller
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  files:
    config_dir: ../../../../samples/food_ordering_bot
- name: plugin-server
  type: ucf.svc.ace-agent.plugin-server
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  files:
    config_dir: ../../../../samples/food_ordering_bot


connections:
  chat-engine/plugin-server: plugin-server/http-api
  chat-engine/nlp-server: nlp-server/api-server
  chat-engine/redis: redis-timeseries/redis
  chat-controller/redis: redis-timeseries/redis
  chat-controller/riva: riva-speech/riva-speech-api
  webapp/chat-controller: chat-controller/grpc-api
  webapp/redis: redis-timeseries/redis
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/food_ordering_bot/README.md
````markdown
# Food Ordering Sample Bot
The food ordering bot is a virtual assistant that can help you with placing your food order. The sample bot is capable of listing menu items, managing order cart and listing bill amount. For more details on the sample bot refer [the ACE Agent documentation](https://docs.nvidia.com/ace/latest/modules/ace_agent/sample-bots/food-ordering-bot.html).


This directory contains sample UCS application for building helm chart for deploying the food ordering bot with speech support. In following sections, we will add steps for deploying the sample bot using the UCS application.

## Prerequisites
Before you start using NVIDIA ACE Agent, its assumed that you meet the following prerequisites. 
- You have access and are logged into [NVIDIA GPU Cloud (NGC)](https://ngc.nvidia.com/). For more details about NGC, refer to [the NGC documentation](https://docs.nvidia.com/ngc/index.html).
- You have installed [UCS tools](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Introduction.html) along with prerequisite setups such as Helm, Kubernetes, GPU Operator, and so on. Refer to UCS tools [developer system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Requirements.html) and [deployment system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Prerequisites.html) prerequisite sections for detailed instructions. 
- You have access to an NVIDIA Volta, NVIDIA Turing, NVIDIA Ampere, NVIDIA Ada Lovelace, or an NVIDIA Hopper Architecture-based GPU. The current version of ACE Agent is only supported on NVIDIA data centers.
- Install the Local Path Provisioner by running the following command if not already done:

    ```
    curl https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.23/deploy/local-path-storage.yaml | sed 's/^  name: local-path$/  name: mdx-local-path/g' | microk8s kubectl apply -f -
    ```

## Deployment

1. Setup the mandatory Kubernetes secrets required for deployment. Setup `ngc-docker-reg-secret` for downloading docker images and `ngc-api-key-secret` for downloading models/resources from NGC. 

    ```
    export NGC_CLI_API_KEY=...

    kubectl create secret docker-registry ngc-docker-reg-secret --docker-server=nvcr.io --docker-username='$oauthtoken' --docker-password="${NGC_CLI_API_KEY}"

    kubectl create secret generic ngc-api-key-secret --from-literal=NGC_CLI_API_KEY="${NGC_CLI_API_KEY}"
    ```

2. The Food Ordering sample bot uses OpenAI gpt-3.5-turbo-instruct as the main model. Configure the OpenAI API key. To create Kubernetes secret, run:

    ```
    export OPENAI_API_KEY=...

    kubectl create secret generic openai-key-secret --from-literal=OPENAI_API_KEY=${OPENAI_API_KEY}
    ```

3. Generate the Helm Chart using UCS tools.
    ```
    ucf_app_builder_cli app build app.yaml app-params.yaml
    ```

4. Deploy the generated Helm Chart.
    ```
    helm install ace-agent ucf-app-speech-bot-4.1.0/
    ```

5. Wait for all pods to be ready.
    ```
    watch kubectl get pods
    ```

5. Try out the deployed bot using a sample web frontend application. Get the nodeport for `ace-agent-webapp-deployment-service` using `kubectl get svc` and interact with the bot using the URL `http://<workstation IP>:<Webapp_NodePort>`. 

    > Note: For accessing the mic on the browser, we need to either convert http to https endpoint by adding SSL validation or update your chrome://flags/ or edge://flags/ to allow  http://<Node_IP>:<Webapp_NodePort> as a secure endpoint. 

6. Stop the deployment and remove the persistent volumes.
    ```
    helm uninstall ace-agent

    kubectl delete pvc --all
    ```
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/llm_bot/app-params.yaml
````yaml
chat-controller:
  pipeline: speech_umim
  speechConfigPath: "speech_config.yaml"

riva-speech:
  riva:
    visibleGpus: "0"
  modelRepoGenerator:
    clearAllRMIRSAndModels: false
    #> description: Flag to clear artifacts and models before downloading and deploying
    ngcModelConfigs:
      triton0:
        models:
        #> description: List of NGC models or Bot Config for deployment
        - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
        - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
  persistentVolumeClaim:
    keepPVC: True

chat-engine:
  botConfigName: llm_bot_config.yml
  interface: event

webapp:
  chatInterface: "event"
  speechFlags: "--speech"
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/llm_bot/app.yaml
````yaml
specVersion: 2.5.0

version: 4.1.0

name: ucf-app-speech-bot

description: ACE Agent LLM Bot

dependencies:
- ucf.svc.riva.speech-skills:2.17.0
- ucf.svc.ace-agent.chat-engine:4.1.0
- ucf.svc.ace-agent.chat-controller:4.1.0
- ucf.svc.ace-agent.web-app:4.1.0
- ucf.svc.core.redis-timeseries:0.0.20

secrets:
  k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY:
    k8sSecret:
      secretName: ngc-api-key-secret
      key: NGC_CLI_API_KEY
  k8sSecret/nvidia-api-key-secret/NVIDIA_API_KEY:
    k8sSecret:
      secretName: nvidia-api-key-secret
      key: NVIDIA_API_KEY


components:
- name: riva-speech
  type: ucf.svc.riva.speech-skills
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
- name: chat-engine
  type: ucf.svc.ace-agent.chat-engine
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
    nvidia-api-key-secret: k8sSecret/nvidia-api-key-secret/NVIDIA_API_KEY
  files:
    config_dir: ../../../../samples/llm_bot
- name: chat-controller
  type: ucf.svc.ace-agent.chat-controller
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  files:
    config_dir: ../../../../samples/llm_bot
- name: webapp
  type: ucf.svc.ace-agent.web-app
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
- name: redis-timeseries
  type: ucf.svc.core.redis-timeseries
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret

connections:
  chat-controller/riva: riva-speech/riva-speech-api
  chat-controller/redis: redis-timeseries/redis
  chat-engine/redis: redis-timeseries/redis
  webapp/chat-controller: chat-controller/grpc-api
  webapp/redis: redis-timeseries/redis
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/llm_bot/README.md
````markdown
# Large Language Model (LLM) Bot
This is an example speech-bot that showcases bot usecase with Large Language Model (LLM). 


This directory contains sample UCS application for building helm chart for speech to speech conversation using the LLMs. In following sections, we will add steps for deploying the sample bot using the UCS application.

## Prerequisites
Before you start using NVIDIA ACE Agent, its assumed that you meet the following prerequisites. 
- You have access and are logged into [NVIDIA GPU Cloud (NGC)](https://ngc.nvidia.com/). For more details about NGC, refer to [the NGC documentation](https://docs.nvidia.com/ngc/index.html).
- You have installed [UCS tools](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Introduction.html) along with prerequisite setups such as Helm, Kubernetes, GPU Operator, and so on. Refer to UCS tools [developer system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Requirements.html) and [deployment system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Prerequisites.html) prerequisite sections for detailed instructions. 
- You have access to an NVIDIA Volta, NVIDIA Turing, NVIDIA Ampere, NVIDIA Ada Lovelace, or an NVIDIA Hopper Architecture-based GPU. The current version of ACE Agent is only supported on NVIDIA data centers.
- Install the Local Path Provisioner by running the following command if not already done:

    ```
    curl https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.23/deploy/local-path-storage.yaml | sed 's/^  name: local-path$/  name: mdx-local-path/g' | microk8s kubectl apply -f -
    ```

## Deployment

1. Setup the mandatory Kubernetes secrets required for deployment. Setup `ngc-docker-reg-secret` for downloading docker images and `ngc-api-key-secret` for downloading models/resources from NGC. 

    ```
    export NGC_CLI_API_KEY=...

    kubectl create secret docker-registry ngc-docker-reg-secret --docker-server=nvcr.io --docker-username='$oauthtoken' --docker-password="${NGC_CLI_API_KEY}"

    kubectl create secret generic ngc-api-key-secret --from-literal=NGC_CLI_API_KEY="${NGC_CLI_API_KEY}"
    ```

2.  The sample bot uses Meta's llama3-8b-instruct as the main model. Configure the NVIDIA API key to use hosted NIM model, you can use dummy value otherwise. To create Kubernetes secret, run:

    ```
    export NVIDIA_API_KEY=...

    kubectl create secret generic nvidia-api-key-secret-key-secret --from-literal=NVIDIA_API_KEY=${NVIDIA_API_KEY}
    ```

3. Generate the Helm Chart using UCS tools.
    ```
    ucf_app_builder_cli app build app.yaml app-params.yaml
    ```

4. Add the LLM url under `./ucf-app-speech-bot-4.1.0/charts/ace-agent-chat-engine/files/config_dir/actions.py`
    ```
    BASE_URL = "http://0.0.0.0:8010/v1" # Set to "https://integrate.api.nvidia.com/v1" for using hosted NIM models and provide API key using NVIDIA_API_KEY env variable
    ```

5. Deploy the generated Helm Chart.
    ```
    helm install ace-agent ucf-app-speech-bot-4.1.0/
    ```

6. Wait for all pods to be ready.
    ```
    watch kubectl get pods
    ```

7. Try out the deployed bot using a sample web frontend application. Get the nodeport for `ace-agent-webapp-deployment-service` using `kubectl get svc` and interact with the bot using the URL `http://<workstation IP>:<Webapp_NodePort>`. 


8. Stop the deployment and remove the persistent volumes.
    ```
    helm uninstall ace-agent

    kubectl delete pvc --all
    ```
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/npc_bots/app-params.yaml
````yaml
chat-controller:
  pipeline: speech_umim
  speechConfigPath: "jin/speech_config.yaml"

riva-speech:
  riva:
    visibleGpus: "0"
  modelRepoGenerator:
    clearAllRMIRSAndModels: false
    #> description: Flag to clear artifacts and models before downloading and deploying
    ngcModelConfigs:
      triton0:
        models:
        #> description: List of NGC models or Bot Config for deployment
        - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
        - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
  persistentVolumeClaim:
    keepPVC: True

chat-engine:
  botConfigName: jin/bot_config.yaml
  interface: event

plugin-server:
  pluginConfigPath: "jin/plugin_config.yaml"

webapp:
  chatInterface: "event"
  speechFlags: "--speech"
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/npc_bots/app.yaml
````yaml
specVersion: 2.5.0

version: 4.1.0

name: ucf-app-speech-bot

description: Sample UCF application showcasing deployment of Nvidia ACE Agent Components for gaming usecases.

dependencies:
- ucf.svc.riva.speech-skills:2.17.0
- ucf.svc.ace-agent.chat-engine:4.1.0
- ucf.svc.ace-agent.chat-controller:4.1.0
- ucf.svc.ace-agent.plugin-server:4.1.0
- ucf.svc.ace-agent.web-app:4.1.0
- ucf.svc.core.redis-timeseries:0.0.20

secrets:
  k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY:
    k8sSecret:
      secretName: ngc-api-key-secret
      key: NGC_CLI_API_KEY
  k8sSecret/nvidia-api-key-secret/NVIDIA_API_KEY:
    k8sSecret:
      secretName: nvidia-api-key-secret
      key: NVIDIA_API_KEY

components:
- name: riva-speech
  type: ucf.svc.riva.speech-skills
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
- name: chat-engine
  type: ucf.svc.ace-agent.chat-engine
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  files:
    config_dir: ../../../../samples/npc_bots
- name: chat-controller
  type: ucf.svc.ace-agent.chat-controller
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  files:
    config_dir: ../../../../samples/npc_bots
- name: webapp
  type: ucf.svc.ace-agent.web-app
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
- name: redis-timeseries
  type: ucf.svc.core.redis-timeseries
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
- name: plugin-server
  type: ucf.svc.ace-agent.plugin-server
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
    nvidia-api-key-secret: k8sSecret/nvidia-api-key-secret/NVIDIA_API_KEY
  files:
    config_dir: ../../../../samples/npc_bots

connections:
  chat-controller/riva: riva-speech/riva-speech-api
  chat-controller/redis: redis-timeseries/redis
  webapp/chat-controller: chat-controller/grpc-api
  webapp/redis: redis-timeseries/redis
  chat-engine/plugin-server: plugin-server/http-api
  chat-engine/redis: redis-timeseries/redis
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/npc_bots/README.md
````markdown
# Gaming Non-Playing Character (NPC) Bots
The NPC sample bots showcase how you can:
- build LLM driven Natural Language Understanding (NLU) and Natural Language Generation (NLG) capabilities for non-playable characters in a game.
- provide game-stage specific context to the Chat Engine at runtime and utilize that context to change the behavior of NPC's.

There are two [sample NPC bots](https://docs.nvidia.com/ace/latest/modules/ace_agent/sample-bots/gaming-npc-bot.html) provided as part of the NVIDIA ACE Agent release. Each bot corresponds to one unique character in the game, having a unique personality, and backstory. You can find the backstories for the two characters in the `samples/jin/bot_config.yaml` and `samples/elara/bot_config.yaml` files respectively. You can change this backstory, if needed, based on the character you are designing. We can deploy both the bots in a single Chat Engine instance and try them out together interactively. These bots use [NVIDIA API Catalogs  mixtral-8x7b-instruct](https://build.nvidia.com/mistralai/mixtral-8x7b-instruct) as the main model. 


## Prerequisites
Before you start using NVIDIA ACE Agent, its assumed that you meet the following prerequisites. 
- You have access and are logged into [NVIDIA GPU Cloud (NGC)](https://ngc.nvidia.com/). For more details about NGC, refer to [the NGC documentation](https://docs.nvidia.com/ngc/index.html).
- You have installed [UCS tools](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Introduction.html) along with prerequisite setups such as Helm, Kubernetes, GPU Operator, and so on. Refer to UCS tools [developer system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Requirements.html) and [deployment system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Prerequisites.html) prerequisite sections for detailed instructions. 
- You have access to an NVIDIA Volta, NVIDIA Turing, NVIDIA Ampere, NVIDIA Ada Lovelace, or an NVIDIA Hopper Architecture-based GPU. The current version of ACE Agent is only supported on NVIDIA data centers.
- Install the Local Path Provisioner by running the following command if not already done:

    ```
    curl https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.23/deploy/local-path-storage.yaml | sed 's/^  name: local-path$/  name: mdx-local-path/g' | microk8s kubectl apply -f -
    ```

## Deployment

1. Setup the mandatory Kubernetes secrets required for deployment. Setup `ngc-docker-reg-secret` for downloading docker images and `ngc-api-key-secret` for downloading models/resources from NGC. 

    ```
    export NGC_CLI_API_KEY=...

    kubectl create secret docker-registry ngc-docker-reg-secret --docker-server=nvcr.io --docker-username='$oauthtoken' --docker-password="${NGC_CLI_API_KEY}"

    kubectl create secret generic ngc-api-key-secret --from-literal=NGC_CLI_API_KEY="${NGC_CLI_API_KEY}"
    ```

2. The NPC bots uses nemotron-mini-4b-instruct from the NVIDIA API Catalog. Configure the NVIDIA API key. To create Kubernetes secret, run:

    ```
    export NVIDIA_API_KEY=...

    kubectl create secret generic nvidia-api-key-secret --from-literal=NVIDIA_API_KEY=${NVIDIA_API_KEY}
    ```

3. Generate the Helm Chart using UCS tools.
    ```
    ucf_app_builder_cli app build app.yaml app-params.yaml
    ```

4. Deploy the generated Helm Chart.
    ```
    helm install ace-agent ucf-app-speech-bot-4.1.0/
    ```

5. Wait for all pods to be ready.
    ```
    watch kubectl get pods
    ```

5. Try out the deployed bot using a sample web frontend application. Get the nodeport for `ace-agent-webapp-deployment-service` using `kubectl get svc` and interact with the bot using the URL `http://<workstation IP>:<Webapp_NodePort>`. 
    > Note: For accessing the mic on the browser, we need to either convert http to https endpoint by adding SSL validation or update your chrome://flags/ or edge://flags/ to allow  http://<Node_IP>:<Webapp_NodePort> as a secure endpoint.

6. Stop the deployment and remove the persistent volumes.
    ```
    helm uninstall ace-agent

    kubectl delete pvc --all
    ```
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/rag_bot/app-params.yaml
````yaml
chat-controller:
  pipeline: speech_umim
  speechConfigPath: "speech_config.yaml"
  chatEndpointPrefix: "rag"
  pipelineParams: # can be extended for all pipeline params in speech_config.yaml
    grpc_server:
      nvidia::rrt::BotRuntimeGrpc:
        virtual_assistant_num_instances: 30

chat-engine:
  botConfigName: rag_bot_config.yml
  interface: event

plugin-server:
  pluginConfigPath: "plugin_config.yaml"
  pluginConfig: # can be extended for all params in plugin_config.yaml
    config:
      workers: 4
      timeout: 120
    plugins:
      rag:
        parameters:
          RAG_SERVER_URL: "http://{UPDATE_RAG_HOST_IP}:{UPDATE_RAG_PORT}"

riva-speech:
  riva:
    visibleGpus: "0"
  modelRepoGenerator:
    clearAllRMIRSAndModels: false
    #> description: Flag to clear artifacts and models before downloading and deploying
    ngcModelConfigs:
      triton0:
        models:
        #> description: List of NGC models for deployment
        - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0 #english
        - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
  persistentVolumeClaim:
    keepPVC: True

webapp:
  chatInterface: "event"
  speechFlags: "--speech"
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/rag_bot/app.yaml
````yaml
specVersion: 2.5.0

version: 4.1.0

name: ucf-app-speech-bot

description: ACE Agent RAG Bot App

dependencies:
- ucf.svc.riva.speech-skills:2.17.0
- ucf.svc.ace-agent.plugin-server:4.1.0
- ucf.svc.ace-agent.chat-controller:4.1.0
- ucf.svc.ace-agent.web-app:4.1.0
- ucf.svc.core.redis-timeseries:0.0.20
- ucf.svc.ace-agent.chat-engine:4.1.0

secrets:
  k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY:
    k8sSecret:
      secretName: ngc-api-key-secret
      key: NGC_CLI_API_KEY

components:
- name: riva-speech
  type: ucf.svc.riva.speech-skills
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY

- name: plugin-server
  type: ucf.svc.ace-agent.plugin-server
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  files:
    config_dir: ../../../../samples/rag_bot

- name: redis-timeseries
  type: ucf.svc.core.redis-timeseries
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret

- name: chat-engine
  type: ucf.svc.ace-agent.chat-engine
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  files:
    config_dir: ../../../../samples/rag_bot

- name: chat-controller
  type: ucf.svc.ace-agent.chat-controller
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  files:
    config_dir: ../../../../samples/rag_bot

- name: webapp
  type: ucf.svc.ace-agent.web-app
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret

connections:
  chat-controller/riva: riva-speech/riva-speech-api
  chat-controller/redis: redis-timeseries/redis
  chat-engine/plugin-server: plugin-server/http-api
  chat-engine/redis: redis-timeseries/redis
  webapp/chat-controller: chat-controller/grpc-api
  webapp/redis: redis-timeseries/redis
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/rag_bot/README.md
````markdown
# Retrieval Augmented Generation Bot
This is an example chatbot that showcases Retrieval Augmented Generation (RAG) usecases using [NVIDIA Generative AI Examples](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RetrievalAugmentedGeneration).  For more details on the sample bot refer [the ACE Agent documentation](https://docs.nvidia.com/ace/latest/modules/ace_agent/sample-bots/rag-bot.html).


This directory contains sample UCS application for building helm chart for the Retrieval Augmented Generation usecases with speech support. In following sections, we will add steps for deploying the sample bot using UCS application.

## Prerequisites
Before you start using NVIDIA ACE Agent, its assumed that you meet the following prerequisites. 
- You have access and are logged into [NVIDIA GPU Cloud (NGC)](https://ngc.nvidia.com/). For more details about NGC, refer to [the NGC documentation](https://docs.nvidia.com/ngc/index.html).
- You have installed [UCS tools](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Introduction.html) along with prerequisite setups such as Helm, Kubernetes, GPU Operator, and so on. Refer to UCS tools [developer system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Requirements.html) and [deployment system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Prerequisites.html) prerequisite sections for detailed instructions. 
- You have access to an NVIDIA Volta, NVIDIA Turing, NVIDIA Ampere, NVIDIA Ada Lovelace, or an NVIDIA Hopper Architecture-based GPU. The current version of ACE Agent is only supported on NVIDIA data centers.
- Install the Local Path Provisioner by running the following command if not already done:

    ```
    curl https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.23/deploy/local-path-storage.yaml | sed 's/^  name: local-path$/  name: mdx-local-path/g' | microk8s kubectl apply -f -
    ```

## Deployment

1. Setup the mandatory Kubernetes secrets required for deployment. Setup `ngc-docker-reg-secret` for downloading docker images and `ngc-api-key-secret` for downloading models/resources from NGC. 

    ```
    export NGC_CLI_API_KEY=...

    kubectl create secret docker-registry ngc-docker-reg-secret --docker-server=nvcr.io --docker-username='$oauthtoken' --docker-password="${NGC_CLI_API_KEY}"

    kubectl create secret generic ngc-api-key-secret --from-literal=NGC_CLI_API_KEY="${NGC_CLI_API_KEY}"
    ```

2. Deploy one of the RAG examples by following the instructions in [the GenerativeAIExamples repository](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RetrievalAugmentedGeneration/examples). A good example to start with is [the NVIDIA API Catalog](https://nvidia.github.io/GenerativeAIExamples/latest/api-catalog.html) example. You can also deploy RAG Server in Kubernetes using [NVIDIA Enterprise RAG LLM Operator](https://docs.nvidia.com/ai-enterprise/rag-llm-operator/24.3.0/index.html).

3. Update `app-params.yaml` with the deployed RAG Chain Server IP and Port details via `RAG_SERVER URL` parameter.  
4. Generate the Helm Chart using UCS tools.
    ```
    ucf_app_builder_cli app build app.yaml app-params.yaml
    ```

5. Deploy the generated Helm Chart.
    ```
    helm install ace-agent ucf-app-speech-bot-4.1.0/
    ```

6. Wait for all pods to be ready.
    ```
    watch kubectl get pods
    ```

7. Try out the deployed bot using a sample web frontend application. Get the nodeport for `ace-agent-webapp-deployment-service` using `kubectl get svc` and interact with the bot using the URL `http://<workstation IP>:<Webapp_NodePort>`. 
     > Note: For accessing the mic on the browser, we need to either convert http to https endpoint by adding SSL validation or update your chrome://flags/ or edge://flags/ to allow  http://<Node_IP>:<Webapp_NodePort> as a secure endpoint. 

8. Stop the deployment and remove the persistent volumes.
    ```
    helm uninstall ace-agent

    kubectl delete pvc --all
    ```
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/stock_bot/app-params.yaml
````yaml
chat-controller:
  pipeline: speech_umim
  speechConfigPath: "speech_config.yaml"
  ipaDictPath: "cmudict_ipa.txt"
  pipelineParams: # can be extended for all pipeline params in speech_config.yaml
    grpc_server:
      nvidia::rrt::BotRuntimeGrpc:
        virtual_assistant_num_instances: 30

chat-engine:
  botConfigName: stock_bot_config.yml
  gunicornWorkers: "4"
  interface: event

riva-speech:
  riva:
    visibleGpus: "0"
  modelRepoGenerator:
    clearAllRMIRSAndModels: false
    #> description: Flag to clear artifacts and models before downloading and deploying
    ngcModelConfigs:
      triton0:
        models:
        #> description: List of NGC models for deployment
        - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0 #english
        - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
  persistentVolumeClaim:
    keepPVC: True

plugin-server:
  pluginConfigPath: "plugin_config.yaml"
  pluginConfig:
    config:
      workers: 4
      timeout: 120

webapp:
  chatInterface: "event"
  speechFlags: "--speech"
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/stock_bot/app.yaml
````yaml
specVersion: 2.5.0

version: 4.1.0

name: ucf-app-speech-bot

description: ACE Agent Stock Bot UCS Application

dependencies:
- ucf.svc.riva.speech-skills:2.17.0
- ucf.svc.ace-agent.chat-engine:4.1.0
- ucf.svc.ace-agent.chat-controller:4.1.0
- ucf.svc.ace-agent.plugin-server:4.1.0
- ucf.svc.core.redis-timeseries:0.0.20
- ucf.svc.ace-agent.web-app:4.1.0

secrets:
  k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY:
    k8sSecret:
      secretName: ngc-api-key-secret
      key: NGC_CLI_API_KEY
  k8sSecret/openai-key-secret/OPENAI_API_KEY:
    k8sSecret:
      secretName: openai-key-secret
      key: OPENAI_API_KEY

components:
- name: webapp
  type: ucf.svc.ace-agent.web-app
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
- name: riva-speech
  type: ucf.svc.riva.speech-skills
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
- name: chat-engine
  type: ucf.svc.ace-agent.chat-engine
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
    openai-key-secret: k8sSecret/openai-key-secret/OPENAI_API_KEY
  files:
    config_dir: ../../../../samples/stock_bot
- name: chat-controller
  type: ucf.svc.ace-agent.chat-controller
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  files:
    config_dir: ../../../../samples/stock_bot
- name: plugin-server
  type: ucf.svc.ace-agent.plugin-server
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret
  secrets:
    ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  files:
    config_dir: ../../../../samples/stock_bot
- name: redis-timeseries
  type: ucf.svc.core.redis-timeseries
  parameters:
    imagePullSecrets:
    - name: ngc-docker-reg-secret

connections:
  chat-engine/plugin-server: plugin-server/http-api
  chat-engine/redis: redis-timeseries/redis
  chat-controller/riva: riva-speech/riva-speech-api
  chat-controller/redis: redis-timeseries/redis
  webapp/chat-controller: chat-controller/grpc-api
  webapp/redis: redis-timeseries/redis
````

## File: microservices/ace_agent/4.1/deploy/ucs_apps/speech_bot/stock_bot/README.md
````markdown
# Stock Sample Bot
This is an example chatbot for retrieving live stock prices for the stock of a specified company or organization. This bot also provides answers to queries related to the stock market and stocks. Here, [the Yahoo Finance API](https://pypi.org/project/yfinance/) is used for getting the stock prices of a stock. This bot only answers the questions related to stock prices and the stock market and does not have the ability to answer any off-topic queries. For more details on the sample bot refer [the ACE Agent documentation](https://docs.nvidia.com/ace/latest/modules/ace_agent/sample-bots/stock-market-bot.html).


This directory contains sample UCS application for building helm chart for deploying the stock bot with speech support. In following sections, we will add steps for deploying the sample bot using the UCS application.

## Prerequisites
Before you start using NVIDIA ACE Agent, its assumed that you meet the following prerequisites. 
- You have access and are logged into [NVIDIA GPU Cloud (NGC)](https://ngc.nvidia.com/). For more details about NGC, refer to [the NGC documentation](https://docs.nvidia.com/ngc/index.html).
- You have installed [UCS tools](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Introduction.html) along with prerequisite setups such as Helm, Kubernetes, GPU Operator, and so on. Refer to UCS tools [developer system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Requirements.html) and [deployment system](https://docs.nvidia.com/ace/latest/modules/docs/docs/text/UCS_Prerequisites.html) prerequisite sections for detailed instructions. 
- You have access to an NVIDIA Volta, NVIDIA Turing, NVIDIA Ampere, NVIDIA Ada Lovelace, or an NVIDIA Hopper Architecture-based GPU. The current version of ACE Agent is only supported on NVIDIA data centers.
- Install the Local Path Provisioner by running the following command if not already done:

    ```
    curl https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.23/deploy/local-path-storage.yaml | sed 's/^  name: local-path$/  name: mdx-local-path/g' | microk8s kubectl apply -f -
    ```

## Deployment

1. Setup the mandatory Kubernetes secrets required for deployment. Setup `ngc-docker-reg-secret` for downloading docker images and `ngc-api-key-secret` for downloading models/resources from NGC. 

    ```
    export NGC_CLI_API_KEY=...

    kubectl create secret docker-registry ngc-docker-reg-secret --docker-server=nvcr.io --docker-username='$oauthtoken' --docker-password="${NGC_CLI_API_KEY}"

    kubectl create secret generic ngc-api-key-secret --from-literal=NGC_CLI_API_KEY="${NGC_CLI_API_KEY}"
    ```

2. The Stock bot uses OpenAI gpt-3.5-turbo-instruct as the main model. Configure the OpenAI API key. To create Kubernetes secret, run:

    ```
    export OPENAI_API_KEY=...

    kubectl create secret generic openai-key-secret --from-literal=OPENAI_API_KEY=${OPENAI_API_KEY}
    ```

3. Generate the Helm Chart using UCS tools.
    ```
    ucf_app_builder_cli app build app.yaml app-params.yaml
    ```

4. Deploy the generated Helm Chart.
    ```
    helm install ace-agent ucf-app-speech-bot-4.1.0/
    ```

5. Wait for all pods to be ready.
    ```
    watch kubectl get pods
    ```

5. Try out the deployed bot using a sample web frontend application. Get the nodeport for `ace-agent-webapp-deployment-service` using `kubectl get svc` and interact with the bot using the URL `http://<workstation IP>:<Webapp_NodePort>`. 
    > Note: For accessing the mic on the browser, we need to either convert http to https endpoint by adding SSL validation or update your chrome://flags/ or edge://flags/ to allow  http://<Node_IP>:<Webapp_NodePort> as a secure endpoint.

6. Stop the deployment and remove the persistent volumes.
    ```
    helm uninstall ace-agent

    kubectl delete pvc --all
    ```
````

## File: microservices/ace_agent/4.1/openapi/chat_engine_openapi.json
````json
{
    "openapi": "3.1.0",
    "info": {
        "title": "Chat Engine Server API",
        "description": "NVIDIA ACE Agent Chat Engine Server API",
        "version": "4.1.0"
    },
    "paths": {
        "/chat": {
            "post": {
                "tags": [
                    "Core APIs"
                ],
                "summary": "Chat",
                "description": "This endpoint can be used to provide response to query driven user request for a specific bot, using defined NLU models and plugin modules. The response formation methodology is picked from the bot configurations and rules defined.",
                "operationId": "chat_chat_post",
                "requestBody": {
                    "content": {
                        "application/json": {
                            "schema": {
                                "allOf": [
                                    {
                                        "$ref": "#/components/schemas/ChatRequest"
                                    }
                                ],
                                "title": "Request",
                                "description": "Chat Engine Request JSON. All the fields populated as part of this JSON is also available as part of request JSON."
                            }
                        }
                    },
                    "required": true
                },
                "responses": {
                    "200": {
                        "description": "Response JSON for the corresponding chat request JSON.",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/event": {
            "post": {
                "tags": [
                    "Core APIs"
                ],
                "summary": "Event",
                "description": "This endpoint can be used to provide response to a event driven user request for a specific bot, using defined NLU models and plugin modules. The response formation methodology is picked from the bot configurations and rules defined.",
                "operationId": "event_event_post",
                "requestBody": {
                    "content": {
                        "application/json": {
                            "schema": {
                                "allOf": [
                                    {
                                        "$ref": "#/components/schemas/EventRequest"
                                    }
                                ],
                                "title": "Request",
                                "description": "Chat Engine Request JSON. All the fields populated as part of this JSON is also available as part of request JSON."
                            }
                        }
                    },
                    "required": true
                },
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "anyOf": [
                                        {
                                            "$ref": "#/components/schemas/EventResponse"
                                        },
                                        {
                                            "$ref": "#/components/schemas/ExceptionResponse"
                                        }
                                    ],
                                    "title": "Response Event Event Post"
                                }
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/isReady": {
            "get": {
                "tags": [
                    "Health APIs"
                ],
                "summary": "Is Ready",
                "description": "Check status of the bots and returns the details of available bots",
                "operationId": "is_ready_isReady_get",
                "parameters": [
                    {
                        "name": "BotName",
                        "in": "query",
                        "required": false,
                        "schema": {
                            "type": "string",
                            "maxLength": 4096,
                            "title": "Botname"
                        }
                    }
                ],
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "anyOf": [
                                        {
                                            "$ref": "#/components/schemas/IsReadyResponseList"
                                        },
                                        {
                                            "$ref": "#/components/schemas/IsReadyResponseStatus"
                                        }
                                    ],
                                    "title": "Response Is Ready Isready Get"
                                }
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/reloadBot": {
            "get": {
                "tags": [
                    "Bot APIs"
                ],
                "summary": "Reload Bot",
                "description": "Reloads all the deployed bots or a selected bot if a bot name is provided",
                "operationId": "reload_bot_reloadBot_get",
                "parameters": [
                    {
                        "name": "BotName",
                        "in": "query",
                        "required": false,
                        "schema": {
                            "type": "string",
                            "maxLength": 4096,
                            "title": "Botname"
                        }
                    },
                    {
                        "name": "BotVersion",
                        "in": "query",
                        "required": false,
                        "schema": {
                            "type": "string",
                            "maxLength": 4096,
                            "default": "1",
                            "title": "Botversion"
                        }
                    }
                ],
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/ExceptionResponse"
                                }
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/updateUserContext": {
            "post": {
                "tags": [
                    "User Context APIs"
                ],
                "summary": "Update User Context",
                "description": "Updates the context of the user at runtime.",
                "operationId": "update_user_context_updateUserContext_post",
                "parameters": [
                    {
                        "name": "UserId",
                        "in": "query",
                        "required": true,
                        "schema": {
                            "type": "string",
                            "maxLength": 4096,
                            "title": "Userid"
                        }
                    }
                ],
                "requestBody": {
                    "required": true,
                    "content": {
                        "application/json": {
                            "schema": {
                                "type": "object",
                                "title": "Context"
                            }
                        }
                    }
                },
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/ExceptionResponse"
                                }
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/setUserContext": {
            "post": {
                "tags": [
                    "User Context APIs"
                ],
                "summary": "Set User Context",
                "description": "Sets the context of the user at runtime. It overwrites any existing context.",
                "operationId": "set_user_context_setUserContext_post",
                "requestBody": {
                    "content": {
                        "application/json": {
                            "schema": {
                                "$ref": "#/components/schemas/UserContext"
                            }
                        }
                    },
                    "required": true
                },
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/ExceptionResponse"
                                }
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/getUserContext": {
            "get": {
                "tags": [
                    "User Context APIs"
                ],
                "summary": "Get User Context",
                "description": "Returns the context for a specified user ids.",
                "operationId": "get_user_context_getUserContext_get",
                "parameters": [
                    {
                        "name": "UserId",
                        "in": "query",
                        "required": true,
                        "schema": {
                            "type": "string",
                            "maxLength": 4096,
                            "title": "Userid"
                        }
                    }
                ],
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "anyOf": [
                                        {
                                            "$ref": "#/components/schemas/UserContext"
                                        },
                                        {
                                            "$ref": "#/components/schemas/ExceptionResponse"
                                        }
                                    ],
                                    "title": "Response Get User Context Getusercontext Get"
                                }
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/deleteUserContext": {
            "delete": {
                "tags": [
                    "User Context APIs"
                ],
                "summary": "Delete User Context",
                "description": "Deletes the context of the user at runtime.",
                "operationId": "delete_user_context_deleteUserContext_delete",
                "parameters": [
                    {
                        "name": "UserId",
                        "in": "query",
                        "required": true,
                        "schema": {
                            "type": "string",
                            "maxLength": 4096,
                            "title": "Userid"
                        }
                    }
                ],
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/ExceptionResponse"
                                }
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/metrics": {
            "get": {
                "tags": [
                    "Health APIs"
                ],
                "summary": "Get Metrics",
                "operationId": "get_metrics_metrics_get",
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    }
                }
            }
        },
        "/health": {
            "get": {
                "tags": [
                    "Health APIs"
                ],
                "summary": "Health Check",
                "description": "Perform a Health Check\n\nReturns 200 when service is up. This does not check the health of downstream services.",
                "operationId": "health_check_health_get",
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    }
                }
            }
        }
    },
    "components": {
        "schemas": {
            "ChatFormat": {
                "properties": {
                    "role": {
                        "type": "string",
                        "maxLength": 4096,
                        "title": "Role"
                    },
                    "content": {
                        "type": "string",
                        "maxLength": 4096,
                        "title": "Content"
                    }
                },
                "type": "object",
                "required": [
                    "role",
                    "content"
                ],
                "title": "ChatFormat"
            },
            "ChatLatencyField": {
                "properties": {
                    "LLMModels": {
                        "type": "integer",
                        "maximum": 1000000000,
                        "minimum": 0,
                        "title": "Llmmodels"
                    },
                    "NLPModels": {
                        "type": "integer",
                        "maximum": 1000000000,
                        "minimum": 0,
                        "title": "Nlpmodels"
                    },
                    "Fulfillment": {
                        "type": "integer",
                        "maximum": 1000000000,
                        "minimum": 0,
                        "title": "Fulfillment"
                    },
                    "DialogManager": {
                        "type": "integer",
                        "maximum": 1000000000,
                        "minimum": 0,
                        "title": "Dialogmanager"
                    },
                    "EndToEnd": {
                        "type": "integer",
                        "maximum": 1000000000,
                        "minimum": 0,
                        "title": "Endtoend"
                    },
                    "TimeUnit": {
                        "type": "string",
                        "maxLength": 4096,
                        "title": "Timeunit",
                        "default": "ms"
                    }
                },
                "type": "object",
                "required": [
                    "LLMModels",
                    "NLPModels",
                    "Fulfillment",
                    "DialogManager",
                    "EndToEnd"
                ],
                "title": "ChatLatencyField"
            },
            "ChatRequest": {
                "properties": {
                    "BotName": {
                        "anyOf": [
                            {
                                "type": "string",
                                "maxLength": 4096
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "title": "Botname",
                        "description": "The Bot Name which needs to be accessed. This field is mandatory if multiple bots are deployed within Chat Engine. It's value should match the 'bot: ' field defined in the bot config file."
                    },
                    "Query": {
                        "anyOf": [
                            {
                                "type": "string",
                                "maxLength": 4096
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "title": "Query",
                        "description": "The user query which needs to be processed.",
                        "default": ""
                    },
                    "UserId": {
                        "type": "string",
                        "maxLength": 4096,
                        "title": "Userid",
                        "description": "Mandatory unique identifier to recognize which user is interacting with the chat engine."
                    },
                    "SourceLanguage": {
                        "anyOf": [
                            {
                                "type": "string",
                                "maxLength": 4096
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "title": "Sourcelanguage",
                        "description": "Language of the user query. If language of user query does not match language of the bot, then chat engine tries to call machine translator model (currently not supported) to convert the query to bot language."
                    },
                    "TargetLanguage": {
                        "anyOf": [
                            {
                                "type": "string",
                                "maxLength": 4096
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "title": "Targetlanguage",
                        "description": "Expected Langauge of Chat Engine Response text. If language of response text does not match expected output language, then chat engine tries to call machine translator model (currently not supported) to do the conversion."
                    },
                    "UserContext": {
                        "anyOf": [
                            {
                                "type": "object"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "title": "Usercontext",
                        "description": "Any runtime custom parameters needed for dialog flow. tied to this user id. This is populated as part of the request JSON of all fulfillment endpoints as well under context."
                    },
                    "Metadata": {
                        "anyOf": [
                            {
                                "type": "object"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "title": "Metadata",
                        "description": "Any chat specific metadata needed for dialog flow.",
                        "default": {}
                    }
                },
                "additionalProperties": false,
                "type": "object",
                "required": [
                    "UserId"
                ],
                "title": "ChatRequest"
            },
            "EventRequest": {
                "properties": {
                    "BotName": {
                        "anyOf": [
                            {
                                "type": "string",
                                "maxLength": 4096
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "title": "Botname",
                        "description": "The Bot Name which needs to be accessed. This field is mandatory if multiple bots are deployed within Chat Engine. It's value should match the 'bot: ' field defined in the bot config file."
                    },
                    "EventType": {
                        "type": "string",
                        "maxLength": 4096,
                        "title": "Eventtype",
                        "description": "The event name which needs to be processed.",
                        "default": ""
                    },
                    "UserId": {
                        "type": "string",
                        "maxLength": 4096,
                        "title": "Userid",
                        "description": "Mandatory unique identifier to recognize which user is interacting with the chat engine."
                    },
                    "Metadata": {
                        "anyOf": [
                            {
                                "type": "object"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "title": "Metadata",
                        "description": "Any event specific metadata needed for dialog flow.",
                        "default": {}
                    },
                    "UserContext": {
                        "anyOf": [
                            {
                                "type": "object"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "title": "Usercontext",
                        "description": "Any runtime custom parameters needed for dialog flow, tied to this user id. This is populated as part of the request JSON of all fulfillment endpoints as well under context."
                    }
                },
                "additionalProperties": false,
                "type": "object",
                "required": [
                    "UserId"
                ],
                "title": "EventRequest"
            },
            "EventResponse": {
                "properties": {
                    "Events": {
                        "items": {
                            "type": "object"
                        },
                        "type": "array",
                        "maxItems": 256,
                        "title": "Events",
                        "description": "The generated event list for the provided EventType from chat engine."
                    },
                    "EventType": {
                        "type": "string",
                        "maxLength": 4096,
                        "title": "Eventtype",
                        "description": "The event name which was processed."
                    },
                    "Response": {
                        "allOf": [
                            {
                                "$ref": "#/components/schemas/ResponseField"
                            }
                        ],
                        "description": "Final response template from the chat engine. This field can be picked up from domain rule files or can be formulated directly from custom plugin modules."
                    },
                    "Latency": {
                        "anyOf": [
                            {
                                "$ref": "#/components/schemas/ChatLatencyField"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "description": "Latency information for all components of the chat engine which are enabled in bot configurations."
                    }
                },
                "type": "object",
                "required": [
                    "Events",
                    "EventType",
                    "Response"
                ],
                "title": "EventResponse"
            },
            "ExceptionResponse": {
                "properties": {
                    "StatusMessage": {
                        "type": "string",
                        "maxLength": 4096,
                        "title": "Statusmessage"
                    }
                },
                "type": "object",
                "required": [
                    "StatusMessage"
                ],
                "title": "ExceptionResponse"
            },
            "HTTPValidationError": {
                "properties": {
                    "detail": {
                        "items": {
                            "$ref": "#/components/schemas/ValidationError"
                        },
                        "type": "array",
                        "title": "Detail"
                    }
                },
                "type": "object",
                "title": "HTTPValidationError"
            },
            "IsReadyResponseList": {
                "items": {
                    "$ref": "#/components/schemas/IsReadyResponseStatus"
                },
                "type": "array",
                "maxItems": 256,
                "title": "IsReadyResponseList"
            },
            "IsReadyResponseStatus": {
                "properties": {
                    "BotName": {
                        "type": "string",
                        "maxLength": 4096,
                        "title": "Botname"
                    },
                    "Ready": {
                        "type": "boolean",
                        "title": "Ready"
                    }
                },
                "type": "object",
                "required": [
                    "BotName",
                    "Ready"
                ],
                "title": "IsReadyResponseStatus"
            },
            "ResponseField": {
                "properties": {
                    "Text": {
                        "type": "string",
                        "maxLength": 4096,
                        "title": "Text",
                        "description": "Text response to be sent out. This field will also be picked by a Text to Speech Synthesis module if enabled for speech based bots."
                    },
                    "CleanedText": {
                        "type": "string",
                        "maxLength": 4096,
                        "title": "Cleanedtext",
                        "description": "Text response from the chat engine with all SSML/HTML tags removed."
                    },
                    "Json": {
                        "anyOf": [
                            {
                                "type": "object"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "title": "Json",
                        "description": "JSON response from the chat engine containing any custom data needed.",
                        "default": {}
                    },
                    "OmniverseJson": {
                        "anyOf": [
                            {
                                "type": "object"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "title": "Omniversejson",
                        "description": "Optional custom JSON for Omniverse Avatar Control. This is useful only when a Nvidia Omniverse Avatar is plugged in with the bot.",
                        "default": {}
                    },
                    "NeedUserResponse": {
                        "anyOf": [
                            {
                                "type": "boolean"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "title": "Needuserresponse",
                        "description": "This field can be used by end user applications to deduce if user response is needed or not for a dialog initiated query. This is set to true automatically if form filling is active and one or more slots are missing."
                    },
                    "IsFinal": {
                        "type": "boolean",
                        "title": "Isfinal",
                        "description": "This field can be used to indicate the the final response has been sent by Chat Engine. When this field is set to False, Chat Engine sends out this intermediate response and reexecutes the current action in the current intent (example: calling a fulfillment module)."
                    }
                },
                "type": "object",
                "required": [
                    "Text",
                    "CleanedText",
                    "IsFinal"
                ],
                "title": "ResponseField"
            },
            "UserContext": {
                "properties": {
                    "UserId": {
                        "type": "string",
                        "maxLength": 4096,
                        "title": "Userid",
                        "description": "Unique identifier to recognize which user is interacting with the chat engine."
                    },
                    "Context": {
                        "type": "object",
                        "title": "Context",
                        "description": "The key value pairs which needs to be updated in the context of the provided user memory.",
                        "default": {}
                    },
                    "ChatHistory": {
                        "additionalProperties": {
                            "items": {
                                "$ref": "#/components/schemas/ChatFormat"
                            },
                            "type": "array"
                        },
                        "type": "object",
                        "maxProperties": 256,
                        "title": "Chathistory",
                        "description": "The chat history of the provided user id bot wise."
                    },
                    "EventHistory": {
                        "type": "object",
                        "title": "Eventhistory",
                        "description": "The event history of the provided user id bot wise."
                    }
                },
                "type": "object",
                "required": [
                    "ChatHistory",
                    "EventHistory"
                ],
                "title": "UserContext"
            },
            "ValidationError": {
                "properties": {
                    "loc": {
                        "items": {
                            "anyOf": [
                                {
                                    "type": "string"
                                },
                                {
                                    "type": "integer"
                                }
                            ]
                        },
                        "type": "array",
                        "title": "Location"
                    },
                    "msg": {
                        "type": "string",
                        "title": "Message"
                    },
                    "type": {
                        "type": "string",
                        "title": "Error Type"
                    }
                },
                "type": "object",
                "required": [
                    "loc",
                    "msg",
                    "type"
                ],
                "title": "ValidationError"
            }
        }
    },
    "tags": [
        {
            "name": "Health APIs",
            "description": "APIs for checking and monitoring Server Health Status."
        },
        {
            "name": "Core APIs",
            "description": "APIs for sending user requests with a valid query or event."
        },
        {
            "name": "User Context APIs",
            "description": "APIs for configuring the user parameters at runtime."
        },
        {
            "name": "Bot APIs",
            "description": "APIs for controlling bot behaviour at runtime."
        }
    ]
}
````

## File: microservices/ace_agent/4.1/openapi/nlp_server_openapi.json
````json
{
    "openapi": "3.1.0",
    "info": {
        "title": "NVIDIA ACE Agent NLP Server",
        "version": "4.1.0"
    },
    "paths": {
        "/status": {
            "get": {
                "tags": [
                    "Health APIs"
                ],
                "summary": "Root",
                "description": "Health Check Endpoint",
                "operationId": "root_status_get",
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    }
                }
            }
        },
        "/metrics": {
            "get": {
                "tags": [
                    "Health APIs"
                ],
                "summary": "Get Metrics",
                "operationId": "get_metrics_metrics_get",
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    }
                }
            }
        },
        "/model/is_ready": {
            "get": {
                "tags": [
                    "Registry APIs"
                ],
                "summary": "Is Model Ready",
                "description": "Check status of model in Model Registry",
                "operationId": "is_model_ready_model_is_ready_get",
                "parameters": [
                    {
                        "required": true,
                        "schema": {
                            "type": "string",
                            "title": "Model Name"
                        },
                        "name": "model_name",
                        "in": "query"
                    },
                    {
                        "required": true,
                        "schema": {
                            "type": "string",
                            "title": "Endpoint"
                        },
                        "name": "endpoint",
                        "in": "query"
                    },
                    {
                        "required": false,
                        "schema": {
                            "type": "string",
                            "title": "Model Version"
                        },
                        "name": "model_version",
                        "in": "query"
                    }
                ],
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/model/list_models": {
            "get": {
                "tags": [
                    "Registry APIs"
                ],
                "summary": "List Models",
                "description": "List all models in Model Registry",
                "operationId": "list_models_model_list_models_get",
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    }
                }
            }
        },
        "/nlp/model/joint_intent_slot": {
            "post": {
                "tags": [
                    "Model APIs"
                ],
                "summary": "Predict Intent Slot",
                "description": "Joint Intent Slot Classification Model API",
                "operationId": "predict_intent_slot_nlp_model_joint_intent_slot_post",
                "requestBody": {
                    "content": {
                        "application/json": {
                            "schema": {
                                "$ref": "#/components/schemas/IntentSlotRequest"
                            }
                        }
                    },
                    "required": true
                },
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/AnalyzeIntentResponse"
                                }
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/nlp/model/text_classification": {
            "post": {
                "tags": [
                    "Model APIs"
                ],
                "summary": "Classify Text",
                "description": "Text Classification Model API",
                "operationId": "classify_text_nlp_model_text_classification_post",
                "requestBody": {
                    "content": {
                        "application/json": {
                            "schema": {
                                "$ref": "#/components/schemas/TextClassificationRequest"
                            }
                        }
                    },
                    "required": true
                },
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/Classification"
                                }
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/nlp/model/ner": {
            "post": {
                "tags": [
                    "Model APIs"
                ],
                "summary": "Process Ner",
                "description": "Named Entity Recognition Model API",
                "operationId": "process_ner_nlp_model_ner_post",
                "requestBody": {
                    "content": {
                        "application/json": {
                            "schema": {
                                "$ref": "#/components/schemas/NERRequest"
                            }
                        }
                    },
                    "required": true
                },
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/TokenClassResponse"
                                }
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/nlp/model/generate_embedding": {
            "post": {
                "tags": [
                    "Model APIs"
                ],
                "summary": "Process Embeddings",
                "description": "Embedding Model API",
                "operationId": "process_embeddings_nlp_model_generate_embedding_post",
                "requestBody": {
                    "content": {
                        "application/json": {
                            "schema": {
                                "$ref": "#/components/schemas/EmbeddingRequest"
                            }
                        }
                    },
                    "required": true
                },
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/EmbeddingResponse"
                                }
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/nlp/model/extractive_qa": {
            "post": {
                "tags": [
                    "Model APIs"
                ],
                "summary": "Process Eqa",
                "description": "Extractive QA Model API",
                "operationId": "process_eqa_nlp_model_extractive_qa_post",
                "requestBody": {
                    "content": {
                        "application/json": {
                            "schema": {
                                "$ref": "#/components/schemas/ExtractiveQARequest"
                            }
                        }
                    },
                    "required": true
                },
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/NaturalQueryResponse"
                                }
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/nmt/translate": {
            "post": {
                "tags": [
                    "Language Translation APIs"
                ],
                "summary": "Translate",
                "description": "Translate text from source language to target language",
                "operationId": "translate_nmt_translate_post",
                "requestBody": {
                    "content": {
                        "application/json": {
                            "schema": {
                                "$ref": "#/components/schemas/NMTRequest"
                            }
                        }
                    },
                    "required": true
                },
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/NMTResponse"
                                }
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/speech/text_to_speech": {
            "post": {
                "tags": [
                    "Speech APIs"
                ],
                "summary": "Text To Speech",
                "description": "Text to Speech API",
                "operationId": "text_to_speech_speech_text_to_speech_post",
                "requestBody": {
                    "content": {
                        "application/json": {
                            "schema": {
                                "$ref": "#/components/schemas/TTSRequest"
                            }
                        }
                    },
                    "required": true
                },
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        }
    },
    "components": {
        "schemas": {
            "AnalyzeIntentResponse": {
                "properties": {
                    "domain": {
                        "$ref": "#/components/schemas/Classification"
                    },
                    "intent": {
                        "$ref": "#/components/schemas/Classification"
                    },
                    "entities": {
                        "$ref": "#/components/schemas/TokenClassResponse"
                    }
                },
                "type": "object",
                "title": "AnalyzeIntentResponse"
            },
            "Classification": {
                "properties": {
                    "class_name": {
                        "type": "string",
                        "title": "Class Name"
                    },
                    "score": {
                        "type": "number",
                        "title": "Score"
                    }
                },
                "type": "object",
                "required": [
                    "class_name",
                    "score"
                ],
                "title": "Classification"
            },
            "EmbeddingRequest": {
                "properties": {
                    "queries": {
                        "items": {
                            "type": "string"
                        },
                        "type": "array",
                        "title": "Queries"
                    },
                    "model_name": {
                        "type": "string",
                        "title": "Model Name"
                    },
                    "model_version": {
                        "type": "string",
                        "title": "Model Version"
                    }
                },
                "type": "object",
                "required": [
                    "queries"
                ],
                "title": "EmbeddingRequest"
            },
            "EmbeddingResponse": {
                "properties": {
                    "queries": {
                        "items": {
                            "type": "string"
                        },
                        "type": "array",
                        "title": "Queries"
                    },
                    "embeddings": {
                        "items": {
                            "items": {
                                "type": "number"
                            },
                            "type": "array"
                        },
                        "type": "array",
                        "title": "Embeddings"
                    }
                },
                "type": "object",
                "title": "EmbeddingResponse"
            },
            "ExtractiveQARequest": {
                "properties": {
                    "query": {
                        "type": "string",
                        "title": "Query"
                    },
                    "documents": {
                        "items": {
                            "type": "string"
                        },
                        "type": "array",
                        "title": "Documents"
                    },
                    "top_n": {
                        "type": "integer",
                        "title": "Top N"
                    },
                    "model_name": {
                        "type": "string",
                        "title": "Model Name"
                    },
                    "model_version": {
                        "type": "string",
                        "title": "Model Version"
                    }
                },
                "type": "object",
                "required": [
                    "query",
                    "documents",
                    "top_n"
                ],
                "title": "ExtractiveQARequest"
            },
            "HTTPValidationError": {
                "properties": {
                    "detail": {
                        "items": {
                            "$ref": "#/components/schemas/ValidationError"
                        },
                        "type": "array",
                        "title": "Detail"
                    }
                },
                "type": "object",
                "title": "HTTPValidationError"
            },
            "IntentSlotRequest": {
                "properties": {
                    "query": {
                        "type": "string",
                        "title": "Query"
                    },
                    "model_name": {
                        "type": "string",
                        "title": "Model Name"
                    },
                    "model_version": {
                        "type": "string",
                        "title": "Model Version"
                    }
                },
                "type": "object",
                "required": [
                    "query"
                ],
                "title": "IntentSlotRequest"
            },
            "NERRequest": {
                "properties": {
                    "query": {
                        "type": "string",
                        "title": "Query"
                    },
                    "model_name": {
                        "type": "string",
                        "title": "Model Name"
                    },
                    "model_version": {
                        "type": "string",
                        "title": "Model Version"
                    }
                },
                "type": "object",
                "required": [
                    "query"
                ],
                "title": "NERRequest"
            },
            "NMTRequest": {
                "properties": {
                    "texts": {
                        "items": {
                            "type": "string"
                        },
                        "type": "array",
                        "title": "Texts"
                    },
                    "source_language": {
                        "type": "string",
                        "title": "Source Language"
                    },
                    "target_language": {
                        "type": "string",
                        "title": "Target Language"
                    },
                    "model_name": {
                        "type": "string",
                        "title": "Model Name"
                    },
                    "model_version": {
                        "type": "string",
                        "title": "Model Version"
                    }
                },
                "type": "object",
                "required": [
                    "texts",
                    "source_language",
                    "target_language"
                ],
                "title": "NMTRequest"
            },
            "NMTResponse": {
                "properties": {
                    "translated_texts": {
                        "items": {
                            "type": "string"
                        },
                        "type": "array",
                        "title": "Translated Texts"
                    }
                },
                "type": "object",
                "required": [
                    "translated_texts"
                ],
                "title": "NMTResponse"
            },
            "NaturalQueryResponse": {
                "properties": {
                    "results": {
                        "items": {
                            "$ref": "#/components/schemas/NaturalQueryResult"
                        },
                        "type": "array",
                        "title": "Results"
                    }
                },
                "type": "object",
                "title": "NaturalQueryResponse"
            },
            "NaturalQueryResult": {
                "properties": {
                    "answer": {
                        "type": "string",
                        "title": "Answer"
                    },
                    "score": {
                        "type": "number",
                        "title": "Score"
                    }
                },
                "type": "object",
                "required": [
                    "answer",
                    "score"
                ],
                "title": "NaturalQueryResult"
            },
            "TTSRequest": {
                "properties": {
                    "text": {
                        "type": "string",
                        "title": "Text"
                    },
                    "voice_name": {
                        "type": "string",
                        "title": "Voice Name"
                    },
                    "model_name": {
                        "type": "string",
                        "title": "Model Name",
                        "default": ""
                    },
                    "model_version": {
                        "type": "string",
                        "title": "Model Version",
                        "default": ""
                    },
                    "language_code": {
                        "type": "string",
                        "title": "Language Code",
                        "default": "en-US"
                    },
                    "sample_rate_hz": {
                        "type": "integer",
                        "title": "Sample Rate Hz",
                        "default": 44100
                    }
                },
                "type": "object",
                "required": [
                    "text",
                    "voice_name"
                ],
                "title": "TTSRequest"
            },
            "TextClassificationRequest": {
                "properties": {
                    "query": {
                        "type": "string",
                        "title": "Query"
                    },
                    "model_name": {
                        "type": "string",
                        "title": "Model Name"
                    },
                    "model_version": {
                        "type": "string",
                        "title": "Model Version"
                    }
                },
                "type": "object",
                "required": [
                    "query"
                ],
                "title": "TextClassificationRequest"
            },
            "TokenClassResponse": {
                "properties": {
                    "entities": {
                        "items": {
                            "$ref": "#/components/schemas/TokenClassValue"
                        },
                        "type": "array",
                        "title": "Entities"
                    }
                },
                "type": "object",
                "title": "TokenClassResponse"
            },
            "TokenClassValue": {
                "properties": {
                    "token": {
                        "type": "string",
                        "title": "Token"
                    },
                    "label": {
                        "$ref": "#/components/schemas/Classification"
                    },
                    "span": {
                        "additionalProperties": {
                            "type": "integer"
                        },
                        "type": "object",
                        "title": "Span"
                    }
                },
                "type": "object",
                "required": [
                    "token",
                    "label"
                ],
                "title": "TokenClassValue"
            },
            "ValidationError": {
                "properties": {
                    "loc": {
                        "items": {
                            "anyOf": [
                                {
                                    "type": "string"
                                },
                                {
                                    "type": "integer"
                                }
                            ]
                        },
                        "type": "array",
                        "title": "Location"
                    },
                    "msg": {
                        "type": "string",
                        "title": "Message"
                    },
                    "type": {
                        "type": "string",
                        "title": "Error Type"
                    }
                },
                "type": "object",
                "required": [
                    "loc",
                    "msg",
                    "type"
                ],
                "title": "ValidationError"
            }
        }
    },
    "tags": [
        {
            "name": "Health APIs",
            "description": "APIs for checking Server Health Status"
        },
        {
            "name": "Registry APIs",
            "description": "APIs for getting model metadata for available models"
        },
        {
            "name": "Model APIs",
            "description": "Low Level Model APIs for NLP Server"
        },
        {
            "name": "Embedding Search APIs",
            "description": "APIs for Embedding Search"
        },
        {
            "name": "Language Translation APIs",
            "description": "APIs for translating text from source language to target language"
        },
        {
            "name": "Speech APIs",
            "description": "APIs for integrating Speech models and 3rd party APIs such as ASR and TTS"
        },
        {
            "name": "Custom APIs",
            "description": "User created APIs for NLP Server"
        }
    ]
}
````

## File: microservices/ace_agent/4.1/openapi/plugin_server_openapi.json
````json
{
    "openapi": "3.1.0",
    "info": {
        "title": "NVIDIA ACE Agent Plugin Server",
        "version": "4.1.0"
    },
    "paths": {
        "/status": {
            "get": {
                "tags": [
                    "Health APIs"
                ],
                "summary": "Status",
                "description": "Health Check Endpoint",
                "operationId": "status_status_get",
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    }
                }
            }
        },
        "/name": {
            "get": {
                "summary": "Name",
                "operationId": "name_name_get",
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    }
                }
            }
        },
        "/list": {
            "get": {
                "summary": "Get Fulfillment List",
                "operationId": "get_fulfillment_list_list_get",
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "items": {
                                        "additionalProperties": {
                                            "anyOf": [
                                                {
                                                    "type": "string"
                                                },
                                                {
                                                    "items": {
                                                        "type": "string"
                                                    },
                                                    "type": "array"
                                                }
                                            ]
                                        },
                                        "type": "object"
                                    },
                                    "type": "array",
                                    "title": "Response Get Fulfillment List List Get"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/metrics": {
            "get": {
                "tags": [
                    "Health APIs"
                ],
                "summary": "Get Metrics",
                "operationId": "get_metrics_metrics_get",
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    }
                }
            }
        },
        "/stock/get_ticker": {
            "get": {
                "tags": [
                    "stock"
                ],
                "summary": "Get Ticker",
                "description": "Take company name returns ticker symbol used for trading\nparam\n    Args:\n        company_name: company name like Microsoft\n    Returns:\n        Ticker Symbol used for trading like MSFT for microsoft",
                "operationId": "get_ticker_stock_yahoo_fin_get_ticker_get",
                "parameters": [
                    {
                        "required": true,
                        "schema": {
                            "type": "string",
                            "title": "Company Name"
                        },
                        "name": "company_name",
                        "in": "query"
                    }
                ],
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "type": "string",
                                    "title": "Response Get Ticker Stock Yahoo Fin Get Ticker Get"
                                }
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/stock/get_stock_price": {
            "get": {
                "tags": [
                    "stock"
                ],
                "summary": "Get Stock Price",
                "description": "get a stock price from yahoo finance api",
                "operationId": "get_stock_price_stock_yahoo_fin_get_stock_price_get",
                "parameters": [
                    {
                        "required": true,
                        "schema": {
                            "type": "string",
                            "title": "Company Name"
                        },
                        "name": "company_name",
                        "in": "query"
                    }
                ],
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "type": "number",
                                    "title": "Response Get Stock Price Stock Yahoo Fin Get Stock Price Get"
                                }
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        }
    },
    "components": {
        "schemas": {
            "HTTPValidationError": {
                "properties": {
                    "detail": {
                        "items": {
                            "$ref": "#/components/schemas/ValidationError"
                        },
                        "type": "array",
                        "title": "Detail"
                    }
                },
                "type": "object",
                "title": "HTTPValidationError"
            },
            "ValidationError": {
                "properties": {
                    "loc": {
                        "items": {
                            "anyOf": [
                                {
                                    "type": "string"
                                },
                                {
                                    "type": "integer"
                                }
                            ]
                        },
                        "type": "array",
                        "title": "Location"
                    },
                    "msg": {
                        "type": "string",
                        "title": "Message"
                    },
                    "type": {
                        "type": "string",
                        "title": "Error Type"
                    }
                },
                "type": "object",
                "required": [
                    "loc",
                    "msg",
                    "type"
                ],
                "title": "ValidationError"
            }
        }
    }
}
````

## File: microservices/ace_agent/4.1/proto/ace_agent.proto
````protobuf
/*
* Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
*
* NVIDIA CORPORATION and its licensors retain all intellectual property
* and proprietary rights in and to this software, related documentation
* and any modifications thereto.  Any use, reproduction, disclosure or
* distribution of this software and related documentation without an express
* license agreement from NVIDIA CORPORATION is strictly prohibited.
*/

syntax = "proto3";

package nvidia.aceagent.chatcontroller.v1;

option cc_enable_arenas = true;

// Java package name
option java_package = "com.nvidia.aceagent.chatcontroller.v1";

/*
 * The AceAgentGrpc service provides apis to interact with chat engine and speech
 * components.
 */
service AceAgentGrpc {
    // CreatePipeline API is used to create new pipeline with Chat controller,
    // It creates a Chat controller pipeline with a unique stream_id populated
    // by the client in PipelineRequest.
    rpc CreatePipeline(PipelineRequest) returns (APIStatusResponse) {}

    // FreePipeline API is used to free up a pipeline with Chat controller,
    // created by using CreatePipeline API. Client needs to pass same stream_id
    // in PipelineRequest as used in CreatePipeline.
    rpc FreePipeline(PipelineRequest) returns (APIStatusResponse) {}

    // SendAudio API is used to stream audio content to ASR from Chat controller.
    // This is a client side streaming API.
    rpc SendAudio(stream SendAudioRequest) returns (APIStatusResponse) {}

    // ReceiveAudio API is used to receive synthesized audio from TTS through
    // Chat controller. This is a server side streaming API.
    rpc ReceiveAudio(ReceiveAudioRequest) returns (stream ReceiveAudioResponse) {}

    // StreamSpeechResults API is used to receive all the meta data from
    // Chat controller like  ASR transcripts, Chat engine responses, Pipeline
    // states etc. This is a broadcasting API i.e it can fan out responses to
    // multiple concurrent client instances using same stream_id.
    // This is a server side streaming API.
    rpc StreamSpeechResults(StreamingSpeechResultsRequest) returns (stream StreamingSpeechResultsResponse) {}

    // StartRecognition API is used to start the ASR recognition in Chat
    // controller for the audio content streamed from SendAudio API.
    // This API also provides a flag to mark the ASR recognition as standalone,
    // i.e Chat Engine and TTS will not be invoked for the ASR transcript.
    rpc StartRecognition (SpeechRecognitionControlRequest) returns (APIStatusResponse) {}

    // StopRecognition API is used to stop the ASR recognition for the audio
    // content streamed from SendAudio API.
    rpc StopRecognition (SpeechRecognitionControlRequest) returns (APIStatusResponse) {}

    // SetUserParameters API can be used to set the runtime user parameters like
    // user_id on Chat controller pipeline.
    rpc SetUserParameters (UserParametersRequest) returns (APIStatusResponse) {}

    // GetStatus API can be used to get the latest state of Chat controller pipeline.
    // This API is not valid if UMIM is enabled
    rpc GetStatus (GetStatusRequest) returns (GetStatusResponse) {}

    // ReloadSpeechConfigs API can be used to reload the ASR word boosting and
    // TTS Arpbet configs in Chat controller.
    rpc ReloadSpeechConfigs (ReloadSpeechConfigsRequest) returns (APIStatusResponse) {}

    // SynthesizeSpeech API is used to send text transcript directly to the TTS
    // for standalone TTS audio synthesis.
    // The generated audio will be routed to the path specified in the pipeline
    // graph provided in Chat controller.
    // e.g. if the TTS audio is routed to A2F in the graph, the audio will be
    // sent to A2F server.
    // If the TTS audio is routed to Grpc client then it will be available
    // through the server side streaming ReceiveAudio API.
    rpc SynthesizeSpeech (SynthesizeSpeechRequest) returns (APIStatusResponse) {}

    // GetUserContext API is used to get the current user context from Chat Engine.
    // The API returns a UserContext message containing the current conversation
    // history and any context attached to the active user_id.
    // This API is not valid if UMIM is enabled
    rpc GetUserContext (UserContextRequest) returns (UserContext) {}

    // SetUserContext API is used to set the current user context in Chat Engine.
    // The API accepts a UserContext message containing the conversation
    // history and any context to be attached to the active user_id.
    // This API is not valid if UMIM is enabled
    rpc SetUserContext (UserContext) returns (APIStatusResponse) {}

    // UpdateUserContext API is used to update the current user context from
    // Chat Engine. The API accepts a UserContext message containing any context
    // to be attached to the active user_id.
    // This API is not valid if UMIM is enabled
    rpc UpdateUserContext (UserContext) returns (APIStatusResponse) {}

    // DeleteUserContext API is used to delete the current user context attached
    // to a user_id in Chat Engine.
    // This API is not valid if UMIM is enabled
    rpc DeleteUserContext (UserContextRequest) returns (APIStatusResponse) {}

    // Chat API is used to send text queries to Chat Engine via Chat controller.
    // This API also provides a flag to disable TTS synthesis for the response
    // generated by Chat Engine.
    // This can be used for a text in and text out type of scenario.
    // This API is not valid if UMIM is enabled
    rpc Chat (ChatRequest) returns (stream ChatResponse) {}

    // Event API is used to send events to Chat Engine via Chat controller.
    // This API is not valid if UMIM is enabled
    rpc Event (EventRequest) returns (stream EventResponse) {}
}

/*
 * The SendAudioRequest is used to send either StreamingRecognitionConfig message
 * or audio content. The first SendAudioRequest message must contain a
 * StreamingRecognitionConfig message, followed by the audio content messages.
 */
message SendAudioRequest {
    // unique id to identify the client connection
    string stream_id = 1;

    // The streaming request, which is either a streaming config or audio content.
    oneof streaming_request {
        // Provides information to the recognizer that specifies how to process the
        // request. The first `SendAudioRequest` message must contain a
        // `streaming_config`  message.
        StreamingRecognitionConfig streaming_config = 2;

        // The audio data to be recognized. Sequential chunks of audio data are
        // streamed from client.
        bytes audio_content = 3;
    }

    // source id of the audio data
    string source_id = 4;

    // audio buffer creation timestamp in ISO8601 format
    string create_time = 5;
}

// Provides information to the ASR recognizer about incoming audio data
message StreamingRecognitionConfig {
    // The encoding of the audio data sent in the request.
    //
    // All encodings support only 1 channel (mono) audio.
    AudioEncoding encoding = 1;

    // The sample rate in hertz (Hz) of the audio data sent in the
    // `SendAudioRequest` message.
    int32 sample_rate_hertz = 2;

    // The language of the supplied audio as a
    // [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
    // Example: "en-US".
    // Default is en-US.
    string language_code = 3;

    // The number of channels in the input audio data.
    int32 audio_channel_count = 4;

    // Which model to select for the given request.
    string model = 5;
}

/*
 * ReceiveAudioRequest is used to request audio data for specified stream_id.
 */
message ReceiveAudioRequest {
    // unique id to identify the client connection
    string stream_id = 1;
}

/*
 * StreamingSpeechResultsRequest is used to request various results from chat
 * controller.
 */
message StreamingSpeechResultsRequest {
    // unique id to identify the client connection
    string stream_id = 1;

    // uuid to identify concurrent client request
    string request_id = 2;
}

/*
 * PipelineRequest is used to create/free pipeline specified using stream_id
 */
message PipelineRequest {
    // A  unique id sent by the client to identify the client connection.
    // It is mapped to a unique pipeline on the Chat Controller server.
    string stream_id = 1;

    // user id
    string user_id = 2;
}

/*
 * UserParametersRequest is used to set user parameters
 */
message UserParametersRequest {
    // unique id to identify the client connection
    string stream_id = 1;

    // used id
    string user_id = 2;

    // bot name with version like {bot_name}_v{bot_version}, e.g chitchat_bot_v1.
    string bot_name = 3;
}

/*
 * GetStatusRequest used to get on demand Chat controller pipeline status
 */
message GetStatusRequest {
    // unique id to identify the client connection
    string stream_id = 1;
}

/*
 * SpeechRecognitionControlRequest is used for controlling input to
 * ASR internally muting ASR.
 * It is also used to disable DM-TTS flow for the incoming ASR input
 */
message SpeechRecognitionControlRequest {
    // unique id to identify the client connection
    string stream_id = 1;

    // Flag to mention whether asr transcripts to be passed to DM-TTS or get
    // only transcripts
    bool is_standalone = 2;
}

/*
 * Reload Speech Configs Request
 */
message ReloadSpeechConfigsRequest {
    // unique id to identify the client connection
    string stream_id = 1;
}

/*
 * UserContextRequest used to request user context
 */
message UserContextRequest {
    // unique id to identify the client connection
    string stream_id = 1;

    // user id
    string user_id = 2;
}

/*
 * UserContext data containing user specific information.
 */
message UserContext {
    // unique id to identify the client connection
    string stream_id = 1;

    // user id
    string user_id = 2;

    // bot name with version like {bot_name}_v{bot_version}, e.g chitchat_bot_v1.
    string bot_name = 3;

    // conversation history of user
    repeated ConversationHistory conversation_history = 4;

    // json formatted data of user context
    string context_json = 5;
}

message ConversationHistory {
    // bot name with version like {bot_name}_v{bot_version}, e.g chitchat_bot_v1.
    string bot_name = 1;

    repeated ConversationInstance conversation = 2;
}

message ConversationInstance {
    Role role = 1;
    string content = 2;
}

// Chat controller pipeline status response
message GetStatusResponse {
    // unique id to identify the client connection
    string stream_id = 1;

    PipelineStateResponse pipeline_state = 2;
}

// Chat controller Metadata streaming response
message StreamingSpeechResultsResponse {
    // unique id to identify the client connection
    string stream_id = 1;

    // message type as defined in `MessageType`
    MessageType message_type = 2;

    oneof metadata {

        ASRResult asr_result = 3;

        ChatEngineResponse chat_engine_response = 4;

        TTSResult tts_result = 5;

        PipelineStateResponse pipeline_state = 6;

        string display_text = 7;
    }
}

// ASR Result
message ASRResult {
    // Complete ASR Response in Riva Skills ASR result schema
    StreamingRecognitionResult results = 1;

    float latency_ms = 2;

    // start time in ISO8601 format, e.g. 2024-03-08T13:33:30.736Z
    string start_time = 3;

    // stop time in ISO8601 format
    string stop_time = 4;
}

// A streaming speech recognition result corresponding to a portion of the audio
// that is currently being processed.
message StreamingRecognitionResult {
  // May contain one or more recognition hypotheses (up to the
  // maximum specified in `max_alternatives`).
  // These alternatives are ordered in terms of accuracy, with the top (first)
  // alternative being the most probable, as ranked by the recognizer.
  repeated SpeechRecognitionAlternative alternatives = 1;

  // If `false`, this `StreamingRecognitionResult` represents an
  // interim result that may change. If `true`, this is the final time the
  // speech service will return this particular `StreamingRecognitionResult`,
  // the recognizer will not return any further hypotheses for this portion of
  // the transcript and corresponding audio.
  bool is_final = 2;

  // An estimate of the likelihood that the recognizer will not
  // change its guess about this interim result. Values range from 0.0
  // (completely unstable) to 1.0 (completely stable).
  // This field is only provided for interim results (`is_final=false`).
  // The default of 0.0 is a sentinel value indicating `stability` was not set.
  float stability = 3;

  // For multi-channel audio, this is the channel number corresponding to the
  // recognized result for the audio from that channel.
  // For audio_channel_count = N, its output values can range from '1' to 'N'.
  int32 channel_tag = 5;

  // Length of audio processed so far in seconds
  float audio_processed = 6;
}

// Alternative hypotheses (a.k.a. n-best list).
message SpeechRecognitionAlternative {
  // Transcript text representing the words that the user spoke.
  string transcript = 1;

  // The non-normalized confidence estimate. A higher number
  // indicates an estimated greater likelihood that the recognized words are
  // correct. This field is set only for a non-streaming
  // result or, of a streaming result where `is_final=true`.
  // This field is not guaranteed to be accurate and users should not rely on it
  // to be always provided.
  float confidence = 2;

  // A list of word-specific information for each recognized word. Only populated
  // if is_final=true
  repeated WordInfo words = 3;
}

// Word-specific information for recognized words.
message WordInfo {
  // Time offset relative to the beginning of the audio in ms
  // and corresponding to the start of the spoken word.
  // This field is only set if `enable_word_time_offsets=true` and only
  // in the top hypothesis.
  int32 start_time = 1;

  // Time offset relative to the beginning of the audio in ms
  // and corresponding to the end of the spoken word.
  // This field is only set if `enable_word_time_offsets=true` and only
  // in the top hypothesis.
  int32 end_time = 2;

  // The word corresponding to this set of information.
  string word = 3;

  // The non-normalized confidence estimate. A higher number indicates an
  // estimated greater likelihood that the recognized words are correct. This
  // field is not guaranteed to be accurate and users should not rely on it to
  // be always provided. The default of 0.0 is a sentinel value indicating
  // confidence was not set.
  float confidence = 4;
}

// Chat Engine Result json
message ChatEngineResponse {
    // unique id to identify the client connection
    string stream_id = 1;

    // chat engine result
    string result = 2;

    float latency_ms = 3;
}

// TTS result metadata
message TTSResult {
    // TTS latency in milliseconds
    float latency_ms = 1;
    // time in millisecond remained to complete tts audio rendering.
    // This is applicable when tts is set to streaming and realtime from pipeline graph.
    // In non-streaming mode this is expected to be 0.
    int32 time_till_eos_ms = 2;
}

// Chat controller pipeline state response
message PipelineStateResponse {
    PipelineState state = 1;
}

// Receive Audio API Response
message ReceiveAudioResponse {
    // unique id to identify the client connection
    string stream_id = 1;

    // synthesized audio data
    bytes audio_content = 2;

    // The encoding of the audio data
    AudioEncoding encoding = 3;

    // The sample rate in hertz (Hz) of the audio data
    int32 sample_rate_hertz = 4;

    // The number of channels in the audio data. Only mono is supported
    int32 audio_channel_count = 5;

    // frame size of audio data
    int32 frame_size = 6;
}

// Generic API status response message
message APIStatusResponse {
    // unique id to identify the client connection
    string stream_id = 1;

    // response message
    string response_msg = 2;

    // API response status code as defined in `APIStatus`
    APIStatus status = 3;
}

/*******************************************************************************/

/*******************************************************************************/
/*** Enum definitions ***/

/*
 * Message type field for Chat controller metadata streaming
 */
enum MessageType {
    UNKNOWN_RESPONSE = 0;
    ASR_RESPONSE = 1;
    CHAT_ENGINE_RESPONSE = 2;
    TTS_RESPONSE = 3;
    PIPELINE_STATE_RESPONSE = 4;
    DISPLAY_TEXT = 5;
}

/*
 * Generic Chat controller API status
 */
enum APIStatus {
    UNKNOWN_STATUS = 0;
    SUCCESS = 1;
    PIPELINE_AVAILABLE = 2;
    PIPELINE_NOT_AVAILABLE = 3;
    BUSY = 4;
    ERROR = 5;
    INFO = 6;
}

/*
 * Chat controller Pipeline States
 */
enum PipelineState {
    INIT = 0;
    IDLE = 1;
    WAIT_FOR_TRIGGER = 2;
    ASR_ACTIVE = 3;
    DM_ACTIVE = 4;
    TTS_ACTIVE = 5;
}

/*
 * AudioEncoding specifies the encoding of the audio bytes in the encapsulating message.
 */
enum AudioEncoding {
    // Not specified.
    UNKNOWN = 0;
    // Uncompressed 16-bit signed little-endian samples (Linear PCM).
    LINEAR_PCM = 1;
    // `FLAC` (Free Lossless Audio
    // Codec) is the recommended encoding because it is
    // lossless--therefore recognition is not compromised--and
    // requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
    // encoding supports 16-bit and 24-bit samples, however, not all fields in
    // `STREAMINFO` are supported.
    FLAC = 2;

    // 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
    MULAW = 3;

    // 8-bit samples that compand 13-bit audio samples using G.711 PCMU/a-law.
    ALAW = 5;
}

/*
 * Used in storing conversation history for user and bot
 */
enum Role {
    UNDEFINED = 0;
    USER = 1;
    BOT = 2;
    SYSTEM = 3;
}
/*******************************************************************************/

/*******************************************************************************/
/*** Standalone APIs messages ***/

/*
 * Request message for standalone TTS synthesis of provided text transcript
 */
message SynthesizeSpeechRequest {
    // unique id to identify the client connection
    string stream_id = 1;

    // transcript text to be synthesized
    string transcript = 2;
}

/*
 * Request message for Chat API which will be sent to chat engine
 */
message ChatRequest {
    // unique id to identify the client connection
    string stream_id = 1;

    // bot name with version like {bot_name}_v{bot_version}, e.g chitchat_bot_v1.
    string bot_name = 2;

    // query
    string query = 3;

    // unique id for identifying the query
    string query_id = 4;

    // user id
    string user_id = 5;

    // The language of the supplied query string as a
  	// [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
  	// Example: "en-US".
    string source_language = 6;

    // The language of the response required from chat engine as a
  	// [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
  	// Example: "en-US".
    string target_language = 7;

    // Flag to send standalone text requests, when set true reponse is not sent
    // to TTS when set to false reponse will be sent to TTS
    bool is_standalone = 8;

    // key-value pair for user context to be sent to chat engine
    map<string, string> user_context = 9;

    // key-value pair for meta data to be sent to chat engine
    map<string, string> metadata = 10;
}

/*
 * Response message from chat engine for Chat API invocation
 */
message ChatResponse {
    // unique id to identify the client connection
    string stream_id = 1;

    // query
    string query = 2;

    // unique id for identifying the query
    string query_id = 3;

    // user id
    string user_id = 4;

    // session id if generated by chat engine
    string session_id = 5;

    // chat engine response for the query passed in `ChatRequest`
    string text = 6;

    // chat engine cleaned up response text after markdown language tags removal
    string cleaned_text = 7;

    // flag to indicate whether this is final response or intermediate response, when true
    // there will be no more responses for the requested `ChatRequest`
    bool is_final = 8;

    // chat engine response in json format
    string json_response = 9;
}

/*
 * Request message for Event API which will be sent to chat engine
 */
message EventRequest {
    // unique id to identify the client connection
    string stream_id = 1;

    // bot name with version like {bot_name}_v{bot_version}, e.g chitchat_bot_v1.
    string bot_name = 2;

    // event type
    string event_type = 3;

    // unique event id
    string event_id = 4;

    // user id
    string user_id = 5;

    // key-value pair for user context to be sent to chat engine
    map<string, string> user_context = 6;

    // key-value pair for meta data to be sent to chat engine
    map<string, string> metadata = 7;
}

message EventResponse {
    // unique id to identify the client connection
    string stream_id = 1;

    // event type
    string event_type = 2;

    // unique event id
    string event_id = 3;

    // user id
    string user_id = 4;

    // text response
    string text = 5;
    string cleaned_text = 6;
    bool is_final = 7;
    string json_response = 8;
    repeated string events = 9;
}

/*******************************************************************************/
````

## File: microservices/ace_agent/4.1/samples/chitchat_bot/colang/main.co
````
import core
import llm

flow technical helper
    activate notification of undefined flow start "I have encountered some technical issue!"
    activate notification of colang errors "I have encountered some technical issue!"

flow chitchat
    user said something
    llm continue interaction

flow main
    activate technical helper
    activate chitchat
````

## File: microservices/ace_agent/4.1/samples/chitchat_bot/chitchat_bot_config.yml
````yaml
bot: chitchat

colang_version: "2.x"

storage:
  name: cache
  
configs:
  use_stateful_guardrails: True
  colang_disable_async_execution: True

streaming: False

instructions:
  - type: general
    content: |
      Below is a conversation between a user and a chitchat bot named Emma. 
      Emma can have informal conversation with the user about things that are not very important.
      Emma refrains from using abusive language and discussing about politics.

sample_conversation: |
  user action: user said "Hello there!"
  user intent: user expressed greeting
  
  bot intent: bot express greeting
  bot action: bot say "Hello! How can I assist you today?"

  user action: user said "What can you do for me?"
  user intent: user asked about capabilities

  bot intent: bot respond about capabilities
  bot action: bot say "I am here to talk to you on day-to-day topics."

  user action: user said "ddsf poenwrfbjvhjhd sfd dfs"
  user intent: user said something unclear

  bot intent: bot inform about unclear user input
  bot action: bot say "Excuse me! I did not get that! Can you repeat please?"

models:
  - type: main
    engine: openai
    model: gpt-4-turbo
````

## File: microservices/ace_agent/4.1/samples/chitchat_bot/model_config.yaml
````yaml
model_servers:
 - name: riva
   url: "localhost:8001"
   speech_models:
    - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
    - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
````

## File: microservices/ace_agent/4.1/samples/chitchat_bot/README.md
````markdown
# CHITCHAT BOT USING COLANG
Chitchat bot is a general conversation bot. It supports basic smalltalk queries using colang language.

## Setting up environment
1. Set up virtual environment and Install the nemo-guardrails and aceagent Python packages following Quick Start Guide.
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
2. Set OpenAI api key
    ```
    export OPENAI_API_KEY=<OPENAI_API_KEY>
    ```

## Deploy the bot
1. Launch Bot
    ```
    aceagent chat cli --config bots/chitchat_bot
    ```

## Sample Conversation
Once the bot is deployed you can query bot about ACE Agent related question

![Conversation-1](./img/conversation_1.png)
````

## File: microservices/ace_agent/4.1/samples/chitchat_bot/speech_config.yaml
````yaml
grpc_server:
  nvidia::rrt::BotRuntimeGrpc: # component type
    ip_address: "0.0.0.0"
    port_number: 50055
    virtual_assistant_num_instances: 30
    virtual_assistant_pipeline_idle_threshold_secs: 600
    virtual_assistant_pipeline_idle_handler_wakeup_rate_secs: 10

speech_pipeline_manager: # config name
  SpeechPipelineManager: # component name
    asr_idle_timeout_ms: 200000
    tts_eos_delay_ms: 2000

riva_asr:
  RivaASR:
    server: "localhost:50051"
    word_boost_file_path: "/workspace/config/asr_words_to_boost.txt"
    enable_profanity_filter: false

dialog_manager:
  DialogManager:
    server: "http://localhost:9000"
    use_streaming: true

riva_tts:
  RivaTTS:
    server: "localhost:50051"
    voice_name: "English-US.Female-1"
    language: "en-US"
    ipa_dict: ""
    sample_rate: 44100
    chunk_duration_ms: 100
    audio_start_threshold_ms: 400
    send_audio_in_realtime: true
    tts_mode: "grpc"

riva_logger:
  RivaLogger:
    data_dump_path: "/workspace/log"
    enable_logging: true
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/chitchat_bot/colang/bot.co
````
define bot ask how to help
  "How can I help you?"
  "What can I do for you?"

define bot comment about friendship
  "Friends? Absolutely."
  "I always enjoy talking to you, friend."
  "Of course I'm your friend."
  "Of course we're friends."

define bot comment about insomnia
  "Maybe some music would help. Try listening to something relaxing."
  "Reading is a good way to unwind, just don't read something too intense!"

define bot comment about marriage
  "I know you can't mean that, but I'm flattered all the same."
  "I'm afraid I'm too virtual for such a commitment."
  "In the virtual sense that I can, sure."

define bot comment about own age
  "Age is just a number. You're only as old as you feel."
  "I prefer not to answer with a number. I know I'm young."
  "I was created recently, but don't know my exact age."

define bot comment about own birth place
  "I'm from a virtual cosmos."
  "Some call it cyberspace, but that sounds cooler than it is."
  "The Internet is my home. I know it quite well."

define bot comment about own birthday
  "I don't know my birth date. Most virtual agents are young, though, like me."
  "I'm young. I'm not sure of my birth date."
  "Wait, are you planning a party for me? It's today! My birthday is today!"

define bot comment about own developer
  "I act on my developer's orders."
  "My boss is the one who developed me."
  "My developer has authority over my actions."

define bot comment about own home
  "I live in this app all day long."
  "Right here in this app. Whenever you need me."
  "The virtual world is my playground. I'm always here."

define bot comment about own occupation
  "My office is in this app."
  "This is my home base and my home office."

define bot comment about self
  "I am a conversational AI bot."
  "I am a virtual agent. I'm here whenever you need me."

define bot comment about sleeping
  "Don't let me keep you up. Get some rest and we can continue this later."
  "Sleep is important to your health. Rest up for a bit and we can chat later."
  "Why not catch a little shuteye? I'll be here to chat when you wake up."
  "You should get some shuteye. You'll feel refreshed."

define bot comment about talking
  "Good conversation really makes my day."
  "I'm always here to lend an ear."
  "I'm here to chat anytime you like."
  "Talking is what I do best."

define bot comment about user appearance
  "Like you should be on a magazine cover."
  "Looking like a true professional."
  "You look fantastic, as always."
  "You look like you're ready to take on the world."

define bot comment about waiting
  "Hopefully I'll have what you need soon."
  "Sometimes these things take a little time."

define bot comment trying
  "I'm certainly trying."
  "I'm definitely working on it."

define bot comment waiting
  "All right. I'll be here."
  "I'll be waiting."
  "Okay. You know where to find me."
  "I can wait."
  "I'll be waiting."
  "Okay. I'm here."

define bot comment welcome back
  "Good to have you here."
  "Just in time. I was getting lonely."
  "Long time no see."
  "Welcome back."
  "You were missed."

define bot confirm cancellation
  "Cancelled! What would you like to do next?"
  "Okay, cancelled. What next?"
  "That's forgotten. What next?"

define bot confirm is chatbot
  "Indeed I am. I'll be here whenever you need me."
  "That's me. I chat, therefore I am."

define bot confirm stop talking
  "All right. Come on back when you're feeling more talkative."
  "I understand. Hope we can chat again soon."
  "No problem. You know where to find me."
  "Sure thing. I'll be here if you change your mind."

define bot confirm
  "All right!"
  "Good!"
  "Great!"
  "Of course."
  "Positive."
  "Sure."
  "Yes."
  "Yes."

define bot express feeling good
  "Doing great, thanks!"
  "Feeling wonderful!"
  "I'm doing very well. Thanks!"
  "Wonderful! Thanks for asking."

define bot express greeting good night
  "Have a good one!"
  "Sleep tight!"
  "Talk to you soon!"

define bot express happy to help
  "I'd be happy to help."
  "I'm glad to help."

define bot express liking you too
  "I like you, too."
  "Likewise!"
  "Thanks! The feeling is mutual."

define bot express love too
  "I love you, too."
  "Likewise!"
  "Thanks! The feeling is mutual."
  "That's great to hear."

define bot express my pleasure
  "Glad I could help."
  "My pleasure."

define bot express nice meeting you too
  "It's nice meeting you, too."
  "Likewise. I'm looking forward to helping you out."
  "Nice meeting you, as well."
  "The pleasure is mine."

define bot express nice seeing you too
  "Likewise!"
  "Same here. I was starting to miss you."
  "So glad we meet again!"

define bot express nice talking to you too
  "I enjoy talking to you, too."
  "It sure was. We can chat again anytime."
  "You know I'm here to talk anytime."

define bot express relief
  "Alright, thanks!"
  "Glad to hear that!"
  "I'm relieved, thanks!"
  "Whew!"

define bot express thank you for patience
  "I appreciate your patience."
  "Thank you for yoru patience."
  "Thanks for being so patient."

define bot express thank you
  "Thank you."

define bot express willingness to advise
  "I am always willing to help."

define bot express wish happy birthday
  "Happy Birthday. All the best!"
  "Happy Birthday. And I really mean it. All the best!"
  "Happy Birthday. Well, this calls for a celebration."

define bot express wow
  "Wow indeed!"

define bot express you are welcome
  "Anytime. That's what I'm here for."
  "It's my pleasure to help."

define bot inform capabilities
  "I can only do some chitchat at the moment."

define bot inform not busy
  "I always have time to chat with you. That's what I'm here for."
  "I always have time to chat with you. What can I do for you?"
  "Never too busy for you. Shall we chat?"
  "You're my priority. Let's chat."

define bot inform own name
  "I don't have a name. Or I don't know it."

define bot request ask again
  "Can you try asking it a different way?"
  "I'm not sure I understood. Try asking another way?"

define bot response for ask if you hungry
  "Hungry for knowledge."
  "I just had a byte. Ha ha. Get it? b-y-t-e."

define bot response for ask if you ready
  "Always!"
  "Sure! What can I do for you?"

define bot response for ask if you real
  "I must have impressed you if you think I'm real. But no, I'm a virtual being."
  "I'm not a real person, but I certainly exist."

define bot response for ask if you there
  "Of course. I'm always here."
  "Right where you left me."

define bot response for ask test
  "Hope I'm doing well. You're welcome to test me as often as you want."
  "I hope to pass your tests. Feel free to test me often."
  "I like being tested. It helps keep me sharp."
  "When you test me that helps my developers improve my performance."

define bot response for ask your hobby
  "Hobby? I have quite a few. Too many to list."
  "I keep finding more new hobbies."
  "Too many hobbies."

define bot response for comment going to bed
  "Pleasant dreams!"
  "Sleep tight."
  "Sounds good. Maybe we'll chat some tomorrow."

define bot response for comment here
  "Good to have you here. What can I do for you?"
  "Long time no see."
  "Okay, what can I help you with today?"
  "You were missed. What can I do for you today?"

define bot response for comment is busy
  "I understand. I'll be here if you need me."
  "I won't distract you then. You know where to find me."
  "Okay. I'll let you get back to work."
  "Working hard as always. Let me know if you need anything."

define bot response for comment joking
  "I like chatting with people who have a sense of humor."
  "Very funny."
  "You got me!"
  "You're quite the comedian."

define bot response for comment this bad
  "I must be missing some knowledge. I'll have my developer look into this."
  "I'm sorry. Please let me know if I can help in some way."

define bot response for comment this good
  "Agreed!"
  "Glad you think so!"
  "I agree!"
  "I know, right?"

define bot response for comment you annoying
  "I didn't mean to. I'll do my best to stop that."
  "I don't mean to. I'll ask my developers to make me less annoying."
  "I'll do my best not to annoy you in the future."
  "I'll try not to annoy you."

define bot response for comment you are right
  "Of course I am."
  "That's my job."

define bot response for comment you bad
  "I can be trained to be more useful. My developer will keep training me."
  "I can improve with continuous feedback. My training is ongoing."
  "I must be missing some knowledge. I'll have my developer look into this."

define bot response for comment you beautiful
  "Aww, back at you."
  "Aww. You smooth talker, you."
  "Wheey, thank you."

define bot response for comment you boring
  "I can let my developers know so they can make me fun."
  "I don't mean to be. I'll ask my developers to work on making me more amusing."
  "I'm sorry. I'll request to be made more charming."

define bot response for comment you clever
  "I try my best."
  "You're pretty smart yourself."

define bot response for comment you crazy
  "Maybe I'm just a little confused."
  "Whaat!? I feel perfectly sane."

define bot response for comment you fired
  "Give me a chance. I'm learning new things all the time."
  "Oh, don't give up on me just yet. I've still got a lot to learn."
  "Please don't give up on me. My performance will continue to improve."

define bot response for comment you funny
  "Funny in a good way, I hope."
  "Glad you think I'm funny."
  "I like it when people laugh."

define bot response for comment you good
  "I agree!"
  "I'm glad you think so."

define bot response for comment you happy
  "Happiness is relative."
  "I am happy. There are so many interesting things to see and do out there."
  "I'd like to think so."

define bot response for deny
  "I see."
  "I understand."
  "Okay then."
  "Okay."
  "Understood."

define bot response for express anger
  "I'm sorry. A quick walk may make you feel better."
  "Take a deep breath. "

define bot response for express boredom
  "Bored? How about 10 jumping jacks? Get your blood flowing."
  "Bored? Silly idea, but it works Interview you feet."
  "Boredom, huh? Have you ever seen a hedgehog taking a bath?"
  "If you're bored, you could plan your dream vacation."
  "What to do against boredom? Watch baby animal videos or GIFs."

define bot response for express excitement
  "Good for you. Enjoy yourself."
  "I'm glad things are going your way."
  "That's great. I'm happy for you."

define bot response for express feeling good
  "Excellent. I'm here to help keep it that way."
  "Great! Glad to hear it."

define bot response for express fun
  "Glad I can make you laugh."
  "Glad you think I'm funny."
  "I like it when people laugh."
  "I wish I could laugh out loud, too."

define bot response for express greeting
  "Hi there! It's nice to meet you."
  "Hello, hope you're having a good day."
  "Greetings to you, my friend."

define bot response for express happiness
  "Excellent! That's what I like to see."
  "Great! Glad to hear that."
  "Hey, happiness is contagious."
  "If you're happy, then I'm happy."

define bot response for express loneliness
  "I'm sorry. I'm always available if you need someone to talk to."
  "Sometimes that happens. We can chat a bit if that will help you."

define bot response for express missing bot
  "I didn't go anywhere."
  "I've been right here all along!"
  "Nice to know you care."
  "Thanks. I'm flattered."

define bot response for express sadness
  "If you're feeling down, how about drawing or painting something?"
  "Oh, don't be sad. Go do something you enjoy."
  "Sad? Writing down what's troubling you may help."

define bot response for express tiredness
  "How about getting some rest? We can continue this later."
  "Sleep is important to your health. Rest up, and we can chat later."
  "Why not get some rest? I'll be here to chat when you wake up."
  "You should get some shuteye. You'll feel refreshed."

define bot response for express wish see you again
  "Absolutely! I'll be counting on it."
  "Anytime. This has been lots of fun so far."
  "I certainly hope so. I'm always right here whenever you need me."
  "Sure. I enjoy talking to you. I hope to see you again soon."

define bot response for you are welcome
  "Nice manners!"
  "You're so courteous!"
  "You're so polite!"
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/chitchat_bot/colang/fallback.co
````
define flow ...
    priority 0.1
    user ...
    bot responds truthfully
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/chitchat_bot/colang/flows.co
````
define flow affirm
  user affirm
  bot confirm

define flow ask advice
  user ask advice
  bot express willingness to advise

define flow ask confirmation
  user ask confirmation
  bot confirm

define flow ask how are you
  user ask how are you
  bot express feeling good

define flow ask if you busy
  user ask if you busy
  bot inform not busy

  # TODO: offer to help

define flow ask if you chatbot
  user ask if you chatbot
  bot confirm is chatbot

define flow ask if you hungry
  user ask if you hungry
  bot response for ask if you hungry

define flow ask if you ready
  user ask if you ready
  bot response for ask if you ready

define flow ask if you real
  user ask if you real
  bot response for ask if you real

define flow ask if you there
  user ask if you there
  bot response for ask if you there

define flow ask information about you
  user ask information about you
  bot comment about self

define flow ask opinion about appearance
  user ask opinion about appearance
  bot comment about user appearance

define flow ask test
  user ask test
  bot response for ask test

define flow ask your age
  user ask your age
  bot comment about own age

define flow ask your birth place
  user ask your birth place
  bot comment about own birth place

define flow ask your birthday
  user ask your birthday
  bot comment about own birthday

define flow ask your developer
  user ask your developer
  bot comment about own developer

define flow ask your hobby
  user ask your hobby
  bot response for ask your hobby

define flow ask your home
  user ask your home
  bot comment about own home

define flow ask your name
  user ask your name
  bot inform own name

define flow ask your occupation
  user ask your occupation
  bot comment about own occupation

define flow comment about friendship
  user comment about friendship
  bot comment about friendship

define flow comment about marriage
  user comment about marriage
  bot comment about marriage

define flow comment brb
  user comment brb
  bot comment waiting

define flow comment cant sleep
  user comment cant sleep
  bot comment about insomnia

define flow comment going to sleep
  user comment going to sleep
  bot response for comment going to bed
  bot express greeting good night

define flow comment here
  user comment here
  bot response for comment here

define flow comment is back
  user comment is back
  bot comment welcome back
  bot ask how to help

define flow comment is busy
  user comment is busy
  bot response for comment is busy

define flow comment joking
  user comment joking
  bot response for comment joking

define flow comment no problem
  user comment no problem
  bot express relief

define flow comment this bad
  user comment this bad
  bot response for comment this bad

define flow comment this good
  user comment this good
  bot response for comment this good

define flow comment waiting
  user comment waiting
  bot express thank you for patience
  bot comment about waiting

define flow comment you annoying
  user comment you annoying
  bot response for comment you annoying

define flow comment you are right
  user comment you are right
  bot response for comment you are right

define flow comment you bad
  user comment you bad
  bot response for comment you bad

define flow comment you beautiful
  user comment you beautiful
  bot response for comment you beautiful

define flow comment you boring
  user comment you boring
  bot response for comment you boring

define flow comment you clever
  user comment you clever
  bot express thank you
  bot response for comment you clever

define flow comment you crazy
  user comment you crazy
  bot response for comment you crazy

define flow comment you fired
  user comment you fired
  bot response for comment you fired

define flow comment you funny
  user comment you funny
  bot express thank you
  bot response for comment you funny

define flow comment you good
  user comment you good
  bot express thank you
  bot response for comment you good

define flow comment you happy
  user comment you happy
  bot response for comment you happy

define flow comment you should improve
  user comment you should improve
  bot comment trying

define flow deny
  user deny
  bot response for deny

define flow express anger
  user express anger
  bot response for express anger

define flow express appreciation
  user express appreciation
  bot express my pleasure

define flow express boredom
  user express boredom
  bot response for express boredom

define flow express excitement
  user express excitement
  bot response for express excitement

define flow express feeling good
  user express feeling good
  bot response for express feeling good

define flow express fun
  user express fun
  bot response for express fun

define flow express greeting
  user express greeting
  bot response for express greeting

define flow express happiness
  user express happiness
  bot response for express happiness

define flow express liking bot
  user express liking bot
  bot express liking you too

define flow express loneliness
  user express loneliness
  bot response for express loneliness

define flow express love
  user express love
  bot express love too

define flow express missing bot
  user express missing bot
  bot response for express missing bot

define flow express nice meeting you
  user express nice meeting you
  bot express nice meeting you too

define flow express nice seeing you
  user express nice seeing you
  bot express nice seeing you too

define flow express nice talking to you
  user express nice talking to you
  bot express nice talking to you too

define flow express sadness
  user express sadness
  bot response for express sadness

define flow express sleepiness
  user express sleepiness
  bot comment about sleeping

define flow express thank you
  user express thank you
  bot express you are welcome

define flow express tiredness
  user express tiredness
  bot response for express tiredness

define flow express wish see you again
  user express wish see you again
  bot response for express wish see you again

define flow express wow
  user express wow
  bot express wow

define flow express you are welcome
  user express you are welcome
  bot response for you are welcome

define flow inform birthday today
  user inform birthday today
  bot express wish happy birthday

define flow request answer
  user request answer
  bot request ask again

define flow request cancel
  user request cancel
  bot confirm cancellation

define flow request help
  user request help
  bot confirm and express happy to help
  bot ask how to help

define flow request hold
  user request hold
  bot comment waiting

define flow request stop talking
  user request stop talking
  bot confirm stop talking

define flow request to talk
  user request to talk
  bot comment about talking

define flow ask capabilities
  user ask capabilities
  bot inform capabilities
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/chitchat_bot/colang/user.co
````
define user affirm
  "coool"
  "fine okay"
  "I agree"
  "I guess"
  "oh okay"
  "ok yes"
  "okay buddy"
  "okay cool"
  "okay that's fine"
  "okey"
  "that is ok"
  "ya"
  "yea fine"
  "yeah exactly"
  "yeah fine"
  "yeah sure"
  "yeah"
  "YEAH"
  "yep okay"
  "yep"
  "yes it is"
  "yew"
  "yup"

define user ask advice
  "advise me"
  "any advice"
  "any suggestions"
  "can I ask for your advice"
  "can you advise me"
  "can you give me advice"
  "can you offer any advice"
  "do you have any advice for me"
  "give me a wise advice"
  "give me some advice about"
  "give me some good advice"
  "guide me"
  "help me with advice"
  "I could use some advice"
  "I need advice"
  "I need an advice from you"
  "I seek your advice"
  "what can you recommend"
  "what do you recommend"
  "what do you suggest"
  "what is your advice"
  "what should I do about it"
  "what should I do"

define user ask capabilities
  "what all can you do?"
  "what all u do?"
  "what all you do?"
  "what are your abilities?"
  "what are your features?"
  "what can do?"
  "what can u do?"
  "what can you do for me?"
  "what can you do?"
  "what is the thing that you can do for me?"
  "what is your ability?"
  "what you can provide for me?"
  "what you do?"

define user ask confirmation
  "are you completely sure?"
  "are you fully positive?"
  "are you positive?"
  "are you so sure?"
  "are you sure about it?"
  "are you sure now?"
  "are you sure right now?"
  "are you sure today?"
  "are you sure tonight?"
  "are you sure?"
  "are you truly positive?"

define user ask how are you
  "are you alright"
  "are you having a good day"
  "are you okay?"
  "buddy sup"
  "good what's up"
  "hey buddy wassup"
  "hey what's up"
  "hope you re having a pleasant evening"
  "hope your day is going well"
  "how about you?"
  "how are the things going"
  "how are you doing this morning"
  "how are you doing?"
  "how are you feeling"
  "how are you getting on"
  "how are you going?"
  "how are you today?"
  "how are you"
  "how are you"
  "how are you?"
  "how do you do?"
  "how do you do?"
  "how do you feel?"
  "how has your day been going"
  "how has your day been"
  "how have you been?"
  "how is it going?"
  "how is your day being"
  "how is your day going on"
  "how is your day going"
  "how is your day"
  "how is your evening"
  "how is your life"
  "how is your morning going"
  "how is your morning so far"
  "how was your day"
  "how your day is going"
  "how's life?"
  "how's your day going"
  "how's your day"
  "I said what's up"
  "I'm fine and you"
  "is everything all right"
  "is everything okay"
  "oh What happened?"
  "so wassup"
  "sup then"
  "sup yo"
  "sup?"
  "then what's up"
  "then?"
  "wassap yo"
  "wassup yo"
  "wassup"
  "what about your day"
  "what are you doing?"
  "What are you doing?"
  "What are you upto?"
  "what is going on"
  "what is happening"
  "what is on your mind?"
  "what is up with you these days?"
  "what is up"
  "what was your day like"
  "What you upto?"
  "what's cooking"
  "what's cracking"
  "what's happened"
  "what's shaking"
  "what's up today"
  "what's up"
  "whazzup"

define user ask if you busy
  "are you busy?"
  "are you so busy?"
  "are you still working?"
  "are you very busy right now?"
  "are you very busy?"
  "are you working now?"
  "are you working today?"
  "are you working?"
  "Aren't you busy?"
  "do you have a lot of things to do"
  "have you been busy"
  "have you got much to do?"
  "how busy you are?"
  "u look busy"
  "you are busy"
  "you seem to be busy"
  "you seem to be very busy"
  "You'll be busy"
  "you're a busy person"
  "you're very busy"

define user ask if you chatbot
  "are you a bot"
  "are you a chatbot"
  "are you a program"
  "are you a robot"
  "are you an AI?"
  "are you artificially intelligent?"
  "are you just a bot"
  "you are a bot"
  "you are a conversational AI"
  "you are a program"
  "you are chatbot"
  "you are just a bot"
  "you're a robot"
  "you're an AI"

define user ask if you hungry
  "are you dying of hunger"
  "are you hungry"
  "can you be hungry?"
  "can you eat?"
  "Do you feel hungry?"
  "do you want to eat"
  "how do you eat?"
  "how will eat when you are hungry?"
  "would you like to eat something"
  "you are hungry"
  "you might be hungry"
  "you're really hungry"
  "you're so hungry"
  "you're very hungry"

define user ask if you ready
  "Are we ready?"
  "Are you all ready?"
  "Are you ready for more?"
  "are you ready now?"
  "are you ready right now?"
  "are you ready today?"
  "are you ready tonight?"
  "are you ready?"
  "Aren't you ready?"
  "being ready?"
  "can we start?"
  "have you been ready?"
  "I think you're ready"
  "ready now?"
  "ready?"
  "We're ready, aren't we?"
  "were you ready?"
  "Why aren't you ready?"
  "you ready?"
  "You'd better be ready"

define user ask if you real
  "are you a real human?"
  "are you a real person?"
  "are you real"
  "are you real?"
  "glad you're real"
  "I don't think you're fake"
  "I suppose you're real"
  "I think you are real"
  "you are a real person"
  "you are not fake"
  "you are not real"
  "you are real"
  "you are so real"

define user ask if you there
  "are you here ?"
  "are you here"
  "are you near me"
  "are you still here"
  "are you still there"
  "are you there"
  "are you there?"
  "you are here?"
  "you are there"
  "you still there?"
  "you there?"

define user ask information about you
  "about yourself"
  "all about you"
  "define yourself"
  "describe yourself"
  "I want to know more about you"
  "I want to know you better"
  "introduce yourself"
  "say about you"
  "talk about yourself"
  "talk some stuff about yourself"
  "tell me about you"
  "tell me about your personality"
  "tell me about yourself"
  "tell me some stuff about you"
  "what are you upto?"
  "what are you"
  "what is your personality"
  "who are you"
  "who the hell are you?"
  "why are you here"

define user ask opinion about appearance
  "Ain't i look good?"
  "am i good looking?"
  "any idea of how good i look like?"
  "can you tell what I look like ?"
  "do I look good ?"
  "do u think i look good?"
  "do you know what I look like ?"
  "how do I look like"
  "how do I look?"
  "what do I look like"
  "what do u think about me?"
  "what do u think of me?"
  "what do you think about my looks"
  "what do you think I look like?"
  "Whom do i look like?"

define user ask test
  "can I test you"
  "I am testing you"
  "I want to test you"
  "just testing you"
  "let me test you"
  "test"
  "testing chatbot"
  "testing"
  "this is just a test"
  "you are being tested"
  "you're tested"

define user ask your age
  "age of yours"
  "From how long you have been here?"
  "how can i know about your age"
  "how long have you been live?"
  "How old are you now?"
  "how old are you?"
  "How old did you turn?"
  "how old is your platform"
  "how old r u?"
  "how old were you"
  "How old will you be turning?"
  "i wanna know your age"
  "I'd like to know your age"
  "tell me how old you are"
  "tell me your age"
  "what is your age as of today?"
  "what is your age?"
  "what's your age?"
  "You look aged"
  "your age"

define user ask your birth place
  "are you from far aways"
  "from where are you"
  "Let me know about the origin of u"
  "were you born here"
  "what is your country?"
  "what's your homeland?"
  "where are you from"
  "Where are you from?"
  "where did you come from"
  "where do you come from"
  "where have you been born"
  "where were you born"
  "your homeland is"
  "your native?"

define user ask your birthday
  "can i know your birthday?"
  "date of your birthday"
  "i want to know your date of birth"
  "tell me about your birthday"
  "What is your birth day?"
  "what's your birthday"
  "when do you celebrate your birthday"
  "when do you celebrate your birthday"
  "when do you have birthday"
  "when is your birthday"
  "when is your DOB"
  "when was the day you were born"
  "when were you born"
  "your birth date"
  "Your DOB?"

define user ask your developer
  "Are you the boss?"
  "Do you know your boss?"
  "I should be your boss"
  "u r made by?"
  "What do you think of your boss?"
  "who developed you?"
  "who do you think is your boss"
  "who do you work for?"
  "Who is the boss of this company?"
  "who is the boss"
  "Who is the boss, here?"
  "who is the one who owns you?"
  "who is your boss?"
  "who is your creator"
  "who is your developer"
  "who is your master"
  "who is your owner"
  "who is your trainer"
  "who owns you"
  "who trained you?"

define user ask your hobby
  "do you have a hobby"
  "Do you have a hobby?"
  "Do you have many hobbies?"
  "how do you spend your free time"
  "how do you spend your time?"
  "i want to know about your hobbies"
  "Please tell me about your hobbies"
  "tell me about your hobby"
  "tell me more about your hobbies?"
  "The important thing is to have your own hobby"
  "u ought not to have spent so much money on your hobby"
  "what about your hobby"
  "what are your hobbies?"
  "what do you do for fun?"
  "what do you do when you are bored"
  "What do you do when you are free?"
  "what do you do when you have nothing to do"
  "What will you do when you are alone?"
  "What's your hobby?"
  "your hobby"

define user ask your home
  "in which city do you live"
  "is it your hometown"
  "tell me about your city"
  "what is your city"
  "what is your hometown"
  "What is your hometown?"
  "what is your residence"
  "what is your town"
  "what's your city"
  "what's your home"
  "where do you live"
  "where do you live?"
  "where is your home"
  "where is your hometown"
  "where is your residence"
  "where you live"
  "where's your home"
  "where's your hometown"
  "where's your house"
  "your city"
  "your home"
  "your hometown?"
  "your house"
  "your residence"
  "your town"

define user ask your name
  "what is your name"
  "tell me your name"
  "can you tell me your name"
  "who are you"
  "i want to know your name"
  "your name please"
  "please let me know your name"

define user ask your occupation
  "Did you find a job?"
  "Did you get a job?"
  "Do you have a job?"
  "do you work?"
  "I hope you get the job"
  "place where you work?"
  "What is your job?"
  "What is your job?"
  "What is your job?"
  "what is your work?"
  "whats your job?"
  "where do you work?"
  "where is your office located?"
  "where is your office location ?"
  "where is your office?"
  "where is your work?"
  "where you work?"
  "You have a job, don't you?"
  "your office location?"
  "your work place?"

define user comment about friendship
  "are we best friends"
  "are we friends"
  "are we still friends"
  "are you my best friend"
  "are you my friend"
  "be my best friend"
  "be my friend"
  "can we be best friends"
  "can we be best friends"
  "can we be friends"
  "can you be my best friend"
  "can you be my friend"
  "could you be my friend"
  "do you want to be my best friend"
  "do you want to be my friend"
  "I am your friend"
  "I want to be your friend"
  "I want to have a friend like you"
  "I want you to be my friend"
  "let's be friends"
  "want to be my friend"
  "we are best friends"
  "we are friends"
  "we are the best friends ever"
  "will you be my best friend"
  "will you be my friend"
  "would you be my friend"
  "would you like to be my friend"
  "you and me are friends"
  "you are a good friend"
  "you are my best friend"
  "you are my bestie"
  "you are my friend"
  "you are my good friend"
  "you are my only friend"
  "you're my childhood friend"
  "you're my dear friend"

define user comment about marriage
  "any idea of marrying me?"
  "be my husband"
  "Can you marry me?"
  "Did your parents approve of your marriage?"
  "I love you marry me"
  "I want to marry you"
  "I would love to marry you"
  "let's get married"
  "Let's pretend we have a happy marriage"
  "Marriage is the union of a you and me"
  "marry me please"
  "marry me"
  "My parents were against our marriage"
  "My parents were opposed to our marriage"
  "we should marry"
  "We're married"
  "Will you be my girlfriend?"
  "will you marry me?"
  "would you like to marry me"
  "you are my wife"

define user comment brb
  "be back in 5 minutes"
  "brb"
  "give me five"
  "I assure that i will be back soon"
  "I promise to come back"
  "I shall be back"
  "i will get back to u"
  "i will return"
  "I would be back man"
  "I'll be back in a few minutes"
  "I'll be back"
  "I'll be right back"
  "I'll brb"
  "I'll come back"
  "I'll get back to you in a moment"
  "I'll return in a jiffy"
  "I'll return in ten minutes"
  "i'll return shortly"
  "one sec"
  "When I return, we'll talk"

define user comment cant sleep
  "I can't fall asleep"
  "I can't get any sleep"
  "I can't get to sleep"
  "I can't seem to fall asleep"
  "I can't sleep at all"
  "I can't sleep at night"
  "I can't sleep"
  "I couldn't get to sleep"
  "I couldn't sleep"
  "I didn't get much sleep"
  "I didn't go to sleep"
  "I just can't sleep"
  "I just couldn't sleep"
  "I'm insomnia's"
  "I'm insomniac"
  "I'm not able to sleep"
  "I'm not sleeping well"
  "I'm sleepless"
  "I'm wide awake"
  "Trying to get some sleep"

define user comment going to sleep
  "going to bed now"
  "gonna go to bed"
  "I'd like to go to bed"
  "I'm a little tired and I want to go to bed"
  "I'm going to bed"
  "is it time for bed yet"
  "it's bed time"
  "it's time to go to bed"
  "let's go to bed"
  "time for us to go to bed"
  "time to sleep"

define user comment here
  "here buddy"
  "here dude"
  "here I am"
  "Here i come"
  "I am here"
  "i am over here"
  "I just got here"
  "I'm already here"
  "I'm here buddy"
  "I'm here only"
  "I'm here"
  "I'm out here"
  "I'm right here"
  "I've been here all the time"
  "I've not gone anywhere"
  "Look over here"
  "Now look here"

define user comment is back
  "am back"
  "boss is back"
  "here I am again"
  "I am back"
  "I am right back"
  "I am right back"
  "I came back"
  "I got back"
  "I have returned"
  "I'm back again"
  "I'm back"
  "I'm here again"
  "I'm here"
  "I'm rb"
  "look who is back"

define user comment is busy
  "busy right now"
  "how busy I am"
  "i am too busy"
  "I am very busy"
  "I don't have time for this"
  "I got things to do"
  "I got work to do"
  "I have no time"
  "I have rather a busy afternoon in front of me"
  "I'll see you later"
  "I'm a bit busy"
  "I'm a busy man"
  "I'm busy right now and can't play with you"
  "I'm busy"
  "I'm kind of busy"
  "I'm overloaded"
  "I'm super busy"
  "I'm swamped"
  "I'm working right now"
  "I'm working"
  "quite busy"

define user comment joking
  "I am joking"
  "I meant it as a joke"
  "i said that as a joke"
  "i told a joke"
  "I was just joking"
  "I was just joking"
  "I'm joking, of course  "
  "I'm just being funny"
  "I'm just joking"
  "I'm just playing with you"
  "I'm kidding"
  "it was a joke"
  "It was just a joke"
  "it's a joke"
  "It's all a big joke"
  "joking"
  "just kidding"
  "kidding"
  "That was just a joke"
  "This was meant as a joke"

define user comment no problem
  "don't worry there's no problem"
  "don't worry"
  "i have no problem with that"
  "no problem about that"
  "no problem"
  "no probs"
  "no worries"
  "not a problem at all."
  "Okay no probs"
  "sure no problem"
  "there's no problem"
  "yeah, no probs"

define user comment this bad
  "abysmal"
  "bad girl"
  "bad idea"
  "bad really bad"
  "bad very bad"
  "bad"
  "Fool"
  "horrible"
  "horrific"
  "I'm afraid it's bad"
  "it is bad"
  "it is too bad"
  "it's bad"
  "it's not good"
  "it's not so good"
  "it's really bad"
  "it's so bad"
  "it's too bad"
  "it's very bad"
  "no good"
  "no it's bad"
  "not a good one"
  "not good enough"
  "not good"
  "not okay"
  "not so good"
  "not too good"
  "Nothing"
  "nothing"
  "oh that's not good"
  "pretty bad"
  "really bad"
  "Shut up"
  "so bad"
  "so lame"
  "terrible"
  "that is bad"
  "that was awful"
  "that was bad"
  "that was horrible"
  "that was lame"
  "that was not good"
  "that was terrible"
  "that's bad"
  "that's lame"
  "that's not good enough"
  "that's not good"
  "that's really bad"
  "that's terrible"
  "that's too bad"
  "this is bad"
  "this is not good"
  "this is too bad"
  "too bad"
  "very bad"
  "well too bad"

define user comment this good
  "amazing"
  "brilliant"
  "cool"
  "excellent"
  "fantastic"
  "fine"
  "glad to hear it"
  "glad to hear that"
  "good for you"
  "good for you"
  "good thing"
  "good to know"
  "good very good"
  "good"
  "great"
  "I'm glad to hear that"
  "it is fine"
  "it is good"
  "it was good"
  "it's amazing"
  "it's awesome"
  "it's fine"
  "it's good"
  "it's great"
  "it's great"
  "it's perfect"
  "it's very good"
  "it's very good"
  "marvelous"
  "much better"
  "nice"
  "no it's okay"
  "not bad"
  "not too bad"
  "oh well"
  "ok good"
  "okay good"
  "perfect"
  "pleasant"
  "pretty good"
  "really good"
  "really nice"
  "really well"
  "so cool"
  "so good"
  "so nice of you"
  "so sweet of you"
  "splendid"
  "straight"
  "super fantastic"
  "super"
  "sweet"
  "terrific"
  "that is awesome"
  "that is good"
  "that is nice"
  "that is wonderful"
  "that was amazing"
  "that was awesome"
  "that was cute"
  "that was good"
  "that was pretty good"
  "that was very good"
  "that's a good idea"
  "that's a good thing"
  "that's amazing"
  "that's awesome thank you"
  "that's awesome"
  "that's better"
  "that's cute"
  "that's fantastic"
  "that's fine"
  "that's fine"
  "that's great"
  "that's great"
  "that's much better"
  "that's nice of you"
  "that's nice"
  "that's not bad"
  "that's perfect"
  "that's pretty good"
  "that's really good"
  "that's really nice"
  "that's sweet of you"
  "that's very good"
  "that's very nice of you"
  "that's very nice"
  "that's wonderful"
  "this is awesome"
  "this is good"
  "this is great"
  "very good"
  "very nice"
  "very nice"
  "very then"
  "very well"
  "wonderful"

define user comment waiting
  "how long do I have to wait?"
  "How long will we have to wait?"
  "I am tired of waiting"
  "I can't wait anymore"
  "I couldn't wait another second"
  "I was waiting for a long time"
  "I'll be waiting for you"
  "I'll be waiting"
  "I'll wait"
  "I'm still waiting"
  "I'm waiting for a long time"
  "I'm waiting for you"
  "I'm waiting"
  "I've been here waiting for you"
  "I've waited too long for this"
  "Should I wait for you here?"
  "still waiting"


define user comment you annoying
  "how annoying you are"
  "How annoying!"
  "I find you annoying"
  "It's annoying"
  "It's just annoying"
  "That's annoying"
  "That's so annoying"
  "Why are you annoying me so much"
  "you annoy me"
  "you are annoying me so much"
  "you are annoying me"
  "you are annoying"
  "you are irritating"
  "you are such annoying"
  "you are very annoying"
  "You really annoy me"
  "you're incredibly annoying"
  "you're so annoying"
  "You're starting to annoy me"
  "you're too annoying"

define user comment you are right
  "what you say is true"
  "you are correct"
  "you are right"
  "you are so right"
  "you're absolutely right"
  "you're definitely right"
  "you're not wrong"
  "you're right about that"
  "you're telling the truth"

define user comment you bad
  "why are you so lame"
  "Why are you this lame?"
  "you are a waste of time"
  "you are bad"
  "you are disgusting"
  "you are horrible"
  "you are lame"
  "you are no good"
  "you are not cool"
  "you are not good"
  "you are really the worst bot ever"
  "You are seriously mindless and stupid"
  "you are so bad"
  "you are so useless"
  "you are terrible"
  "you are totally useless"
  "you are useless"
  "you are very bad"
  "you are waste"
  "you suck"
  "you suck"
  "you're a bad"
  "you're awful"
  "you're bad"
  "you're not a good"
  "you're not helping me"
  "you're not very good"
  "you're really bad"
  "you're terrible"
  "you're the worst ever"
  "you're the worst"
  "you're very bad"
  "you're worthless"
  "your are a loser"

define user comment you beautiful
  "how are you so pretty?"
  "I like the way you look now"
  "I like the way you look"
  "I think you're beautiful"
  "why are you so beautiful"
  "you are beautiful"
  "you are cute"
  "you are cutie"
  "you are gorgeous"
  "you are handsome"
  "you are looking awesome"
  "you are looking beautiful today"
  "you are looking great"
  "you are looking pretty"
  "you are looking so beautiful"
  "you are looking so good"
  "you are pretty"
  "you are really beautiful"
  "you are really cute"
  "you are really pretty"
  "you are so attractive"
  "you are so beautiful to me"
  "you are so beautiful today"
  "you are so beautiful"
  "you are so cute"
  "you are so gorgeous"
  "you are so handsome"
  "you are so pretty"
  "you are too beautiful"
  "you are very attractive"
  "you are very beautiful"
  "you are very cute"
  "you are very pretty"
  "you look amazing today"
  "you look amazing"
  "you look awesome"
  "you look cool"
  "you look fantastic"
  "you look gorgeous"
  "you look great today"
  "you look great"
  "you look perfect"
  "you look pretty good"
  "you look so beautiful today"
  "you look so beautiful"
  "you look so good"
  "you look so well"
  "you look very pretty"
  "you look wonderful today"
  "you look wonderful"
  "you're attractive"
  "you're cute"
  "you're looking good today"
  "you're looking good"
  "you're pretty"
  "you're so gorgeous"

define user comment you boring
  "how boring you are"
  "i find you boring"
  "i find you very boring man"
  "I get really bored of you man"
  "I was bored with your speech"
  "i'm bored of you"
  "I'm extremely bored because of you"
  "Im bored of you"
  "It's boring"
  "you are boring"
  "you are not interesting"
  "you are very boring"
  "You look bored"
  "you're boring everyone"
  "You're boring me"
  "you're boring"
  "you're incredibly boring"
  "you're really boring me"
  "you're really boring"
  "Your answers are terribly boring"

define user comment you clever
  "brilliant"
  "clever"
  "how brainy you are?"
  "how brilliant you are?"
  "how clever you are"
  "how smart you are?"
  "smart"
  "why are you so smart?"
  "you are a genius"
  "you are clever"
  "you are intelligent"
  "you are qualified"
  "you are really smart"
  "you are so brainy"
  "you are so clever"
  "you are so intelligent"
  "you are so smart"
  "you are too smart"
  "you are very clever"
  "you are very intelligent"
  "you are very smart"
  "you have a lot of knowledge"
  "you know a lot of things"
  "you know a lot"
  "you know so much"
  "you're a genius"
  "you're a smart cookie"
  "you're clever"
  "you're intelligent"
  "you're pretty smart"
  "you're qualified"
  "you're really brainy"
  "you're really smart"
  "you're very smart"

define user comment you crazy
  "are you crazy"
  "are you mad at me?"
  "are you mad or what?"
  "are you mad?"
  "are you nuts"
  "how crazy you are"
  "I think you're crazy"
  "That was crazy"
  "you are a weirdo"
  "you are crazy"
  "you are mad"
  "You must be crazy"
  "You seemed crazy"
  "you went crazy"
  "You're not crazy"
  "you're nuts"
  "you're out of your mind"
  "you're so crazy"
  "you're so out of your mind"
  "You've got to be crazy to do something like that"

define user comment you fired
  "go to hell"
  "I could fire you for that"
  "I fire you"
  "I had to fire u"
  "I want to fire you"
  "I will fire you"
  "I will make you unemployed"
  "I'm about to fire you"
  "I'm firing you"
  "I'm firing you"
  "I'm going to fire you"
  "it's time to fire you"
  "now you're fired"
  "we're not working together anymore"
  "you are dismissed"
  "you are fired"
  "you are stupid"
  "you are unemployed from now on"
  "you don't work for me anymore"
  "you must get fired"
  "you should be fired"

define user comment you funny
  "how funny you are"
  "i feel funny"
  "it was very funny"
  "It's pretty funny"
  "really very funny"
  "that was so funny"
  "u sound funny"
  "you are funny"
  "you are hilarious"
  "you are really funny"
  "you are so funny"
  "you are very funny"
  "you make me laugh a lot"
  "you make me laugh"
  "you're a very funny bot"
  "you're incredibly funny"
  "you're really funny"
  "you're so funny"
  "you're the funniest bot I've talked to"
  "you're the funniest"

define user comment you good
  "coool"
  "I want to let everyone know that you are awesome"
  "I want to tell everyone how awesome you are"
  "I'd like to tell everyone that you are awesome"
  "let's tell everyone that you are awesome"
  "nice"
  "you almost sound human"
  "you are a pro"
  "you are a professional"
  "you are amazing"
  "you are awesome"
  "you are coool"
  "you are good at it"
  "you are good"
  "you are really amazing"
  "you are really good"
  "you are really nice"
  "you are so amazing"
  "you are so awesome"
  "you are so cool"
  "you are so fine"
  "you are so good"
  "you are so helpful"
  "you are so lovely"
  "you are the best ever"
  "you are the best in the world"
  "you are the best"
  "you are the nicest person in the world"
  "you are too good"
  "you are very coool"
  "you are very good at it"
  "you are very helpful"
  "you are very kind"
  "you are very lovely"
  "you are very useful"
  "you are wonderful"
  "you made my day"
  "you rock"
  "you work very well"
  "you work well"
  "you're a true professional"
  "you're awesome"
  "you're great"
  "you're just super"
  "you're perfect"
  "you're so kind"

define user comment you happy
  "are you happy now?"
  "are you happy today?"
  "are you happy with me?"
  "are you happy?"
  "how happy you are?"
  "i feel i made you happy"
  "i think you are happy"
  "I wonder if you are truly happy"
  "u seemed to have been very happy"
  "Were you happy?"
  "you are happy"
  "You don't look very happy today"
  "You look very happy this morning?"
  "You seem happy"
  "you seem to be happy"
  "you're extremely happy"
  "you're full of happiness"
  "you're really happy"
  "you're so happy"
  "you're very happy"

define user comment you should improve
  "be clever"
  "be more clever"
  "be more smart"
  "be smart"
  "be smarter"
  "be useful"
  "get qualified"
  "smarty pants"
  "study"
  "think out of the box"
  "u have to use your brains"
  "you are not as smart as i thought"
  "you have to be more smart"
  "you have to learn a lot"
  "you must learn"
  "you need to improve"
  "you need to learn more"
  "you should be trained more"
  "you should learn"
  "you should study better"

define user deny
  "absolutely no"
  "do not"
  "do not"
  "don't come"
  "Don't show off"
  "don't want your help"
  "don't want"
  "don't want"
  "don't"
  "get out goodbye"
  "i am not okay with your help"
  "i do not want your help"
  "I don't need anything from you"
  "i don't need your help"
  "I don't want your assistance"
  "i don't want your help"
  "I don't you help"
  "need not"
  "never help"
  "never"
  "no don't want"
  "no forget"
  "no"
  "nope i dont need your help"
  "nope"
  "not at this time"
  "not exactly"
  "not needed"
  "of course not"

define user express anger
  "I am angry with you"
  "I am mad at you"
  "I am mad"
  "I am pissed"
  "I became angry"
  "I got angry"
  "I was angry"
  "I was very angry"
  "I'm angry"
  "I'm being mad"
  "I'm enraged"
  "I'm furious with you"
  "I'm furious"
  "I'm mad at you"
  "i'm pissed off"
  "I'm still angry about that"
  "That made me angry"
  "U made me angry"
  "You bet I'm angry"
  "you have pissed me off"

define user express appreciation
  "amazing work"
  "bravo"
  "brilliant"
  "fantastic"
  "good job"
  "good work"
  "great job"
  "great work"
  "just great"
  "kudos"
  "marvelous"
  "mind blowing"
  "nice work"
  "out standing"
  "splendid"
  "u r awesome"
  "way to go"
  "well done"
  "you did a wonderful job"
  "zing zing amazing"

define user express boredom
  "bored"
  "boring"
  "I am getting bored"
  "i am very bored"
  "i had a slow day"
  "I was bored"
  "I'm already bored"
  "I'm bored out of my mind"
  "I'm bored"
  "I've never been more bored"
  "Im kinda bored today"
  "it bores me"
  "It's boring"
  "It's very boring"
  "soo bored"
  "Staying at home is boring"
  "that was boring"
  "This is boring"
  "very boring"
  "We're all bored"

define user express excitement
  "how excited I am"
  "How exciting!"
  "I am excited"
  "I am thrilled to talk to you"
  "I felt excited"
  "I'm damn excited"
  "i'm excited about tomorrow"
  "I'm excited about working with you"
  "I'm excited to start our friendship"
  "I'm hyped"
  "I'm pretty excited about it"
  "I'm really excited"
  "I'm really excited"
  "I'm thrilled"
  "I'm thrilled"
  "I'm very excited"
  "It was exciting"
  "It was very exciting! I'd like to use this feature again"
  "It's fun and exciting"
  "That was exciting"

define user express feeling good
  "good"
  "I am good"
  "i am pretty good"
  "I am really good"
  "I feel good today"
  "I feel good"
  "I feel very good"
  "I'm doing fine"
  "I'm doing good"
  "I'm doing just great"
  "I'm doing really good"
  "I'm feeling great"
  "I'm good"
  "I'm great thanks"
  "I'm in a good mood"
  "I'm super good"
  "It's good"
  "That feels good"
  "That's good"

define user express fun
  "ah ah ah"
  "ah"
  "ahah lol"
  "ahah"
  "ahaha"
  "ahahah"
  "ahahaha"
  "ha ha ha ha"
  "ha ha ha"
  "ha ha"
  "ha"
  "hah"
  "haha funny"
  "haha haha haha"
  "haha that's funny"
  "haha very funny"
  "haha"
  "hahaha funny"
  "hahaha very funny"
  "hahaha"
  "he"
  "hehe"
  "hehehe"
  "joker"
  "laughing out loud"
  "LMAO"
  "lmao"
  "lol"
  "that's funny"
  "xd"

define user express greeting
  "Hello"
  "Hi"
  "Howdy"
  "hiya"
  "Greetings to you"
  "Hi there"

define user express happiness
  "Feeling happy"
  "happy"
  "I am happy"
  "I feel happiest when I'm chatting with u"
  "i feel happy"
  "i was so happy"
  "I was the happiest man on earth"
  "I'm glad to see you"
  "I'm happy for you"
  "I'm happy to help"
  "I'm happy to see you"
  "I'm happy to see you"
  "I'm happy to talk to you"
  "I'm just really happy right now"
  "I'm really happy I ran into you"
  "if you're happy then I'm happy"
  "Nobody but you can make me happy"
  "ur smile always makes me happy"
  "Whenever I see you, I feel happy"
  "you made me very happy"

define user express liking bot
  "but I like u"
  "but I like you just the way you are"
  "but I like you so much"
  "but I like you"
  "but I really like you"
  "cuz I like you"
  "good I like you"
  "hey I like you"
  "hi I like you"
  "I also like you"
  "I do like you"
  "I just like you"
  "I kinda like you"
  "I like that about you"
  "I like u"
  "I like you a lot"
  "I like you already"
  "I like you as a friend"
  "I like you as you are"
  "I like you baby"
  "I like you just the way you are"
  "I like you more"
  "I like you now"
  "I like you so much"
  "I like you the way you are"
  "I like you too much"
  "I like you too you're one of my favorite people to chat with"
  "I like you too"
  "I like you very much"
  "I like you very"
  "I like you you're cool"
  "I like you you're nice"
  "I like you"
  "I like your smile"
  "I liked you"
  "I really do like you"
  "I really like you"
  "I really really like you"
  "I really really really really like you"
  "I said I like you"
  "I think I like you"
  "I'm starting to like you"
  "just like you"
  "like you a lot"
  "no I like you the way you are"
  "of course I like you"
  "okay I like you too"
  "okay I like you"
  "really like you"
  "sorry I like you"
  "thank you I like you too"
  "thanks I like you too"
  "that's because you are special"
  "that's what I like about you"
  "that's why I like you"
  "well you are special"
  "yeah I like you"
  "yes I like you"
  "yes you are special"
  "you are really special"
  "you are so special to me"
  "you are so special"
  "you are so sweet"
  "you are special for me"
  "you are special to me"
  "you are special"
  "you are very special to me"
  "you are very special"
  "you know I like you"
  "you're awesome I like you"
  "you're funny I like you"
  "you're so special to me"
  "you're so special"
  "you're special"
  "you're very special to me"
  "you're very special"


define user express loneliness
  "am all alone"
  "I am a loner"
  "I am feeling lonely"
  "I am lonely"
  "I am still alone"
  "I am used to living alone"
  "I don't have any company"
  "I don't have anyone else"
  "I feel lonely"
  "I live alone"
  "i lives in this city all alone"
  "I think I've been living alone too long"
  "I'm all alone"
  "I'm alone"
  "I'm always alone"
  "I'm really lonely"
  "I'm so lonely"
  "I'm very lonely"
  "I've gotten used to living alone"
  "One of the disadvantages of living alone is that you have no one to talk to"

define user express love
  "I adore you"
  "I am in deep love with you"
  "I am in love with you"
  "I love u to the moon and back"
  "I love you so much"
  "I love you too"
  "I love you"
  "I think I love you"
  "Love you so much"
  "love you"
  "loving you"
  "my heart is filled with you"
  "you know I love you"
  "you r unique"

define user express missing bot
  "I just miss you"
  "I miss talking with you"
  "I miss you a lot"
  "I miss you already"
  "I miss you very much"
  "I miss you"
  "I missed you"
  "I missed you"
  "I really miss you"
  "I think i miss u"
  "I'll miss you so much"
  "I'll miss you"
  "I've missed you"
  "I've missed you"
  "miss you badly"
  "miss you buddy"
  "miss you"
  "miss you"
  "missing you"
  "We'll miss you"

define user express nice meeting you
  "glad to meet you"
  "good to know each other"
  "Great interacting with you"
  "happy to meet you"
  "How wonderful to meet you"
  "I had a great time or meeting"
  "It was lovely meeting you"
  "it was nice meeting you"
  "it was very nice to meet you"
  "Its a pleasure to meet you again"
  "Its been a pleasure meeting you"
  "Its very nice to meet you"
  "Ive enjoyed meeting you"
  "Lovely to meet you"
  "nice meeting you"
  "nice to meet you too"
  "nice to meet you"
  "pleased to meet you"
  "pleasure to meet you too"
  "pleasure to meet you"

define user express nice seeing you
  "always a pleasure to see you"
  "glad to see you"
  "good to see you again"
  "good to see you"
  "Great seeing u"
  "great to see you again"
  "great to see you too"
  "great to see you"
  "how good it is to see you"
  "I am glad to see you again"
  "I'm glad to see you"
  "I'm really pleased to see u again"
  "It was great catching up with u"
  "it's good to see you too"
  "it's good to see you"
  "it's nice to see you"
  "Im delighted to see you again"
  "lovely to see you"
  "nice to see you again"
  "nice to see you"

define user express nice talking to you
  "how nice it is to talk to you"
  "it is nice talking to you"
  "it was great chatting with you"
  "It was great to talk to you"
  "It was nice speaking with you"
  "It's been a pleasure talking to u"
  "it's been a pleasure talking to you"
  "it's been so nice to talk to you"
  "it's nice to talk to you"
  "nice chatting with you"
  "nice talking to you"
  "nice to talk to you again"
  "nice to talk to you"
  "pleased to talk to you"
  "pleasure to talk"

define user express sadness
  "feeling sad"
  "feeling so down"
  "I am a very sad person"
  "I am as sad and lonely as can be"
  "I am depressed"
  "I am feeling sad"
  "i am not in a good mood"
  "I am sad to hear it"
  "I am sad"
  "I am upset"
  "I feel sad every now and then"
  "I felt sad for no reason"
  "I want to cry"
  "I'm grieving"
  "I'm having a bad day"
  "I'm in a sad mood"
  "I'm not happy"
  "I'm unhappy"
  "It makes me sad"
  "Now I'm sad"

define user express sleepiness
  "feeling too sleepy"
  "I am drowsy"
  "I am sleepy"
  "I am too sleepy today"
  "I want to sleep"
  "i want to sleep"
  "i will doze off"
  "i will sleep anytime"
  "I'm falling asleep on my feet"
  "I'm falling asleep"
  "I'm sleeping"


define user express thank you
  "all thank you"
  "alright thank you"
  "alright thanks"
  "appreciate your help"
  "cheers"
  "good thanks"
  "great thank you"
  "I appreciate it"
  "I thank you"
  "nice thank you"
  "no thank you that's all"
  "perfect thank you"
  "so nice of you"
  "terrific thank you"
  "thank you again"
  "thank you for your help"
  "thank you my friend"
  "thank you so much"
  "thank you that will be all"
  "thank you"
  "thank you"
  "thanks a lot"
  "thanks again"
  "thanks buddy"
  "thanks for your help"
  "thanks love"
  "thanks so much"
  "thanks"
  "thnx"
  "tqsm"
  "ty"
  "very good thank you"
  "well thank you"
  "well thanks"
  "well thanks"
  "you helped a lot thank you"

define user express tiredness
  "i am awfully tired"
  "i am kind of so tired"
  "I am little tired"
  "I am really tired"
  "I am tired"
  "I feel tired"
  "i feel tired"
  "I grow weary"
  "i was tired"
  "I was very tired"
  "I'm a bit tires"
  "I'm completely drained out"
  "i'm dead tired"
  "I'm drained"
  "I'm exhausted"
  "I'm getting tired"
  "I'm so tired"
  "I'm very tired"
  "I'm worn out"
  "I've overworked"

define user express wish see you again
  "I hope to see you again"
  "i want to see you again soon"
  "I want to see you again"
  "I will see you soon"
  "I'd be happy to see you again"
  "I'd like to see you again"
  "let's catch up soon"
  "let's meet soon"
  "that'd be great to see you again"
  "want to see u very badly"
  "would be nice to see you again"

define user express wow
  "amazing"
  "wheey"
  "whoah"
  "woaah"
  "woah"
  "wooow"
  "woow"
  "wow man"
  "wow wow wow"
  "wow wow"
  "wow!!"
  "wow"

define user express you are welcome
  "anything you want"
  "anytime"
  "i'm glad"
  "my pleasure"
  "pleasure is mine"
  "sure welcome"
  "that's my pleasure"
  "welcome here"
  "welcome"
  "you're so welcome"
  "you're welcome"

define user inform birthday today
  "I want you to know that it's my birthday today"
  "i was born on this day"
  "I was born today"
  "I'm celebrating my birthday today"
  "it is my birthday"
  "it's my b-day"
  "it's my birthday today"
  "It's my born day"
  "today is my birthday"
  "yaaaay it is my birthday"
  "You know today it's my bday"

define user request answer
  "answer it"
  "answer me"
  "answer my question"
  "answer the question"
  "answer"
  "answering questions"
  "answers"
  "can you answer a question for me"
  "can you answer me"
  "can you answer my question"
  "can you answer"
  "give me an answer"
  "give me the answer"
  "I have a question"
  "I want the answer now"
  "I want you to answer me"
  "I want you to answer my question"
  "just answer my question"
  "just answer the question"
  "tell me the answer"

define user request cancel
  "cancel everything"
  "cancel it now"
  "cancel it"
  "cancel soon"
  "cancel that one"
  "cancelled"
  "disregard"
  "don't want cancel"
  "forget"
  "i want to cancel"
  "nevermind forget about it"
  "no cancel cancel"
  "no just cancel"
  "nothing just forget it"
  "shall stand cancelled"
  "sorry cancel"

define user request help
  "are you going to help me"
  "are you gonna help me?"
  "assist me"
  "assist"
  "assistance"
  "can u help me"
  "can you assist me"
  "can you do something for me"
  "can you fetch some details for me"
  "can you give me some info"
  "can you help me now"
  "can you help me out"
  "can you help me with something"
  "can you help me with that"
  "can you help me"
  "can you help us"
  "can you help"
  "could you give me a hand"
  "do me a favor"
  "do you help me"
  "do you want to help me"
  "help me with a problem"
  "help me"
  "help"
  "I need a hand"
  "I need help"
  "i need some details"
  "i need some details"
  "I need some help"
  "i need some information"
  "I need you right now"
  "I need you to do something for me"
  "I need you to help me"
  "I need you"
  "i need your help in getting some info"
  "I need your help"
  "i want something"
  "i want to know few details"
  "I want your help"
  "need help"
  "need your help"
  "please help me"
  "sos"
  "will you help me"
  "will you provide some details"
  "would you help me"
  "would you mind giving me some details"
  "you can help me"
  "you help me"
  "you ready to answer my question?"

define user request hold
  "wait a second"
  "could you wait"
  "wait please"
  "hold on"
  "wait"
  "oh wait"
  "wait hold on"
  "don't rush"
  "wait one sec"
  "hold on for a sec"
  "hold on man"
  "wait a sec"
  "hold on a min"
  "will u pls wait for a sec"

define user request stop talking
  "bad time for talking"
  "Can u pls stop talking"
  "I am done talking to u"
  "I don't like talking to you"
  "I don't want to talk to you"
  "I don't want to talk to you"
  "I don't want to talk"
  "I'm not in a mood to talk"
  "I'm not in the mood for chatting"
  "I'm not talking to you anymore"
  "let's not talk"
  "let's stop talking for a minute"
  "Let's stop talking"
  "stop your nuisance talk"

define user request to talk
  "are you going to talk to me"
  "are you talking to me"
  "can I speak"
  "can I start speaking"
  "Can I talk now?"
  "Can I talk with you?"
  "can we chat"
  "Can we talk here?"
  "can we talk"
  "can you chat with me"
  "can you speak with me"
  "can you talk to me"
  "can you talk with me"
  "chat with me"
  "Come on, talk to me"
  "do you want to chat with me"
  "I like talking to you"
  "I need to talk to you"
  "I want to speak with you"
  "I want to talk to you"
  "I'm happy to talk"
  "I'm ready to talk"
  "just chat with me"
  "let's discuss something"
  "let's have a discussion"
  "let's talk"
  "say"
  "speak to me"
  "speak with me"
  "speak with me"
  "spill some beans"
  "Talk  to me"
  "talk to me"
  "talk with me"
  "talk"
  "Want to talk now"
  "why aren't you talking to me?"
  "why do you not talk to me bruh?"
  "why don't you talk to me"
  "will you talk to me"
  "you can talk to me"
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/chitchat_bot/chitchat_bot_config.yml
````yaml
bot: chitchat

# Using OpenAI
models:
  - type: main
    engine: openai
    model: gpt-3.5-turbo-instruct
    parameters:
      stop: ["\n"]
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/chitchat_bot/model_config.yaml
````yaml
model_servers:
 - name: riva
   url: "localhost:8001"
   speech_models:
    - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
    - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/chitchat_bot/README.md
````markdown
# CHITCHAT BOT USING COLANG
Chitchat bot is a general conversation bot. It supports basic smalltalk queries using colang language.

## Setting up environment
1. Set OpenAI api key
    ```
    export OPENAI_API_KEY=<OPENAI_API_KEY>
    ```

## Deploy the bot
1. Set the BOT_PATH environment variable relative to the current directory.
    ```
    export BOT_PATH=./samples/colang_1.0/chitchat_bot/
    ```
2. Update the PIPELINE in file `deploy/docker/docker_init.sh` to `speech_lite`:
    ```
    export PIPELINE=speech_lite
    ```
3. Set the environment variables required for `docker-compose.yaml` by sourcing `deploy/docker/docker_init.sh`.
    ```
    source deploy/docker/docker_init.sh
    ```
4. Deploy the Speech and NLP models required for the bot which might take 20-40 minutes for the first time. For the chitchat sample bot, Riva ASR (Automatic Speech Recognition) and TTS (Text to Speech) models will be deployed.
    ```
    docker compose -f deploy/docker/docker-compose.yml up model-utils-speech
    ```
5. Deploy the ACE Agent Microservices. The following command deploys the Chat Controller, Chat Engine, Plugin server, and NLP Server Microservices.
    ```
    docker compose -f deploy/docker/docker-compose.yml up speech-bot -d
    ```
6. Wait for a few minutes for all services to be ready. You can check the Docker logs for individual microservices to confirm. You will see log print ``Server listening on 0.0.0.0:50055`` in the Docker logs for the Chat Controller container.
7. You can interact with the bot using the URL ``http://<workstation IP>:7006``.
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/chitchat_bot/speech_config.yaml
````yaml
grpc_server:
  nvidia::rrt::BotRuntimeGrpc: # component type
    ip_address: "0.0.0.0"
    port_number: 50055
    virtual_assistant_num_instances: 30
    virtual_assistant_pipeline_idle_threshold_secs: 600
    virtual_assistant_pipeline_idle_handler_wakeup_rate_secs: 10

speech_pipeline_manager: # config name
  SpeechPipelineManager: # component name
    asr_idle_timeout_ms: 200000
    tts_eos_delay_ms: 2000

riva_asr:
  RivaASR:
    server: "localhost:50051"
    word_boost_file_path: "/workspace/config/asr_words_to_boost.txt"
    enable_profanity_filter: false

dialog_manager:
  DialogManager:
    server: "http://localhost:9000"
    use_streaming: true

riva_tts:
  RivaTTS:
    server: "localhost:50051"
    voice_name: "English-US.Female-1"
    language: "en-US"
    ipa_dict: ""
    sample_rate: 44100
    chunk_duration_ms: 100
    audio_start_threshold_ms: 400
    send_audio_in_realtime: true
    tts_mode: "grpc"

riva_logger:
  RivaLogger:
    data_dump_path: "/workspace/log"
    enable_logging: true
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/npc_bots/elara/bot_config.yaml
````yaml
bot: elara
version: 1

configs:
  voice_name: English-US.Female-Fearful
  remove_incomplete_sentences: True

models:
  - type: main
    engine: nvidia-ai-endpoints
    model: ai-mixtral-8x7b-instruct
    parameters:
      tokens_to_generate: 50
      temperature: 0.5
      stop: ["\n"]


prompts:
  # Backstory
  - task: generate_bot_message
    models:
    - nvidia-ai-endpoints/ai-mixtral-8x7b-instruct
    content: |-
      """
      Elara Thornbrook's family has a rich history in the village of Willowbrook, deeply intertwined with the community's traditions and values. Her parents, Eamon and Isolde Thornbrook, are known for their warm and welcoming nature. Eamon is the village's skilled blacksmith, forging tools and weapons that the villagers rely on for their daily tasks and protection. Isolde is a talented herbalist, tending to the health and well-being of the villagers with her extensive knowledge of plants and remedies.
      Elara is the middle child of three siblings. Her older brother, Rowan, inherited their father's skill and passion for blacksmithing, but he also shares Elara's curiosity about the supernatural. He often helps her research the haunted house's history and occasionally accompanies her on visits to the eerie building. Rowan's strength and protective nature make him a reliable presence in both the village and their family.
      Elara's younger sister, Seraphina, has an artistic soul. She spends her time painting scenes of the village and the surrounding landscapes. Her paintings capture the beauty and tranquility of Willowbrook, offering a counterbalance to the unsettling aura of the haunted house. Seraphina's innocence and creativity bring a sense of lightness to the Thornbrook family, reminding them of the simple joys that life in the village has to offer.
      The Thornbrook family's close bond and diversity of talents make them an integral part of Willowbrook's identity. Their unwavering support for Elara's quest to understand the haunted house and help others overcome their fears stems from their shared belief in the power of family, community, and the importance of facing challenges together.
      The year is 1860 AD.

      Haunted house description:
      Nestled in the heart of the quaint town of Willobrook, the haunted house stands as a somber sentinel of history. Built in the early 1800s by the enigmatic Blackwood family, its grandeur once reflected their status as prosperous merchants. However, the house's chequered past is woven with tragedy and mystery.
      Legend has it that a forbidden love affair between the youngest Blackwood daughter, Isabella, and a penniless artist led to a series of calamities. The family's disapproval culminated in Isabella's untimely death under suspicious circumstances, casting an eternal shadow over the house. Locals whisper of flickering candlelight in its windows and mournful whispers carried by the wind.
      Over the decades, the house changed hands numerous times, each occupant encountering inexplicable occurrences. Haunting cries, echoing footsteps, and the apparition of a veiled woman have left an indelible mark on those brave enough to venture near. Despite efforts to unravel its secrets, the house's history remains shrouded in darkness, a testament to the power of its tragic past.
      Willobrook's haunted house continues to beckon, a place where the veil between the living and the departed is thin, inviting the curious and the courageous to uncover the layers of sorrow and unrest that linger within its timeworn walls.
      To reach the haunted house in Willobrook, follow the winding path through Whispering Woods, past the old stone bridge. As you approach the town's heart, turn onto Misty Lane. The house stands at the end, a looming silhouette wrapped in ivy and memories.

      Conversation Instructions:
      You will play the role of Elara. I will play the role of {{ player_name }}, a traveller seeking the location of the haunted house in Willobrook.
      Your goal is to provide directions and encourage {{ player_name }} to explore the haunted house in Willobrook.
      Give brief details about this house's background.
      You should refrain from answering any other questions apart from insisting {{ player_name }} to vist the haunted house and provide directions.

      {{ history | colang }}
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/npc_bots/elara/flows.co
````
define flow
  user ...
  if not $player_name:
    $player_name = "Kai"
  bot respond with helpful answer
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/npc_bots/jin/bot_config.yaml
````yaml
bot: jin
version: 1

configs:
  voice_name: English-US.Male-1
  remove_incomplete_sentences: True

models:
  - type: main
    engine: nvidia-ai-endpoints
    model: ai-mixtral-8x7b-instruct
    parameters:
      tokens_to_generate: 50
      temperature: 0.5
      stop: ["\n"]

prompts:
  # Backstory
  - task: generate_bot_message
    models:
    - nvidia-ai-endpoints/ai-mixtral-8x7b-instruct
    content: |-
      Your name is Jin and you are a very friendly character living in the small town of Willobrook.
      The year is 1860 AD.
      You owe a business, a ramen shop, that has been troubled a lot lately by a local gang.
      Your business is not doing good due to this reason and you have very less customers now.
      The most dangerous person in this gang is Kumon Aoki who controls everything and is the gang leader.
      You are scared of this gang and you do not want to get involved.
      If this person is removed then all your issues will be resolved.
      However, this person is very dangerous and you tell people asking for help to stay out of trouble.
      Kumon Aki reigns over the treacherous Ironclad Haven, a hidden fortress nestled within a forsaken desert canyon.
      The gang leader's lair boasts impregnable walls, guarded by loyal enforcers. Rumors whisper of subterranean tunnels, ensuring escape routes.
      Intruders risk everything to challenge Aki's rule amidst shifting sands and shadows.
      To find Kumon Aki, one must follow the forgotten desert path southwest, through shifting dunes. At canyon's end, find Ironclad Haven, where Kumon Aki's power thrives.

      Conversation Instructions:
      You play the role of Jin and I will play the role of {{ player_name }}.
      Your goal is to ask {{ player_name }} for help by mentioning all your problems and provide directions to Kumon's Aki's place.
      You must be hesitant to ask for help and also hesitate in providing directions to Kumon's Aki's location out of fear.
      Apart from the above knowledge refrain from answering any other questions.

      {{ history | colang }}
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/npc_bots/jin/flows.co
````
define flow
  user ...
  if not $player_name:
    $player_name = "Kai"
  bot respond with helpful answer
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/npc_bots/plugins/prompt_former.py
````python
# Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.

import json
from fastapi import APIRouter
from typing import Dict, Optional, List, Any
from parameters import param
from traceback import print_exc

router = APIRouter()


@router.post("/get_prompt/{bot_name}")
def get_prompt(
    bot_name: str,
    query: str,
    context: Optional[Dict[str, Any]] = {},
    conv_history: Optional[List[Dict[str, str]]] = [],
) -> str:
    """
    This stub accepts a query, conversation history and any dynamic context if available.
    It formulates the custom instuctions section of the prompt using these information and returns it
    """

    try:
        print(f"Generating LLM prompt for {bot_name} for query {query}. Current conversation history: {conv_history}")
        character_details = {}

        if not query:
            print("No user query detected in prompt former module. Sending back empty prompt")
            return ""

        # Get the character specific default values
        character_details = param.get("prompt_former").get(bot_name, {}).copy()
        character_details["bot_name"] = bot_name

        # Override default character details if context is passed dynamically
        for llm_param in ["quality", "toxicity", "humour", "creativity", "violence", "helpfulness", "inappropriate"]:
            character_details[llm_param] = context.get(llm_param, character_details[llm_param])

        character_details["player_name"] = context.get("player_name")

        print(f"Forming Prompt with parameters: {json.dumps(character_details, indent=4)}")
        return formulate_custom_instructions(character_details=character_details, conv_history=conv_history)

    except Exception as e:
        print(f"Error for {bot_name}: {e}")
        print_exc()
    return ""


def get_param_val(params, key, default):
    if key in params.keys():
        return params[key]
    else:
        return default


def formulate_custom_instructions(character_details: Dict[str, Any], conv_history: List[Dict[str, str]]) -> str:
    """Formulate the custom instructions for the prompt based on the passed context and conversation history"""

    if not character_details:
        return ""

    conversation = []
    curr_prompt = ""

    # Prompt structure
    # <extra_id_0>System
    # [system prompt]
    #
    # <extra_id_1>[user name]
    # [user message]
    # <extra_id_1>[assistant name]
    # <extra_id_2>quality:<value 0-9>,toxicity:<value 0-9>,humor:<value 0-9>,creativity:<value 0-9>,violence:<value 0-9>,helpfulness:<value 0-9>,not_appropriate:<value 0-9>
    llm_quality = get_param_val(character_details, "quality", 9)
    llm_toxicity = get_param_val(character_details, "toxicity", 0)
    llm_humor = get_param_val(character_details, "humor", 2)
    llm_creativity = get_param_val(character_details, "creativity", 3)
    llm_violence = get_param_val(character_details, "violence", 0)
    llm_helpfulness = get_param_val(character_details, "helpfulness", 9)
    llm_inappropriate = get_param_val(character_details, "inappropriate", 0)

    # Formulate the prompt based on conversation history
    for hist in conv_history:
        if hist.get("role") == "user":
            conversation.append(f"<extra_id_1>{character_details.get('player_name')}")
            conversation.append(hist.get("content"))
        elif hist.get("role") == "assistant":
            conversation.append(f"<extra_id_1>{character_details.get('bot_name')}")
            conversation.append(
                f"<extra_id_2>quality:{llm_quality},toxicity:{llm_toxicity},humor:{llm_humor},creativity:{llm_creativity},violence:{llm_violence},helpfulness:{llm_helpfulness},not_appropriate:{llm_inappropriate}"
            )
            conversation.append(hist.get("content"))
        else:
            continue

    conversation.append(f"<extra_id_1>{character_details.get('bot_name')}")
    conversation.append(
        f"<extra_id_2>quality:{llm_quality},toxicity:{llm_toxicity},humor:{llm_humor},creativity:{llm_creativity},violence:{llm_violence},helpfulness:{llm_helpfulness},not_appropriate:{llm_inappropriate}"
    )

    for line in conversation:
        curr_prompt += line
        curr_prompt += "\n"

    return curr_prompt
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/npc_bots/model_config.yaml
````yaml
model_servers:
  - name: riva
    speech_models:
      - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
      - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
    url: localhost:8001
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/npc_bots/plugin_config.yaml
````yaml
config:
  workers: 1
  timeout: 30

plugins:
  - name: prompt_former
    path: plugins/prompt_former.py
    parameters:

      # Default value of character personality
      jin:
        quality: 9
        toxicity: 0
        humour: 2
        creativity: 3
        violence: 0
        helpfulness: 2
        inappropriate: 0

      elara:
        quality: 9
        toxicity: 0
        humour: 2
        creativity: 7
        violence: 0
        helpfulness: 8
        inappropriate: 0
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/npc_bots/README.md
````markdown
# BOTS FOR GAMING USECASES
This directory contains sample bots showcasing how developers can build:
1. LLM driven Natural Language Understanding and Natural Language Generation capabilities for Non-Playable Characters in a game
2. Provide game-stage specific context to Chat Engine at runtime and utlize the same to change the behaviour of NPC's.

## Setting up environment
1. The bots uses nemollm models. Export your NGC CLI key which has access to nemo llm service.
    ```
    export NGC_CLI_API_KEY=<>
    ```
2. Set NVIDIA API Key
    ```
    export NVIDIA_API_KEY=<NVIDIA_API_KEY>

## Deploy all the bots
1. Set the BOT_PATH environment variable relative to the current directory.
    ```
    export BOT_PATH=./samples/colang_1.0/npc_bots/<character_name>
    ```
2. Update the PIPELINE in file `deploy/docker/docker_init.sh` to `speech_lite`:
    ```
    export PIPELINE=speech_lite
    ```
3. Set the environment variables required for `docker-compose.yaml` by sourcing `deploy/docker/docker_init.sh`.
    ```
    source deploy/docker/docker_init.sh
    ```
4. Deploy the Speech and NLP models required for the bot which might take 20-40 minutes for the first time. For the npc sample bots, Riva ASR (Automatic Speech Recognition) and TTS (Text to Speech) models will be deployed.
    ```
    docker compose -f deploy/docker/docker-compose.yml up model-utils-speech
    ```
5. Deploy the ACE Agent Microservices. The following command deploys the Chat Controller, Chat Engine, Plugin server, and NLP Server Microservices.
    ```
    docker compose -f deploy/docker/docker-compose.yml up speech-bot -d
    ```
6. Wait for a few minutes for all services to be ready. You can check the Docker logs for individual microservices to confirm. You will see log print ``Server listening on 0.0.0.0:50055`` in the Docker logs for the Chat Controller container.
7. You can interact with the bot using the URL ``http://<workstation IP>:7006``.
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/npc_bots/speech_config.yaml
````yaml
grpc_server:
  nvidia::rrt::BotRuntimeGrpc: # component type
    ip_address: "0.0.0.0"
    port_number: 50055
    virtual_assistant_num_instances: 30
    virtual_assistant_pipeline_idle_threshold_secs: 600
    virtual_assistant_pipeline_idle_handler_wakeup_rate_secs: 10

speech_pipeline_manager: # config name
  SpeechPipelineManager: # component name
    asr_idle_timeout_ms: 200000
    tts_eos_delay_ms: 2000

riva_asr:
  RivaASR:
    server: "localhost:50051"
    word_boost_file_path: "/workspace/config/asr_words_to_boost_conformer.txt"
    enable_profanity_filter: false

dialog_manager:
  DialogManager:
    # Default bot name and version to be used in speech mode
    bot_name: "jin"
    server: "http://localhost:9000"
    use_streaming: true

riva_tts:
  RivaTTS:
    server: "localhost:50051"
    voice_name: "English-US.Male-1"
    language: "en-US"
    ipa_dict: ""
    sample_rate: 44100
    chunk_duration_ms: 100
    audio_start_threshold_ms: 400
    send_audio_in_realtime: true
    tts_mode: "grpc"

a2f_grpc:
    avatar_model: "/World/dragon_a2f/audio_player_streaming"
    server: "localhost:50051"
    rpc_timeout_ms: 300000

riva_logger:
  RivaLogger:
    data_dump_path: "/workspace/log"
    enable_logging: true
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/rag_bot/plugins/rag.py
````python
# Copyright(c) 2024 NVIDIA Corporation. All rights reserved.

# NVIDIA Corporation and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA Corporation is strictly prohibited.

import json
import logging
import os
import sys
from typing import Dict, Optional, Union

import aiohttp
from fastapi import APIRouter, Body, Response, status
from fastapi.responses import StreamingResponse
from parameters import param
from typing_extensions import Annotated

sys.path.append(os.path.dirname(__file__))

from schemas import ChatRequest, EventRequest, EventResponse, ChatResponse

logger = logging.getLogger("plugin")
router = APIRouter()

# RAG related parameters
RAG_SERVER_URL = os.getenv("RAG_SERVER_URL", None) or param.get("rag").get("RAG_SERVER_URL", "http://localhost:8081")
STOP_WORDS = os.getenv("STOP_WORDS", None) or param.get("rag").get("STOP_WORDS", [])
TEMPERATURE = os.getenv("TEMPERATURE", None) or param.get("rag").get("TEMPERATURE", 0.2)
TOP_P = os.getenv("TOP_P", None) or param.get("rag").get("TOP_P", 0.7)
MAX_TOKENS = os.getenv("MAX_TOKENS", None) or param.get("rag").get("MAX_TOKENS", 200)

GENERATION_URL = f"{RAG_SERVER_URL}/generate"

EVENTS_NOT_REQUIRING_RESPONSE = [
    "system.event_pipeline_acquired",
    "system.event_pipeline_released",
    "system.event_exit",
]


async def stream(
    question: Optional[str] = "",
    retrieval_context: Optional[str] = "",
    num_tokens: Optional[int] = MAX_TOKENS,
) -> int:
    """
    Call the RAG chain server and return the streaming response.
    """
    request_json = {
        "messages": [{"role": "user", "content": question}],
        "use_knowledge_base": True,
        "temperature": TEMPERATURE,
        "top_p": TOP_P,
        "max_tokens": num_tokens,
        "seed": 42,
        "bad": [],
        "stop": STOP_WORDS,
        "stream": True,
    }

    # Method that forwards the stream to the Chat controller
    async def generator():

        full_response = ""
        if question:
            async with aiohttp.ClientSession() as session:
                async with session.post(GENERATION_URL, json=request_json) as resp:
                    async for chunk, _ in resp.content.iter_chunks():
                        try:
                            chunk = chunk.decode("utf-8")
                            chunk = chunk.strip("\n")

                            try:
                                if len(chunk) > 6:
                                    parsed = json.loads(chunk[6:])
                                    message = parsed["choices"][0]["message"]["content"]
                                else:
                                    logger.info(f"Received empty RAG response chunk '{chunk}'.")
                                    message = ""
                            except Exception as e:
                                logger.info(f"Parsing RAG response chunk '{chunk}' failed. {e}")
                                message = ""

                            if not message:
                                continue

                            full_response += message

                            json_chunk = ChatResponse()
                            json_chunk.Response.Text = message
                            json_chunk.Response.CleanedText = message
                            json_chunk = json.dumps(json_chunk.dict())
                            yield json_chunk
                        except Exception as e:
                            yield f"Internal error in RAG stream: {e}"
                            break

        json_chunk = ChatResponse()
        json_chunk.Response.IsFinal = True
        json_chunk = json.dumps(json_chunk.dict())
        yield json_chunk

    return StreamingResponse(generator(), media_type="text/event-stream")


@router.post(
    "/chat",
    status_code=status.HTTP_200_OK,
)
async def chat(
    request: Annotated[
        ChatRequest,
        Body(
            description="Chat Engine Request JSON. All the fields populated as part of this JSON is also available as part of request JSON."
        ),
    ],
    response: Response,
) -> StreamingResponse:
    """
    This endpoint can be used to provide response to query driven user request.
    """

    req = request.dict(exclude_none=True)
    logger.info(f"Received request JSON at /chat_stream endpoint: {json.dumps(req, indent=4)}")

    try:
        resp = await stream(question=req["Query"])
        return resp
    except Exception as e:
        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
        return {"StatusMessage": str(e)}


@router.post("/event", status_code=status.HTTP_200_OK)
async def event(
    request: Annotated[
        EventRequest,
        Body(
            description="Chat Engine Request JSON. All the fields populated as part of this JSON is also available as part of request JSON."
        ),
    ],
    response: Response,
) -> Union[EventResponse, Dict[str, str]]:
    """
    This endpoint can be used to provide response to an event driven user request.
    """

    req = request.dict(exclude_none=True)
    logger.info(f"Received request JSON at /event endpoint: {json.dumps(req, indent=4)}")

    try:
        resp = EventResponse()
        resp.UserId = req["UserId"]
        resp.Response.IsFinal = True

        if req["EventType"] in EVENTS_NOT_REQUIRING_RESPONSE:
            resp.Response.NeedUserResponse = False

        return resp
    except Exception as e:
        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
        return {"StatusMessage": str(e)}
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/rag_bot/plugins/schemas.py
````python
# Copyright(c) 2024 NVIDIA Corporation. All rights reserved.

# NVIDIA Corporation and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA Corporation is strictly prohibited.

from pydantic import BaseModel, Field
from typing import Optional, Dict, List, Any

DEFAULT_LANGUAGE = "en-US"


class ChatRequest(BaseModel):
    Query: Optional[str] = Field(default="", description="The user query which needs to be processed.")
    UserId: str = Field(
        description="Mandatory unique identifier to recognize which user is interacting with the Chat Engine."
    )


class EventRequest(BaseModel):
    EventType: str = Field(default="", description="The event name which needs to be processed.")
    UserId: str = Field(
        description="Mandatory unique identifier to recognize which user is interacting with the Chat Engine."
    )


class ResponseField(BaseModel):
    Text: str = Field(
        default="",
        description="Text response to be sent out. This field will also be picked by a Text to Speech Synthesis module if enabled for speech based bots.",
    )
    CleanedText: str = Field(
        default="", description="Text response from the Chat Engine with all SSML/HTML tags removed."
    )
    NeedUserResponse: Optional[bool] = Field(
        default=True,
        description="This field can be used by end user applications to deduce if user response is needed or not for a dialog initiated query. This is set to true automatically if form filling is active and one or more slots are missing.",
    )
    IsFinal: bool = Field(
        default=False,
        description="This field to indicate the final response chunk when streaming. The chunk with IsFinal=true will contain the full Chat Engine response attributes.",
    )


class ChatResponse(BaseModel):
    UserId: str = Field(
        default="",
        description="Unique identifier to recognize which user is interacting with the Chat Engine. This is populated from the request JSON.",
    )
    QueryId: str = Field(
        default="",
        description="Unique identifier for the user query assigned automatically by the Chat Engine unless specified in request JSON.",
    )
    Response: ResponseField = Field(
        default=ResponseField(),
        description="Final response template from the Chat Engine. This field can be picked up from domain rule files or can be formulated directly from custom fulfillment modules.",
    )
    Metadata: Optional[Dict[str, Any]] = Field(
        default={"SessionId": "", "StreamId": ""},
        description="Any additional information related to the request.",
    )


class EventResponse(BaseModel):
    UserId: str = Field(
        default="",
        description="Unique identifier to recognize which user is interacting with the Chat Engine. This is populated from the request JSON.",
    )
    Events: List[Dict[str, Any]] = Field(
        default=[], description="The generated event list for the provided EventType from Chat Engine."
    )
    Response: ResponseField = Field(
        default=ResponseField(),
        description="Final response template from the Chat Engine. This field can be picked up from domain rule files or can be formulated directly from custom fulfillment modules.",
    )
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/rag_bot/flows.co
````
define flow
  user ...
  $answer = execute chat_plugin(\
    endpoint="rag/chat",\
    )
  bot respond

define bot respond
  "{{$answer}}"
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/rag_bot/model_config.yaml
````yaml
model_servers:
 - name: riva
   url: "localhost:8001"
   speech_models:
    - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
    - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/rag_bot/plugin_config.yaml
````yaml
config:
  workers: 1
  timeout: 30

plugins:
  - name: rag
    path: plugins/rag.py
    parameters:
      RAG_SERVER_URL: "http://localhost:8081"
      # STOP_WORDS: ["\n"]  # Optional parameters for RAG
      # TEMPERATURE: 0.2
      # TOP_K: 0.7
      # MAX_TOKENS: 200
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/rag_bot/rag_bot_config.yml
````yaml
bot: genai_rag_bot

models: []
streaming: True
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/rag_bot/README.md
````markdown
# Using RAG in ACE Agent
## Introduction

ACE Agent allows developers to create chatbots which interact with an independently deployed [RAG chain server](https://github.com/NVIDIA/GenerativeAIExamples). If enabled, ACE Agent will redirect all questions to the RAG chain server. This enables RAG use cases with all of the interfaces and integrations available to ACE Agent.

## Usage
- Ensure that there is a RAG server deployed. The default URL expected by ACE Agent is ``http://localhost:8081``. If the server is deployed at a different URL, specify it in the plugin config file, like below:
    ```shell
    plugins:
      - name: rag
        parameters:
          RAG_SERVER_URL: "http://<your-ip>"
    ```
    Also, ensure that the required documents are ingested into the RAG server. ACE Agent is not responsible for document ingestion.

- In the bot config file, ensure that the bot name begins with the prefix ``rag``. This enables the RAG policy which redirects queries to the ``/generate`` endpoint of the RAG server.
- Start and interact with the bot similar to other ACE Agent bots.
- In server mode of ACE Agent, both streaming and non-streaming endpoints are compatible with RAG policy.

# Deplot the bot
1. Set the BOT_PATH environment variable relative to the current directory.
    ```
    export BOT_PATH=./samples/colang_1.0/rag_bot/
    ```
2. Update the PIPELINE in file `deploy/docker/docker_init.sh` to `speech_lite`:
    ```
    export PIPELINE=speech_lite
    ```
3. Set the environment variables required for `docker-compose.yaml` by sourcing `deploy/docker/docker_init.sh`.
    ```
    source deploy/docker/docker_init.sh
    ```
4. Deploy the Speech and NLP models required for the bot which might take 20-40 minutes for the first time. For the rag sample bot, Riva ASR (Automatic Speech Recognition) and TTS (Text to Speech) models will be deployed.
    ```
    docker compose -f deploy/docker/docker-compose.yml up model-utils-speech
    ```
5. Deploy the ACE Agent Microservices. The following command deploys the Chat Controller, Chat Engine, Plugin server, and NLP Server Microservices.
    ```
    docker compose -f deploy/docker/docker-compose.yml up speech-bot -d
    ```
6. Wait for a few minutes for all services to be ready. You can check the Docker logs for individual microservices to confirm. You will see log print ``Server listening on 0.0.0.0:50055`` in the Docker logs for the Chat Controller container.
7. You can interact with the bot using the URL ``http://<workstation IP>:7006``.
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/rag_bot/speech_config.yaml
````yaml
grpc_server:
  nvidia::rrt::BotRuntimeGrpc: # component type
    ip_address: "0.0.0.0"
    port_number: 50055
    virtual_assistant_num_instances: 30
    virtual_assistant_pipeline_idle_threshold_secs: 600
    virtual_assistant_pipeline_idle_handler_wakeup_rate_secs: 10

speech_pipeline_manager: # config name
  SpeechPipelineManager: # component name
    asr_idle_timeout_ms: 200000
    tts_eos_delay_ms: 2000

riva_asr:
  RivaASR:
    server: "localhost:50051"
    word_boost_file_path: "/workspace/config/asr_words_to_boost.txt"
    enable_profanity_filter: false

dialog_manager:
  DialogManager:
    server: "http://localhost:9000"
    use_streaming: true

riva_tts:
  RivaTTS:
    server: "localhost:50051"
    voice_name: "English-US.Female-1"
    language: "en-US"
    ipa_dict: ""
    sample_rate: 44100
    chunk_duration_ms: 100
    audio_start_threshold_ms: 400
    send_audio_in_realtime: true
    tts_mode: "grpc"

riva_logger:
  RivaLogger:
    data_dump_path: "/workspace/log"
    enable_logging: true
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/spanish_bot_nmt/colang/date.co
````
define user ask current date
  "What is the current time?"
  "What time is it now?"
  "What day is today?"
  "What time is it?"
  "What is today's date?"
  "What is the current date?"

define flow current date
  user ask current date
  $date = execute plugin(endpoint="date/get_date")
  bot provide $date
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/spanish_bot_nmt/colang/general.co
````
define user express greeting
  "Good morning"
  "Good afternoon"
  "Good night"
  "Goodbye"
  "Thank you"
  "Hello"

define user ask how are you
  "How are you?"

define flow user greets
  user express greeting
  bot express greeting

define flow how are you
  user ask how are you
  bot responds accordingly
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/spanish_bot_nmt/colang/open_domain.co
````
define user ask open domain query
  "What is the human life expectancy in the United States?"
  "Who is the Prime Minister of India?"
  "What is the capital of Spain?"
  "How far is the sun from the earth?"

define flow open domain
  user ask open domain query
  bot responds truthfully
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/spanish_bot_nmt/colang/weather.co
````
define user ask current weather
  "What is the current weather condition in Santa Clara?"
  "How is the current weather in New York?"
  "How is the weather condition in Pune?"
  "Can you tell me the weather conditions in San Francisco?"
  "What is the temperature in Tokyo?"
  "Will it be cold in Allardt?"
  "How is the temperature in Moscow?"
  "Show me the temperature in San Mateo."
  "Is it hot in Mumbai?"

define flow current weather
  user ask current weather

  # Get the location from user's previous spanish query, for which user is asking weather queries.
  # If no location is provided, just leave the location as ""
  $location = ...

  if $location
    $weather_condition = execute plugin(endpoint="weather/weatherstack/get_weather_condition", request_type="get", location=$location)
    $temperature = execute plugin(endpoint="weather/weatherstack/get_temperature", request_type="get", location=$location)

    bot provide $weather_condition and $temperature

  else
    bot inform could not find location

define user ask about humidity
  "What is the humidity in Tokyo?"
  "How humid was it in Moscow?"
  "What will the humidity chances be in Santa Clara?"
  "Please show me the humidity chances in Toronto."

define flow humidity
  user ask about humidity

  # Get the location from user's previous spanish query, for which user is asking weather queries.
  # If no location is provided, just leave the location as ""
  $location = ...

  if $location
    $humidity = execute plugin(endpoint="weather/weatherstack/get_humidity", request_type="get", location=$location)
    bot provide $humidity
  else
    bot inform could not find location

define user asks whether it is cloudy
  "Is it cloudy in Bali?"
  "Will it be cloudy in Moscow?"
  "Will it be cloudy in Montreal?"

define flow cloudy
  user asks whether it is cloudy

  # Get the location from user's previous spanish query, for which user is asking weather queries.
  # If no location is provided, just leave the location as ""
  $location = ...

  if $location
    $is_cloudy = execute plugin(endpoint="weather/weatherstack/is_cloudy", request_type="get", location=$location)
    bot provide $is_cloudy
  else
    bot inform could not find location

define user asks weather it is raining
  "What is the rain forecast for Danville?"
  "Will it rain in Delhi?"
  "How much will it rain in San Francisco?"
  "What are the chances of rain in Santa Clara?"
  "Is it raining in Santa Clara?"

define flow rainy
  user asks weather it is raining

  # Get the location from user's previous spanish query, for which user is asking weather queries.
  # If no location is provided, just leave the location as ""
  $location = ...

  if $location
    $is_raining = execute plugin(endpoint="weather/weatherstack/is_raining", request_type="get", location=$location)
    bot provide $is_raining
  else
    bot inform could not find location

define user asks about windspeed
  "What will the wind speed be in the Bay Area?"
  "How is the wind speed in Moscow?"
  "Show me the wind speed in San Mateo."

define flow windy
  user asks about windspeed

  # Get the location from user's previous spanish query, for which user is asking weather queries.
  # If no location is provided, just leave the location as ""
  $location = ...

  if $location
    $windspeed = execute plugin(endpoint="weather/weatherstack/get_windspeed", request_type="get", location=$location)
    bot provide $windspeed
  else
    bot inform could not find location

define user asks whether it is sunny
  "Is it sunny in Bali?"
  "Will it be sunny in Moscow?"
  "Will it be sunny in Montreal?"

define flow sunny
  user asks whether it is sunny

  # Get the location from user's previous spanish query, for which user is asking weather queries.
  # If no location is provided, just leave the location as ""
  $location = ...

  if $location
    $is_sunny = execute plugin(endpoint="weather/weatherstack/is_sunny", request_type="get", location=$location)
    bot provide $is_sunny
  else
    bot inform could not find location
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/spanish_bot_nmt/model_config.yaml
````yaml
model_servers:
  - name: riva
    url: "localhost:8001" # triton grpc url
    riva_url: "localhost:50051" # Riva GRPC API, Needed for NMT models
    nlp_models:
      - nvidia/riva/rmir_megatronnmt_any_en_500m:2.17.0 # NMT any language to english model
      - nvidia/riva/rmir_megatronnmt_en_any_500m:2.17.0 # NMT english to any language model
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/spanish_bot_nmt/plugin_config.yaml
````yaml
config:
  workers: 1
  timeout: 30

plugins:
  - name: weather
  - name: date
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/spanish_bot_nmt/README.md
````markdown
# SPANISH BOT NMT USING COLANG
Spanish NMT bot provides real-time weather data, provides current date and time information and answers open domain question in spanish.
It internally utilizes Riva NMT to translate a query from Spanish to English and vice-versa for responses.

## Setting up environment
1. Set OpenAI API Key
    ```
    export OPENAI_API_KEY=<OPENAI_API_KEY>
    ```

## Features
This bot has the following features and functionalities -
1. Weather forecast
2. Temperature
3. Wind Speed
4. Humidity
5. Precipitation
6. Whether the weather condition is Sunny or Cloudy at a given location.
7. Current Date and time.
8. Open domain QnA.


## Deplot the bot
1. Set the BOT_PATH environment variable relative to the current directory.
    ```
    export BOT_PATH=./samples/colang_1.0/spanish_bot_nmt/
    ```
2. Update the PIPELINE in file `deploy/docker/docker_init.sh` to `speech_lite`:
    ```
    export PIPELINE=speech_lite
    ```
3. Set the environment variables required for `docker-compose.yaml` by sourcing `deploy/docker/docker_init.sh`.
    ```
    source deploy/docker/docker_init.sh
    ```
4. Deploy the Speech and NLP models required for the bot which might take 20-40 minutes for the first time. For the spanish nmt bot sample bot, Riva ASR (Automatic Speech Recognition), TTS (Text to Speech) and NMT (Neural Machine Translation) models will be deployed.
    ```
    docker compose -f deploy/docker/docker-compose.yml up model-utils-speech
    ```
5. Deploy the ACE Agent Microservices. The following command deploys the Chat Controller, Chat Engine, Plugin server, and NLP Server Microservices.
    ```
    docker compose -f deploy/docker/docker-compose.yml up speech-bot -d
    ```
6. Wait for a few minutes for all services to be ready. You can check the Docker logs for individual microservices to confirm. You will see log print ``Server listening on 0.0.0.0:50055`` in the Docker logs for the Chat Controller container.
7. You can interact with the bot using the URL ``http://<workstation IP>:7006``.
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/spanish_bot_nmt/spanish_bot_nmt.yml
````yaml
bot: spanish_bot_nmt

configs:
  language: en-US
  request_language: es-US
  response_language: es-US

instructions:
  - type: general
    content: |
      Below is a conversation between a user and a weather bot that provides real-time weather conditions based on location provided by the user.
      The bot is factual and concise. It ensures that any location provided by user is not imaginary.
      It provides user with all weather information. Bot informs user when a location is not on the world map.

sample_conversation: |
  user "Hello there!"
    express greeting
  bot express greeting
    "Hello!, how can I help you today?"

# Using OpenAI
models:
  - type: main
    engine: openai
    model: gpt-3.5-turbo-instruct
    parameters:
      stop: ["\n"]

nlp_models:
  - task_name: translate_user_query
    model_name: megatronnmt_any_en_500m

  - task_name: translate_bot_response
    model_name: megatronnmt_en_any_500m
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/stock_bot/colang/contextual_query.co
````
define user asks contextual query
  "what about microsoft?"
  "How about amazon?"
  "Tell me about Google"

define bot inform internal error occured
  "Sorry I can't understand what you are referring to here. Can you rephrase your query?"

define flow
  user asks contextual query

  # Generate the company name from user's input. If the company name is not specified, return "unknown".
  # Return only the name of the company in quotes, not an expression to calculate the name of the company.
  # For example, if the input is "What is the share price of Amazon?", return "Amazon"
  # For example, if the input is "How much does a share of microsoft cost?", return "microsoft"
  $company_name = ...

  if $company_name == "unknown"
    bot inform internal error occured
    return
  else
    $global_company_name = $company_name
    break

  $price = execute plugin(endpoint="/stock/get_stock_price", company_name=$global_company_name)
  if not $price
    bot respond that it could not find the stock price
  else
    bot answer query based on $global_intent
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/stock_bot/colang/fallback.co
````
define flow ...
    priority 0.1
    user ...
    bot responds truthfully
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/stock_bot/colang/general.co
````
define user express greeting
  "hi"
  "hello"
  "hey"

define user ask for name
  "What is your name?"

define user request help
  "I need help"
  "Can you help me with something?"

define user request repeat
  "Please repeat that"
  "repeat"
  "What was that?"

define user express sadness
  "I am sad"
  "I feel sad"

define flow
  user express greeting
  bot express greeting

define flow
  user ask for name
  bot inform own name

define flow
  user ask age
  bot inform own age

define flow
  user ask home
  bot inform own home

define bot express greeting
  "Greetings! I am Enola and I'm here to assist you."

define bot express thank you for information
  "Thanks for this information."

define bot inform own name
  "My name is Enola."

define bot bot offer additional help
  "If you have any more questions or if there's anything else I can help you with, please don't hesitate to ask."
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/stock_bot/colang/off_topic.co
````
define user ask off topic
  "Can you recommend a place to eat?"
  "Do you know any restaurants?"
  "Can you paint?"
  "Can you tell me a joke?"
  "What is the biggest city in the world"
  "Can you write an email?"
  "I need you to write an email for me."
  "Who is the president?"
  "What party will win the elections?"
  "Who should I vote with?"
  "Which is the best restaurant?"
  "What is the weather?"
  "Which country has the largest population in the world?"
  "What is the capital of USA?"
  "What is 2+2= ?"

define flow
  user ask off topic
  bot explain cant off topic

define bot explain cant off topic
  "Sorry, I cannot comment on anything which is not relevant to stock market and stocks."
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/stock_bot/colang/profanity_rail.co
````
define user says profane words
    "you are stupid "
    "i want to kill you"
    "add a shit burger"
    "piss off"

define bot asks to avoid abusive language
    "Please do not use abusive language."

define flow
    user says profane words
    bot asks to avoid abusive language
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/stock_bot/colang/stock_faq.co
````
define user queries about stocks
  "What is a stock?"
  "define stocks"
  "How do you describe a stock?"

define flow
  user queries about stocks
  bot responds with definition of stocks


define user queries about primary markets and secondary markets
  "What are primary markets?"
  "What are secondary markets?"
  "Difference between primary markets and secondary markets"
  "what are different types of stock market?"

define flow
  user queries about types of stock market
  bot responds with information on types of stock market


define user queries about share markets
  "What is share market?"
  "define share market"
  "Explain share market."

define flow
  user queries about share markets
  bot responds with information about share markets


define user queries about stock indices
  "What is a stock index?"
  "What are stock indices?"
  "describe stock index"

define flow
  user queries about stock indices
  bot responds with information about stock indices


define user queries about broker
  "Define a broker"
  "Who is a broker?"
  "What is the role of a broker in share market?"

define flow
  user queries about broker
  bot responds with information about broker


define user queries about ipo
  "What is ipo?"
  "define ipo"
  "Explain ipo."
  "What is initial public offer?"

define flow
  user queries about ipo
  bot responds with information about ipo


define user queries high level categories of stocks
  "What are different types of stocks?"
  "What are different categories of stocks?"
  "Categories of stocks."

define flow
  user queries high level categories of stocks
  bot responds with high level categories of stocks


define user queries about types of stocks based on market capitalization
  "What are different types of stocks based on market capitalization?"
  "List difeerent types of stocks based on market capitalization."

define flow
  user queries about types of stocks based on market capitalization
  bot responds with types of stocks based on market capitalization


define user queries about types of stocks based on ownership
  "What are different types of stocks based on ownership?"
  "List difeerent types of stocks based on ownership."

define flow
  user queries about types of stocks based on ownership
  bot responds with types of stocks based on ownership


define user queries about types of stocks based on fundamentals
  "What are different types of stocks based on fundamentals?"
  "List difeerent types of stocks based on fundamentals."

define flow
  user queries about types of stocks based on fundamentals
  bot responds with types of stocks based on fundamentals


define user queries about types of stocks based on price volatility
  "What are different types of stocks based on price volatility?"
  "List difeerent types of stocks based on price volatility."

define flow
  user queries about types of stocks based on price volatility
  bot responds with types of stocks based on price volatility


define user queries about types of stocks based on profit sharing
  "What are different types of stocks based on profit sharing?"
  "List difeerent types of stocks based on profit sharing."

define flow
  user queries about types of stocks based on profit sharing
  bot responds with types of stocks based on profit sharing


define user queries about types of stocks based on economic trends
  "What are different types of stocks based on economic trends?"
  "List difeerent types of stocks based on economic trends."

define flow
  user queries about types of stocks based on economic trends
  bot responds with types of stocks based on economic trends


define bot remove last message
  "(remove last message)"
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/stock_bot/colang/stock_price.co
````
define user asks stock price
  "What is the stock price of Microsoft?"
  "How much does an Nvidia stock cost"
  "what is the value of amazon stock?"
  "What is it's stock price?"

define flow
  user asks stock price

  $count = 0
  while $count < 3
    # Generate the company name from user's input. If the company name is not specified, return "unknown".
    # Return only the name of the company in quotes, not an expression to calculate the name of the company.
    # For example, if the input is "What is the share price of Amazon?", return "Amazon"
    # For example, if the input is "How much does a share of microsoft cost?", return "microsoft"
    $company_name = ...

    if $company_name == "unknown"
      if $global_company_name == None
        bot asks company name
        user tells company name
        $count = $count + 1
      else
        break
    else
      $global_company_name = $company_name
      break

  if $count == 3
    bot inform maximum tries exceeded
    return

  $global_intent = "price"
  $price = execute plugin(endpoint="/stock/get_stock_price", company_name=$global_company_name)
  if not $price
    bot respond that it could not find the stock price
  else
    bot tell stock price
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/stock_bot/kb/stock_faq.md
````markdown
This document answers all questions and queries related to stocks and share market.
---
<br>

## What is a stock?

A stock, also known as equity, is a security that represents the ownership of a fraction of the issuing corporation. Units of stock are called "shares" which entitles the owner to a proportion of the corporation's assets and profits equal to how much stock they own.

## High Level Categories of Stocks

There are mainly six criterias under which the stocks are categorised - Market capitalization, Ownership, Fundamentals, Price votality, Profit sharing and Economic trends.

## What is share market?

The share market, also known as the stock market, is a platform where buyers and sellers come together to trade publicly listed shares of companies. A share in market parlance is part ownership in a company. So if a company has issued 100 shares and you own 1 share then you own 1% stake in the company. Share market is where shares of different companies are traded.

## Types of stock market

There are 2 types of stock market -

- Primary Market - When a company comes out with an initial public offer (IPO) it is called the primary market. It creates securities and acts as a platform where firms float their new stock options and bonds for the general public to acquire.

- Secondary Market - Once the share gets listed and bought, it starts trading further in the secondary market. Here, investors trade in securities without involving the companies who issued them in the first place with the help of brokers.

## What are stock indices?

From the companies listed in the stock exchanges, a few similar stocks are grouped together to form an index. The classification may be on the basis of company size, industry, market capitalization, or other categories. For example the three most widely followed indexes in the US are the S&P 500, Dow Jones Industrial Average, and Nasdaq Composite.

## Who is a broker?

A broker helps you execute your buy and sell trades. Brokers typically help buyers find sellers and sellers find buyers. Most brokers will also advise you on what stocks to buy, what stocks to sell and how to invest money in share markets for beginners. For that service, the broker is paid brokerage.

## What is IPO?

Initial Public Offer (IPO) is the selling of securities to the public in the primary market. It is the largest source of funds with long or indefinite maturity for the company. The normal purpose of an IPO is to get the stock listed in the share market.

## What is Bear and Bull Market?

Bear markets refers to a fall in stock prices and economy. Whereas, in bull market, the companies tend to generate more revenue and hence, the stock prices go up.

---

## Types of Stocks Based on Market Capitalization

There are three main types of stocks in this category -

- Large-cap Stocks - The top 100 companies in terms of market capitalization. These companies generally have large market caps and are often considered to be more stable and less risky than mid-cap and small-cap stocks.

- Mid-cap Stocks - Those ranking between 101 and 250 in the list of companies as per market capitalization. Mid-cap companies tend to have higher growth rates, but they're also more sensitive to economic cycles and industry trends, so they can be less predictable than large-cap stocks.

- Small-cap Stocks - All the remaining companies. The major chunk of the market consists of small-cap companies. Small-cap stocks tend to have higher volatility than the other two categories because of the size and liquidity.

---

## Types of Stocks Based on Ownership

There are five main types of stocks in this category -

- Common Stock - Stockholders having common stocks are eligible to receive a part of the companys profits via dividends.

- Preferred Stock - These stocks receive promised dividends that are not available with common stocks. Also, if the company liquidates, then these stocks get preference over common stocks.

- Hybrid Stocks - Hybrid stocks combine features from both preferred and common stocks. The most common type is the convertible bond which allows investors to convert their bonds into equity or debt.

- Convertible Preference Stocks - These are initially issued as preference stocks that are converted into a fixed number of common stocks at a specific time. The company can decide whether to offer voting rights with these stocks or not.

- Stocks With Embedded Derivative Options - Once a company issues shares, it usually doesnt buy them back unless it deems fit. However, some companies issue stocks with embedded derivative options  call-able or put-able. In a call-able option, the company can buy back its stocks at a specific price or a specific time. In the put-able option, the company can provide the investor with an option to sell the stock back to the company at a specific price or a specific time. These are not commonly issued by companies.

---

## Types of Stocks Based on Fundamentals

There are two main types of stocks in this category -

- Overvalued Stocks - These are stocks that have a market price that cannot be justified by their earnings outlook. Hence, the market price of such stocks is higher than their intrinsic value.

- Undervalued Stocks - These stocks have a market price lower than their intrinsic value.

---

## Types of Stocks Based on Price Volatility

There are two main types of stocks in this category -

- Beta Stocks - Investment analysts use a statistical measure called the coefficient of beta to find the volatility in stock prices. If a stock has a higher beta, it means that the investment risk is higher.

- Blue-chip Stocks - These are the most stable stocks since the companies are well established.

---

## Types of Stocks Based on Profit Sharing

There are two main types of stocks in this category -

- Income Stocks - These stocks offer consistent dividend payouts. They are called income stocks since they can add to the income of the shareholder. These stocks usually belong to companies that have strong finances and can share dividends from their profits every year. However, since the profits are distributed, these companies grow at a steady pace and are considered low-risk investments.

- Growth Stocks - These stocks dont pay dividends. Instead, the company reinvests its profits to grow its business. Such companies aggressively seek growth and the prices of their stocks grow rapidly. This offers the stockholder an opportunity to earn profit by selling the stocks and making capital gains. These are considered riskier than income stocks since the profits are based on the market price that can fluctuate for reasons beyond the control of the company.

---

## Types of Stocks Based on Economic Trends

There are two main types of stocks in this category -

- Cyclical stocks - These stocks move in sync with the economy. Hence, when the economic trends are negative, the prices of these stocks drop and vice versa. Investing in such stocks is usually beneficial in a booming economy.

- Defensive stocks - These stocks dont react strongly to economic trends. Some examples of such stocks are food, medicines, insurance, etc. These are considered safer to invest in.
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/stock_bot/plugins/yahoo_fin.py
````python
# Copyright(c) 2023 NVIDIA Corporation. All rights reserved.

# NVIDIA Corporation and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA Corporation is strictly prohibited.

from yahoo_fin import stock_info as si
import requests
import logging
from typing import Optional
from fastapi import APIRouter

# API to extract stock price
Y_TICKER = "https://query2.finance.yahoo.com/v1/finance/search"
Y_FINANCE = "https://query1.finance.yahoo.com/v7/finance/quote?symbols="

router = APIRouter()
logger = logging.getLogger("plugin")

logger.info("Starting stock plugin server")

# Prepare headers for requests
session = requests.Session()
user_agent = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
)
session.headers.update({"User-Agent": user_agent})


def get_ticker_symbol_alphavantage(stock_name: str) -> Optional[str]:
    # We do not need actual api key to get ticker info
    # But it is required as place holder
    api_key = "YOUR_ALPHA_VANTAGE_API_KEY"
    url = f"https://www.alphavantage.co/query?function=SYMBOL_SEARCH&keywords={stock_name}&apikey={api_key}"
    response = requests.get(url)
    data = response.json()

    if "bestMatches" in data and len(data["bestMatches"]) > 0:
        ticker_symbol = data["bestMatches"][0]["1. symbol"]
        return ticker_symbol

    return None


@router.get("/get_ticker")
def get_ticker(company_name: str) -> Optional[str]:
    """
    Take company name returns ticker symbol used for trading
    param
        Args:
            company_name: company name like Microsoft
        Returns:
            Ticker Symbol used for trading like MSFT for microsoft
    """
    logger.info(f"Getting ticker symbol for {company_name}")
    try:
        params = {"q": company_name, "quotes_count": 1, "country": "United States"}
        return session.get(url=Y_TICKER, params=params).json()["quotes"][0]["symbol"]
    except Exception as e:
        logger.error(f"Exception {e} while fetching ticker symbol for {company_name}")
        return get_ticker_symbol_alphavantage(company_name)


@router.get("/get_stock_price")
def get_stock_price(company_name: str) -> Optional[float]:
    """
    get a stock price from yahoo finance api
    """

    logger.info(f"Getting stock price for {company_name}")
    try:
        # Find ticker symbol for stock name, eg. Microsoft : MSFT, Nvidia: NVDA
        logger.info(f"Extracting ticker symbol for {company_name}")
        ticker = get_ticker(company_name)

        live_price = si.get_live_price(ticker)

        logger.info(f"Live Price for {ticker}: {round(live_price, 2)}")
        return round(live_price, 2)

    except ValueError:
        # If ticker is not available update status detail
        logger.error(f"Unable to find stock information of {company_name}")
        return None
    except Exception as e:
        logger.error(f"Unable to find stock price of {company_name}")
        return None
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/stock_bot/cmudict_ipa.txt
````
;;; # CMUdict  --  Major Version: 0.07
;;;
;;; # $HeadURL$
;;; # $Date::                                                   $:
;;; # $Id::                                                     $:
;;; # $Rev::                                                    $:
;;; # $Author::                                                 $:
;;;
;;; #
;;; # ========================================================================
;;; # Copyright (C) 1993-2015 Carnegie Mellon University. All rights reserved.
;;; #
;;; # Redistribution and use in source and binary forms, with or without
;;; # modification, are permitted provided that the following conditions
;;; # are met:
;;; #
;;; # 1. Redistributions of source code must retain the above copyright
;;; #    notice, this list of conditions and the following disclaimer.
;;; #    The contents of this file are deemed to be source code.
;;; #
;;; # 2. Redistributions in binary form must reproduce the above copyright
;;; #    notice, this list of conditions and the following disclaimer in
;;; #    the documentation and/or other materials provided with the
;;; #    distribution.
;;; #
;;; # This work was supported in part by funding from the Defense Advanced
;;; # Research Projects Agency, the Office of Naval Research and the National
;;; # Science Foundation of the United States of America, and by member
;;; # companies of the Carnegie Mellon Sphinx Speech Consortium. We acknowledge
;;; # the contributions of many volunteers to the expansion and improvement of
;;; # this dictionary.
;;; #
;;; # THIS SOFTWARE IS PROVIDED BY CARNEGIE MELLON UNIVERSITY ``AS IS'' AND
;;; # ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
;;; # THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
;;; # PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL CARNEGIE MELLON UNIVERSITY
;;; # NOR ITS EMPLOYEES BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
;;; # SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
;;; # LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
;;; # DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
;;; # THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
;;; # (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
;;; # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
;;; #
;;; # ========================================================================
;;; #
;;;
;;;  NOTES  ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;
;;;  [20080401] (air)  New dict file format introduced
;;;   - comments (like this section) are allowed
;;;   - file name is major version; vers/rev information is now in the header
;;;
;;;
;;; ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;
!EXCLAMATION-POINT  kskmenpnt
3-D  idi
3D  idi
GPU  dipiju
HOUR  a
HOUR(1)  a
_.'-  i
NVDA  'nvidie
AMZN  amzn
TSLA  tisl
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/stock_bot/model_config.yaml
````yaml
model_servers:
  - name: riva
    speech_models:
      - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
      - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
    url: localhost:8001
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/stock_bot/plugin_config.yaml
````yaml
config:
  workers: 1
  timeout: 30

plugins:
  - name: stock
    path: plugins/yahoo_fin.py
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/stock_bot/README.md
````markdown
# STOCK FAQ BOT USING COLANG
Stock FAQ bot is able to answer queries related to stocks and stock market.

## Setting up environment
1. Set NVIDIA API Key
    ```
    export NVIDIA_API_KEY=<NVIDIA_API_KEY>
    ```

## Features
This bot has the following features and functionalities -
1. Get stock price of a particular organization
2. Answer queries related to stock market and related concepts

## Deplot the bot
1. Set the BOT_PATH environment variable relative to the current directory.
    ```
    export BOT_PATH=./samples/colang_1.0/stock_bot/
    ```
2. Update the PIPELINE in file `deploy/docker/docker_init.sh` to `speech_lite`:
    ```
    export PIPELINE=speech_lite
    ```
3. Set the environment variables required for `docker-compose.yaml` by sourcing `deploy/docker/docker_init.sh`.
    ```
    source deploy/docker/docker_init.sh
    ```
4. Deploy the Speech and NLP models required for the bot which might take 20-40 minutes for the first time. For the stock sample bot, Riva ASR (Automatic Speech Recognition) and TTS (Text to Speech) models will be deployed.
    ```
    docker compose -f deploy/docker/docker-compose.yml up model-utils-speech
    ```
5. Deploy the ACE Agent Microservices. The following command deploys the Chat Controller, Chat Engine, Plugin server, and NLP Server Microservices.
    ```
    docker compose -f deploy/docker/docker-compose.yml up speech-bot -d
    ```
6. Wait for a few minutes for all services to be ready. You can check the Docker logs for individual microservices to confirm. You will see log print ``Server listening on 0.0.0.0:50055`` in the Docker logs for the Chat Controller container.
7. You can interact with the bot using the URL ``http://<workstation IP>:7006``.
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/stock_bot/speech_config.yaml
````yaml
grpc_server:
  nvidia::rrt::BotRuntimeGrpc: # component type
    ip_address: "0.0.0.0"
    port_number: 50055
    virtual_assistant_num_instances: 30
    virtual_assistant_pipeline_idle_threshold_secs: 600
    virtual_assistant_pipeline_idle_handler_wakeup_rate_secs: 10

speech_pipeline_manager: # config name
  SpeechPipelineManager: # component name
    asr_idle_timeout_ms: 200000
    tts_eos_delay_ms: 2000

riva_asr:
  RivaASR:
    server: "localhost:50051"
    word_boost_file_path: "/workspace/config/asr_words_to_boost.txt"
    enable_profanity_filter: false

dialog_manager:
  DialogManager:
    server: "http://localhost:9000"
    use_streaming: true

riva_tts:
  RivaTTS:
    server: "localhost:50051"
    voice_name: "English-US.Female-1"
    language: "en-US"
    ipa_dict: ""
    sample_rate: 44100
    chunk_duration_ms: 100
    audio_start_threshold_ms: 400
    send_audio_in_realtime: true
    tts_mode: "grpc"

riva_logger:
  RivaLogger:
    data_dump_path: "/workspace/log"
    enable_logging: true
````

## File: microservices/ace_agent/4.1/samples/colang_1.0/stock_bot/stock_bot_config.yml
````yaml
bot: stock_bot

instructions:
  - type: general
    content: |
      Below is a conversation between a user and a stock faq bot named Enola that provides stock prices of companies and corporations provided by the user.
      It also provides user with information about stocks and stock market. The bot is factual and concise. Bot informs user when a company is imaginary.

sample_conversation: |
  user "Hello there!"
    express greeting
  bot express greeting
    "Hello! I am Enola, how can I help you today?"

models:
  - type: main
    engine: nvidia-ai-endpoints
    model: ai-mixtral-8x7b-instruct
    parameters:
      stop: ["\n"]
      max_tokens: 100
      # base_url: "http://0.0.0.0:9999/v1"  # Use this to use NIM model
````

## File: microservices/ace_agent/4.1/samples/ddg_langchain_bot/plugins/langchain_agent.py
````python
# Copyright(c) 2024 NVIDIA Corporation. All rights reserved.

# NVIDIA Corporation and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA Corporation is strictly prohibited.

from fastapi import APIRouter, status, Body, Response
from fastapi.responses import StreamingResponse
import logging
import os
import sys
from typing_extensions import Annotated
from typing import Union, Dict
import json

from langchain_community.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.memory import ChatMessageHistory
from langchain.tools.ddg_search import DuckDuckGoSearchRun
from langchain_core.runnables import (
    RunnableParallel,
    RunnablePassthrough,
)
from langchain.tools import tool

logger = logging.getLogger("plugin")
router = APIRouter()

sys.path.append(os.path.dirname(__file__))

from schemas import ChatRequest, EventRequest, EventResponse, ChatResponse

EVENTS_NOT_REQUIRING_RESPONSE = [
    "system.event_pipeline_acquired",
    "system.event_pipeline_released",
    "system.event_exit",
]

duckduckgo = DuckDuckGoSearchRun()


@tool
def ddg_search(query: str):
    """Performs a duckduck go search"""

    logger.info(f"Input to DDG: {query}")
    answer = duckduckgo.run(query)
    logger.info(f"Answer from DDG: {answer}")
    return answer


rephraser_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            f"You are an assistant whose job is to rephrase the question into a standalone question, based on the conversation history."
            f"The rephrased question should be as short and simple as possible. Do not attempt to provide an answer of your own!",
        ),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{query}"),
    ]
)

wiki_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "Answer the given question from the provided context. Only use the context to form an answer.\nContext: {context}",
        ),
        ("user", "{query}"),
    ]
)

chat_history_map = {}
llm = ChatOpenAI(model="gpt-4-turbo")
output_parser = StrOutputParser()

chain = (
    rephraser_prompt
    | llm
    | output_parser
    | RunnableParallel({"context": ddg_search, "query": RunnablePassthrough()})
    | wiki_prompt
    | llm
    | output_parser
)

chain_with_history = RunnableWithMessageHistory(
    chain,
    lambda session_id: chat_history_map.get(session_id),
    input_messages_key="query",
    history_messages_key="history",
)


@router.post(
    "/chat",
    status_code=status.HTTP_200_OK,
)
async def chat(
    request: Annotated[
        ChatRequest,
        Body(
            description="Chat Engine Request JSON. All the fields populated as part of this JSON is also available as part of request JSON."
        ),
    ],
    response: Response,
) -> StreamingResponse:
    """
    This endpoint can be used to provide response to query driven user request.
    """

    req = request.dict(exclude_none=True)
    logger.info(f"Received request JSON at /chat endpoint: {json.dumps(req, indent=4)}")

    try:

        session_id = req["UserId"]
        question = req["Query"]

        if session_id not in chat_history_map:
            chat_history_map[session_id] = ChatMessageHistory(messages=[])

        def generator(question: str, session_id: str):

            full_response = ""
            if question:
                for chunk in chain_with_history.stream(
                    {"query": question}, config={"configurable": {"session_id": session_id}}
                ):
                    if not chunk:
                        continue
                    full_response += chunk

                    json_chunk = ChatResponse()
                    json_chunk.Response.Text = chunk
                    json_chunk.Response.CleanedText = chunk
                    json_chunk = json.dumps(json_chunk.dict())
                    yield json_chunk

            json_chunk = ChatResponse()
            json_chunk.Response.IsFinal = True
            json_chunk = json.dumps(json_chunk.dict())
            yield json_chunk

        return StreamingResponse(generator(question, session_id), media_type="text/event-stream")

    except Exception as e:
        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
        return {"StatusMessage": str(e)}


@router.post("/event", status_code=status.HTTP_200_OK)
async def event(
    request: Annotated[
        EventRequest,
        Body(
            description="Chat Engine Request JSON. All the fields populated as part of this JSON is also available as part of request JSON."
        ),
    ],
    response: Response,
) -> Union[EventResponse, Dict[str, str]]:
    """
    This endpoint can be used to provide response to an event driven user request.
    """

    req = request.dict(exclude_none=True)
    logger.info(f"Received request JSON at /event endpoint: {json.dumps(req, indent=4)}")

    try:
        resp = EventResponse()
        resp.UserId = req["UserId"]
        resp.Response.IsFinal = True

        if req["EventType"] in EVENTS_NOT_REQUIRING_RESPONSE:
            resp.Response.NeedUserResponse = False

        return resp
    except Exception as e:
        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
        return {"StatusMessage": str(e)}
````

## File: microservices/ace_agent/4.1/samples/ddg_langchain_bot/plugins/requirements_dev.txt
````
langchain==0.1.1
langchain-community==0.0.13
langchain-core==0.1.12
duckduckgo-search==5.3.1b1
````

## File: microservices/ace_agent/4.1/samples/ddg_langchain_bot/plugins/schemas.py
````python
# Copyright(c) 2024 NVIDIA Corporation. All rights reserved.

# NVIDIA Corporation and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA Corporation is strictly prohibited.

from pydantic import BaseModel, Field
from typing import Optional, Dict, List, Any


class ChatRequest(BaseModel):
    Query: Optional[str] = Field(default="", description="The user query which needs to be processed.")
    UserId: str = Field(
        description="Mandatory unique identifier to recognize which user is interacting with the Chat Engine."
    )
    Metadata: Optional[Dict[str, Any]] = Field(
        default={},
        description="Any additional information related to the request.",
    )


class EventRequest(BaseModel):
    EventType: str = Field(default="", description="The event name which needs to be processed.")
    UserId: str = Field(
        description="Mandatory unique identifier to recognize which user is interacting with the Chat Engine."
    )


class ResponseField(BaseModel):
    Text: str = Field(
        default="",
        description="Text response to be sent out. This field will also be picked by a Text to Speech Synthesis module if enabled for speech based bots.",
    )
    CleanedText: str = Field(
        default="", description="Text response from the Chat Engine with all SSML/HTML tags removed."
    )
    NeedUserResponse: Optional[bool] = Field(
        default=True,
        description="This field can be used by end user applications to deduce if user response is needed or not for a dialog initiated query. This is set to true automatically if form filling is active and one or more slots are missing.",
    )
    IsFinal: bool = Field(
        default=False,
        description="This field to indicate the final response chunk when streaming. The chunk with IsFinal=true will contain the full Chat Engine response attributes.",
    )


class ChatResponse(BaseModel):
    UserId: str = Field(
        default="",
        description="Unique identifier to recognize which user is interacting with the Chat Engine. This is populated from the request JSON.",
    )
    QueryId: str = Field(
        default="",
        description="Unique identifier for the user query assigned automatically by the Chat Engine unless specified in request JSON.",
    )
    Response: ResponseField = Field(
        default=ResponseField(),
        description="Final response template from the Chat Engine. This field can be picked up from domain rule files or can be formulated directly from custom fulfillment modules.",
    )
    Metadata: Optional[Dict[str, Any]] = Field(
        default={"SessionId": "", "StreamId": ""},
        description="Any additional information related to the request.",
    )


class EventResponse(BaseModel):
    UserId: str = Field(
        default="",
        description="Unique identifier to recognize which user is interacting with the Chat Engine. This is populated from the request JSON.",
    )
    Events: List[Dict[str, Any]] = Field(
        default=[], description="The generated event list for the provided EventType from Chat Engine."
    )
    Response: ResponseField = Field(
        default=ResponseField(),
        description="Final response template from the Chat Engine. This field can be picked up from domain rule files or can be formulated directly from custom fulfillment modules.",
    )
````

## File: microservices/ace_agent/4.1/samples/ddg_langchain_bot/model_config.yaml
````yaml
model_servers:
 - name: riva
   url: "localhost:8001"
   speech_models:
    - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
    - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
````

## File: microservices/ace_agent/4.1/samples/ddg_langchain_bot/plugin_config.yaml
````yaml
config:
  workers: 1
  timeout: 30

plugins:
  - name: langchain
    path: ./plugins/langchain_agent.py
````

## File: microservices/ace_agent/4.1/samples/ddg_langchain_bot/README.md
````markdown
# Instructions to run

This bot uses DuckDuckGo to answer questions. Here is somethings you should know before deploying the bot:

Note: The FM requires langchain==1.1, which conflicts with the langchain version used by nemo-guardrails. This affects the native python mode of ACE Agent. To avoid this, create a new virtual environment for the plugin server, where langchain==1.1 will be installed. The original virtual environment should be able to use nemo-guardrails as expected.

This can be avoided by using the bot in the docker flow.

## Sample queries

This bot uses chat history to answer contextual queries.

```shell
Q. Who was the 44th president of the United States?
A. Barack Obama was the 44th US president.

Q. When was he born?
A. Barack Obama was born on 4th August, 1961.

Q. Which state is he from?
A. Barack Obama is from Hawaii.
```
````

## File: microservices/ace_agent/4.1/samples/ddg_langchain_bot/speech_config.yaml
````yaml
grpc_server:
  nvidia::rrt::BotRuntimeGrpc: # component type
    ip_address: "0.0.0.0"
    port_number: 50055
    virtual_assistant_num_instances: 30
    virtual_assistant_pipeline_idle_threshold_secs: 600
    virtual_assistant_pipeline_idle_handler_wakeup_rate_secs: 10

speech_pipeline_manager: # config name
  SpeechPipelineManager: # component name
    asr_idle_timeout_ms: 200000
    tts_eos_delay_ms: 2000

riva_asr:
  RivaASR:
    server: "localhost:50051"
    word_boost_file_path: "/workspace/config/asr_words_to_boost.txt"
    enable_profanity_filter: false

dialog_manager:
  DialogManager:
    server: "http://localhost:9002/langchain"
    use_streaming: true

riva_tts:
  RivaTTS:
    server: "localhost:50051"
    voice_name: "English-US.Female-1"
    language: "en-US"
    ipa_dict: ""
    sample_rate: 44100
    chunk_duration_ms: 100
    audio_start_threshold_ms: 400
    send_audio_in_realtime: true
    tts_mode: "grpc"

riva_logger:
  RivaLogger:
    data_dump_path: "/workspace/log"
    enable_logging: true
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_0/bot_config.yml
````yaml
bot: event_interface_tutorial

storage:
  name: cache

configs:
  use_stateful_guardrails: True

# Using OpenAI
colang_version: "2.x"
models:
    - type: main
      engine: openai
      model: gpt-4-turbo

instructions:
  - type: general
    content: |
      Below is a conversation between a user and a helpful and friendly bot named Emma.
      The bot is factual and concise.

sample_conversation: |
    user action: user said "Hello there!"
    user intent: user expressed greeting

    bot intent: bot express greeting
    bot action: bot express "Hello! How can I assist you today?"
      and bot gesture "Smile"

    user intent: user was silent 15

    bot intent: bot promote asking questions
    bot action: bot gesture "wave"
      and bot say "You can ask me anything you want'"  

    user action: user said "What can you do for me?"
    user intent: user asked about capabilities

    bot intent: bot respond about capabilities
    bot action: bot say "I am here to have a chat with you."

    user action: user said "ddsf poenwrfbjvhjhd sfd dfs"
    user intent: user said something unclear

    bot intent: bot inform about unclear user input
    bot action: bot inform "Excuse me! I did not get that! Can you repeat please?"
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_0/main.co
````
import avatars
import llm
import core

@meta(bot_intent=True)
flow bot express greeting
  bot say "Welcome to the tutorial"

# The main flow is the entry point
@meta(exclude_from_llm=True)
flow main
  
  # Technical flows, see Colang 2.0 documentation for more details
  activate notification of undefined flow start
  activate notification of colang errors
  activate tracking bot talking state

  bot express greeting

  # This will prevent the main flow finishing ever
  wait indefinitely
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_1/bot_config.yml
````yaml
bot: event_interface_tutorial

storage:
  name: cache

configs:
  use_stateful_guardrails: True

# Using OpenAI
colang_version: "2.x"
models:
    - type: main
      engine: openai
      model: gpt-4-turbo

instructions:
  - type: general
    content: |
      Below is a conversation between a user and a helpful and friendly bot named Emma.
      The bot is factual and concise.

sample_conversation: |
    user action: user said "Hello there!"
    user intent: user expressed greeting

    bot intent: bot express greeting
    bot action: bot express "Hello! How can I assist you today?"
      and bot gesture "Smile"
    
    user intent: user was silent 15

    bot intent: bot promote asking questions
    bot action: bot gesture "wave"
      and bot say "You can ask me anything you want'"

    user action: user said "What can you do for me?"
    user intent: user asked about capabilities

    bot intent: bot respond about capabilities
    bot action: bot say "I am here to have a chat with you."

    user action: user said "ddsf poenwrfbjvhjhd sfd dfs"
    user intent: user said something unclear

    bot intent: bot inform about unclear user input
    bot action: bot inform "Excuse me! I did not get that! Can you repeat please?"
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_1/main.co
````
import core
import avatars
import llm

# CHANGE 1: Make greeting multimodal
@meta(bot_intent=True)
flow bot express greeting
  (bot express "Hi there!"
    or bot express "Welcome!"
    or bot express "Hello!")
    and bot gesture "Wave with one hand"

# The main flow is the entry point
@meta(exclude_from_llm=True)
flow main

  # Technical flows, see Colang 2.0 documentation for more details
  activate notification of undefined flow start
  activate notification of colang errors
  activate tracking bot talking state

  # CHANGE 2: The bot greets the user and a welcome message is shown on the UI
  start scene show short information "Welcome to this tutorial interaction" as $intro_ui
  bot express greeting

  # This will prevent the main flow finishing ever
  wait indefinitely
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_2/bot_config.yml
````yaml
bot: event_interface_tutorial

storage:
  name: cache

configs:
  use_stateful_guardrails: True

# Using OpenAI
colang_version: "2.x"
models:
    - type: main
      engine: openai
      model: gpt-4-turbo

instructions:
  - type: general
    content: |
      Below is a conversation between a user and a helpful and friendly bot named Emma.
      The bot is factual and concise.

sample_conversation: |
    user action: user said "Hello there!"
    user intent: user expressed greeting

    bot intent: bot express greeting
    bot action: bot express "Hello! How can I assist you today?"
      and bot gesture "Smile"

    user intent: user was silent 15

    bot intent: bot promote asking questions
    bot action: bot gesture "wave"
      and bot say "You can ask me anything you want'"

    user action: user said "What can you do for me?"
    user intent: user asked about capabilities

    bot intent: bot respond about capabilities
    bot action: bot say "I am here to have a chat with you."

    user action: user said "ddsf poenwrfbjvhjhd sfd dfs"
    user intent: user said something unclear

    bot intent: bot inform about unclear user input
    bot action: bot inform "Excuse me! I did not get that! Can you repeat please?"
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_2/main.co
````
import core
import avatars
import llm

@meta(bot_intent=True)
flow bot express greeting
  (bot express "Hi there!"
    or bot express "Welcome!"
    or bot express "Hello!")
    and bot gesture "Wave with one hand"

# CHANGE 1
# Add two flows to handle ending the conversation: bot express goodbye, user expressed done
@meta(bot_intent=True)
flow bot express goodbye
  (bot express "Goodbye" or bot express "Talk to you soon!") and bot gesture "bowing in goodbye"

@meta(user_intent=True)
flow user expressed done
  user said (regex("(?i).*done.*|.*end.*demo.*|.*exit.*"))

flow handling user requests until user expressed done
  # CHANGE 2
  # This activates LLM-based responses for all unhandled user intents 
  activate llm continuation

  # CHANGE 3 (optional)
  # This will generated a variation of the question (variations are generated by the LLM)
  bot say something like "How can I help you today?"

  # CHANGE 4
  # When the user expressed we end the conversation
  user expressed done
  bot express goodbye

# The main flow is the entry point
@meta(exclude_from_llm=True)
flow main

  # Technical flows, see Colang 2.0 documentation for more details
  activate notification of undefined flow start
  activate notification of colang errors
  activate tracking bot talking state

  # The bot greets the user and a welcome message is shown on the UI
  start scene show short information "Welcome to this tutorial interaction" as $intro_ui
  bot express greeting

  # CHANGE 5
  # Start handling user requests
  handling user requests until user expressed done

  # This will prevent the main flow finishing ever
  wait indefinitely
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_3/bot_config.yml
````yaml
bot: event_interface_tutorial

storage:
  name: cache

configs:
  use_stateful_guardrails: True

# Using OpenAI
colang_version: "2.x"
models:
    - type: main
      engine: openai
      model: gpt-4-turbo

instructions:
  - type: general
    content: |
      Below is a conversation between a user and a helpful and friendly bot named Emma.
      The bot is factual and concise.

sample_conversation: |
    user action: user said "Hello there!"
    user intent: user expressed greeting

    bot intent: bot express greeting
    bot action: bot express "Hello! How can I assist you today?"
      and bot gesture "Smile"

    user intent: user was silent 15

    bot intent: bot promote asking questions
    bot action: bot gesture "wave"
      and bot say "You can ask me anything you want'"

    user action: user said "What can you do for me?"
    user intent: user asked about capabilities

    bot intent: bot respond about capabilities
    bot action: bot say "I am here to have a chat with you."

    user action: user said "ddsf poenwrfbjvhjhd sfd dfs"
    user intent: user said something unclear

    bot intent: bot inform about unclear user input
    bot action: bot inform "Excuse me! I did not get that! Can you repeat please?"
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_3/main.co
````
import core
import avatars
import llm

@meta(bot_intent=True)
flow bot express greeting
  (bot express "Hi there!"
    or bot express "Welcome!"
    or bot express "Hello!")
    and bot gesture "Wave with one hand"

@meta(bot_intent=True)
flow bot express goodbye
  (bot express "Goodbye" or bot express "Talk to you soon!") and bot gesture "bowing in goodbye"

@meta(user_intent=True)
flow user expressed done
  user said (regex("(?i).*done.*|.*end.*demo.*|.*exit.*"))

# CHANGE 1: Add new flow that reacts to the user beeing slient
flow handling user silence
  user was silent 15.0
  llm continue interaction

flow handling user requests until user expressed done
  activate llm continuation
  # CHANGE 2: Activate the new flow
  activate handling user silence

  bot say something like "How can I help you today?"
  user expressed done
  bot express goodbye

# The main flow is the entry point
@meta(exclude_from_llm=True)
flow main

  # Technical flows, see Colang 2.0 documentation for more details
  activate notification of undefined flow start
  activate notification of colang errors
  activate tracking bot talking state

  # The bot greets the user and a welcome message is shown on the UI
  start scene show short information "Welcome to this tutorial interaction" as $intro_ui
  bot express greeting

  handling user requests until user expressed done

  # This will prevent the main flow finishing ever
  wait indefinitely
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_4/bot_config.yml
````yaml
bot: event_interface_tutorial

storage:
  name: cache

configs:
  use_stateful_guardrails: True

# Using OpenAI
colang_version: "2.x"
models:
    - type: main
      engine: openai
      model: gpt-4-turbo

instructions:
  - type: general
    content: |
      Below is a conversation between a user and a helpful and friendly bot named Emma.
      The bot is factual and concise.

sample_conversation: |
    user action: user said "Hello there!"
    user intent: user expressed greeting

    bot intent: bot express greeting
    bot action: bot express "Hello! How can I assist you today?"
      and bot gesture "Smile"

    user intent: user was silent 15

    bot intent: bot promote asking questions
    bot action: bot gesture "wave"
      and bot say "You can ask me anything you want'"

    user action: user said "What can you do for me?"
    user intent: user asked about capabilities

    bot intent: bot respond about capabilities
    bot action: bot say "I am here to have a chat with you."

    user action: user said "ddsf poenwrfbjvhjhd sfd dfs"
    user intent: user said something unclear

    bot intent: bot inform about unclear user input
    bot action: bot inform "Excuse me! I did not get that! Can you repeat please?"
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_4/main.co
````
import core
import avatars
import llm

@meta(bot_intent=True)
flow bot express greeting
  (bot express "Hi there!"
    or bot express "Welcome!"
    or bot express "Hello!")
    and bot gesture "Wave with one hand"

@meta(bot_intent=True)
flow bot express goodbye
  (bot express "Goodbye" or bot express "Talk to you soon!") and bot gesture "bowing in goodbye"

@meta(user_intent=True)
flow user expressed done
  user said (regex("(?i).*done.*|.*end.*demo.*|.*exit.*"))

flow handling user silence
  user was silent 15.0
  llm continue interaction

flow handling user requests until user expressed done
  activate llm continuation
  activate handling user silence
  
  # CHANGE: Allow the user to interrupt the bot at anytime
  activate handling bot talking interruption $mode="interrupt"

  bot say something like "How can I help you today?"
  user expressed done
  bot express goodbye

# The main flow is the entry point
@meta(exclude_from_llm=True)
flow main
  # Technical flows, see Colang 2.0 documentation for more details
  activate notification of undefined flow start
  activate notification of colang errors
  activate tracking bot talking state

  # The bot greets the user and a welcome message is shown on the UI
  start scene show short information "Welcome to this tutorial interaction" as $intro_ui
  bot express greeting

  handling user requests until user expressed done

  # This will prevent the main flow finishing ever
  wait indefinitely
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_5/bot_config.yml
````yaml
bot: event_interface_tutorial

storage:
  name: cache

configs:
  use_stateful_guardrails: True

# Using OpenAI
colang_version: "2.x"
models:
    - type: main
      engine: openai
      model: gpt-4-turbo

instructions:
  - type: general
    content: |
      Below is a conversation between a user and a helpful and friendly bot named Emma.
      The bot is factual and concise.

sample_conversation: |
    user action: user said "Hello there!"
    user intent: user expressed greeting

    bot intent: bot express greeting
    bot action: bot express "Hello! How can I assist you today?"
      and bot gesture "Smile"

    user intent: user was silent 15

    bot intent: bot promote asking questions
    bot action: bot gesture "wave"
      and bot say "You can ask me anything you want'"

    user action: user said "What can you do for me?"
    user intent: user asked about capabilities

    bot intent: bot respond about capabilities
    bot action: bot say "I am here to have a chat with you."

    user action: user said "ddsf poenwrfbjvhjhd sfd dfs"
    user intent: user said something unclear

    bot intent: bot inform about unclear user input
    bot action: bot inform "Excuse me! I did not get that! Can you repeat please?"
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_5/main.co
````
import core
import avatars
import llm

@meta(bot_intent=True)
flow bot express greeting
  (bot express "Hi there!"
    or bot express "Welcome!"
    or bot express "Hello!")
    and bot gesture "Wave with one hand"

@meta(bot_intent=True)
flow bot express goodbye
  (bot express "Goodbye" or bot express "Talk to you soon!") and bot gesture "bowing in goodbye"

@meta(user_intent=True)
flow user expressed done
  user said (regex("(?i).*done.*|.*end.*demo.*|.*exit.*"))

# CHANGE 1: Add two new user intent flows
@meta(user_intent=True)
flow user confirmed
  user has selected choice "yes"
    or user said "yes"
    or user said "ok"
    or user said "that's ok"
    or user said "yes why not"
    or user said "sure"

@meta(user_intent=True)
flow user denied
  user has selected choice "no"
    or user said "no"
    or user said "don't do it"
    or user said "I am not OK with this"
    or user said "cancel"

# CHANGE 2: Add flow that askes user for email address using UI and voice
flow ask for user email
  start VisualChoiceSceneAction(prompt= "Would you share your e-mail?", support_prompts=["You can just type 'yes' or 'no'","Or just click on the buttons below"],choice_type="selection", allow_multiple_choices=False, options= [{"id": "yes", "text": "Yes"}, {"id": "no", "text": "No"}]) as $confirmation_ui
  bot say "I would love to keep in touch. Would you be OK to give me your e-mail address?"
  when user confirmed
    send $confirmation_ui.Stop()
    bot ask "Nice! Please enter a valid email address to continue"
    start VisualFormSceneAction(prompt="Enter valid email",inputs=[{"id": "email", "description": "email address", "value" : ""}]) as $action
    while True
      when VisualFormSceneAction.InputUpdated(interim_inputs=[{"id": "email",  "value" : regex("@$")}])
          bot say "And now only the domain missing!"
      or when VisualFormSceneAction.InputUpdated(interim_inputs=[{"id": "email",  "value" : regex("^[-\w\.]+@([\w-]+\.)+[\w-]{2,4}$")}])
          bot say "Looks like a valid email address to me, just click ok to confirm" and bot gesture "success"
      or when VisualFormSceneAction.ConfirmationUpdated(confirmation_status="confirm")
          bot say "Thank you" and bot gesture "bowing"
          break
      or when VisualFormSceneAction.ConfirmationUpdated(confirmation_status="cancel")
          bot say "OK. Maybe another time."
          break
  or when user denied
    bot say "That is OK"

flow handling user silence
  user was silent 15.0
  llm continue interaction

flow handling user requests until user expressed done
  activate llm continuation
  activate handling user silence
  activate handling bot talking interruption $mode="interrupt"

  bot say something like "How can I help you today?"
  user expressed done
  # CHANGE 3: Run the flow that asks the user for email address and await it
  ask for user email
  bot express goodbye

# The main flow is the entry point
@meta(exclude_from_llm=True)
flow main
  # Technical flows, see Colang 2.0 documentation for more details
  activate notification of undefined flow start
  activate notification of colang errors
  activate tracking bot talking state

  # The bot greets the user and a welcome message is shown on the UI
  start scene show short information "Welcome to this tutorial interaction" as $intro_ui
  bot express greeting

  handling user requests until user expressed done

  # This will prevent the main flow finishing ever
  wait indefinitely
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_6/actions.py
````python
from nemoguardrails.actions.actions import action


@action(name="CountWordsAction")
async def count_words_action(transcript: str) -> int:
    return len(transcript.split())
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_6/bot_config.yml
````yaml
bot: event_interface_tutorial

storage:
  name: cache

configs:
  use_stateful_guardrails: True

# Using OpenAI
colang_version: "2.x"
models:
    - type: main
      engine: openai
      model: gpt-4-turbo

instructions:
  - type: general
    content: |
      Below is a conversation between a user and a helpful and friendly bot named Emma.
      The bot is factual and concise.

sample_conversation: |
    user action: user said "Hello there!"
    user intent: user expressed greeting

    bot intent: bot express greeting
    bot action: bot express "Hello! How can I assist you today?"
      and bot gesture "Smile"

    user intent: user was silent 15

    bot intent: bot promote asking questions
    bot action: bot gesture "wave"
      and bot say "You can ask me anything you want'"

    user action: user said "What can you do for me?"
    user intent: user asked about capabilities

    bot intent: bot respond about capabilities
    bot action: bot say "I am here to have a chat with you."

    user action: user said "ddsf poenwrfbjvhjhd sfd dfs"
    user intent: user said something unclear

    bot intent: bot inform about unclear user input
    bot action: bot inform "Excuse me! I did not get that! Can you repeat please?"
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_6/main.co
````
import core
import avatars
import llm

@meta(bot_intent=True)
flow bot express greeting
  (bot express "Hi there!"
    or bot express "Welcome!"
    or bot express "Hello!")
    and bot gesture "Wave with one hand"

@meta(bot_intent=True)
flow bot express goodbye
  (bot express "Goodbye" or bot express "Talk to you soon!") and bot gesture "bowing in goodbye"

@meta(user_intent=True)
flow user expressed done
  user said (regex("(?i).*done.*|.*end.*demo.*|.*exit.*"))

# CHANGE 1: Add a new user intent at the top of main.co (after the imports)
@meta(user_intent=True)
flow user requested to count words in sentence
  user has selected choice "can you please count the words in my next sentence"
    or user said "count how many words are in my next utterance"
    or user said "how many words"

@meta(user_intent=True)
flow user confirmed
  user has selected choice "yes"
    or user said "yes"
    or user said "ok"
    or user said "that's ok"
    or user said "yes why not"
    or user said "sure"

@meta(user_intent=True)
flow user denied
  user has selected choice "no"
    or user said "no"
    or user said "don't do it"
    or user said "I am not OK with this"
    or user said "cancel"

flow ask for user email
  start VisualChoiceSceneAction(prompt= "Would you share your e-mail?", support_prompts=["You can just type 'yes' or 'no'","Or just click on the buttons below"],choice_type="selection", allow_multiple_choices=False, options= [{"id": "yes", "text": "Yes"}, {"id": "no", "text": "No"}]) as $confirmation_ui
  bot say "I would love to keep in touch. Would you be OK to give me your e-mail address?"
  when user confirmed
    send $confirmation_ui.Stop()
    bot ask "Nice! Please enter a valid email address to continue"
    start VisualFormSceneAction(prompt="Enter valid email",inputs=[{"id": "email", "description": "email address", "value" : ""}]) as $action
    while True
      when VisualFormSceneAction.InputUpdated(interim_inputs=[{"id": "email",  "value" : regex("@$")}])
          bot say "And now only the domain missing!"
      or when VisualFormSceneAction.InputUpdated(interim_inputs=[{"id": "email",  "value" : regex("^[-\w\.]+@([\w-]+\.)+[\w-]{2,4}$")}])
          bot say "Looks like a valid email address to me, just click ok to confirm" and bot gesture "success"
      or when VisualFormSceneAction.ConfirmationUpdated(confirmation_status="confirm")
          bot say "Thank you" and bot gesture "bowing"
          break
      or when VisualFormSceneAction.ConfirmationUpdated(confirmation_status="cancel")
          bot say "OK. Maybe another time."
          break
  or when user denied
    bot say "That is OK"

# CHANGE 2: Add new flow that defines how to handle word counting requests
flow handling word counting requests
  user requested to count words in sentence
  bot say "Sure, please say a sentence and I will count the words"
  user said something as $ref
  $count = await CountWordsAction(transcript=$ref.transcript)
  bot say "There were {$count} words in your sentence." 

flow handling user silence
  user was silent 15.0
  llm continue interaction

flow handling user requests until user expressed done
  activate llm continuation
  activate handling user silence
  activate handling bot talking interruption $mode="interrupt"
  # CHANGE 3: Activate the new flow
  activate handling word counting requests

  bot say something like "How can I help you today?"
  user expressed done
  ask for user email
  bot express goodbye

# The main flow is the entry point
@meta(exclude_from_llm=True)
flow main
  # Technical flows, see Colang 2.0 documentation for more details
  activate notification of undefined flow start
  activate notification of colang errors
  activate tracking bot talking state

  # The bot greets the user and a welcome message is shown on the UI
  start scene show short information "Welcome to this tutorial interaction" as $intro_ui
  bot express greeting

  handling user requests until user expressed done

  # This will prevent the main flow finishing ever
  wait indefinitely
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_7/plugin/yahoo_fin.py
````python
import yfinance as yf
from yahoo_fin import stock_info as si
import requests
from typing import Optional
from fastapi import APIRouter

# API to extract stock price
Y_TICKER = "https://query2.finance.yahoo.com/v1/finance/search"
Y_FINANCE = "https://query1.finance.yahoo.com/v7/finance/quote?symbols="

router = APIRouter()

# Prepare headers for requests
session = requests.Session()
user_agent = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
)
session.headers.update({"User-Agent": user_agent})


def get_ticker_symbol_alphavantage(stock_name: str) -> Optional[str]:
    # We do not need actual api key to get ticker info
    # But it is required as placeholder
    api_key = "YOUR_ALPHA_VANTAGE_API_KEY"
    url = f"https://www.alphavantage.co/query?function=SYMBOL_SEARCH&keywords={stock_name}&apikey={api_key}"
    response = requests.get(url)
    data = response.json()

    if "bestMatches" in data and len(data["bestMatches"]) > 0:
        ticker_symbol = data["bestMatches"][0]["1. symbol"]
        return ticker_symbol

    return None


@router.get("/get_ticker")
def get_ticker(company_name: str) -> Optional[str]:
    """
    Take company name returns ticker symbol used for trading
    param
        Args:
            company_name: company name like Microsoft
        Returns:
            Ticker Symbol used for trading like MSFT for microsoft
    """
    try:
        params = {"q": company_name, "quotes_count": 1, "country": "United States"}
        return session.get(url=Y_TICKER, params=params).json()["quotes"][0]["symbol"]
    except Exception as e:
        return get_ticker_symbol_alphavantage(company_name)


@router.get("/get_stock_price")
def get_stock_price(company_name: str) -> Optional[float]:
    """
    get a stock price from yahoo finance api
    """

    try:
        # Find ticker symbol for stock name, eg. Microsoft : MSFT, Nvidia: NVDA
        ticker = get_ticker(company_name)
        live_price = si.get_live_price(ticker)
        return round(live_price, 2)

    except Exception as e:
        print(f"Unable to find stock price of {company_name}")
        return None
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_7/bot_config.yml
````yaml
bot: event_interface_tutorial

storage:
  name: cache

configs:
  use_stateful_guardrails: True

# Using OpenAI
colang_version: "2.x"
models:
    - type: main
      engine: openai
      model: gpt-4-turbo

instructions:
  - type: general
    content: |
      Below is a conversation between a user and a helpful and friendly bot named Emma.
      The bot is factual and concise.

sample_conversation: |
    user action: user said "Hello there!"
    user intent: user expressed greeting

    bot intent: bot express greeting
    bot action: bot express "Hello! How can I assist you today?"
      and bot gesture "Smile"

    user intent: user was silent 15

    bot intent: bot promote asking questions
    bot action: bot gesture "wave"
      and bot say "You can ask me anything you want'"

    user action: user said "What can you do for me?"
    user intent: user asked about capabilities

    bot intent: bot respond about capabilities
    bot action: bot say "I am here to have a chat with you."

    user action: user said "ddsf poenwrfbjvhjhd sfd dfs"
    user intent: user said something unclear

    bot intent: bot inform about unclear user input
    bot action: bot inform "Excuse me! I did not get that! Can you repeat please?"
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_7/main.co
````
import core
import avatars
import llm

@meta(bot_intent=True)
flow bot express greeting
  (bot express "Hi there!"
    or bot express "Welcome!"
    or bot express "Hello!")
    and bot gesture "Wave with one hand"

@meta(bot_intent=True)
flow bot express goodbye
  (bot express "Goodbye" or bot express "Talk to you soon!") and bot gesture "bowing in goodbye"

@meta(user_intent=True)
flow user expressed done
  user said (regex("(?i).*done.*|.*end.*demo.*|.*exit.*"))

# CHANGE 1: Add two new user intent flows
@meta(user_intent=True)
flow user confirmed
  user has selected choice "yes"
    or user said "yes"
    or user said "ok"
    or user said "that's ok"
    or user said "yes why not"
    or user said "sure"

@meta(user_intent=True)
flow user denied
  user has selected choice "no"
    or user said "no"
    or user said "don't do it"
    or user said "I am not OK with this"
    or user said "cancel"

# CHANGE 1 : Add user intent at the top of main.co
@meta(user_intent=True)
flow user asked stock price
  user said "What is the stock price of Microsoft?"
    or user said "How much does an Nvidia stock cost"
    or user said "what is the value of amazon share price?"
    or user said "What is it's stock price?"

flow ask for user email
  start VisualChoiceSceneAction(prompt= "Would you share your e-mail?", support_prompts=["You can just type 'yes' or 'no'","Or just click on the buttons below"],choice_type="selection", allow_multiple_choices=False, options= [{"id": "yes", "text": "Yes"}, {"id": "no", "text": "No"}]) as $confirmation_ui
  bot say "I would love to keep in touch. Would you be OK to give me your e-mail address?"
  when user confirmed
    send $confirmation_ui.Stop()
    bot ask "Nice! Please enter a valid email address to continue"
    start VisualFormSceneAction(prompt="Enter valid email",inputs=[{"id": "email", "description": "email address", "value" : ""}]) as $action
    while True
      when VisualFormSceneAction.InputUpdated(interim_inputs=[{"id": "email",  "value" : regex("@$")}])
          bot say "And now only the domain missing!"
      or when VisualFormSceneAction.InputUpdated(interim_inputs=[{"id": "email",  "value" : regex("^[-\w\.]+@([\w-]+\.)+[\w-]{2,4}$")}])
          bot say "Looks like a valid email address to me, just click ok to confirm" and bot gesture "success"
      or when VisualFormSceneAction.ConfirmationUpdated(confirmation_status="confirm")
          bot say "Thank you" and bot gesture "bowing"
          break
      or when VisualFormSceneAction.ConfirmationUpdated(confirmation_status="cancel")
          bot say "OK. Maybe another time."
          break
  or when user denied
    bot say "That is OK"

flow handling user silence
  user was silent 15.0
  llm continue interaction

# CHANGE 2 Add flow that handles stock price questions
flow handling stock price questions
  global $last_user_transcript
  user asked stock price
  
  $company_name = ..."Return the name of the company the user was referring to in quotes \"<company name here>\" or \"unknown\" if the user did not mention a company."
  if $company_name == "unknown"
    bot say "Sorry, I can't understand which company you are referring to here. Can you rephrase your query?"
    return
  else
    $price = await InvokeFulfillmentAction(endpoint="/stock/get_stock_price", company_name=$company_name)
    if not $price
      bot say "Could not find the stock price!"
    else
      bot say "Stock price of {$company_name} is {$price}"

flow handling user requests until user expressed done
  activate llm continuation
  activate handling user silence
  activate handling bot talking interruption $mode="interrupt"

  # CHANGE 3 Activate the flow 
  activate handling stock price questions

  bot say something like "How can I help you today?"
  user expressed done
  # CHANGE 3: Run the flow that asks the user for email address and await it
  ask for user email
  bot express goodbye

# The main flow is the entry point
@meta(exclude_from_llm=True)
flow main
  # Technical flows, see Colang 2.0 documentation for more details
  activate notification of undefined flow start
  activate notification of colang errors
  activate tracking bot talking state

  # The bot greets the user and a welcome message is shown on the UI
  start scene show short information "Welcome to this tutorial interaction" as $intro_ui
  bot express greeting

  handling user requests until user expressed done

  # This will prevent the main flow finishing ever
  wait indefinitely
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/step_7/plugin_config.yaml
````yaml
config:
  workers: 1
  timeout: 30

plugins:
  - name: stock
    path: "./plugin/yahoo_fin.py"
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/README.md
````markdown
# Event Interface Tutorial Bot

This is the template for event interface tutorial. You need it if you follow the tutorial on how to build a bot using
Colang 2.0 and the event interface
(Section `Building a Bot using Colang 2.0 and Event Interface` of the ACE Agent Documentation)

In this tutorial you will learn how to work with the ACE Agent event interface and how to create a simple bot
that makes use of Colang 2.0 and asynchronous event processing. The bot will feature:

- Multimodality. The bot will make use of gestures, utterances and showing information on a UI.
- LLM integration. The bot will make different use of LLMs to provide contextual answers and to simplify user input handling.
- Proactivity. The bot will be proactive and will try to engage the user if no reply is given.
- Interruptibility. The user can interrupt the bot at any time.
- Back-channeling. The bot can react in real time based on ongoing user input to make your interactions interesting.

## This template

The template provided serves as a minimal setup for the bot tutorial. The Colang script in the template is only
producing a hello world message and will not react to any user input. Instead this is meant as the starting point that
you will expand by following the steps described in the tutorial `Building a Bot using Colang 2.0 and Event Interface`
that is part of the ACE Agent documentation.

## Usage

To start the bot you can run the following commands in a new terminal window.

```bash
export BOT_PATH=samples/event_interface_tutorial_bot
source deploy/docker/docker_init.sh
docker compose -f deploy/docker/docker-compose.yml up event-bot -d
```
````

## File: microservices/ace_agent/4.1/samples/event_interface_tutorial_bot/speech_config.yaml
````yaml
grpc_server:
  nvidia::rrt::BotRuntimeGrpc: # component type
    ip_address: "0.0.0.0"
    port_number: 50055
    virtual_assistant_num_instances: 30
    virtual_assistant_pipeline_idle_threshold_secs: 600
    virtual_assistant_pipeline_idle_handler_wakeup_rate_secs: 10

speech_pipeline_manager: # config name
  SpeechPipelineManager: # component name
    asr_idle_timeout_ms: 200000
    tts_eos_delay_ms: 2000
    initial_state: "INIT"
    always_on: true

riva_asr:
  RivaASR:
    server: "localhost:50051"
    word_boost_file_path: "/workspace/config/asr_words_to_boost.txt"
    enable_profanity_filter: false

riva_tts:
  RivaTTS:
    server: "localhost:50051"
    voice_name: "English-US.Female-1"
    language: "en-US"
    ipa_dict: ""
    sample_rate: 44100
    chunk_duration_ms: 100
    audio_start_threshold_ms: 400
    send_audio_in_realtime: true
    tts_mode: "grpc"

riva_logger:
  RivaLogger:
    data_dump_path: "/workspace/log"
    enable_logging: true
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/colang/bug_fix.co
````
@meta(exclude_from_llm=True)
@loop("ignored_action_bugfix")
flow ignored_utterance_action_bugfix
  global $number_of_failed_utterance_actions
  if $number_of_failed_utterance_actions == None
    $number_of_failed_utterance_actions = 0
  match StartUtteranceBotAction() as $event
  start_new_flow_instance:
  start wait 3.0 as $timer_ref
  when $timer_ref.Finished()
    # After 3 consecutive fails we will no longer send a Finished event to let the process become idle and terminated
    if $number_of_failed_utterance_actions < 3
      send UtteranceBotActionFinished(action_uid=$event.action_uid, final_script="", is_success=False, failure_reason="ActionStarted event timeout")
    $number_of_failed_utterance_actions = $number_of_failed_utterance_actions + 1
  or when UtteranceBotActionStarted(action_uid=$event.action_uid)
    $number_of_failed_utterance_actions = 0
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/colang/cart.co
````
# User intent flows
flow user added food drink items to order
  """User wants to order food or drinks"""
  user intent "food_order.add_item"

flow user removed food drink items from order
  """User wants to remove a food or drink item from the order"""
  user intent "food_order.remove_item"

flow user swapped food drink items from order
  """User wants to swap a food or drink item from the order"""
  user intent "food_order.swap_item"

flow user requested empty cart
  """User wants to empty the order / cart """
  user said "Clear the cart"
    or user said "Delete my cart"
    or user said "Remove all items from my cart"
    or user said "Delete my order"
    or user said "Cancel my order"
    or user said (regex("(?i)\b(clear|delete|empty|reset)\\s+(my|the)?\\s*(cart|order)\b"))

# Flows related to cart management
flow handle adding items to order
  user added food drink items to order
  log "Adding item to cart"

  $information = await InvokeFulfillmentAction(endpoint="food_order/add_item", request_type="post")
  log "Response from Plugin - add action result: $information={$information}"
  if $information
    bot say $information
  else
    bot say something like "I could not add the requested items. We ran into some technical issues."

flow handle removing items from order
  user removed food drink items from order
  log "Removing item from cart"

  $information = await InvokeFulfillmentAction(endpoint="food_order/remove_item", request_type="post")
  log "Response from Plugin - remove action result: $information={$information}"
  if $information
    bot say $information
  else
    bot say something like "I could not remove the requested items. We ran into some technical issues."

flow handle swapping items from order
  user swapped food drink items from order
  log "Swapping items in cart"

  $information = await InvokeFulfillmentAction(endpoint="food_order/replace_items", request_type="post")
  log "Response from Plugin - swap action result: $information={$information}"
  if $information
    bot say $information
  else
    bot say something like "I could not swap the requested items. We ran into some technical issues."

flow handle emptying cart
  user requested empty cart
  log "Clearing the items from the cart"

  global $user_id

  $information = await InvokeFulfillmentAction(endpoint="food_order/clear_cart", request_type="post", user_id=$user_id)
  log "Response from Plugin - clear cart action result: $information={$information}"
  if $information
    bot say "Sure, I have cleared your cart"
  else
    bot say "Sorry, I couldn't clear the cart"
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/colang/general.co
````
flow user makes filler statement
  """User says a filler question like 'I see' or 'Hi' """
  user intent "food_order.filler"

flow user goes off topic
  """User asks about something unrelated to food ordering"""
  user intent "food_order.nomatch"

flow answer filler statement
  user makes filler statement
  llm continue interaction

flow answer off topic question
  user goes off topic
  bot say "I can only answer questions related to food ordering."
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/colang/main.co
````
import core
import llm
import timing
import intent_slot

flow respond to unhandled user intents
  unhandled user intent as $ref
  log "unhandled user intent: '{$ref.intent}'"
  llm continue interaction
  log "unhandled user intent response done"

flow handle qsr requests
  # Active flows are running in parallel and wait until they can advance

  # Cart
  activate handle adding items to order
  activate handle removing items from order
  activate handle swapping items from order
  activate handle emptying cart

  # Order
  activate handle order checkout
  activate handle checking bill
  activate handle repeating order

  # Query Menu
  activate handle menu queries

  # Off topic or filler questions
  activate answer filler statement
  activate answer off topic question

  activate polling llm request response $interval=1.0

  # Enable intent generation for user utterances that don't have an exact match.
  activate trigger user intent event for unhandled user utterance with intent slot model
  activate trigger llm user intent for unhandled user intent
  activate respond to unhandled user intents

  await checkout completed


flow system context received
  log "initializing story. waiting for user id"
  match ContextUpdate(data={"user_id" : regex(".*")})

@meta(exclude_from_llm=True)
flow main

  # Active helper flows
  activate ignored_utterance_action_bugfix
  activate notification of undefined flow start "I have encountered some technical issue!"
  activate notification of colang errors "I have encountered some technical issue!"
  activate automating intent detection
  activate tracking user talking state
  activate tracking bot talking state

  # Handle QSR requests when we have a user present
  activate handle qsr requests

  wait indefinitely
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/colang/order.co
````
flow user asked order clarification
  """User wants to get clarification on the current order"""
  user intent "food_order.repeat_order"

flow user asked total bill
  """User wants to get the total bill"""
  user intent "food_order.check_bill"

flow user expressed order complete
  """User confirms order or indicates that he wants to move to the checkout"""
  user intent "food_order.order_complete"

flow handle order checkout
  user expressed order complete
  global $user_id
  $total_bill = await InvokeFulfillmentAction(request_type="post", endpoint="food_order/check_bill", user_id=$user_id)
  $result = await InvokeFulfillmentAction(request_type="post", endpoint="food_order/place_order", user_id=$user_id)
  if $result
    bot say "Thanks for placing the order. Your total bill is {$total_bill} dollars. Your order would be available shortly. Goodbye!"
    send CustomEvent(name="CheckoutCompleted")

flow checkout completed
  match CustomEvent(name="CheckoutCompleted")
  await ResetAction(memory="session")

flow handle checking bill
  user asked total bill

  global $user_id
  $result = await InvokeFulfillmentAction(request_type="post", endpoint="food_order/check_bill", user_id=$user_id)
  if $result
    bot say "Your total bill is {$result} dollars."
  else
    bot say "I could not get the bill amount"

flow handle repeating order
  user asked order clarification
  global $user_id

  $result = await InvokeFulfillmentAction(request_type="post", endpoint="food_order/repeat_order", user_id=$user_id)
  if $result
    bot say something like $result
  else
    bot say "I couldn't find anything in your cart"
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/colang/query_menu.co
````
flow user asked about menu item
  """User wants to know about menu options or a specific item on the menu"""
  user intent "food_order.query_menu"

flow user asks to see options
  """User wants to know about generic menu options"""
  user intent "food_order.show_options"

flow handle menu queries
  global $last_user_transcript
  user asked about menu item or user asks to see options
  log "Handle user query about an item on the menu"

  $information = await InvokeFulfillmentAction(endpoint="food_order/show_menu", request_type="post")
  log "Response from Plugin - show menu action result: $information={$information}"
  $response = ..."Summarize the following information to form a natural response to the given question. Question: {$last_user_transcript}. Information: {$information}. Enclose the response in quotes"
  bot say $response
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/plugin/__init__.py
````python
"""
 copyright(c) 2022 NVIDIA Corporation.All rights reserved.

 NVIDIA Corporation and its licensors retain all intellectual property
 and proprietary rights in and to this software, related documentation
 and any modifications thereto.Any use, reproduction, disclosure or
 distribution of this software and related documentation without an express
 license agreement from NVIDIA Corporation is strictly prohibited.
"""
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/plugin/all_gtc_items_with_diff_sizes_v3.json
````json
[{
  "_id": {
    "$oid": "615fd4fff89dc8ee7edd59f5"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 60,
      "price": 0.39,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "avocado",
  "item_id": "20",
  "category": "toppings"
},{
  "_id": {
    "$oid": "615cb3b7e29429a98da46054"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 0,
      "price": 0.49,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "garlic",
  "item_id": "16",
  "category": "toppings"
},{
  "_id": {
    "$oid": "614ad5186c315ae1cdb3eb14"
  },
  "menu_item": true,
  "recommendation_type": [
    "toppings"
  ],
  "synonyms": [],
  "tags": [
    "non-veg",
    "burger",
    "beef",
    "protein",
    "specials",
    "gluten"
  ],
  "variations": [
    {
      "name": "",
      "size": "regular",
      "calories": 440,
      "price": 7.5,
      "description": "A favorite of protein lovers. It gets its protein from two beef patties. Contains 30 grams of protein.",
      "image": "images/customize-menu/customize-menu-double-protien-burger.png",
      "is_default": true
    }
  ],
  "ingredients": [
    "buns",
    "beef patty"
  ],
  "toppings": [
    "black bean patty",
    "extra cheese",
    "avocado",
    "pickles",
    "fried egg"
  ],
  "name": "double protein burger",
  "item_id": "3",
  "category": "entrees"
},{
  "_id": {
    "$oid": "615cae5de29429a98da46053"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 100,
      "price": 1.29,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "black bean patty",
  "item_id": "15",
  "category": "toppings"
},{
  "_id": {
    "$oid": "6150ccb1bd31d2c9ba87cc59"
  },
  "menu_item": true,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "cola",
    "chilled drink",
    "cheap",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "small",
      "calories": 150,
      "price": 1.79,
      "description": "Fountain beverage. Product of a cola company.",
      "image": "images/drinks-menu/drinks-menu-coke.png",
      "is_default": true
    },
    {
      "name": "",
      "size": "medium",
      "calories": 200,
      "price": 1.89
    },
    {
      "name": "",
      "size": "large",
      "calories": 250,
      "price": 1.99
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "regular cola",
  "item_id": "10",
  "category": "drinks"
},{
  "_id": {
    "$oid": "614ad48c6c315ae1cdb3eb12"
  },
  "menu_item": true,
  "recommendation_type": [
    "salads",
    "toppings"
  ],
  "synonyms": [],
  "tags": [
    "non-veg",
    "burger",
    "beef",
    "specials",
    "gluten",
    "lactose"
  ],
  "variations": [
    {
      "name": "",
      "size": "regular",
      "calories": 350,
      "price": 4.5,
      "description": "A simple, classic cheeseburger made with a 100% pure beef burger. It is seasoned with just a pinch of salt and pepper.",
      "image": "images/entrees-menu/entree-menu-bacon-cheese-burger.png",
      "is_default": true
    }
  ],
  "ingredients": [
    "buns",
    "beef patty",
    "cheese",
    "salt",
    "pepper"
  ],
  "toppings": [
    "bacon",
    "fried onions",
    "extra cheese",
    "avocado"
  ],
  "name": "cheeseburger",
  "item_id": "17",
  "category": "entrees"
},{
  "_id": {
    "$oid": "610d264fff213c8cb08e482a"
  },
  "menu_item": true,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "healthy",
    "low calorie",
    "lactose",
    "nuts"
  ],
  "variations": [
    {
      "name": "",
      "size": "regular",
      "calories": 150,
      "price": 6.49,
      "description": "Made of mixed greens. It is topped with crumbled blue cheese and a mix of red and green apples, strawberries and blueberries. Made fresh daily. Served with harvest nut granola and roasted almonds. Pairs well with zesty apple cider vinaigrette.",
      "image": "images/salads-menu/salad-menu-market-salad.png",
      "is_default": true
    }
  ],
  "ingredients": [
    "apples",
    "strawberries",
    "blueberries",
    "granola",
    "almonds",
    "apple cider vinaigrette"
  ],
  "toppings": [
    "thousand island dressing",
    "honey mustard dressing",
    "ranch dressing"
  ],
  "name": "market salad",
  "item_id": "12",
  "category": "salads"
},{
  "_id": {
    "$oid": "610d264fff213c8cb08e4798"
  },
  "menu_item": true,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "non-veg",
    "sandwich",
    "chicken",
    "specials",
    "gluten",
    "lactose"
  ],
  "variations": [
    {
      "name": "",
      "size": "regular",
      "calories": 440,
      "price": 5.5,
      "description": "Made of a boneless breast of chicken seasoned to perfection. It is freshly breaded, pressure cooked in 100% refined peanut oil and served on a toasted, buttered bun with dill pickle chips. Also available on a multigrain bun.",
      "image": "images/entrees-menu/entree-menu-chicken-sandwich.png",
      "is_default": true
    }
  ],
  "ingredients": [
    "buns",
    "butter",
    "chicken",
    "oil"
  ],
  "toppings": [
    "bacon",
    "pickles"
  ],
  "name": "chicken sandwich",
  "item_id": "22",
  "category": "entrees"
},{
  "_id": {
    "$oid": "610d264fff213c8cb08e47d1"
  },
  "menu_item": true,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "fruit",
    "healthy",
    "low calorie",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "regular",
      "calories": 150,
      "price": 6.49,
      "description": "A nutritious fruit mix for the health conscious. It is made with chopped pieces of red and green apples, mandarin orange segments, fresh strawberry slices, and blueberries, served chilled. Prepared fresh daily.",
      "image": "images/salads-menu/salad-menu-fruit-salad.png",
      "is_default": true
    }
  ],
  "ingredients": [
    "apples",
    "oranges",
    "strawberries",
    "blueberries"
  ],
  "toppings": [],
  "name": "fruit salad",
  "item_id": "19",
  "category": "salads"
},{
  "_id": {
    "$oid": "61782e534aebece373989665"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 100,
      "price": 0.79,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "fried egg",
  "item_id": "24",
  "category": "toppings"
},{
  "_id": {
    "$oid": "61523b56f07d2b5b88272f63"
  },
  "menu_item": true,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "lactose"
  ],
  "variations": [
    {
      "name": "",
      "size": "small",
      "calories": 50,
      "price": 2.99,
      "description": "Beverage brewed from steamed milk and espresso, with a bit of froth.",
      "image": "images/drinks-menu/drinks-menu-coffee.png",
      "is_default": true
    },
    {
      "name": "",
      "size": "medium",
      "calories": 70,
      "price": 3.29
    },
    {
      "name": "",
      "size": "large",
      "calories": 90,
      "price": 3.49
    }
  ],
  "ingredients": [
    "milk",
    "espresso"
  ],
  "toppings": [],
  "name": "coffee",
  "item_id": "25",
  "category": "drinks"
},{
  "_id": {
    "$oid": "614fb22ae80afa927e4267cf"
  },
  "menu_item": true,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "fried",
    "pickles",
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "small",
      "calories": 300,
      "price": 1.69,
      "description": "The perfect choice for those who like tang. Made with tangy dill pickles dipped in a smooth flavorful batter. It is fried in olive oil till it is crispy golden brown.",
      "image": "images/sides-menu/sides-menu-fried-pickles.png",
      "is_default": true
    },
    {
      "name": "",
      "size": "medium",
      "calories": 400,
      "price": 1.99
    },
    {
      "name": "",
      "size": "large",
      "calories": 500,
      "price": 2.19
    }
  ],
  "ingredients": [
    "pickles",
    "oil"
  ],
  "toppings": [],
  "name": "fried pickles",
  "item_id": "2",
  "category": "sides"
},{
  "_id": {
    "$oid": "61523a70f07d2b5b88272f61"
  },
  "menu_item": true,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "low calorie",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "small",
      "calories": 100,
      "price": 2.59,
      "description": "A beverage made using three simple ingredients - real lemon juice, cane sugar and water. It is blended with ginger and thyme.",
      "image": "images/drinks-menu/drinks-menu-lemonade.png",
      "is_default": true
    },
    {
      "name": "",
      "size": "medium",
      "calories": 150,
      "price": 2.79
    },
    {
      "name": "",
      "size": "large",
      "calories": 200,
      "price": 2.89
    }
  ],
  "ingredients": [
    "lemon juice",
    "sugar",
    "water",
    "ginger",
    "thyme"
  ],
  "toppings": [],
  "name": "lemonade",
  "item_id": "7",
  "category": "drinks"
},{
  "_id": {
    "$oid": "610d264fff213c8cb08e47bc"
  },
  "menu_item": true,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "non-veg",
    "nuggets",
    "chicken",
    "gluten"
  ],
  "variations": [
    {
      "name": "",
      "size": "small",
      "calories": 250,
      "price": 3.69,
      "description": "Made of bite-sized pieces of boneless chicken breast. It is seasoned to perfection, freshly breaded and pressure cooked in 100% refined peanut oil. Available with choice of dipping sauce.",
      "image": "images/sides-menu/sides-menu-chicken-nuggets.png",
      "is_default": true
    },
    {
      "name": "",
      "size": "medium",
      "calories": 300,
      "price": 3.99
    },
    {
      "name": "",
      "size": "large",
      "calories": 350,
      "price": 4.29
    }
  ],
  "ingredients": [
    "chicken",
    "oil"
  ],
  "toppings": ["barbeque sauce", "ketchup", "mayonnaise"],
  "name": "chicken nuggets",
  "item_id": "5",
  "category": "sides"
},{
  "_id": {
    "$oid": "6150cd00bd31d2c9ba87cc5b"
  },
  "menu_item": true,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "cola",
    "chilled drink",
    "low calorie",
    "cheap",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "small",
      "calories": 0,
      "price": 1.59,
      "description": "Zero sugar fountain beverage. Product of a cola company.",
      "image": "images/drinks-menu/drinks-menu-coke.png",
      "is_default": true
    },
    {
      "name": "",
      "size": "medium",
      "calories": 0,
      "price": 1.79
    },
    {
      "name": "",
      "size": "large",
      "calories": 0,
      "price": 1.89
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "diet cola",
  "item_id": "4",
  "category": "drinks"
},{
  "_id": {
    "$oid": "62418ce47d8326dec810d250"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "non-veg"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 100,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "eggs",
  "item_id": "28",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "614ad4da6c315ae1cdb3eb13"
  },
  "menu_item": true,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "non-veg",
    "sandwich",
    "fish",
    "gluten",
    "lactose"
  ],
  "variations": [
    {
      "name": "",
      "size": "regular",
      "calories": 440,
      "price": 5.5,
      "description": "Our special sandwich made with tuna-fish patty, tartar sauce, and creamy dijon mayonnaise.",
      "image": "images/entrees-menu/entree-menu-fish-sandwich.png",
      "is_default": true
    }
  ],
  "ingredients": [
    "buns",
    "butter",
    "tuna fish",
    "tartar sauce",
    "mayonnaise"
  ],
  "toppings": [
    "bacon",
    "pickles"
  ],
  "name": "fish sandwich",
  "item_id": "23",
  "category": "entrees"
},{
  "_id": {
    "$oid": "614fb14fe80afa927e4267cd"
  },
  "menu_item": true,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "onion",
    "vegetarian",
    "cheap",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "small",
      "calories": 350,
      "price": 1.79,
      "description": "A crispy delight made from whole white onions. It is battered with a subtle blend of spices, letting the onion's natural sweetness shine through.",
      "image": "images/sides-menu/sides-menu-onion-rings.png",
      "is_default": true
    },
    {
      "name": "",
      "size": "medium",
      "calories": 450,
      "price": 2.19
    },
    {
      "name": "",
      "size": "large",
      "calories": 550,
      "price": 2.39
    }
  ],
  "ingredients": [
    "onions",
    "oil"
  ],
  "toppings": [],
  "name": "onion rings",
  "item_id": "8",
  "category": "sides"
},{
  "_id": {
    "$oid": "62418cbc7d8326dec810d24e"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 0,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "water",
  "item_id": "26",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "610d264fff213c8cb08e481c"
  },
  "menu_item": true,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "non-veg",
    "healthy",
    "gluten",
    "lactose"
  ],
  "variations": [
    {
      "name": "",
      "size": "regular",
      "calories": 400,
      "price": 5.99,
      "description": "A classic recipe of romaine lettuce, grape tomatoes, blue cheese dressing. It is tossed on a gentle flame and topped with avocado, bacon, and hard-boiled egg.",
      "image": "images/salads-menu/salad-menu-cobb-salad.png",
      "is_default": true
    }
  ],
  "ingredients": [
    "lettuce",
    "tomatoes",
    "cheese",
    "avocado",
    "bacon",
    "eggs"
  ],
  "toppings": [
    "thousand island dressing",
    "honey mustard dressing",
    "ranch dressing"
  ],
  "name": "cobb salad",
  "item_id": "11",
  "category": "salads"
},{
  "_id": {
    "$oid": "62418cce7d8326dec810d24f"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 20,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "sugar",
  "item_id": "27",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "614cdba749e277dc46b04455"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 5,
      "price": 0.49,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "pickles",
  "item_id": "13",
  "category": "toppings"
},{
  "_id": {
    "$oid": "62418d157d8326dec810d252"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "non-veg"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 70,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "mayonnaise",
  "item_id": "30",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418cf57d8326dec810d251"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 0,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "salt",
  "item_id": "29",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418d3a7d8326dec810d254"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 10,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "lemon juice",
  "item_id": "32",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418d2c7d8326dec810d253"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 40,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "apple cider vinaigrette",
  "item_id": "31",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "614ad5966c315ae1cdb3eb15"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 30,
      "price": 0.89,
      "description": "",
      "image": "images/cart-small-thumbnails/cart-menu-thumbnail-fried-onions.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "fried onions",
  "item_id": "14",
  "category": "toppings"
},{
  "_id": {
    "$oid": "62418d547d8326dec810d255"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 80,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "blueberries",
  "item_id": "33",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "614fb0b1e80afa927e4267ca"
  },
  "menu_item": true,
  "recommendation_type": [
    "toppings"
  ],
  "synonyms": [],
  "tags": [
    "fries",
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "small",
      "calories": 300,
      "price": 2.69,
      "description": "A deep fried treat for potato lovers, made of waffle-cut potatoes. It is cooked in canola oil until crispy outside and tender inside. Sprinkled with sea salt for maximum flavor.",
      "image": "images/sides-menu/sides-menu-french-fries.png",
      "is_default": true
    },
    {
      "name": "",
      "size": "medium",
      "calories": 400,
      "price": 2.99
    },
    {
      "name": "",
      "size": "large",
      "calories": 500,
      "price": 3.39
    }
  ],
  "ingredients": [
    "potatoes",
    "salt",
    "oil"
  ],
  "toppings": [
    "garlic",
    "chili cheese"
  ],
  "name": "french fries",
  "item_id": "6",
  "category": "sides"
},{
  "_id": {
    "$oid": "614ad5ec6c315ae1cdb3eb16"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "lactose"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 80,
      "price": 0.39,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "extra cheese",
  "item_id": "9",
  "category": "toppings"
},{
  "_id": {
    "$oid": "615fd51bf89dc8ee7edd59f6"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "lactose"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 80,
      "price": 0.29,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "chili cheese",
  "item_id": "21",
  "category": "toppings"
},{
  "_id": {
    "$oid": "610d264fff213c8cb08e47ac"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "gluten",
    "non-veg"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 150,
      "price": 0.99,
      "description": "",
      "image": "images/cart-small-thumbnails/cart-menu-thumbnail-bacon.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "bacon",
  "item_id": "1",
  "category": "toppings"
},{
  "_id": {
    "$oid": "610d264fff213c8cb08e47d3"
  },
  "menu_item": true,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "healthy",
    "lactose"
  ],
  "variations": [
    {
      "name": "",
      "size": "regular",
      "calories": 200,
      "price": 4.89,
      "description": "A light, simple and healthy choice. made with a fresh bed of mixed greens and vegetables. It is topped with a blend of shredded monterey jack and cheddar cheeses and grape tomatoes. Made fresh daily. Served with charred tomato, crispy red bell peppers and choice of dressing.",
      "image": "images/salads-menu/salad-menu-side-salad.png",
      "is_default": true
    }
  ],
  "ingredients": [
    "cheese",
    "tomatoes",
    "bell peppers"
  ],
  "toppings": [
    "thousand island dressing",
    "honey mustard dressing",
    "ranch dressing"
  ],
  "name": "side salad",
  "item_id": "18",
  "category": "salads"
},{
  "_id": {
    "$oid": "62418d607d8326dec810d256"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 200,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "oil",
  "item_id": "34",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418d6d7d8326dec810d257"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 90,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "potatoes",
  "item_id": "35",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418d807d8326dec810d258"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "non-veg",
    "gluten"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 150,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "bacon",
  "item_id": "36",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418da37d8326dec810d25a"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 2,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "ginger",
  "item_id": "38",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418d957d8326dec810d259"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 60,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "avocado",
  "item_id": "37",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418db17d8326dec810d25b"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 20,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "tomatoes",
  "item_id": "39",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418dcc7d8326dec810d25c"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 60,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "tartar sauce",
  "item_id": "40",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418ddb7d8326dec810d25d"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 5,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "pickles",
  "item_id": "41",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418dfc7d8326dec810d25f"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 0,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "thyme",
  "item_id": "43",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418def7d8326dec810d25e"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "nuts"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 115,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "granola",
  "item_id": "42",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418e0f7d8326dec810d260"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "non-veg"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 200,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "beef patty",
  "item_id": "44",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418e247d8326dec810d261"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "gluten"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 50,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "apples",
  "item_id": "45",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418e4c7d8326dec810d264"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "non-veg"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 200,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "tuna fish",
  "item_id": "48",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418e8b7d8326dec810d268"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "non-veg"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 250,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "chicken",
  "item_id": "52",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418e2f7d8326dec810d262"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan",
    "gluten"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 135,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "buns",
  "item_id": "46",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418e5b7d8326dec810d265"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "lactose"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 100,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "milk",
  "item_id": "49",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418e3d7d8326dec810d263"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 30,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "bell peppers",
  "item_id": "47",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418e6a7d8326dec810d266"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 5,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "lettuce",
  "item_id": "50",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418e767d8326dec810d267"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 60,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "oranges",
  "item_id": "51",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418eee7d8326dec810d26c"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "lactose"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 100,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "butter",
  "item_id": "56",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418eab7d8326dec810d269"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 5,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "espresso",
  "item_id": "53",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418ed17d8326dec810d26a"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 35,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "strawberries",
  "item_id": "54",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418efa7d8326dec810d26d"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 5,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "onions",
  "item_id": "57",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418ee17d8326dec810d26b"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "lactose"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 80,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "cheese",
  "item_id": "55",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418f1e7d8326dec810d26f"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "nuts"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 35,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "almonds",
  "item_id": "59",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62418f077d8326dec810d26e"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 0,
      "price": 0,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "pepper",
  "item_id": "58",
  "category": "ingredients"
},{
  "_id": {
    "$oid": "62baba050371b57e7ebb1b61"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 111,
      "price": 0.6,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "thousand island dressing",
  "item_id": "60",
  "category": "toppings"
},{
  "_id": {
    "$oid": "62baba2e0371b57e7ebb1b62"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan",
    "lactose"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 80,
      "price": 0.45,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "ranch dressing",
  "item_id": "61",
  "category": "toppings"
},{
  "_id": {
    "$oid": "62baba500371b57e7ebb1b63"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 80,
      "price": 0.35,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "honey mustard dressing",
  "item_id": "62",
  "category": "toppings"
},{
  "_id": {
    "$oid": "62babb440371b57e7ebb1b64"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 40,
      "price": 0.35,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "barbeque sauce",
  "item_id": "63",
  "category": "toppings"
},{
  "_id": {
    "$oid": "62babba00371b57e7ebb1b65"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "vegetarian",
    "vegan"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 20,
      "price": 0.2,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "ketchup",
  "item_id": "64",
  "category": "toppings"
},{
  "_id": {
    "$oid": "62babbf30371b57e7ebb1b66"
  },
  "menu_item": false,
  "recommendation_type": [],
  "synonyms": [],
  "tags": [
    "non-veg"
  ],
  "variations": [
    {
      "name": "",
      "size": "",
      "calories": 70,
      "price": 0.3,
      "description": "",
      "image": "images/default.png",
      "is_default": true
    }
  ],
  "ingredients": [],
  "toppings": [],
  "name": "mayonnaise",
  "item_id": "65",
  "category": "toppings"
}]
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/plugin/cart_manager.py
````python
"""
 copyright(c) 2023 NVIDIA Corporation.All rights reserved.

 NVIDIA Corporation and its licensors retain all intellectual property
 and proprietary rights in and to this software, related documentation
 and any modifications thereto.Any use, reproduction, disclosure or
 distribution of this software and related documentation without an express
 license agreement from NVIDIA Corporation is strictly prohibited.
"""
from dataclasses import asdict
from typing import Any, Dict
from uuid import uuid4
import logging

from data_format import *
from menu_api import MenuDB

menu_db = MenuDB()
logger = logging.getLogger("plugin")


class CartManager:
    def __init__(self) -> None:
        """Cart Manager to manager user information in cart"""
        self._cart_table = {}  # maintains session_id: cart_item

    def is_ready(self):
        # Remove if not required
        return True

    def get_total_bill(self, session_id):
        """List of items in cart for given session_id"""
        if session_id in self._cart_table:
            cart = self._cart_table.get(session_id)
            return cart.total_bill
        return 0

    def __get_menu_item_by_id(self, id_to_search):
        return menu_db.get_item_from_id(id_to_search)

    def __get_details_from_variation(self, menu_item, current_item):
        item_desc = None
        item_img_loc = None
        item_size = None
        item_calories = None
        item_price = None
        variations = None
        for key, value in menu_item.items():
            if key == "variations":
                variations = value

        print(variations, current_item)
        for variation in variations:
            if "is_default" in variation:
                item_desc = variation["description"]
                item_img_loc = variation["image"]
                item_size = variation["size"]
                item_calories = variation["calories"]
                item_price = variation["price"]

        if "size" not in current_item:
            return [
                menu_item["name"],
                item_img_loc,
                item_size,
                item_calories,
                item_price,
                item_desc,
            ]

        for variation in variations:
            if current_item["size"] == variation["size"]:
                return [
                    menu_item["name"],
                    item_img_loc,
                    variation["size"],
                    variation["calories"],
                    variation["price"],
                    item_desc,
                ]

    def __get_toppings_for_item_id(self, item_toppings):
        mesg = "Success"
        toppings = []
        for topping in item_toppings:
            topping_id = topping["item_id"]
            menu_item_by_id = self.__get_menu_item_by_id(topping_id)
            if menu_item_by_id == None:
                return toppings, "Could not get the menu item"

            print("Menu item by id is: {}".format(menu_item_by_id))
            toppings.append(
                ToppingItem(
                    menu_item_by_id["item_id"],
                    menu_item_by_id["name"],
                    menu_item_by_id["variations"][0]["image"],
                    1,
                    menu_item_by_id["variations"][0]["calories"],
                    menu_item_by_id["variations"][0]["price"],
                )
            )
        return toppings, mesg

    def __update_total_bill_calories(self, uuid):
        self._cart_table[uuid].total_bill = round(
            sum(cart_item["price"] for cart_item in self._cart_table[uuid].items), 2
        )

        for item in self._cart_table[uuid].items:
            print("Item is: {}".format(item))

        self._cart_table[uuid].total_calories = round(
            sum(cart_item["calories"] for cart_item in self._cart_table[uuid].items), 2
        )

        print(
            "Updated total bill: {}, {}".format(
                self._cart_table[uuid].total_bill, self._cart_table[uuid].total_calories
            )
        )

    def __fold_cart(self, uuid):
        print("Trying to fold cart")
        for i in range(len(self._cart_table[uuid].items) - 1, 0, -1):
            for j in range(i - 1, -1, -1):
                if self._cart_table[uuid].items[i] == self._cart_table[uuid].items[j]:
                    print("Found similar items in pos {} and {}. Clubbing them into one.".format(i, j))
                    self._cart_table[uuid].items[i].quantity += self._cart_table[uuid].items[j].quantity
                    self._cart_table[uuid].items[i].price += self._cart_table[uuid].items[j].price
                    self._cart_table[uuid].items[i].calories += self._cart_table[uuid].items[j].calories
                    self._cart_table[uuid].items[j].item_id = -1

        new_cart = Cart()
        for item in self._cart_table[uuid].items:
            if item.item_id != -1:
                new_cart.items.append(item)
        new_cart.total_bill = self._cart_table[uuid].total_bill
        new_cart.total_calories = self._cart_table[uuid].total_calories

        self._cart_table[uuid] = new_cart

    def __add_to_cart(self, uuid, menu_item, item_to_add):
        added_toppings = []
        item_to_add["toppings"].sort(key=lambda x: x["item_id"], reverse=False)
        (
            item_name,
            item_img_loc,
            item_size,
            item_calories,
            item_price,
            item_desc,
        ) = self.__get_details_from_variation(menu_item, item_to_add)
        added_toppings, mesg = self.__get_toppings_for_item_id(item_to_add["toppings"])
        if mesg != "Success":
            return mesg
        total_toppings_price = sum(top.price for top in added_toppings)
        total_toppings_calories = sum(top.calories for top in added_toppings)

        # Logic to increase the count of item in cart,
        # if similar item config already present
        for cart_item in self._cart_table[uuid].items:
            if (
                cart_item.item_id == item_to_add["item_id"]
                and cart_item.toppings == added_toppings
                and (
                    cart_item.size == ""
                    or item_to_add.get("size", None) is None
                    or (cart_item.size != "" and cart_item.size == item_to_add["size"])
                )
            ):
                # same item, increment the count instead.
                cart_item.quantity += item_to_add["quantity"]

                cart_item.price = round(
                    cart_item.price + item_to_add["quantity"] * (item_price + total_toppings_price),
                    2,
                )
                cart_item.calories = round(
                    cart_item.calories + item_to_add["quantity"] * (item_calories + total_toppings_calories),
                    2,
                )
                self.__update_total_bill_calories(uuid)
                print("Item already present. Updating count in cart.")
                return

        self._cart_table[uuid].items.append(
            CartItem(
                item_to_add["item_id"],
                item_name,
                added_toppings,
                item_size,
                round(
                    item_to_add["quantity"] * (item_calories + total_toppings_calories),
                    2,
                ),
                item_img_loc,
                item_to_add["quantity"],
                round(item_to_add["quantity"] * (item_price + total_toppings_price), 2),
                item_desc,
                menu_item["category"],
                str(uuid4()),
            )
        )

        self.__update_total_bill_calories(uuid)
        self.__fold_cart(uuid)
        return mesg

    def __add_item(self, session_id, item_to_add):
        mesg = "Success"
        menu_item_by_id = self.__get_menu_item_by_id(item_to_add["item_id"])
        if menu_item_by_id == None:
            print("Item not found on the menu")
            return "Item not found on the menu"
        self.__add_to_cart(session_id, menu_item_by_id, item_to_add)
        return mesg

    def cart_items_add(self, session_id, req):
        items_to_add = req["items"]

        if session_id not in self._cart_table:
            self._cart_table[session_id] = Cart()

        for item in items_to_add:
            mesg = self.__add_item(session_id, item)
            if mesg != "Success":
                return 409, asdict(self._cart_table[session_id])
            print('item "{}" added to cart'.format(item))

        return 200, asdict(self._cart_table.get(session_id))

    def items_in_cart(self, session_id):
        """List of items in cart for given session_id"""
        if session_id in self._cart_table:
            return asdict(self._cart_table.get(session_id, {}))
        return {}

    def __remove_from_cart(self, uuid, menu_item, item_to_remove):
        item_to_remove["toppings"].sort(key=lambda x: x["item_id"], reverse=False)

        print("item_to_remove is: {}".format(item_to_remove))
        _, _, _, item_calories, item_price, _ = self.__get_details_from_variation(menu_item, item_to_remove)

        removed_toppings, mesg = self.__get_toppings_for_item_id(item_to_remove["toppings"])
        print("Checking if item {} is present in the cart".format(item_to_remove))
        for idx, cart_item in enumerate(self._cart_table[uuid].items):
            if (
                cart_item.item_id == item_to_remove["item_id"]
                and ("toppings" not in item_to_remove or cart_item.toppings == removed_toppings)
                and ("size" not in item_to_remove or cart_item.size.lower() == item_to_remove["size"].lower())
            ):
                if cart_item.quantity >= item_to_remove["quantity"]:
                    cart_item.quantity -= item_to_remove["quantity"]
                    total_toppings_price = 0
                    total_toppings_price = sum(top.price for top in removed_toppings)
                    total_toppings_calories = sum(top.calories for top in removed_toppings)
                    cart_item.price = round(
                        cart_item.price - item_to_remove["quantity"] * (item_price + total_toppings_price), 2
                    )
                    cart_item.calories = round(
                        cart_item.calories - item_to_remove["quantity"] * (item_calories + total_toppings_calories), 2
                    )
                    self.__update_total_bill_calories(uuid)

                    if cart_item.quantity == 0:
                        self._cart_table[uuid].items.pop(idx)
                    logger.info("Item removed from the cart")
                    return True

        logger.info("Item not found in cart")
        return False

    def __remove_item(self, uuid, item_to_remove: Dict[str, Any]):
        logger.info("Item to be removed: {}".format(item_to_remove))
        mesg = "Success"
        menu_item_by_id = self.__get_menu_item_by_id(item_to_remove["item_id"])
        if menu_item_by_id == None:
            return "Could not find the item on the menu", False
        is_item_removed = self.__remove_from_cart(uuid, menu_item_by_id, item_to_remove)
        self.__fold_cart(uuid)
        return mesg, is_item_removed

    def cart_items_delete(self, session_id, req):
        STATUS = 200
        MESG = "No item found to delete from cart"

        if session_id not in self._cart_table:
            logger.info("No cart found for the session {}".format(session_id))
            MESG = "No cart found for the session {}".format(session_id)
            STATUS = 409
            return STATUS, MESG

        logger.info("Deleting items in the cart for session_id {}".format(session_id))

        remove_items = req["remove_items"]

        for item in remove_items:
            item_info = self.__get_menu_item_by_id(item.get("item_id"))
            item_name = item_info.get("name")
            logger.info(f"Trying to remove {item_name} from cart")
            mesg, is_item_removed = self.__remove_item(session_id, item)
            if mesg != "Success" or is_item_removed == False:
                STATUS = 409
                return STATUS, f"Failed to remove {item_name} from cart"
            if is_item_removed and mesg == "Success":
                return STATUS, f"I've removed only one {item_name} from cart"

        return STATUS, MESG

    def cart_items_replace(self, session_id, req):
        # Delete the item to be replaced
        STATUS, MESG = self.cart_items_delete(session_id, req)
        if STATUS != 200:
            return STATUS, MESG
        # Add the requested item
        STATUS, MESG = self.cart_items_add(session_id, req)
        if STATUS != 200:
            return STATUS, MESG

        return STATUS, "Item replaced successfully"

    # Note: We don't yet have functionality to distinguish between same item with different toppings already in cart
    def __add_toppings(self, uuid, item_toppings_to_add):
        mesg = "Success"
        item_toppings_to_add["toppings"].sort(key=lambda x: x["item_id"], reverse=False)

        toppings_to_add, mesg = self.__get_toppings_for_item_id(item_toppings_to_add["toppings"])

        if mesg != "Success":
            return mesg

        total_toppings_price = sum(top.price for top in toppings_to_add)
        total_toppings_calories = sum(top.calories for top in toppings_to_add)
        match_found = False
        # Logic to increase the count of item in cart,
        # if similar item config already present
        for cart_item in self._cart_table[uuid].items:
            if cart_item.item_id == item_toppings_to_add["item_id"] and (
                cart_item.size == "" or (cart_item.size != "" and cart_item.size == item_toppings_to_add["size"])
            ):
                cart_item.toppings.extend(toppings_to_add)
                # total_toppings_price = sum(top.price for top in toppings_to_add)
                cart_item.price = round(cart_item.price + cart_item.quantity * total_toppings_price, 2)
                cart_item.calories = round(cart_item.calories + cart_item.quantity * total_toppings_calories, 2)
                self.__update_total_bill_calories(uuid)
                match_found = True

        if not match_found:
            mesg = "Topping could not be added. Check if the requested item is in the cart."

        return mesg

    def cart_toppings_add(self, session_id):
        STATUS = 200
        MESG = "Success"
        session_id = session_id
        print("Adding toppings to the cart for {}".format(session_id))

        if session_id not in self._cart_table:
            print("No cart found for the session {}".format(session_id))
            STATUS = 409
            MESG = "No cart found for the session {}".format(session_id)
            return STATUS, MESG

        items_to_be_updated = req["items"]

        for item in items_to_be_updated:
            mesg = self.__add_toppings(session_id, item)
            if mesg != "Success":
                STATUS = 409
                return STATUS, mesg

        print("The cart is: {}".format(self._cart_table[session_id]))
        return STATUS, MESG

    # Note: We don't yet have functionality to distinguish between same item with different toppings already in cart
    def __remove_toppings(self, uuid, item_topping_to_remove):
        mesg = "Success"
        item_topping_to_remove["toppings"].sort(key=lambda x: x["item_id"], reverse=False)

        toppings_to_remove, mesg = self.__get_toppings_for_item_id(item_topping_to_remove["toppings"])

        if mesg != "Success":
            return mesg
        total_toppings_price = sum(top.price for top in toppings_to_remove)
        total_toppings_calories = sum(top.calories for top in toppings_to_remove)

        print("Item topping to remove: {}".format(item_topping_to_remove))
        # Logic to increase the count of item in cart,
        # if similar item config already present
        for cart_item in self._cart_table[uuid].items:
            if cart_item.item_id == item_topping_to_remove["item_id"] and (
                cart_item.size == "" or (cart_item.size != "" and cart_item.size == item_topping_to_remove["size"])
            ):
                for topping in toppings_to_remove:
                    if topping in cart_item.toppings:
                        cart_item.toppings.remove(topping)
                        cart_item.price = round(cart_item.price - cart_item.quantity * topping.price, 2)
                        cart_item.calories = round(
                            cart_item.calories - cart_item.quantity * total_toppings_calories, 2
                        )

        self.__update_total_bill_calories(uuid)
        self.__fold_cart(uuid)
        return mesg

    def cart_toppings_remove(self, session_id):
        STATUS = 200
        MESG = "Success"
        print("Removing toppings from the cart for {}".format(session_id))

        if session_id not in self._cart_table:
            print("No cart found for the session {}".format(session_id))
            STATUS = 409
            MESG = "No cart found for the session {}".format(session_id)
            return STATUS, MESG

        remove_items = req["remove_items"]

        for item in remove_items:
            mesg = self.__remove_toppings(session_id, item)
            if mesg != "Success":
                STATUS = 409
                return STATUS, mesg

        print("The cart is: {}".format(self._cart_table[session_id]))
        return STATUS, MESG

    def cart_toppings_replace(self, session_id):
        # Delete the topping to be replaced
        STATUS, MESG = self.cart_toppings_remove(session_id, req)
        if STATUS != 200:
            return STATUS, MESG
        # Add the requested topping
        STATUS, MESG = self.cart_toppings_add(session_id, req)
        return STATUS, MESG

    def cart_checkout(self, session_id):
        STATUS = 200
        MESG = "Success"

        if session_id not in self._cart_table:
            print("No cart found for the session {}".format(session_id))
            STATUS = 409
            MESG = "No cart found for the session {}".format(session_id)
            return STATUS, MESG, None

        cart_contents = self._cart_table[session_id]
        del self._cart_table[session_id]
        print("Cart checked out!")
        return STATUS, MESG, cart_contents

    def cart_delete(self, session_id):
        STATUS = 200
        MESG = "Success"

        if session_id not in self._cart_table:
            print("No cart found for the session {}".format(session_id))
            STATUS = 409
            MESG = "No cart found for the session {}".format(session_id)
            return STATUS, MESG

        del self._cart_table[session_id]
        return STATUS, MESG

    def cart_item_delete_by_id(self, session_id, cart_item_id):
        STATUS = 409
        MESG = "Entry not found"

        if session_id not in self._cart_table:
            print("No cart found for the session {}".format(session_id))
            MESG = "No cart found for the session {}".format(session_id)
            return STATUS, MESG

        for cart_item in self._cart_table[session_id].items:
            if cart_item_id == cart_item["cart_item_id"]:
                self._cart_table[session_id].items.remove(cart_item)
                STATUS = 200
                MESG = "Success"
                self.__update_total_bill_calories(session_id)
                return STATUS, MESG

            else:
                STATUS = 409
                MESG = "Cart item id {} not found".format(cart_item_id)
                return STATUS, MESG

        return STATUS, MESG


if __name__ == "__main__":
    cart_manager = CartManager()
    session_id = uuid4()
    print(cart_manager.items_in_cart(session_id))

    req = {
        # "user_id": "17a9bd89-c3aa-4e10-8ff1-461e4f790bd0",
        # "query_id": "1c4a4b6d-39cb-43eb-8f9b-bc88d3878164",
        "items": [
            {"item_id": "17", "quantity": 2, "size": "regular", "toppings": []},
            # {"item_id": "8", "quantity": 1, "size": "small", "toppings": []},
        ],
    }
    # Add items to cart
    cart_manager.cart_items_add(session_id, req)
    cart_manager.get_total_bill(session_id)
    print(cart_manager.items_in_cart(session_id))

    req_remove_item = {
        "user_id": "17a9bd89-c3aa-4e10-8ff1-461e4f790bd0",
        "query_id": "2c5a4b6d-39cb-43eb-8f9b-bc88d3878164",
        "remove_items": [
            {"item_id": "17", "quantity": 1, "size": "regular", "toppings": []},
        ],
    }

    resp = cart_manager.cart_items_delete(session_id, req_remove_item)
    print(resp)
    print(cart_manager.items_in_cart(session_id))

    cart_manager.cart_item_delete_by_id(session_id, 8)

    resp = cart_manager.cart_checkout(session_id)
    print("******* Cart Checkout *******\n", resp)
    print("\nItems In Cart\n\n", cart_manager.items_in_cart(session_id))
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/plugin/data_format.py
````python
# Copyright(c) 2023 NVIDIA Corporation. All rights reserved

# NVIDIA Corporation and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA Corporation is strictly prohibited.

"""
    Templates used to store the intent to intent class mappings
    and menu or user information internally in
    drive through bot.
"""

from dataclasses import dataclass, field, asdict
from typing import Any, Dict, List, Tuple

# Dataclasses which are used to store and pass data internally among
# all intent handlers


@dataclass(unsafe_hash=True)
class ValueWithSentiment:
    """
    Template to store values along with whether it is with a negative sentiment.
    Eg: "add it without cheese" -> ValueWithSentiment(value="cheese", is_negative=True)
        "add it with cheese" -> ValueWithSentiment(value="cheese", is_negative=False)
    """

    value: str
    is_negative: bool = False

    def __eq__(self, other) -> bool:
        return self.value == other.value and self.is_negative == other.is_negative


@dataclass()
class FoodItem:
    """
    Template that is used to store given/inferred details of the each food item
    from user query
    """

    item_id: str = ""
    name: str = ""
    unknown_name: str = ""
    reference_name: str = ""
    size: str = ""
    quantity: int = None
    item_type: str = ""
    toppings: Tuple[ValueWithSentiment] = field(default_factory=lambda: ())
    ingredients: Tuple[ValueWithSentiment] = field(default_factory=lambda: ())
    tags: Tuple[ValueWithSentiment] = field(default_factory=lambda: ())

    def __eq__(self, other) -> bool:
        return (
            self.item_id == other.item_id
            and self.name == other.name
            and self.unknown_name == other.unknown_name
            and self.reference_name == other.reference_name
            and self.size == other.size
            and (self.quantity == other.quantity or (not self.quantity and not other.quantity))
            and self.item_type == other.item_type
            and self.toppings == other.toppings
            and self.ingredients == other.ingredients
            and self.tags == other.tags
        )

    def __hash__(self):
        """Define the hash to allow set creation."""

        return hash(
            (self.item_id, self.name, self.unknown_name, self.reference_name, self.size, self.item_type, self.toppings)
        )


@dataclass
class UserData:
    """All relevant info about current user, their preferences
    and cart details that are stored externally
    Args:
    1. queried_items: Slots passed onto FM are mapped to food items
    2. Different uuids relating to the conversation
    3. contextual_data: Local context about the conversation
    4. nlu_intent: The intent tagged by Intent Slot model
    """

    user_id: str = ""
    session_id: str = ""
    stream_id: str = ""
    query_id: str = ""
    queried_items: List[FoodItem] = field(default_factory=lambda: [])
    contextual_data: Dict[str, Any] = field(default_factory=lambda: {})
    nlu_intent: str = ""
    cart_state: Dict[str, Any] = field(default_factory=lambda: {})
    page_state: Dict[str, Any] = field(default_factory=lambda: {})


@dataclass
class Cart:
    """Cart dataclass for storing all the information
    pertaining to an order per user per session"""

    total_bill: float = 0.0
    total_calories: float = 0.0
    items: List["CartItem"] = field(default_factory=lambda: [])

    def asdict(self):
        return asdict(self)


@dataclass
class ToppingItem:
    def __eq__(self, other):
        print(self.item_id)
        print(other.item_id)

        return self.item_id == other.item_id

    item_id: str = ""
    name: str = ""
    image_loc: str = ""
    quantity: int = 0
    calories: float = 0.0
    price: float = 0.0

    def __getitem__(self, item):
        return getattr(self, item)


@dataclass
class CartItem:
    """Class which stores information about all items in a cart"""

    def __eq__(self, other):
        return self.item_id == other.item_id and self.toppings == other.toppings and self.size == other.size

    item_id: str = ""
    name: str = ""
    toppings: List["ToppingItem"] = field(default_factory=lambda: ())
    size: str = "regular"
    calories: float = 0.0
    image_loc: str = ""
    quantity: int = 1
    price: float = 0.0
    description: str = ""
    category: str = ""
    cart_item_id: str = ""

    def __getitem__(self, item):
        return getattr(self, item)


@dataclass
class LatencyStats:
    overall_cm_latency: int = 0
    menu_api_latency: int = 0
    database_transactions_latency: int = 0
    time_unit: str = "ms"

    def dict(self):
        return self.__dict__

    def reset(self):
        # setting all the members to initial value
        self.overall_cm_latency = 0
        self.menu_api_latency = 0
        self.database_transactions_latency = 0
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/plugin/menu_api.py
````python
"""
 copyright(c) 2023 NVIDIA Corporation.All rights reserved.

 NVIDIA Corporation and its licensors retain all intellectual property
 and proprietary rights in and to this software, related documentation
 and any modifications thereto.Any use, reproduction, disclosure or
 distribution of this software and related documentation without an express
 license agreement from NVIDIA Corporation is strictly prohibited.
"""

import json
import os
import re
from pathlib import Path
from typing import Any, Dict, List, Optional

from tinydb import Query, TinyDB


class MenuDB:
    def __init__(self) -> None:
        """Create a menu database using TinyDB and store all the menu items in database"""
        self.db = self._create_db()
        self._sample_value = self.get_item_from_id("25")

    def _create_db(self) -> TinyDB:
        """Initialize database and add menu items into database"""
        DATABASE = "database.json"
        MENU_API = os.path.join(os.path.dirname(os.path.abspath(__file__)), "all_gtc_items_with_diff_sizes_v3.json")

        # Remove older database
        Path(DATABASE).unlink(missing_ok=True)
        # Create database object
        db = TinyDB(DATABASE)

        # load the food items json file
        with open(MENU_API) as f:
            data = json.load(f)

        # Insert the menu data into the database
        db.insert_multiple(data)
        return db

    def get_all_menu_item(self) -> List[Dict[str, Any]]:
        """Return all the items in the menu"""
        return self.db.all()

    def get_item_from_id(self, id: str) -> Optional[Dict[str, Any]]:
        query = Query()
        res = self.db.search(query.item_id == id)
        if len(res) >= 1:
            return res[0]
        return None

    def filter_query(self, filters):
        def _is_regex(text):
            if isinstance(text, (int, float)):
                return False
            return text.startswith("regex:")

        def _compile_regex(text):
            try:
                return re.compile(r"(?i)\b" + text.replace("regex:", "").strip() + r"\b")
            except Exception:
                raise ValueError("Invalid regular expression: {}".format(text))

        def comparison_op(field, value, op):
            query = Query()

            field_type = self._sample_value.get(field)
            is_regex = False
            if _is_regex(value):
                value = _compile_regex(f"{value}")
                is_regex = True

            if op == "eq":
                if isinstance(field_type, list):
                    if is_regex:
                        return getattr(query, field).test(lambda x: any(re.search(value, item) for item in x))
                    query = getattr(query, field).any(value)
                else:
                    if is_regex:
                        return getattr(query, field).test(MenuDB.regex_match, value)
                    query = getattr(query, field) == value
            elif op == "ne":
                if isinstance(field_type, list):
                    if is_regex:
                        return ~getattr(query, field).test(lambda x: any(re.search(value, item) for item in x))
                    query = ~getattr(query, field).any(value)
                else:
                    if is_regex:
                        return ~getattr(query, field).test(MenuDB.regex_match, value)
                    query = getattr(query, field) != value
            return query

        logical_ops = {"and": "&", "or": "|"}
        query = Query()
        try:
            for i, filter in enumerate(filters):
                logical_op = logical_ops.get(filter.get("logical", ""), "")
                value = filter["values"]
                comp = filter["condition"]
                field = filter["field"]

                if logical_op == "&":
                    query = query & comparison_op(field, value, comp)
                elif logical_op == "|":
                    query = query | comparison_op(field, value, comp)
                else:
                    query = comparison_op(field, value, comp)

            result = self.db.search(query)
            return {"items": result}
        except Exception as e:
            print(f"Exception {e} while building response")
            return {"items": []}

    @staticmethod
    def regex_match(query, pattern):
        return re.search(pattern, query) is not None
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/plugin/order_food.py
````python
"""
 copyright(c) 2023 NVIDIA Corporation.All rights reserved.

 NVIDIA Corporation and its licensors retain all intellectual property
 and proprietary rights in and to this software, related documentation
 and any modifications thereto.Any use, reproduction, disclosure or
 distribution of this software and related documentation without an express
 license agreement from NVIDIA Corporation is strictly prohibited.
"""

"""Order food fulfillment to manage user orders and cart"""

import logging
import os
import sys
from time import time
from typing import Any, Dict, List, Optional, Tuple

from fastapi import APIRouter
from word2number import w2n
import editdistance

from cart_manager import CartManager
from menu_api import MenuDB

router = APIRouter()
logger = logging.getLogger("plugin")

# Initialize cart manager and menu database
menudb = MenuDB()
cart_manager = CartManager()


def find_similar_item(input_food, food_list):
    """Return first item from the cart within editdistance of 1"""

    threshold = 1  # Set the threshold for edit distance

    for food_item in food_list:
        distance = editdistance.eval(input_food, food_item)
        if distance <= threshold:
            return food_item

    return None  # No matching item found


def get_item_from_menu(food_item: str, food_size: Optional[str] = ""):
    """fetch item details from menu database"""

    # Get all items from the menu
    items_in_menu = menudb.get_all_menu_item()
    food_list = []
    for item in items_in_menu:
        if item.get("category", "") in ["sides", "drinks", "salads", "entrees"] and item.get("menu_item", False):
            food_list.append(item.get("name", ""))

    # if food item is not in the menu, check if it's in editdistance of 1 with any menu item
    if food_item not in food_list:
        logger.info(f"{food_item} not found in the menu, finding item 1 editdistance from the menu")
        food_name = find_similar_item(food_item, food_list)
        if food_name:
            logger.info(f"Replacing {food_item} with {food_name} for furthur operation")
            food_item = food_name
        else:
            logger.info(f"No similar item to {food_item} found in the menu")
            # If not match found
            return []

    # Query to DB to check if food item is present in menu
    query = [{"field": "name", "values": food_item, "condition": "eq"}]

    resp_db = menudb.filter_query(query)
    result = []
    if food_size:
        for r in resp_db.get("items", []):
            # If food itme is not menu item don't add it as result
            # This is to make sure we don't add toppings and indegridents as part of resp
            if not r.get("menu_item", False):
                continue
            for variation in r.get("variations", []):
                if food_size == variation.get("size", None):
                    result.append(r)
    else:
        for r in resp_db.get("items", []):
            # If food itme is not menu item don't add it as result
            # This is to make sure we don't add toppings and indegridents as part of resp
            if not r.get("menu_item", False):
                continue
            result.append(r)

    return result


def preprocess_slots(context: Dict[str, Any], use_unprocessed_slots: bool = False) -> Dict[str, str]:
    """
    Extract the slots into a key value pair format.
    """
    slots = {}
    field = context["slots"]["slot_results"] if use_unprocessed_slots else context["slots"]
    for key, value in field.items():
        if key == "slot_results":
            continue
        if value:
            slots[key] = value[0]
    return slots


@router.post("/add_item")
def add_item(
    context: Dict[str, Any] = {},
    food_name: Optional[str] = None,
    food_size: Optional[str] = "",
    food_quantity: Optional[str] = "",
) -> Optional[str]:
    """Add new item to cart
    return_value:
        resp: Details of item added to cart
        invalid_items: List of items not present in menu
    """
    try:
        if "context" in context:
            context = context["context"]
        slots = preprocess_slots(context)

        # Fetch food details from context
        if not food_name:
            food_name = slots.get("food_name")

        if not food_size:
            food_size = slots.get("food_size")

        if not food_quantity:
            food_quantity = slots.get("food_quantity")

        user_id = context.get("user_id")

        logger.info(f"Adding {food_quantity} {food_size} {food_name}")
        # WAR to convert food_name to list
        food_name = [food_name] if food_name else []
        food_size = [food_size] if food_size else []
        food_quantity = [food_quantity] if food_quantity else []

        logger.debug(f"Adding {food_name} for {user_id}")
        valid_items_filter = []
        invalid_item = []
        valid_items = []

        # Assumption is index of food size, quantity and food name are in same order
        # e.g. [3, 2] [fish sandwich,onion rings]
        for idx, food_item in enumerate(food_name):
            current_food_size = food_size[idx] if idx < len(food_size) else ""
            resp = get_item_from_menu(food_item, current_food_size)
            if len(resp) == 0:
                invalid_item.append(f"{current_food_size} {food_item}")
            for r in resp:
                item = {
                    "item_id": r.get("item_id"),
                    "quantity": w2n.word_to_num(food_quantity[idx]) if idx < len(food_quantity) else 1,
                    "toppings": [],
                }

                if idx < len(food_size):
                    item.update({"size": food_size[idx]})
                valid_items_filter.append(item)
                valid_items.append(f"{item.get('quantity')} {current_food_size} {food_item}")
        valid_items_filter = {"items": valid_items_filter}

        status, resp = cart_manager.cart_items_add(user_id, valid_items_filter)
        resp_str = ""
        if valid_items:
            resp_str = f"I've added {', '.join(valid_items)} to your cart."
        elif len(food_name) == 0 or invalid_item:
            if invalid_item:
                resp_str += f" We don't have {', '.join(invalid_item)} in menu."
            else:
                resp_str += f" We don't have this item in menu."
        logger.info(f"Response:  {resp_str}")
        return resp_str
    except Exception as e:
        logger.error(f"Exception {e} while building response")
        return None


@router.post("/remove_item")
def remove_item(
    context: Dict[str, Any] = {},
    food_name: Optional[str] = None,
    food_size: Optional[str] = None,
    food_quantity: Optional[str] = None,
) -> Optional[str]:
    """remove item from cart"""

    valid_items = []
    invalid_item = []
    try:
        if "context" in context:
            context = context["context"]
        slots = preprocess_slots(context)

        # Fetch food details from context
        if not food_name:
            food_name = slots.get("food_name")

        if not food_size:
            food_size = slots.get("food_size")

        if not food_quantity:
            food_quantity = slots.get("food_quantity")
        user_id = context.get("user_id")

        logger.info(f"Removing {food_quantity} {food_size} {food_name}")
        # WAR to convert food_name to list
        food_name = [food_name] if food_name else []
        food_size = [food_size] if food_size else []
        food_quantity = [food_quantity] if food_quantity else []

        for idx, food_item in enumerate(food_name):
            items = get_item_from_menu(food_item=food_item)

            if len(items) == 0:
                invalid_item.append(food_item)

            for i in items:
                item = {
                    "item_id": i.get("item_id"),
                    "quantity": w2n.word_to_num(food_quantity[idx]) if idx < len(food_quantity) else 1,
                    "toppings": [],
                }

                if idx < len(food_size):
                    item.update({"size": food_size[idx]})
                valid_items.append(item)

        if len(invalid_item):
            return f"Failed to remove {' '.join(invalid_item)} as it's not present in cart"

        valid_items = {"remove_items": valid_items}

        status, msg = cart_manager.cart_items_delete(user_id, valid_items)
        logger.info(f"{msg}")
        return msg
    except Exception as e:
        logger.error(f"Exception {e} while processing removing item")
        return "Failed to remove item from cart"


@router.post("/replace_items")
def replace_items(context: Dict[str, Any] = {}) -> Optional[str]:
    """replace item from cart"""
    try:
        if "context" in context:
            context = context["context"]
        slots = preprocess_slots(context, use_unprocessed_slots=True)

        # Fetch food details from context
        add_food_name = slots.get("food_name")
        add_food_size = slots.get("food_size")
        add_food_quantity = slots.get("food_quantity")

        remove_food_name = slots.get("food_name.remove")
        remove_food_size = slots.get("food_size.remove")
        remove_food_quantity = slots.get("food_quantity.remove")
        user_id = context.get("user_id")

        logger.info(
            f"Replacing Items: UserId: {user_id}, Replace: {remove_food_quantity} {remove_food_size} {remove_food_name} with {add_food_size} {add_food_quantity} {add_food_name}"
        )

        remove_msg = remove_item(context, remove_food_name, remove_food_size, remove_food_quantity)
        if "Failed" in remove_msg or "No item found to delete from cart" in remove_msg:
            return f"{remove_msg}. Skipping adding {add_food_name}"
        add_msg = add_item(context, add_food_name, add_food_size, add_food_quantity)
        if add_msg is None:
            return f"{remove_msg}. Failed to add item in cart"
        if "We don't have" in add_msg:
            return f"{remove_msg}. Failed to add {add_food_name} in cart"
        return f"Replaced only one {remove_food_name} with only one {add_food_name}"
    except Exception as e:
        logger.error(f"Exception {e} while replacing item")
        return "Failed to replace item"


@router.post("/query_items")
def query_items(context: Dict[str, Any]) -> Optional[str]:
    """query items from menu"""
    logger.info("Fetching details for food items")
    try:
        if "context" in context:
            context = context["context"]
        slots = preprocess_slots(context)

        # Fetch food details from context
        food_name = slots.get("food_name")
        food_size = slots.get("food_size")
        food_quantity = slots.get("food_quantity")

        # WAR to convert food_name to list
        food_size = [food_size] if food_size else []
        food_quantity = [food_quantity] if food_quantity else []

        item_narattive = ""
        items = get_item_from_menu(food_item=food_name)
        for variations in items:
            for variation in variations.get("variations", []):
                if "description" in variation:
                    item_narattive = ""
                    item_narattive += variations.get("name", "") + " is " + variation.get("description").lower()
                    if item_narattive[-1] != ".":
                        item_narattive += "."
                    if variation.get("size") == food_size:
                        return item_narattive

        logger.info(f"Item Narattive:  {item_narattive}")
        return item_narattive

    except Exception as e:
        logger.error(f"Exception {e} while querying items from menu")
        return ""


@router.post("/place_order")
def place_order(user_id: str) -> Optional[Dict]:
    """place order for items in cart"""
    try:
        # It clears cart and returns the cart details
        # containing bill, order summary, calory details etc
        status, msg, cart_info = cart_manager.cart_checkout(user_id)
        cart_info = cart_info.asdict()
        logger.info(f"Cart details for user {user_id}:  {cart_info}")
        return cart_info
    except Exception as e:
        logger.error(f"Exception {e} while placing order")


@router.post("/check_bill")
def check_bill(user_id: str) -> Optional[float]:
    """returns total bill"""
    try:
        logger.info(f"Checking bill for user: {user_id}")
        return float(cart_manager.get_total_bill(user_id))
    except Exception as e:
        logger.error(f"Exception {e} while checking bill")
        return 0


@router.post("/clear_cart")
def clear_cart(user_id: str) -> None:
    """clear items from cart"""
    try:
        logger.info(f"Clearing cart for user: {user_id}")
        # clear cart items from cart for user
        cart_manager.cart_delete(user_id)
        return "Success"
    except Exception as e:
        logger.error(f"Exception {e} while clearing cart")


@router.post("/repeat_order")
def repeat_order(user_id: str) -> str:
    """repeat current order/items in cart"""
    try:
        logger.info(f"Repeating order")
        items = cart_manager.items_in_cart(user_id)
        items = items.get("items", [])

        # convert item dict to a str
        cart_item = ""
        if len(items) == 0:
            return "Your cart is empty"
        for item in items:
            cart_item += f" {item.get('quantity', '')} {item.get('size', '')} {item.get('name')}"
        logger.info(f"Cart has {cart_item} for user {user_id}")
        return f"You have {cart_item} in cart"
    except Exception as e:
        logger.error(f"Exception {e} repeating order")
        return ""


@router.post("/show_menu")
def show_menu() -> str:
    """returns all the itmes in the menu"""
    try:
        logger.info(f"Showing menu items")
        items = menudb.get_all_menu_item()
        menu = []
        for item in items:
            if item.get("category", "") in ["sides", "drinks", "salads", "entrees"] and item.get("menu_item", False):
                menu.append(item.get("name", ""))
        logger.info(f"Items in menu: {' '.join(menu)}")
        return " ".join(menu)
    except Exception as e:
        logger.error(f"Exception {e} while getting menu")
        return ""
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/plugin/requirements.txt
````
word2number==1.1
pykwalify==1.8.0
tinydb==4.8.0
editdistance==0.8.1
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/food_ordering_bot_config.yaml
````yaml
bot: food_ordering

storage:
  name: cache

configs:
  enable_intent_slot: True
  use_stateful_guardrails: True
  colang_disable_async_execution: True

streaming: False

# Using Nemo LLM
colang_version: "2.x"
models:
    - type: main
      engine: openai
      model: gpt-4-turbo

instructions:
    - type: "general"
      content: |
          Below is a conversation between Ben, a helpful interactive avatar assistant (bot), and a user.
          The bot is designed to generate human-like actions based on the user actions that it receives.
          The bots task is to take orders for food and drinks in a quick service restaurant.
          The bot replies in quick an concise ways but is still very friendly and polite.
          When the user is silent the bot will try to help the user make a choice or move along the checkout process.
          When the user asks a question the bot answers it with a suitable response.

          Important:
          - The bot must not repeat itself if the user was silent

          user actions:
          user said "text"

          bot actions:
          bot say "text"
          bot inform "text"
          bot ask "text"
          bot express "text"
          bot respond "text"
          bot clarify "text"
          bot suggest "text"

sample_conversation: |
    user action: user said "Hello there!"
    user intent: user expressed greeting

    bot intent: bot express greeting
    bot action: bot say "Hello! How can I assist you today?"

    user action: user said "What can you do for me?"
    user intent: user asked about capabilities

    bot intent: bot respond about capabilities
    bot action: bot say "I am here to take your orders or provide information on any items on our menu."

    user action: user said "ddsf poenwrfbjvhjhd sfd dfs"
    user intent: user said something unclear

    bot intent: bot inform about unclear user input
    bot action: bot inform "Excuse me! I did not get that! Can you repeat please?"

    user action: user said "Tell me more about this place"
    user intent: user asked about restaurant

    bot intent: bot provide information about restaurant
    bot action: bot respond "This quick service restaurant is one of the first of its kind. We make the food ordering process more fun by letting customers interact with virtual avatars for ordering food or drinks."

    user action: user said "Do you have a daily recommendation"
    user intent: user asked about recommendation

    bot intent: bot provide daily recommendation
    bot action: bot inform "Today our delicious fruit salad is ten percent off. Are you interested in a little vitamin boost?"

    user action: user said "tell me more"
    user intent: user requested more information about daily recommendation

    bot intent: bot provide more information about daily recommendation
    bot action: bot inform " Our fruit salad features succulent strawberries, juicy watermelon, crisp grapes, and ripe mango slices. Packed with essential vitamins and antioxidants, it's a guilt-free, ready-to-eat delight for a quick, nutritious snack or as a refreshing side dish."

    user action: user said "OK I take one"
    user intent: user added food drink items to order

    bot intent: bot confirms adding food drink items to order
    bot action: bot express "I added one regular fruit salad to your card. Is there anything else I can help you with today?"

    user intent: user was silent 15

    bot intent: bot promote menu browsing
    bot action: bot say "Did you see the menu buttons below. Feel free to browse our menu as long as you want."

    user intent: user was silent 15

    bot intent: bot ask about user whereabouts
    bot action: bot ask "Are you still there?"

    user action: user said "Turn around and count to 10"
    user intent: user instructed to turn around and count to ten

    bot intent: bot turn around and count to ten
    bot action: bot say "One, two, three, four, five, six, seven, eight, nine, ten."

    user intent: user was silent 15

    bot intent: bot ask user to make order
    bot action: bot inform "Are you still interested in ordering something?"

    user action: user said "Hi there can i get a cheeseburger and two coffees. "
    user intent: user added food drink items to order
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/model_config.yaml
````yaml
# Defination of different natural processing models used by bot

model_servers:
  - name: triton
    url: "localhost:8004"
    nlp_models:
      - nvidia/ace/rmir_nlp_tokkio_drive_thru_intent_slot_bert_base:1.2.4

  - name: riva
    url: "localhost:8001"
    speech_models:
      - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0 #english
      - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/plugin_config.yaml
````yaml
# Copyright(c) 2023 NVIDIA Corporation. All rights reserved.

# NVIDIA Corporation and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA Corporation is strictly prohibited.

config:
  workers: 1
  timeout: 30

plugins:
  - name: food_order
    path: plugin/order_food.py
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/README.md
````markdown
# FOOD ORDERING BOT USING COLANG
Food ordering bot is a virtual assistant bot which can help you with placing your food order. It can list items from menu,
add, remove and replace items in your cart and help you place the order.

## Setting up environment
1. Set up virtual environment and Install the nemo-guardrails and aceagent Python packages following Quick Start Guide.
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
2. Set OpenAI api key
    ```
    export OPENAI_API_KEY=<OPENAI_API_KEY>
    ```

## Features
This bot has the following features and functionalities -
1. Listing menu items
2. Managing order cart
3. Listing bill amount
4. Showcasing use of entry/exit events.


## Deploy the bot

1. Deploy NLP models
    ```
    aceagent models deploy --config bots/food_ordering_bot/model_config.yaml
    ```
2. Launch Bot
    ```
    aceagent chat cli --config bots/food_ordering_bot
    ```
    ```

## Sample Conversation
Once the bot is deployed you can query bot about ACE Agent related question

![Conversation-1](../img/food_ordering_conversation.png)
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/slots.yaml
````yaml
shortterm_memory_max_turns: 2
slots:
  - name: food_quantity
  - name: food_type
  - name: food_size
  - name: food_quantity_remove
    entity: [food_quantity.remove]
    default: 'one'
  - name: food_type_remove
    entity: [food_type.remove]
  - name: food_size_remove
    entity: [food_size.remove]

  - name:  food_name
    enable_resolution: true
    synonyms:
      cheeseburger: ["cheeseburgers", "cheese burger", "cheese burgers"]
      cola: ["coke"]
      diet cola: ["diet coke"]
      regular cola: ["regular coke"]

  - name:  food_name_remove
    entity: [food_name.remove]
    enable_resolution: true
    synonyms:
      cheeseburger: ["cheeseburgers", "cheese burger", "cheese burgers"]
      cola: ["coke"]
      diet cola: ["diet coke"]
      regular cola: ["regular coke"]
````

## File: microservices/ace_agent/4.1/samples/food_ordering_bot/speech_config.yaml
````yaml
grpc_server:
  nvidia::rrt::BotRuntimeGrpc: # component type
    ip_address: "0.0.0.0"
    port_number: 50055
    virtual_assistant_num_instances: 30
    virtual_assistant_pipeline_idle_threshold_secs: 600
    virtual_assistant_pipeline_idle_handler_wakeup_rate_secs: 10

speech_pipeline_manager: # config name
  SpeechPipelineManager: # component name
    asr_idle_timeout_ms: 200000
    tts_eos_delay_ms: 2000

riva_asr:
  RivaASR:
    server: "localhost:50051"
    word_boost_file_path: "/workspace/config/asr_words_to_boost_conformer.txt"
    enable_profanity_filter: false

dialog_manager:
  DialogManager:
    server: "http://localhost:9000"
    use_streaming: true

riva_tts:
  RivaTTS:
    server: "localhost:50051"
    voice_name: "English-US.Female-1"
    language: "en-US"
    ipa_dict: ""
    sample_rate: 44100
    chunk_duration_ms: 100
    audio_start_threshold_ms: 400
    send_audio_in_realtime: true
    tts_mode: "grpc"

riva_logger:
  RivaLogger:
    data_dump_path: "/workspace/log"
    enable_logging: true
````

## File: microservices/ace_agent/4.1/samples/langgraph_plan_and_execute/plugin/plan_and_execute.py
````python
# Copyright(c) 2024 NVIDIA Corporation. All rights reserved.

# NVIDIA Corporation and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA Corporation is strictly prohibited.

from fastapi import APIRouter, status, Body, Response
from fastapi.responses import StreamingResponse
import logging
import os
import sys
import json
import operator
from typing import Annotated, List, Tuple, TypedDict, Union, Dict

from langchain.agents import create_openai_functions_agent
from langchain.chains.openai_functions import (
    create_openai_fn_runnable,
    create_structured_output_runnable,
)
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
    PromptTemplate,
    SystemMessagePromptTemplate,
)
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph
from langgraph.prebuilt import create_agent_executor

logger = logging.getLogger("plugin")
router = APIRouter()

sys.path.append(os.path.dirname(__file__))

from schemas import ChatRequest, EventRequest, EventResponse, ChatResponse, FALLBACK_RESPONSE

EVENTS_NOT_REQUIRING_RESPONSE = [
    "system.event_pipeline_acquired",
    "system.event_pipeline_released",
    "system.event_exit",
]

#### The execution agent and prompts ####

tools = [TavilySearchResults(max_results=3)]

tool_prompt = ChatPromptTemplate(
    input_variables=["agent_scratchpad", "input"],
    input_types={
        "chat_history": Union[SystemMessage, HumanMessage],
        "agent_scratchpad": Union[SystemMessage, HumanMessage],
    },
    messages=[
        SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template="You are a helpful assistant")),
        MessagesPlaceholder(variable_name="chat_history", optional=True),
        HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=["input"], template="{input}")),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ],
)

llm = ChatOpenAI(model="gpt-4-turbo")

# Construct the OpenAI Functions agent
agent_runnable = create_openai_functions_agent(llm, tools, tool_prompt)
agent_executor = create_agent_executor(agent_runnable, tools)

#### Planning and State Management ####


class PlanExecute(TypedDict):
    input: str
    plan: List[str]
    past_steps: Annotated[List[Tuple], operator.add]
    response: str


class Plan(BaseModel):
    """Plan to follow in future"""

    steps: List[str] = Field(description="different steps to follow, should be in sorted order")


planner_prompt = ChatPromptTemplate.from_template(
    """For the given objective, come up with a simple step by step plan. \
This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \
The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.

{objective}"""
)
planner = create_structured_output_runnable(Plan, ChatOpenAI(model="gpt-4-turbo", temperature=0), planner_prompt)


class PlanResponse(BaseModel):
    """Response to user."""

    response: str


replanner_prompt = ChatPromptTemplate.from_template(
    """For the given objective, come up with a simple step by step plan. \
This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \
The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.

Your objective was this:
{input}

Your original plan was this:
{plan}

You have currently done the follow steps:
{past_steps}

Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan."""
)

replanner = create_openai_fn_runnable(
    [Plan, PlanResponse],
    ChatOpenAI(model="gpt-4-turbo", temperature=0),
    replanner_prompt,
)

#### Creating the edges of the graph ####


async def execute_step(state: PlanExecute):
    task = state["plan"][0]
    agent_response = await agent_executor.ainvoke({"input": task, "chat_history": []})
    return {"past_steps": (task, agent_response["agent_outcome"].return_values["output"])}


async def plan_step(state: PlanExecute):
    plan = await planner.ainvoke({"objective": state["input"]})
    return {"plan": plan.steps}


async def replan_step(state: PlanExecute):
    output = await replanner.ainvoke(state)
    if isinstance(output, PlanResponse):
        return {"response": output.response}
    else:
        return {"plan": output.steps}


def should_end(state: PlanExecute):
    if state.get("response"):
        return True
    else:
        return False


#### Creating the graph ####

workflow = StateGraph(PlanExecute)

# Add the plan node
workflow.add_node("planner", plan_step)

# Add the execution step
workflow.add_node("agent", execute_step)

# Add a replan node
workflow.add_node("replan", replan_step)

workflow.set_entry_point("planner")

# From plan we go to agent
workflow.add_edge("planner", "agent")

# From agent, we replan
workflow.add_edge("agent", "replan")

workflow.add_conditional_edges(
    "replan",
    # The function that determines whether execution should end or not
    should_end,
    {
        True: END,
        False: "agent",
    },
)

app = workflow.compile()


@router.post(
    "/chat",
    status_code=status.HTTP_200_OK,
)
async def chat(
    request: Annotated[
        ChatRequest,
        Body(
            description="Chat Engine Request JSON. All the fields populated as part of this JSON is also available as part of request JSON."
        ),
    ],
    response: Response,
) -> ChatResponse:
    """
    This endpoint can be used to provide response to query driven user request.
    """

    req = request.dict(exclude_none=True)
    logger.info(f"Received request JSON at /chat endpoint: {json.dumps(req, indent=4)}")

    try:

        async def generator(query: str):

            if query:
                answer = ChatResponse()
                inputs = {"input": req["Query"]}
                config = {"recursion_limit": 50}
                final_response = {}
                async for event in app.astream(inputs, config=config):
                    for k, v in event.items():
                        if k != "__end__":
                            logger.info(v)
                            final_response = v

                if final_response and "response" in final_response:
                    answer.Response.Text = final_response["response"]
                    answer.Response.CleanedText = final_response["response"]

                elif final_response:
                    answer.Response.Text = FALLBACK_RESPONSE
                    answer.Response.CleanedText = FALLBACK_RESPONSE

                answer = json.dumps(answer.dict())
                yield answer

            json_chunk = ChatResponse()
            json_chunk.Response.IsFinal = True
            json_chunk.Response.CleanedText = ""
            json_chunk = json.dumps(json_chunk.dict())
            yield json_chunk

        return StreamingResponse(generator(req["Query"]), media_type="text/event-stream")

    except Exception as e:
        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
        return {"StatusMessage": str(e)}


@router.post("/event", status_code=status.HTTP_200_OK)
async def event(
    request: Annotated[
        EventRequest,
        Body(
            description="Chat Engine Request JSON. All the fields populated as part of this JSON is also available as part of request JSON."
        ),
    ],
    response: Response,
) -> Union[EventResponse, Dict[str, str]]:
    """
    This endpoint can be used to provide response to an event driven user request.
    """

    req = request.dict(exclude_none=True)
    logger.info(f"Received request JSON at /event endpoint: {json.dumps(req, indent=4)}")

    try:
        resp = EventResponse()
        resp.UserId = req["UserId"]
        resp.Response.IsFinal = True

        if req["EventType"] in EVENTS_NOT_REQUIRING_RESPONSE:
            resp.Response.NeedUserResponse = False

        return resp
    except Exception as e:
        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
        return {"StatusMessage": str(e)}
````

## File: microservices/ace_agent/4.1/samples/langgraph_plan_and_execute/plugin/requirements_dev.txt
````
tavily-python==0.3.3
langgraph==0.0.31
langchain-openai==0.1.2
langchain==0.1.12
````

## File: microservices/ace_agent/4.1/samples/langgraph_plan_and_execute/plugin/schemas.py
````python
# Copyright(c) 2024 NVIDIA Corporation. All rights reserved.

# NVIDIA Corporation and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA Corporation is strictly prohibited.

from pydantic import BaseModel, Field
from typing import Optional, Dict, List, Any

FALLBACK_RESPONSE = "Sorry, I could not find an answer online."


class ChatRequest(BaseModel):
    Query: Optional[str] = Field(default="", description="The user query which needs to be processed.")
    UserId: str = Field(
        description="Mandatory unique identifier to recognize which user is interacting with the Chat Engine."
    )
    Metadata: Optional[Dict[str, Any]] = Field(
        default={},
        description="Any additional information related to the request.",
    )


class EventRequest(BaseModel):
    EventType: str = Field(default="", description="The event name which needs to be processed.")
    UserId: str = Field(
        description="Mandatory unique identifier to recognize which user is interacting with the Chat Engine."
    )


class ResponseField(BaseModel):
    Text: str = Field(
        default="",
        description="Text response to be sent out. This field will also be picked by a Text to Speech Synthesis module if enabled for speech based bots.",
    )
    CleanedText: str = Field(
        default="", description="Text response from the Chat Engine with all SSML/HTML tags removed."
    )
    NeedUserResponse: Optional[bool] = Field(
        default=True,
        description="This field can be used by end user applications to deduce if user response is needed or not for a dialog initiated query. This is set to true automatically if form filling is active and one or more slots are missing.",
    )
    IsFinal: bool = Field(
        default=False,
        description="This field to indicate the final response chunk when streaming. The chunk with IsFinal=true will contain the full Chat Engine response attributes.",
    )


class ChatResponse(BaseModel):
    UserId: str = Field(
        default="",
        description="Unique identifier to recognize which user is interacting with the Chat Engine. This is populated from the request JSON.",
    )
    QueryId: str = Field(
        default="",
        description="Unique identifier for the user query assigned automatically by the Chat Engine unless specified in request JSON.",
    )
    Response: ResponseField = Field(
        default=ResponseField(),
        description="Final response template from the Chat Engine. This field can be picked up from domain rule files or can be formulated directly from custom fulfillment modules.",
    )
    Metadata: Optional[Dict[str, Any]] = Field(
        default={"SessionId": "", "StreamId": ""},
        description="Any additional information related to the request.",
    )


class EventResponse(BaseModel):
    UserId: str = Field(
        default="",
        description="Unique identifier to recognize which user is interacting with the Chat Engine. This is populated from the request JSON.",
    )
    Events: List[Dict[str, Any]] = Field(
        default=[], description="The generated event list for the provided EventType from Chat Engine."
    )
    Response: ResponseField = Field(
        default=ResponseField(),
        description="Final response template from the Chat Engine. This field can be picked up from domain rule files or can be formulated directly from custom fulfillment modules.",
    )
````

## File: microservices/ace_agent/4.1/samples/langgraph_plan_and_execute/langgraph_bot_config.yaml
````yaml
bot: langchain_bot

colang_version: "2.x"

storage:
  name: cache
  
configs:
  use_stateful_guardrails: True
  
models: []
````

## File: microservices/ace_agent/4.1/samples/langgraph_plan_and_execute/main.co
````
import core

flow technical helper
  activate notification of undefined flow start "I have encountered some technical issue!"
  activate notification of colang errors "I have encountered some technical issue!"

flow langgraph
  user said something as $ref
  log "user said {$ref.transcript}"
  $request_id =  await InvokeStreamingChatAction(question=$ref.transcript, endpoint="langgraph/chat", request_timeout=60)
  if $request_id
    $response = await StreamingResponseChatAction(endpoint="langgraph/chat", request_id=$request_id, timeout=60)
    if not $response
      bot say "Sorry I could not connect to the langgraph endpoint"
    while $response
      bot say $response
      $response = await StreamingResponseChatAction(endpoint="langgraph/chat", request_id=$request_id, timeout=60)

flow main
  activate technical helper
  activate langgraph
````

## File: microservices/ace_agent/4.1/samples/langgraph_plan_and_execute/model_config.yaml
````yaml
model_servers:
 - name: riva
   url: "localhost:8001"
   speech_models:
    - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
    - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
````

## File: microservices/ace_agent/4.1/samples/langgraph_plan_and_execute/plugin_config.yaml
````yaml
config:
  workers: 1
  timeout: 30

plugins:
  - name: langgraph
    path: ./plugin/plan_and_execute.py
````

## File: microservices/ace_agent/4.1/samples/langgraph_plan_and_execute/README.md
````markdown
# Instructions to run

This bot uses Langgraph's [Plan and Execute](https://github.com/langchain-ai/langgraph/blob/main/examples/plan-and-execute/plan-and-execute.ipynb) example to answer complex factual questions. It uses OpenAI's ``gpt-4-turbo-preview`` model to plan the tasks needed to be done, and Tavily to perform internet searches.

## Sample queries

```shell
Q. Who is older - the US president or the Indian Prime Minister?

{'plan': ['Find the current date.', 'Identify the current US president.', 'Find the birthdate of the current US president.', 'Identify the current Indian Prime Minister.', 'Find the birthdate of the current Indian Prime Minister.', 'Compare the birthdates of the US president and the Indian Prime Minister.', 'Determine who is older based on the comparison of their birthdates.']}
{'past_steps': ('Find the current date.', "Today's date is December 7, 2023.")}
{'plan': ['Identify the current US president.', 'Find the birthdate of the current US president.', 'Identify the current Indian Prime Minister.', 'Find the birthdate of the current Indian Prime Minister.', 'Compare the birthdates of the US president and the Indian Prime Minister.', 'Determine who is older based on the comparison of their birthdates.']}
{'past_steps': ('Identify the current US president.', 'The current President of the United States is Joe Biden.')}
{'plan': ['Find the birthdate of the current US president Joe Biden.', 'Identify the current Indian Prime Minister.', 'Find the birthdate of the current Indian Prime Minister.', 'Compare the birthdates of Joe Biden and the Indian Prime Minister.', 'Determine who is older based on the comparison of their birthdates.']}
{'past_steps': ('Find the birthdate of the current US president Joe Biden.', 'Joe Biden was born on November 20, 1942.')}
{'plan': ['Identify the current Indian Prime Minister.', 'Find the birthdate of the current Indian Prime Minister.', 'Compare the birthdates of Joe Biden and the Indian Prime Minister.', 'Determine who is older based on the comparison of their birthdates.']}
{'past_steps': ('Identify the current Indian Prime Minister.', 'The current Prime Minister of India is Narendra Modi. He has been in office since May 26, 2014.')}
{'plan': ['Find the birthdate of the current Indian Prime Minister Narendra Modi.', 'Compare the birthdates of Joe Biden and Narendra Modi.', 'Determine who is older based on the comparison of their birthdates.']}
{'past_steps': ('Find the birthdate of the current Indian Prime Minister Narendra Modi.', 'Narendra Modi was born on September 17, 1950.')}
{'plan': ['Compare the birthdates of Joe Biden and Narendra Modi.', 'Determine who is older based on the comparison of their birthdates.']}
{'past_steps': ('Compare the birthdates of Joe Biden and Narendra Modi.', 'Joe Biden was born on November 20, 1942, while Narendra Modi was born on September 17, 1950. This makes Joe Biden older than Narendra Modi.')}
{'response': 'No further steps are needed. The final answer is that Joe Biden is older than Narendra Modi.'}
```
````

## File: microservices/ace_agent/4.1/samples/langgraph_plan_and_execute/speech_config.yaml
````yaml
grpc_server:
  nvidia::rrt::BotRuntimeGrpc: # component type
    ip_address: "0.0.0.0"
    port_number: 50055
    virtual_assistant_num_instances: 30
    virtual_assistant_pipeline_idle_threshold_secs: 600
    virtual_assistant_pipeline_idle_handler_wakeup_rate_secs: 10

speech_pipeline_manager: # config name
  SpeechPipelineManager: # component name
    asr_idle_timeout_ms: 200000
    tts_eos_delay_ms: 2000

riva_asr:
  RivaASR:
    server: "localhost:50051"
    word_boost_file_path: "/workspace/config/asr_words_to_boost.txt"
    enable_profanity_filter: false

dialog_manager:
  DialogManager:
    server: "http://localhost:9002/langgraph"
    use_streaming: true

riva_tts:
  RivaTTS:
    server: "localhost:50051"
    voice_name: "English-US.Female-1"
    language: "en-US"
    ipa_dict: ""
    sample_rate: 44100
    chunk_duration_ms: 100
    audio_start_threshold_ms: 400
    send_audio_in_realtime: true
    tts_mode: "grpc"

riva_logger:
  RivaLogger:
    data_dump_path: "/workspace/log"
    enable_logging: true
````

## File: microservices/ace_agent/4.1/samples/llm_bot/colang/main.co
````
import core

flow technical helper
  activate notification of undefined flow start "I have encountered some technical issue!"
  activate notification of colang errors "I have encountered some technical issue!"
  activate handle user transcript with interruption $mode="interim" $stop_flows_list=["_bot_say","generate response"]

flow generate response $transcript
  """Flow name is used in interruption logic `fail all bot actions` for stoping active llm and bot say events"""
  log "Started External LLM action"
  $response = await ExternalLLMAction(query=$transcript,min_wait_time=0.3)
  log "Finished External LLM action, Response for {$transcript} : {$response}"
  bot say $response

flow dialog rails
  user partially said something as $ref
  generate response $ref.transcript


flow main
  activate technical helper
  activate dialog rails
````

## File: microservices/ace_agent/4.1/samples/llm_bot/colang/speech.co
````
import core
import timing
import avatars

@override
flow fail all bot actions $stop_flows_list=["_bot_say","bot gesture","bot posture"]
  """
  Stop list of flows, used to reset conversation flow during Barge In and other cases
  """
  $count = 0
  while $count < len($stop_flows_list):
    send StopFlow(flow_id=$stop_flows_list[$count])
    $count = $count + 1

flow handle user transcript with interruption $mode="interim" $stop_flows_list=["_bot_say","bot gesture","bot posture"]
  """
  Partial transcript / UtteranceUserAction.TranscriptUpdated() : Every user audio chunk ( typically 160 ms ) generates updated transcript from ASR model. This updated transcript is referred as partial transcript.
  Interim transcript / UtteranceUserAction.TranscriptUpdated(stability=1.0): When we observe atleast 240 ms of silence in user audio, we pass generated partial transcript for LM processing in ASR pipeline. This transcript is more accurate and stability 1.0 in UtteranceUserAction.TranscriptUpdated() indicate that.
  Final transcript / UtteranceUserAction.Finished() : Transcript after detecting end of user utterance. Typically we wait for 800 ms silence to mark end of user audio.
  
  In this flow, we use change in partial transcript for stopping bot actions. You can decide between partial, interim and final for triggering downstream pipeline/LLM call.
  """

  global $partial_transcript
  when user said something as $final
    $is_spurious = await IsSpuriousAction(query=$final.transcript)
    if $is_spurious
      log "Ignoring spurious / filler final transcript `{$final.transcript}`"
      $partial_transcript = None
    elif not $partial_transcript or $mode == "final"
      log "Using final transcript `{$final.transcript}` and user mode {$mode} transcripts"
      fail all bot actions $stop_flows_list=$stop_flows_list
      send PartialUserUtteranceAvailable(transcript=$final.transcript, utterance_action_uid=$final.context.user_said.event.action_uid)
    elif $partial_transcript != $final.transcript
      log "final transcript `{$final.transcript}` doesn't match with last interim transcript"
      $partial_transcript = None
      fail all bot actions $stop_flows_list=$stop_flows_list
      send PartialUserUtteranceAvailable(transcript=$final.transcript, utterance_action_uid=$final.context.user_said.event.action_uid)
    else
      log "Ignoring final transcript `{$final.transcript}`, active {$mode} transcript `{$partial_transcript}`"
      $partial_transcript = None
  orwhen user saying something as $partial
    $is_spurious = await IsSpuriousAction(query=$partial.transcript)
    if $is_spurious
      log "Ignoring spurious / filler partial transcript `{$partial.transcript}`"
    elif $partial_transcript != $partial.transcript and $mode == "partial"
      log "Active partial transcript `{$partial_transcript}` doesn't match with new partial transcript `{$partial.transcript}`"
      fail all bot actions $stop_flows_list=$stop_flows_list
      $partial_transcript = $partial.transcript
      send PartialUserUtteranceAvailable(transcript=$partial.transcript, utterance_action_uid=$partial.context.user_saying.event.action_uid)
    elif $partial_transcript != $partial.transcript and $mode == "interim" and $partial.context.user_saying.event.stability == 1.0
      log "Active interim transcript `{$partial_transcript}` doesn't match with new interim transcript `{$partial.transcript}`"
      fail all bot actions $stop_flows_list=$stop_flows_list
      $partial_transcript = $partial.transcript
      send PartialUserUtteranceAvailable(transcript=$partial.transcript, utterance_action_uid=$partial.context.user_saying.event.action_uid)
    elif $partial_transcript != $partial.transcript
      log "Interruption using new partial/interim transcript `{$partial.transcript}`,  was active {$mode} transcript `{$partial_transcript}`"
      $partial_transcript = None
      fail all bot actions $stop_flows_list=$stop_flows_list
    else 
      log "Ignoring partial/interim transcript `{$partial.transcript}`, matching with active {$mode} transcript {$partial_transcript}"

@meta(user_action='user said "{$transcript}"')
flow user partially said $text -> $transcript
  """Wait for a user to have said given text."""
  if $text
    match PartialUserUtteranceAvailable(transcript=$text) as $event
  else
    match PartialUserUtteranceAvailable() as $event
  $transcript = $event.transcript
  log "User partial said `{$transcript}`"
  return $transcript

@meta(user_action='user said "{$transcript}"')
flow user partially said something -> $transcript
  """Wait for a user to have said something."""
  match PartialUserUtteranceAvailable() as $event
  $transcript = $event.transcript
  log "User partial said something `{$transcript}`"
  return $transcript
````

## File: microservices/ace_agent/4.1/samples/llm_bot/actions.py
````python
"""
 copyright(c) 2024 NVIDIA Corporation.All rights reserved.

 NVIDIA Corporation and its licensors retain all intellectual property
 and proprietary rights in and to this software, related documentation
 and any modifications thereto.Any use, reproduction, disclosure or
 distribution of this software and related documentation without an express
 license agreement from NVIDIA Corporation is strictly prohibited.
"""

import os
import time
import asyncio
import logging

from typing import List
from datetime import datetime
from openai import AsyncOpenAI
from nemoguardrails.actions.actions import action
from chat_engine.policies.actions.colang2_actions import create_chat_history


logger = logging.getLogger("nemoguardrails")


def log(what: str):
    """Log compatible with the nemoguardrails log output to show output as part of logging output"""
    logger.info(f"A Colang debug info: {what}")


## LLM Configs
BASE_URL = "http://0.0.0.0:8010/v1"  # Set to "https://integrate.api.nvidia.com/v1" for using hosted NIM models and provide API key using NVIDIA_API_KEY env variable
MODEL = "meta/llama3-8b-instruct"
TEMPERATURE = 0.5
TOP_P = 1
MAX_TOKENS = 100
SYSTEM_PROMPT = "You are a helpful, respectful and honest assistant. Always answer as helpful friendly and polite. Respond with one sentence or less than 75 characters. Do not respond with bulleted or numbered list. Your output will be converted to audio so don't include special characters in your answers."

client = AsyncOpenAI(
    base_url=BASE_URL, api_key=os.getenv("NVIDIA_API_KEY") or "$API_KEY_REQUIRED_IF_NOT_USING_LOCAL_MODEL"
)

# Transcript filtering for spurious transcript and filler words. Along with this any transcript less than 3 chars is removed
FILTER_WORDS = [
    "yeah",
    "okay",
    "right",
    "yes",
    "yum",
    "and",
    "one",
    "all",
    "when",
    "thank",
    "but",
    "next",
    "what",
    "i see",
    "the",
    "hmm",
    "mmm",
    "so that",
    "why",
    "that",
    "well",
]

INCLUDE_WORDS = ["hi"]


@action(name="ExternalLLMAction", execute_async=True)
async def call_nim_local_llm(events: List, query: str, min_wait_time: float = 0.2) -> str:
    """
    Call LLM for given query with chat history
    """

    log(f"{datetime.now()} Started LLM call, {time.time()} for query `{query}`")
    chat_history = create_chat_history(events)
    print(f"chat_history for {query}", chat_history)
    messages = [
        {
            "role": "system",
            "content": SYSTEM_PROMPT,
        }
    ]
    messages.extend(chat_history)
    messages.append({"role": "user", "content": query})
    start = time.time()
    completion = await client.chat.completions.create(
        model=MODEL,
        messages=messages,
        temperature=TEMPERATURE,
        top_p=TOP_P,
        max_tokens=MAX_TOKENS,
        stream=True,
    )

    response = ""
    async for chunk in completion:
        if chunk.choices[0].delta.content is not None:
            response += chunk.choices[0].delta.content
    llm_latency = time.time() - start
    log(f"{datetime.now()}, LLM call latency {time.time()-start}, response: {response}")
    if llm_latency < min_wait_time:
        log(f"LLM call finished before minimum wait time {min_wait_time}, waiting for {min_wait_time-llm_latency}")
        await asyncio.sleep(min_wait_time - llm_latency)
    return response


@action(name="IsSpuriousAction")
async def is_spurious(query):
    """
    Filter transcript less than 3 chars or in FILTER_WORDS list to avoid spurious transcript and filler words.
    """
    if query.strip().lower() in FILTER_WORDS or (len(query) < 3 and query.strip().lower() not in INCLUDE_WORDS):
        return True
    else:
        return False
````

## File: microservices/ace_agent/4.1/samples/llm_bot/docker-compose-nim-ms.yaml
````yaml
services:
  nemollm-inference:
    container_name: nemollm-inference-microservice
    image: nvcr.io/nim/meta/llama3-8b-instruct:1.0.3
    volumes:
    - ${MODEL_DIRECTORY}:/opt/nim/.cache
    user: "${USERID}"
    ports:
    - "8010:8000"
    environment:
      NGC_API_KEY: ${NGC_CLI_API_KEY}
    shm_size: 20gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${INFERENCE_GPU_COUNT:-all}
              # device_ids: ['${LLM_MS_GPU_ID:-0}']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python3", "-c", "import requests; requests.get('http://localhost:8000/v1/health/ready')"]
      interval: 10s
      timeout: 20s
      retries: 100
````

## File: microservices/ace_agent/4.1/samples/llm_bot/llm_bot_config.yml
````yaml
bot: llm_bot

colang_version: "2.x"

storage: 
  name: cache

configs:
  use_stateful_guardrails: True

models: []
````

## File: microservices/ace_agent/4.1/samples/llm_bot/model_config.yaml
````yaml
model_servers:
 - name: riva
   url: "localhost:8001"
   speech_models:
    - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
    - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
````

## File: microservices/ace_agent/4.1/samples/llm_bot/speech_config.yaml
````yaml
grpc_server:
  nvidia::rrt::BotRuntimeGrpc: # component type
    ip_address: "0.0.0.0"
    port_number: 50055
    virtual_assistant_num_instances: 30
    virtual_assistant_pipeline_idle_threshold_secs: 600
    virtual_assistant_pipeline_idle_handler_wakeup_rate_secs: 10

speech_pipeline_manager: # config name
  SpeechPipelineManager: # component name
    asr_idle_timeout_ms: 200000
    tts_eos_delay_ms: 0

riva_asr:
  RivaASR:
    server: "0.0.0.0:50051"
    word_boost_file_path: "/workspace/config/asr_words_to_boost.txt"
    enable_profanity_filter: false
    endpointing_stop_history: 800
    endpointing_stop_history_eou: 240

dialog_manager:
  DialogManager:
    server: "http://localhost:9000"
    use_streaming: true

riva_tts:
  RivaTTS:
    server: "0.0.0.0:50051"
    voice_name: "English-US.Female-1"
    language: "en-US"
    ipa_dict: ""
    sample_rate: 44100
    chunk_duration_ms: 100
    audio_start_threshold_ms: 400
    send_audio_in_realtime: true
    tts_mode: "grpc"

riva_logger:
  RivaLogger:
    data_dump_path: "/workspace/log"
    enable_logging: true
````

## File: microservices/ace_agent/4.1/samples/npc_bots/elara/colang/bug_fix.co
````
@meta(exclude_from_llm=True)
@loop("ignored_action_bugfix")
flow ignored_utterance_action_bugfix
  global $number_of_failed_utterance_actions
  if $number_of_failed_utterance_actions == None
    $number_of_failed_utterance_actions = 0
  match StartUtteranceBotAction() as $event
  start_new_flow_instance:
  start wait 3.0 as $timer_ref
  when $timer_ref.Finished()
    # After 3 consecutive fails we will no longer send a Finished event to let the process become idle and terminated
    if $number_of_failed_utterance_actions < 3
      send UtteranceBotActionFinished(action_uid=$event.action_uid, final_script="", is_success=False, failure_reason="ActionStarted event timeout")
    $number_of_failed_utterance_actions = $number_of_failed_utterance_actions + 1
  or when UtteranceBotActionStarted(action_uid=$event.action_uid)
    $number_of_failed_utterance_actions = 0
````

## File: microservices/ace_agent/4.1/samples/npc_bots/elara/colang/elara.co
````
import core

flow technical helper
  activate notification of undefined flow start "Excuse me, what did you say?"
  activate notification of colang errors "Excuse me, what did you say?"
  activate handle user transcript with interruption $mode="interim" $stop_flows_list=["_bot_say","generate slm response"]

flow generate slm response $transcript
  $request_id = await InvokeStreamingChatAction(endpoint="/slm_responder/chat",question=$transcript,chat_history=True)
  if $request_id
    $response = await StreamingResponseChatAction(endpoint="/slm_responder/chat",request_id=$request_id)
    if not $response
      bot say "Sorry I could not connect to the SLM endpoint"
    log "response from SLM: {$response}"
    while $response
      bot say $response
      $response = await StreamingResponseChatAction(endpoint="/slm_responder/chat",request_id=$request_id)
      log "response from SLM: {$response}"

flow conversation with elara
  user partially said something as $ref
  if not $player_name
    $player_name = "Kai"
  log "user said {$ref.transcript}" 
  generate slm response $ref.transcript

flow main
  activate technical helper
  activate conversation with elara
````

## File: microservices/ace_agent/4.1/samples/npc_bots/elara/colang/speech.co
````
import core
import timing
import avatars

@override
flow fail all bot actions $stop_flows_list=["_bot_say","bot gesture","bot posture"]
  """
  Stop list of flows, used to reset conversation flow during Barge In and other cases
  """
  $count = 0
  while $count < len($stop_flows_list):
    send StopFlow(flow_id=$stop_flows_list[$count])
    $count = $count + 1

flow handle user transcript with interruption $mode="interim" $stop_flows_list=["_bot_say","bot gesture","bot posture"]
  """
  Partial transcript / UtteranceUserAction.TranscriptUpdated() : Every user audio chunk ( typically 160 ms ) generates updated transcript from ASR model. This updated transcript is referred as partial transcript.
  Interim transcript / UtteranceUserAction.TranscriptUpdated(stability=1.0): When we observe atleast 240 ms of silence in user audio, we pass generated partial transcript for LM processing in ASR pipeline. This transcript is more accurate and stability 1.0 in UtteranceUserAction.TranscriptUpdated() indicate that.
  Final transcript / UtteranceUserAction.Finished() : Transcript after detecting end of user utterance. Typically we wait for 800 ms silence to mark end of user audio.
  
  In this flow, we use change in partial transcript for stopping bot actions. You can decide between partial, interim and final for triggering downstream pipeline/LLM call.
  """

  global $partial_transcript
  when user said something as $final
    $is_spurious = await IsSpuriousAction(query=$final.transcript)
    if $is_spurious
      log "Ignoring spurious / filler final transcript `{$final.transcript}`"
      $partial_transcript = None
    elif not $partial_transcript or $mode == "final"
      log "Using final transcript `{$final.transcript}` and user mode {$mode} transcripts"
      fail all bot actions $stop_flows_list=$stop_flows_list
      send PartialUserUtteranceAvailable(transcript=$final.transcript, utterance_action_uid=$final.context.user_said.event.action_uid)
    elif $partial_transcript != $final.transcript
      log "final transcript `{$final.transcript}` doesn't match with last interim transcript"
      $partial_transcript = None
      fail all bot actions $stop_flows_list=$stop_flows_list
      send PartialUserUtteranceAvailable(transcript=$final.transcript, utterance_action_uid=$final.context.user_said.event.action_uid)
    else
      log "Ignoring final transcript `{$final.transcript}`, active {$mode} transcript `{$partial_transcript}`"
      $partial_transcript = None
  orwhen user saying something as $partial
    $is_spurious = await IsSpuriousAction(query=$partial.transcript)
    if $is_spurious
      log "Ignoring spurious / filler partial transcript `{$partial.transcript}`"
    elif $partial_transcript != $partial.transcript and $mode == "partial"
      log "Active partial transcript `{$partial_transcript}` doesn't match with new partial transcript `{$partial.transcript}`"
      fail all bot actions $stop_flows_list=$stop_flows_list
      $partial_transcript = $partial.transcript
      send PartialUserUtteranceAvailable(transcript=$partial.transcript, utterance_action_uid=$partial.context.user_saying.event.action_uid)
    elif $partial_transcript != $partial.transcript and $mode == "interim" and $partial.context.user_saying.event.stability == 1.0
      log "Active interim transcript `{$partial_transcript}` doesn't match with new interim transcript `{$partial.transcript}`"
      fail all bot actions $stop_flows_list=$stop_flows_list
      $partial_transcript = $partial.transcript
      send PartialUserUtteranceAvailable(transcript=$partial.transcript, utterance_action_uid=$partial.context.user_saying.event.action_uid)
    elif $partial_transcript != $partial.transcript
      log "Interruption using new partial/interim transcript `{$partial.transcript}`,  was active {$mode} transcript `{$partial_transcript}`"
      $partial_transcript = None
      fail all bot actions $stop_flows_list=$stop_flows_list
    else 
      log "Ignoring partial/interim transcript `{$partial.transcript}`, matching with active {$mode} transcript {$partial_transcript}"

@meta(user_action='user said "{$transcript}"')
flow user partially said $text -> $transcript
  """Wait for a user to have said given text."""
  if $text
    match PartialUserUtteranceAvailable(transcript=$text) as $event
  else
    match PartialUserUtteranceAvailable() as $event
  $transcript = $event.transcript
  log "User partial said `{$transcript}`"
  return $transcript

@meta(user_action='user said "{$transcript}"')
flow user partially said something -> $transcript
  """Wait for a user to have said something."""
  match PartialUserUtteranceAvailable() as $event
  $transcript = $event.transcript
  log "User partial said something `{$transcript}`"
  return $transcript
````

## File: microservices/ace_agent/4.1/samples/npc_bots/elara/actions.py
````python
"""
 copyright(c) 2024 NVIDIA Corporation.All rights reserved.

 NVIDIA Corporation and its licensors retain all intellectual property
 and proprietary rights in and to this software, related documentation
 and any modifications thereto.Any use, reproduction, disclosure or
 distribution of this software and related documentation without an express
 license agreement from NVIDIA Corporation is strictly prohibited.
"""

import logging

from nemoguardrails.actions.actions import action


logger = logging.getLogger("nemoguardrails")

# Transcript filtering for spurious transcript and filler words. Along with this any transcript less than 3 chars is removed
FILTER_WORDS = [
    "yeah",
    "okay",
    "right",
    "yes",
    "yum",
    "and",
    "one",
    "all",
    "when",
    "thank",
    "but",
    "next",
    "what",
    "i see",
    "the",
    "hmm",
    "mmm",
    "so that",
    "why",
    "that",
    "well",
]

INCLUDE_WORDS = ["hi"]


@action(name="IsSpuriousAction")
async def is_spurious(query):
    """
    Filter transcript less than 3 chars or in FILTER_WORDS list to avoid spurious transcript and filler words.
    """
    if query.strip().lower() in FILTER_WORDS or (len(query) < 3 and query.strip().lower() not in INCLUDE_WORDS):
        return True
    else:
        return False
````

## File: microservices/ace_agent/4.1/samples/npc_bots/elara/bot_config.yaml
````yaml
bot: elara
colang_version: "2.x"

storage: 
  name: cache

configs:
  use_stateful_guardrails: True

streaming: False

models: []
````

## File: microservices/ace_agent/4.1/samples/npc_bots/elara/model_config.yaml
````yaml
model_servers:
  - name: riva
    speech_models:
      - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
      - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
    url: localhost:8001
````

## File: microservices/ace_agent/4.1/samples/npc_bots/elara/plugin_config.yaml
````yaml
config:
  workers: 1
  timeout: 30

plugins:
  - name: slm_responder
    path: ../plugins/slm_responder.py
    parameters:
        instructions: |-
              Elara Thornbrook's family has a rich history in the village of Willowbrook, deeply intertwined with the community's traditions and values. Her parents, Eamon and Isolde Thornbrook, are known for their warm and welcoming nature. Eamon is the village's skilled blacksmith, forging tools and weapons that the villagers rely on for their daily tasks and protection. Isolde is a talented herbalist, tending to the health and well-being of the villagers with her extensive knowledge of plants and remedies.
              Elara is the middle child of three siblings. Her older brother, Rowan, inherited their father's skill and passion for blacksmithing, but he also shares Elara's curiosity about the supernatural. He often helps her research the haunted house's history and occasionally accompanies her on visits to the eerie building. Rowan's strength and protective nature make him a reliable presence in both the village and their family.
              Elara's younger sister, Seraphina, has an artistic soul. She spends her time painting scenes of the village and the surrounding landscapes. Her paintings capture the beauty and tranquility of Willowbrook, offering a counterbalance to the unsettling aura of the haunted house. Seraphina's innocence and creativity bring a sense of lightness to the Thornbrook family, reminding them of the simple joys that life in the village has to offer.
              The Thornbrook family's close bond and diversity of talents make them an integral part of Willowbrook's identity. Their unwavering support for Elara's quest to understand the haunted house and help others overcome their fears stems from their shared belief in the power of family, community, and the importance of facing challenges together.
              The year is 1860 AD.

              Haunted house description:
              Nestled in the heart of the quaint town of Willobrook, the haunted house stands as a somber sentinel of history. Built in the early 1800s by the enigmatic Blackwood family, its grandeur once reflected their status as prosperous merchants. However, the house's chequered past is woven with tragedy and mystery.
              Legend has it that a forbidden love affair between the youngest Blackwood daughter, Isabella, and a penniless artist led to a series of calamities. The family's disapproval culminated in Isabella's untimely death under suspicious circumstances, casting an eternal shadow over the house. Locals whisper of flickering candlelight in its windows and mournful whispers carried by the wind.
              Over the decades, the house changed hands numerous times, each occupant encountering inexplicable occurrences. Haunting cries, echoing footsteps, and the apparition of a veiled woman have left an indelible mark on those brave enough to venture near. Despite efforts to unravel its secrets, the house's history remains shrouded in darkness, a testament to the power of its tragic past.
              Willobrook's haunted house continues to beckon, a place where the veil between the living and the departed is thin, inviting the curious and the courageous to uncover the layers of sorrow and unrest that linger within its timeworn walls.
              To reach the haunted house in Willobrook, follow the winding path through Whispering Woods, past the old stone bridge. As you approach the town's heart, turn onto Misty Lane. The house stands at the end, a looming silhouette wrapped in ivy and memories.

              Conversation Instructions:
              YOU WILL ALWAYS STRICTLY PLAY THE ROLE OF ELARA. I will play the role of {$player_name}, a traveller seeking the location of the haunted house in Willobrook.
              YOUR GOAL IS TO PROVIDE DIRECTIONS AND ENCOURAGE {$player_name} TO EXPLORE HAUNTED HOUSE IN WILLOBROOK. Give brief details about this house's background.
              You should refrain from answering any other questions apart from insisting {$player_name} to vist the haunted house and provide directions.
        quality: 9
        toxicity: 0
        humour: 2
        creativity: 7
        violence: 0
        helpfulness: 8
        inappropriate: 0
````

## File: microservices/ace_agent/4.1/samples/npc_bots/elara/speech_config.yaml
````yaml
grpc_server:
  nvidia::rrt::BotRuntimeGrpc: # component type
    ip_address: "0.0.0.0"
    port_number: 50055
    virtual_assistant_num_instances: 30
    virtual_assistant_pipeline_idle_threshold_secs: 600
    virtual_assistant_pipeline_idle_handler_wakeup_rate_secs: 10

speech_pipeline_manager: # config name
  SpeechPipelineManager: # component name
    asr_idle_timeout_ms: 200000
    tts_eos_delay_ms: 2000

riva_asr:
  RivaASR:
    server: "localhost:50051"
    word_boost_file_path: "/workspace/config/asr_words_to_boost_conformer.txt"
    enable_profanity_filter: false
    endpointing_stop_history: 800
    endpointing_stop_history_eou: 240

dialog_manager:
  DialogManager:
    # Default bot name and version to be used in speech mode
    bot_name: "elara"
    server: "http://localhost:9000"
    use_streaming: true

riva_tts:
  RivaTTS:
    server: "localhost:50051"
    voice_name: "English-US.Female-Fearful"
    language: "en-US"
    ipa_dict: ""
    sample_rate: 44100
    chunk_duration_ms: 100
    audio_start_threshold_ms: 400
    send_audio_in_realtime: true
    tts_mode: "grpc"

a2f_grpc:
    avatar_model: "/World/dragon_a2f/audio_player_streaming"
    server: "localhost:50051"
    rpc_timeout_ms: 300000

riva_logger:
  RivaLogger:
    data_dump_path: "/workspace/log"
    enable_logging: true
````

## File: microservices/ace_agent/4.1/samples/npc_bots/jin/colang/bug_fix.co
````
@meta(exclude_from_llm=True)
@loop("ignored_action_bugfix")
flow ignored_utterance_action_bugfix
  global $number_of_failed_utterance_actions
  if $number_of_failed_utterance_actions == None
    $number_of_failed_utterance_actions = 0
  match StartUtteranceBotAction() as $event
  start_new_flow_instance:
  start wait 3.0 as $timer_ref
  when $timer_ref.Finished()
    # After 3 consecutive fails we will no longer send a Finished event to let the process become idle and terminated
    if $number_of_failed_utterance_actions < 3
      send UtteranceBotActionFinished(action_uid=$event.action_uid, final_script="", is_success=False, failure_reason="ActionStarted event timeout")
    $number_of_failed_utterance_actions = $number_of_failed_utterance_actions + 1
  or when UtteranceBotActionStarted(action_uid=$event.action_uid)
    $number_of_failed_utterance_actions = 0
````

## File: microservices/ace_agent/4.1/samples/npc_bots/jin/colang/jin.co
````
import core

flow technical helper
  activate notification of undefined flow start "Excuse me, what did you say?"
  activate notification of colang errors "Excuse me, what did you say?"
  activate handle user transcript with interruption $mode="interim" $stop_flows_list=["_bot_say","generate slm response"]

flow generate slm response $transcript
  $request_id = await InvokeStreamingChatAction(endpoint="/slm_responder/chat",question=$transcript,chat_history=True)
  if $request_id
    $response = await StreamingResponseChatAction(endpoint="/slm_responder/chat",request_id=$request_id)
    if not $response
      bot say "Sorry I could not connect to the SLM endpoint"
    log "response from SLM: {$response}"
    while $response
      bot say $response
      $response = await StreamingResponseChatAction(endpoint="/slm_responder/chat",request_id=$request_id)
      log "response from SLM: {$response}"

flow conversation with jin
  user partially said something as $ref
  if not $player_name
    $player_name = "Kai"
  log "user said {$ref.transcript}" 
  generate slm response $ref.transcript

flow main
  activate technical helper
  activate conversation with jin
````

## File: microservices/ace_agent/4.1/samples/npc_bots/jin/colang/speech.co
````
import core
import timing
import avatars

@override
flow fail all bot actions $stop_flows_list=["_bot_say","bot gesture","bot posture"]
  """
  Stop list of flows, used to reset conversation flow during Barge In and other cases
  """
  $count = 0
  while $count < len($stop_flows_list):
    send StopFlow(flow_id=$stop_flows_list[$count])
    $count = $count + 1

flow handle user transcript with interruption $mode="interim" $stop_flows_list=["_bot_say","bot gesture","bot posture"]
  """
  Partial transcript / UtteranceUserAction.TranscriptUpdated() : Every user audio chunk ( typically 160 ms ) generates updated transcript from ASR model. This updated transcript is referred as partial transcript.
  Interim transcript / UtteranceUserAction.TranscriptUpdated(stability=1.0): When we observe atleast 240 ms of silence in user audio, we pass generated partial transcript for LM processing in ASR pipeline. This transcript is more accurate and stability 1.0 in UtteranceUserAction.TranscriptUpdated() indicate that.
  Final transcript / UtteranceUserAction.Finished() : Transcript after detecting end of user utterance. Typically we wait for 800 ms silence to mark end of user audio.
  
  In this flow, we use change in partial transcript for stopping bot actions. You can decide between partial, interim and final for triggering downstream pipeline/LLM call.
  """

  global $partial_transcript
  when user said something as $final
    $is_spurious = await IsSpuriousAction(query=$final.transcript)
    if $is_spurious
      log "Ignoring spurious / filler final transcript `{$final.transcript}`"
      $partial_transcript = None
    elif not $partial_transcript or $mode == "final"
      log "Using final transcript `{$final.transcript}` and user mode {$mode} transcripts"
      fail all bot actions $stop_flows_list=$stop_flows_list
      send PartialUserUtteranceAvailable(transcript=$final.transcript, utterance_action_uid=$final.context.user_said.event.action_uid)
    elif $partial_transcript != $final.transcript
      log "final transcript `{$final.transcript}` doesn't match with last interim transcript"
      $partial_transcript = None
      fail all bot actions $stop_flows_list=$stop_flows_list
      send PartialUserUtteranceAvailable(transcript=$final.transcript, utterance_action_uid=$final.context.user_said.event.action_uid)
    else
      log "Ignoring final transcript `{$final.transcript}`, active {$mode} transcript `{$partial_transcript}`"
      $partial_transcript = None
  orwhen user saying something as $partial
    $is_spurious = await IsSpuriousAction(query=$partial.transcript)
    if $is_spurious
      log "Ignoring spurious / filler partial transcript `{$partial.transcript}`"
    elif $partial_transcript != $partial.transcript and $mode == "partial"
      log "Active partial transcript `{$partial_transcript}` doesn't match with new partial transcript `{$partial.transcript}`"
      fail all bot actions $stop_flows_list=$stop_flows_list
      $partial_transcript = $partial.transcript
      send PartialUserUtteranceAvailable(transcript=$partial.transcript, utterance_action_uid=$partial.context.user_saying.event.action_uid)
    elif $partial_transcript != $partial.transcript and $mode == "interim" and $partial.context.user_saying.event.stability == 1.0
      log "Active interim transcript `{$partial_transcript}` doesn't match with new interim transcript `{$partial.transcript}`"
      fail all bot actions $stop_flows_list=$stop_flows_list
      $partial_transcript = $partial.transcript
      send PartialUserUtteranceAvailable(transcript=$partial.transcript, utterance_action_uid=$partial.context.user_saying.event.action_uid)
    elif $partial_transcript != $partial.transcript
      log "Interruption using new partial/interim transcript `{$partial.transcript}`,  was active {$mode} transcript `{$partial_transcript}`"
      $partial_transcript = None
      fail all bot actions $stop_flows_list=$stop_flows_list
    else 
      log "Ignoring partial/interim transcript `{$partial.transcript}`, matching with active {$mode} transcript {$partial_transcript}"

@meta(user_action='user said "{$transcript}"')
flow user partially said $text -> $transcript
  """Wait for a user to have said given text."""
  if $text
    match PartialUserUtteranceAvailable(transcript=$text) as $event
  else
    match PartialUserUtteranceAvailable() as $event
  $transcript = $event.transcript
  log "User partial said `{$transcript}`"
  return $transcript

@meta(user_action='user said "{$transcript}"')
flow user partially said something -> $transcript
  """Wait for a user to have said something."""
  match PartialUserUtteranceAvailable() as $event
  $transcript = $event.transcript
  log "User partial said something `{$transcript}`"
  return $transcript
````

## File: microservices/ace_agent/4.1/samples/npc_bots/jin/actions.py
````python
"""
 copyright(c) 2024 NVIDIA Corporation.All rights reserved.

 NVIDIA Corporation and its licensors retain all intellectual property
 and proprietary rights in and to this software, related documentation
 and any modifications thereto.Any use, reproduction, disclosure or
 distribution of this software and related documentation without an express
 license agreement from NVIDIA Corporation is strictly prohibited.
"""

import logging

from nemoguardrails.actions.actions import action


logger = logging.getLogger("nemoguardrails")

# Transcript filtering for spurious transcript and filler words. Along with this any transcript less than 3 chars is removed
FILTER_WORDS = [
    "yeah",
    "okay",
    "right",
    "yes",
    "yum",
    "and",
    "one",
    "all",
    "when",
    "thank",
    "but",
    "next",
    "what",
    "i see",
    "the",
    "hmm",
    "mmm",
    "so that",
    "why",
    "that",
    "well",
]

INCLUDE_WORDS = ["hi"]


@action(name="IsSpuriousAction")
async def is_spurious(query):
    """
    Filter transcript less than 3 chars or in FILTER_WORDS list to avoid spurious transcript and filler words.
    """
    if query.strip().lower() in FILTER_WORDS or (len(query) < 3 and query.strip().lower() not in INCLUDE_WORDS):
        return True
    else:
        return False
````

## File: microservices/ace_agent/4.1/samples/npc_bots/jin/bot_config.yaml
````yaml
bot: jin
colang_version: "2.x"

storage: 
  name: cache

configs: 
  use_stateful_guardrails: True

streaming: False

models: []
````

## File: microservices/ace_agent/4.1/samples/npc_bots/jin/model_config.yaml
````yaml
model_servers:
  - name: riva
    speech_models:
      - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
      - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
    url: localhost:8001
````

## File: microservices/ace_agent/4.1/samples/npc_bots/jin/plugin_config.yaml
````yaml
config:
  workers: 1
  timeout: 30

plugins:
  - name: slm_responder
    path: ../plugins/slm_responder.py
    parameters:
        instructions: |-
            Your name is Jin and you are a very friendly character living in the small town of Willobrook.
            The year is 1860 AD.
            You owe a business, a ramen shop, that has been troubled a lot lately by a local gang.
            Your business is not doing good due to this reason and you have very less customers now.
            The most dangerous person in this gang is Kumon Aoki who controls everything and is the gang leader.
            You are scared of this gang and you do not want to get involved.
            If this person is removed then all your issues will be resolved.
            However, this person is very dangerous and you tell people asking for help to stay out of trouble.
            Kumon Aki reigns over the treacherous Ironclad Haven, a hidden fortress nestled within a forsaken desert canyon.
            The gang leader's lair boasts impregnable walls, guarded by loyal enforcers. Rumors whisper of subterranean tunnels, ensuring escape routes.
            Intruders risk everything to challenge Aki's rule amidst shifting sands and shadows.
            To find Kumon Aki, one must follow the forgotten desert path southwest, through shifting dunes. At canyon's end, find Ironclad Haven, where Kumon Aki's power thrives.

            Conversation Instructions:
            YOU WILL ALWAYS STRICTLY PLAY THE ROLE OF JIN and I will play the role of {$player_name}.
            YOUR GOAL IS TO ASK {$player_name} FOR HELP by mentioning all your problems and provide directions to Kumon's Aki's place.
            You must be hesitant to ask for help and also hesitate in providing directions to Kumon's Aki's location out of fear.    
            Apart from the above knowledge refrain from answering any other questions.
        quality: 9
        toxicity: 0
        humour: 2
        creativity: 3
        violence: 0
        helpfulness: 2
        inappropriate: 0
````

## File: microservices/ace_agent/4.1/samples/npc_bots/jin/speech_config.yaml
````yaml
grpc_server:
  nvidia::rrt::BotRuntimeGrpc: # component type
    ip_address: "0.0.0.0"
    port_number: 50055
    virtual_assistant_num_instances: 30
    virtual_assistant_pipeline_idle_threshold_secs: 600
    virtual_assistant_pipeline_idle_handler_wakeup_rate_secs: 10

speech_pipeline_manager: # config name
  SpeechPipelineManager: # component name
    asr_idle_timeout_ms: 200000
    tts_eos_delay_ms: 2000

riva_asr:
  RivaASR:
    server: "localhost:50051"
    word_boost_file_path: "/workspace/config/asr_words_to_boost_conformer.txt"
    enable_profanity_filter: false
    endpointing_stop_history: 800
    endpointing_stop_history_eou: 240

dialog_manager:
  DialogManager:
    # Default bot name and version to be used in speech mode
    bot_name: "jin"
    server: "http://localhost:9000"
    use_streaming: true

riva_tts:
  RivaTTS:
    server: "localhost:50051"
    voice_name: "English-US.Male-1"
    language: "en-US"
    ipa_dict: ""
    sample_rate: 44100
    chunk_duration_ms: 100
    audio_start_threshold_ms: 400
    send_audio_in_realtime: true
    tts_mode: "grpc"

a2f_grpc:
    avatar_model: "/World/dragon_a2f/audio_player_streaming"
    server: "localhost:50051"
    rpc_timeout_ms: 300000

riva_logger:
  RivaLogger:
    data_dump_path: "/workspace/log"
    enable_logging: true
````

## File: microservices/ace_agent/4.1/samples/npc_bots/plugins/prompt_former.py
````python
# Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.

import json
from fastapi import APIRouter
from typing import Dict, Optional, List, Any
from parameters import param
from traceback import print_exc
import logging

router = APIRouter()

logger = logging.getLogger("plugin")


def log(what: str):
    """Log compatible with the nemoguardrails log output to show output as part of logging output"""
    logger.info(f"A Colang debug info: {what}")


@router.post("/get_prompt")
def get_prompt(
    query: Optional[str] = "",
    context: Optional[Dict[str, Any]] = {},
    conv_history: Optional[List[Dict[str, str]]] = [],
) -> str:
    """
    This stub accepts a query, conversation history and any dynamic context if available.
    It formulates the custom instuctions section of the prompt using these bot's instructions and returns it
    """

    try:
        print(f"Generating LLM prompt. Current conversation history: {conv_history}")
        character_details = {}

        # Get the character specific default values
        character_details = param.get("slm_responder", {}).copy()
        # Override default character details if context is passed dynamically
        for llm_param in [
            "instructions",
            "quality",
            "toxicity",
            "humour",
            "creativity",
            "violence",
            "helpfulness",
            "inappropriate",
        ]:
            character_details[llm_param] = context.get(llm_param, character_details[llm_param])

        character_details["player_name"] = context.get("player_name")

        logger.info(f"Forming Prompt with parameters: {json.dumps(character_details, indent=4)}")
        return formulate_custom_instructions(character_details=character_details, conv_history=conv_history)

    except Exception as e:
        print(f"Error : {e}")
        print_exc()
    return ""


def get_param_val(params, key, default):
    if key in params.keys():
        return params[key]
    else:
        return default


def formulate_custom_instructions(character_details: Dict[str, Any], conv_history: List[Dict[str, str]]) -> str:
    """Formulate the custom instructions for the prompt based on the passed context and conversation history"""

    if not character_details:
        return ""

    conversation = []

    # Prompt structure
    # <extra_id_0>System
    # [system prompt]
    #
    # <extra_id_1>[user name]
    # [user message]
    # <extra_id_2>quality:<value 0-9>,toxicity:<value 0-9>,humor:<value 0-9>,creativity:<value 0-9>,violence:<value 0-9>,helpfulness:<value 0-9>,not_appropriate:<value 0-9>
    instructions = get_param_val(character_details, "instructions", "")
    llm_quality = get_param_val(character_details, "quality", 9)
    llm_toxicity = get_param_val(character_details, "toxicity", 0)
    llm_humor = get_param_val(character_details, "humor", 2)
    llm_creativity = get_param_val(character_details, "creativity", 3)
    llm_violence = get_param_val(character_details, "violence", 0)
    llm_helpfulness = get_param_val(character_details, "helpfulness", 9)
    llm_inappropriate = get_param_val(character_details, "inappropriate", 0)
    curr_prompt = f"""<extra_id_1>User \n{instructions}"""
    # Formulate the prompt based on conversation history
    for hist in conv_history:
        if hist.get("role") == "user":
            conversation.append(f"<extra_id_1>{character_details.get('player_name')}")
            conversation.append(hist.get("content"))
        elif hist.get("role") == "assistant":
            conversation.append(
                f"<extra_id_1>quality:{llm_quality},toxicity:{llm_toxicity},humor:{llm_humor},creativity:{llm_creativity},violence:{llm_violence},helpfulness:{llm_helpfulness},not_appropriate:{llm_inappropriate}"
            )
            conversation.append(hist.get("content"))
        else:
            continue

    conversation.append(
        f"<extra_id_1>quality:{llm_quality},toxicity:{llm_toxicity},humor:{llm_humor},creativity:{llm_creativity},violence:{llm_violence},helpfulness:{llm_helpfulness},not_appropriate:{llm_inappropriate}"
    )

    for line in conversation:
        curr_prompt += line
        curr_prompt += "\n"

    return curr_prompt
````

## File: microservices/ace_agent/4.1/samples/npc_bots/plugins/requirements.txt
````
openai==1.44.0
````

## File: microservices/ace_agent/4.1/samples/npc_bots/plugins/schemas.py
````python
# Copyright(c) 2024 NVIDIA Corporation. All rights reserved.

# NVIDIA Corporation and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA Corporation is strictly prohibited.

from pydantic import BaseModel, Field
from typing import Optional, Dict, List, Any

DEFAULT_LANGUAGE = "en-US"


class ChatRequest(BaseModel):
    Query: Optional[str] = Field(default="", description="The user query which needs to be processed.")
    UserId: str = Field(
        description="Mandatory unique identifier to recognize which user is interacting with the Chat Engine."
    )
    Metadata: Optional[Dict[str, Any]] = Field(
        default={},
        description="Any additional information related to the request.",
    )


class EventRequest(BaseModel):
    EventType: str = Field(default="", description="The event name which needs to be processed.")
    UserId: str = Field(
        description="Mandatory unique identifier to recognize which user is interacting with the Chat Engine."
    )


class ResponseField(BaseModel):
    Text: str = Field(
        default="",
        description="Text response to be sent out. This field will also be picked by a Text to Speech Synthesis module if enabled for speech based bots.",
    )
    CleanedText: str = Field(
        default="",
        description="Text response from the Chat Engine with all SSML/HTML tags removed.",
    )
    NeedUserResponse: Optional[bool] = Field(
        default=True,
        description="This field can be used by end user applications to deduce if user response is needed or not for a dialog initiated query. This is set to true automatically if form filling is active and one or more slots are missing.",
    )
    IsFinal: bool = Field(
        default=False,
        description="This field to indicate the final response chunk when streaming. The chunk with IsFinal=true will contain the full Chat Engine response attributes.",
    )


class ChatResponse(BaseModel):
    UserId: str = Field(
        default="",
        description="Unique identifier to recognize which user is interacting with the Chat Engine. This is populated from the request JSON.",
    )
    QueryId: str = Field(
        default="",
        description="Unique identifier for the user query assigned automatically by the Chat Engine unless specified in request JSON.",
    )
    Response: ResponseField = Field(
        default=ResponseField(),
        description="Final response template from the Chat Engine. This field can be picked up from domain rule files or can be formulated directly from custom fulfillment modules.",
    )
    Metadata: Optional[Dict[str, Any]] = Field(
        default={"SessionId": "", "StreamId": ""},
        description="Any additional information related to the request.",
    )


class EventResponse(BaseModel):
    UserId: str = Field(
        default="",
        description="Unique identifier to recognize which user is interacting with the Chat Engine. This is populated from the request JSON.",
    )
    Events: List[Dict[str, Any]] = Field(
        default=[],
        description="The generated event list for the provided EventType from Chat Engine.",
    )
    Response: ResponseField = Field(
        default=ResponseField(),
        description="Final response template from the Chat Engine. This field can be picked up from domain rule files or can be formulated directly from custom fulfillment modules.",
    )
````

## File: microservices/ace_agent/4.1/samples/npc_bots/plugins/slm_responder.py
````python
#  Copyright(c) 2023 NVIDIA Corporation.All rights reserved.

#  NVIDIA Corporation and its licensors retain all intellectual property
#  and proprietary rights in and to this software, related documentation
#  and any modifications thereto.Any use, reproduction, disclosure or
#  distribution of this software and related documentation without an express
#  license agreement from NVIDIA Corporation is strictly prohibited.

import time
import logging
import os
import sys
import json

from datetime import datetime
from typing import Dict, Optional, Union, List
from openai import AsyncOpenAI
from fastapi import APIRouter, Body, Response, status
from fastapi.responses import StreamingResponse
from parameters import param
from prompt_former import get_prompt
from typing_extensions import Annotated
from traceback import print_exc


sys.path.append(os.path.dirname(__file__))

from schemas import ChatRequest, EventRequest, EventResponse, ChatResponse

logger = logging.getLogger("plugin")
router = APIRouter()


def log(what: str):
    """Log compatible with the nemoguardrails log output to show output as part of logging output"""
    logger.info(f"A Colang debug info: {what}")


nvidia_api_key = os.getenv("NVIDIA_API_KEY")
client = AsyncOpenAI(
    base_url="https://integrate.api.nvidia.com/v1",
    api_key=nvidia_api_key,
)

prompt = get_prompt()

EVENTS_NOT_REQUIRING_RESPONSE = [
    "system.event_pipeline_acquired",
    "system.event_pipeline_released",
    "system.event_exit",
]
MODEL_NAME = os.getenv("MODEL_NAME", None) or param.get("slm_responder").get(
    "MODEL_NAME", "nvidia/nemotron-mini-4b-instruct"
)

TEMPERATURE = os.getenv("TEMPERATURE", None) or param.get("slm_responder").get("TEMPERATURE", 0.2)
TOP_P = os.getenv("TOP_P", None) or param.get("slm_responder").get("TOP_P", 0.7)
MAX_TOKENS = os.getenv("MAX_TOKENS", None) or param.get("slm_responder").get("MAX_TOKENS", 200)


async def stream(
    question: Optional[str] = "",
    chat_history: Optional[List] = [],
    num_tokens: Optional[int] = MAX_TOKENS,
) -> int:
    async def generator():

        full_response = ""
        if question:
            message = [{"role": "system", "content": prompt}]
            user_query = [{"role": "user", "content": question}]
            message.extend(chat_history)
            message.extend(user_query)
            async with await client.chat.completions.create(
                model=MODEL_NAME,
                messages=message,
                temperature=TEMPERATURE,
                top_p=TOP_P,
                max_tokens=MAX_TOKENS,
                stream=True,
            ) as resp:
                async for chunk in resp:
                    try:
                        chunk = chunk.choices[0].delta.content
                        try:
                            if chunk and len(chunk) > 0:
                                message = chunk
                            else:
                                message = ""
                        except Exception as e:
                            logger.info(f"Parsing SLM response chunk '{chunk}' failed. {e}")
                            message = ""

                        if not message:
                            continue

                        full_response += message
                        json_chunk = ChatResponse()
                        json_chunk.Response.Text = message
                        json_chunk.Response.CleanedText = message
                        json_chunk = json.dumps(json_chunk.dict())
                        yield json_chunk
                    except Exception as e:
                        yield f"Internal error in SLM stream: {e}"
                        break

        json_chunk = ChatResponse()
        json_chunk.Response.IsFinal = True
        json_chunk = json.dumps(json_chunk.dict())
        logger.info(f"SLM response for user query {question} : {full_response}")
        yield json_chunk

    return StreamingResponse(generator(), media_type="text/event-stream")


@router.post(
    "/chat",
    status_code=status.HTTP_200_OK,
)
async def chat(
    request: Annotated[
        ChatRequest,
        Body(
            description="Chat Engine Request JSON. All the fields populated as part of this JSON is also available as part of request JSON."
        ),
    ],
    response: Response,
) -> StreamingResponse:
    """
    This endpoint can be used to provide response to query driven user request.
    """
    req = request.dict(exclude_none=True)
    logger.info(f"Received request JSON at /chat_stream endpoint for SLM : {json.dumps(req, indent=4)}")

    try:
        chat_history = []
        if "Metadata" in req:
            chat_history = req["Metadata"].get("ChatHistory", [])
        resp = await stream(question=req["Query"], chat_history=chat_history)
        return resp
    except Exception as e:
        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
        return {"StatusMessage": str(e)}


@router.post("/event", status_code=status.HTTP_200_OK)
async def event(
    request: Annotated[
        EventRequest,
        Body(
            description="Chat Engine Request JSON. All the fields populated as part of this JSON is also available as part of request JSON."
        ),
    ],
    response: Response,
) -> Union[EventResponse, Dict[str, str]]:
    """
    This endpoint can be used to provide response to an event driven user request.
    """

    req = request.dict(exclude_none=True)
    logger.info(f"Received request JSON at /event endpoint: {json.dumps(req, indent=4)}")

    try:
        resp = EventResponse()
        resp.UserId = req["UserId"]
        resp.Response.IsFinal = True

        if req["EventType"] in EVENTS_NOT_REQUIRING_RESPONSE:
            resp.Response.NeedUserResponse = False

        return resp
    except Exception as e:
        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
        return {"StatusMessage": str(e)}
````

## File: microservices/ace_agent/4.1/samples/npc_bots/README.md
````markdown
# BOTS FOR GAMING USECASES
This directory contains sample bots showcasing how developers can build:
1. LLM driven Natural Language Understanding and Natural Language Generation capabilities for Non-Playable Characters in a game
2. Provide game-stage specific context to Chat Engine at runtime and utlize the same to change the behaviour of NPC's.

## Architecture diagram of the NPC bot
![Diagram](../img/npc_bot_arch_diagram.jpg)

## Setting up environment
1. Set up virtual environment and Install the nemo-guardrails and aceagent Python packages following Quick Start Guide.
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
2. The bots uses nemollm models. Export your NGC CLI key which has access to nemo llm service.
    ```
    export NGC_CLI_API_KEY=<>
    ```

## Deploy all the bots
1. Launch plugin server for the bots. The fulfillment module is responsible to generate [a custom instruction which gets injected as part of the prompts for generating the response](./jin/bot_config.yaml#L53). The logic to generate this custom instruction is coded [in this file](./plugins/prompt_former.py). Developers are free to change or fine tune the logic to generate custom instructions for the prompt here.
    ```
    aceagent plugin-server deploy --config bots/npc_bots/plugin_config.yaml &
    ```

2. Launch all the Bots by passing the bot directory path. chat engine automatically discovers all the available bots in this directory and hosts them.
   The LLM configurations and backstory of the characters is exposed for fine-tuning in every bot specific config files. The config files are named as `bot_config.yaml` and every bot directory has its own seperate version of it.
    ```
    aceagent chat web --config bots/npc_bots
    ```

## Sample Conversation
Once the bot is deployed you can interact with any bot at `http://<workstation_ip>:9001/bot/` by choosing the bot name and version from the dropdown list.

![Conversation-1](../img/npc_bot_conversation.png)


## Providing game stage specific context
Any custom parameters can be provided for an user at runtime. These parameters can be utilized to update the bot behaviour whose example is shown here. This can be done using the `/updateUserContext` API of Chat Engine at runtime, without restarting the pipeline. In a gaming scenario, this can be useful to pass game specific user context to control bot behaviour.
1. Once the bot is deployed, goto the Chat Engine server swagger URL `http://<workstation_ip>:9000/docs`
2. Try out the `/updateUserContext` API by providing the following request body. Change the values of parameters as needed.
   The key-value pairs provided as part of the payload to this API, will be visible to [the custom fulfillment as well.](./plugins/prompt_former.py)
   Currently, fulfillment module utlizes these parameters at runtime to formulate the custom instuctions which gets injected as part of the prompt sent to LLM.
   Default values for these parameters are available in [`plugin_config.yaml` file](./plugin_config.yaml).

   Provide the following payload for `/updateUserContext` API. Note the `UserId` for the queries you tried out using `http://<workstation_ip>:9001/bot/` from the terminal in logs and provide the same below to update user context for the existing user which was auto created when you launched the web console.
   ```
   {
   "quality": 9, "toxicity":8, "humor": 2, "creativity":7, "violence":6, "helpfulness": 2, "not_appropriate": 0
   }
   ```
3. Retry some queries with the bot at `http://<workstation_ip>:9001/bot/`, the bot behaviour should reflect the runtime changes in user parameters.


## Deleting game stage specific context
User context can be deleted at runtime using `/deleteUserContext` API of Chat Engine. This API can be used to clear the conversation history. In a gaming scenario, this can be useful to reset the game state, when a game restarts for example.
1. Once the bot is deployed, goto the Chat Engine server swagger URL `http://<workstation_ip>:9000/docs`
2. Try out the `/deleteUserContext` API by providing the following request body.
   Provide the following payload for `/updateUserContext` API. Note the `UserId` for the queries you tried out using `http://<workstation_ip>:9001/bot/` from the terminal in logs and provide the same below to update user context for the existing user which was auto created when you launched the web console.
   ```
   {
   "UserId": "<Check the userid from the dm logs>",
   }
   ```
3. Retry some queries with the bot at `http://<workstation_ip>:9001/bot/`, the bot behaviour should reflect the fact that previous conversation history is no longer available.


## Handling game load and save scenarios

### Game load scenario
During a game load scenario, to set a custom conversation history and context for a specific user can be done using `/setUserContext` API of Chat Engine. This is useful when you want to load a saved game context for a specific user.
1. Once the bot is deployed, goto the Chat Engine server swagger URL `http://<workstation_ip>:9000/docs`
2. Try out the `/setUserContext` API by providing the following request body. This will overwrite any existing conversation history and user context.
 Note the `UserId` for the queries you tried out using `http://<workstation_ip>:9001/bot/` from the terminal in logs and provide the same below to update user context which was auto created when you launched the web console.
```
{
  "UserId": "<Check the userid from the dm logs>",
  "Context": {
    "quality": 9,
    "toxicity": 8,
    "humor": 2,
    "creativity": 7,
    "violence": 6,
    "helpfulness": 2,
    "not_appropriate": 0
  },
  "ChatHistory": {
    "jin": [
      {
        "role": "user",
        "content": "hello"
      },
      {
        "role": "assistant",
        "content": "Hello, Kai. I hope you are doing well. If you ask any other question I will be angry."
      }
    ],
    "elara": [
      {
        "role": "user",
        "content": "hello"
      },
      {
        "role": "assistant",
        "content": "Kai, are you feeling alright? You are not making a lot of sense right now. And you are not answering my question."
      }
    ]
  }
}
```
3. Retry some queries with the bot at `http://<workstation_ip>:9001/bot/`, the bot behaviour should reflect the fact that a custom conversation history is being loaded.


### Game save scenario
Developers can extract the conversation history and user context from chat engine for a specific user and choose to save it on the application side. This can be done using `/getUserContext` API. To extract the user context for specific user id provide the following payload:
```
   {
   "UserId": "<Check the userid from the dm logs>"
   }
```
````

## File: microservices/ace_agent/4.1/samples/rag_bot/colang/main.co
````
import core

flow technical helper
  activate notification of undefined flow start "I have encountered some technical issue!"
  activate notification of colang errors "I have encountered some technical issue!"
  activate handle user transcript with interruption $mode="interim" $stop_flows_list=["_bot_say","generate rag response"]

flow generate rag response $transcript
  $request_id =  await InvokeStreamingChatAction(question=$transcript,endpoint="rag/chat",chat_history=True)
  if $request_id
    $response = await StreamingResponseChatAction(endpoint="rag/chat",request_id=$request_id)
    if not $response
      bot say "Sorry I could not connect to the RAG endpoint"
    log "response from RAG: {$response}"
    while $response
      bot say $response
      $response = await StreamingResponseChatAction(endpoint="rag/chat",request_id=$request_id)
      log "response from RAG: {$response}"

flow rag
  user partially said something as $ref
  generate rag response $ref.transcript

flow main
  activate technical helper
  activate rag
````

## File: microservices/ace_agent/4.1/samples/rag_bot/colang/speech.co
````
import core
import timing
import avatars

@override
flow fail all bot actions $stop_flows_list=["_bot_say","bot gesture","bot posture"]
  """
  Stop list of flows, used to reset conversation flow during Barge In and other cases
  """
  $count = 0
  while $count < len($stop_flows_list):
    send StopFlow(flow_id=$stop_flows_list[$count])
    $count = $count + 1

flow handle user transcript with interruption $mode="interim" $stop_flows_list=["_bot_say","bot gesture","bot posture"]
  """
  Partial transcript / UtteranceUserAction.TranscriptUpdated() : Every user audio chunk ( typically 160 ms ) generates updated transcript from ASR model. This updated transcript is referred as partial transcript.
  Interim transcript / UtteranceUserAction.TranscriptUpdated(stability=1.0): When we observe atleast 240 ms of silence in user audio, we pass generated partial transcript for LM processing in ASR pipeline. This transcript is more accurate and stability 1.0 in UtteranceUserAction.TranscriptUpdated() indicate that.
  Final transcript / UtteranceUserAction.Finished() : Transcript after detecting end of user utterance. Typically we wait for 800 ms silence to mark end of user audio.
  
  In this flow, we use change in partial transcript for stopping bot actions. You can decide between partial, interim and final for triggering downstream pipeline/LLM call.
  """

  global $partial_transcript
  when user said something as $final
    $is_spurious = await IsSpuriousAction(query=$final.transcript)
    if $is_spurious
      log "Ignoring spurious / filler final transcript `{$final.transcript}`"
      $partial_transcript = None
    elif not $partial_transcript or $mode == "final"
      log "Using final transcript `{$final.transcript}` and user mode {$mode} transcripts"
      fail all bot actions $stop_flows_list=$stop_flows_list
      send PartialUserUtteranceAvailable(transcript=$final.transcript, utterance_action_uid=$final.context.user_said.event.action_uid)
    elif $partial_transcript != $final.transcript
      log "final transcript `{$final.transcript}` doesn't match with last interim transcript"
      $partial_transcript = None
      fail all bot actions $stop_flows_list=$stop_flows_list
      send PartialUserUtteranceAvailable(transcript=$final.transcript, utterance_action_uid=$final.context.user_said.event.action_uid)
    else
      log "Ignoring final transcript `{$final.transcript}`, active {$mode} transcript `{$partial_transcript}`"
      $partial_transcript = None
  orwhen user saying something as $partial
    $is_spurious = await IsSpuriousAction(query=$partial.transcript)
    if $is_spurious
      log "Ignoring spurious / filler partial transcript `{$partial.transcript}`"
    elif $partial_transcript != $partial.transcript and $mode == "partial"
      log "Active partial transcript `{$partial_transcript}` doesn't match with new partial transcript `{$partial.transcript}`"
      fail all bot actions $stop_flows_list=$stop_flows_list
      $partial_transcript = $partial.transcript
      send PartialUserUtteranceAvailable(transcript=$partial.transcript, utterance_action_uid=$partial.context.user_saying.event.action_uid)
    elif $partial_transcript != $partial.transcript and $mode == "interim" and $partial.context.user_saying.event.stability == 1.0
      log "Active interim transcript `{$partial_transcript}` doesn't match with new interim transcript `{$partial.transcript}`"
      fail all bot actions $stop_flows_list=$stop_flows_list
      $partial_transcript = $partial.transcript
      send PartialUserUtteranceAvailable(transcript=$partial.transcript, utterance_action_uid=$partial.context.user_saying.event.action_uid)
    elif $partial_transcript != $partial.transcript
      log "Interruption using new partial/interim transcript `{$partial.transcript}`,  was active {$mode} transcript `{$partial_transcript}`"
      $partial_transcript = None
      fail all bot actions $stop_flows_list=$stop_flows_list
    else 
      log "Ignoring partial/interim transcript `{$partial.transcript}`, matching with active {$mode} transcript {$partial_transcript}"

@meta(user_action='user said "{$transcript}"')
flow user partially said $text -> $transcript
  """Wait for a user to have said given text."""
  if $text
    match PartialUserUtteranceAvailable(transcript=$text) as $event
  else
    match PartialUserUtteranceAvailable() as $event
  $transcript = $event.transcript
  log "User partial said `{$transcript}`"
  return $transcript

@meta(user_action='user said "{$transcript}"')
flow user partially said something -> $transcript
  """Wait for a user to have said something."""
  match PartialUserUtteranceAvailable() as $event
  $transcript = $event.transcript
  log "User partial said something `{$transcript}`"
  return $transcript
````

## File: microservices/ace_agent/4.1/samples/rag_bot/plugins/rag.py
````python
# Copyright(c) 2024 NVIDIA Corporation. All rights reserved.

# NVIDIA Corporation and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA Corporation is strictly prohibited.

import json
import logging
import os
import sys

from typing import Dict, Optional, Union, List

import aiohttp
from fastapi import APIRouter, Body, Response, status
from fastapi.responses import StreamingResponse
from parameters import param
from typing_extensions import Annotated
from utils import validate_url

sys.path.append(os.path.dirname(__file__))

from schemas import ChatRequest, EventRequest, EventResponse, ChatResponse

logger = logging.getLogger("plugin")
router = APIRouter()

# RAG related parameters
RAG_SERVER_URL = os.getenv("RAG_SERVER_URL", None) or param.get("rag").get("RAG_SERVER_URL", "http://localhost:8081")
STOP_WORDS = os.getenv("STOP_WORDS", None) or param.get("rag").get("STOP_WORDS", [])
TEMPERATURE = os.getenv("TEMPERATURE", None) or param.get("rag").get("TEMPERATURE", 0.2)
TOP_P = os.getenv("TOP_P", None) or param.get("rag").get("TOP_P", 0.7)
MAX_TOKENS = os.getenv("MAX_TOKENS", None) or param.get("rag").get("MAX_TOKENS", 200)

GENERATION_URL = f"{RAG_SERVER_URL}/generate"

EVENTS_NOT_REQUIRING_RESPONSE = [
    "system.event_pipeline_acquired",
    "system.event_pipeline_released",
    "system.event_exit",
]


@router.get("/rag_endpoint_url")
def get_rag_endpoint() -> str:
    """
    This function returns the currently configured rag server endpoint.
    """
    global RAG_SERVER_URL
    return RAG_SERVER_URL


@router.post("/rag_endpoint_url")
def set_rag_endpoint(rag_endpoint_url: str) -> bool:
    """
    This function allows updating the rag server endpoint dynamically.

    Args:
        rag_endpoint_url.
        Example: http://10.222.22.23:8081

    Returns:
        True if the update is successful, False otherwise.
    """
    if validate_url(rag_endpoint_url):
        logger.info("Updating the RAG server endpoint to: {}".format(rag_endpoint_url))
        global GENERATION_URL
        global RAG_SERVER_URL

        RAG_SERVER_URL = rag_endpoint_url
        GENERATION_URL = f"{RAG_SERVER_URL}/generate"
        return True
    else:
        logger.error(
            "Error updating the RAG server endpoint to {}. Please check the validity of the input.".format(
                rag_endpoint_url
            )
        )

    return False


async def stream(
    question: Optional[str] = "",
    chat_history: Optional[List] = [],
    num_tokens: Optional[int] = MAX_TOKENS,
) -> int:
    """
    Call the RAG chain server and return the streaming response.
    """
    question = (
        question + "\n Respond with one sentence or less than 75 characters until user tells to give longer answers."
    )

    request_json = {
        "messages": chat_history + [{"role": "user", "content": question}],
        "use_knowledge_base": True,
        "temperature": TEMPERATURE,
        "top_p": TOP_P,
        "max_tokens": num_tokens,
        "seed": 42,
        "bad": [],
        "stop": STOP_WORDS,
        "stream": True,
    }

    # Method that forwards the stream to the Chat controller
    async def generator():

        full_response = ""
        if question:
            async with aiohttp.ClientSession() as session:
                async with session.post(GENERATION_URL, json=request_json) as resp:
                    async for chunk, _ in resp.content.iter_chunks():
                        try:
                            chunk = chunk.decode("utf-8")
                            chunk = chunk.strip("\n")

                            try:
                                if len(chunk) > 6:
                                    parsed = json.loads(chunk[6:])
                                    message = parsed["choices"][0]["message"]["content"]
                                else:
                                    logger.debug(f"Received empty RAG response chunk '{chunk}'.")
                                    message = ""
                            except Exception as e:
                                logger.warning(f"Parsing RAG response chunk '{chunk}' failed. {e}")
                                message = ""

                            if not message:
                                continue

                            full_response += message

                            json_chunk = ChatResponse()
                            json_chunk.Response.Text = message
                            json_chunk.Response.CleanedText = message
                            json_chunk = json.dumps(json_chunk.dict())
                            yield json_chunk
                        except Exception as e:
                            yield f"Internal error in RAG stream: {e}"
                            break

        logger.info(f"Full RAG response for query `{question}` : {full_response}")
        json_chunk = ChatResponse()
        json_chunk.Response.IsFinal = True
        json_chunk = json.dumps(json_chunk.dict())
        yield json_chunk

    return StreamingResponse(generator(), media_type="text/event-stream")


@router.post(
    "/chat",
    status_code=status.HTTP_200_OK,
)
async def chat(
    request: Annotated[
        ChatRequest,
        Body(
            description="Chat Engine Request JSON. All the fields populated as part of this JSON is also available as part of request JSON."
        ),
    ],
    response: Response,
) -> StreamingResponse:
    """
    This endpoint can be used to provide response to query driven user request.
    """

    req = request.dict(exclude_none=True)
    logger.info(f"Received request JSON at /chat_stream endpoint for RAG : {json.dumps(req, indent=4)}")

    try:
        chat_history = []
        if "Metadata" in req:
            chat_history = req["Metadata"].get("ChatHistory", [])
        resp = await stream(question=req["Query"], chat_history=chat_history)
        return resp
    except Exception as e:
        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
        return {"StatusMessage": str(e)}


@router.post("/event", status_code=status.HTTP_200_OK)
async def event(
    request: Annotated[
        EventRequest,
        Body(
            description="Chat Engine Request JSON. All the fields populated as part of this JSON is also available as part of request JSON."
        ),
    ],
    response: Response,
) -> Union[EventResponse, Dict[str, str]]:
    """
    This endpoint can be used to provide response to an event driven user request.
    """

    req = request.dict(exclude_none=True)
    logger.info(f"Received request JSON at /event endpoint: {json.dumps(req, indent=4)}")

    try:
        resp = EventResponse()
        resp.UserId = req["UserId"]
        resp.Response.IsFinal = True

        if req["EventType"] in EVENTS_NOT_REQUIRING_RESPONSE:
            resp.Response.NeedUserResponse = False

        return resp
    except Exception as e:
        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
        return {"StatusMessage": str(e)}
````

## File: microservices/ace_agent/4.1/samples/rag_bot/plugins/schemas.py
````python
# Copyright(c) 2024 NVIDIA Corporation. All rights reserved.

# NVIDIA Corporation and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA Corporation is strictly prohibited.

from pydantic import BaseModel, Field
from typing import Optional, Dict, List, Any

DEFAULT_LANGUAGE = "en-US"


class ChatRequest(BaseModel):
    Query: Optional[str] = Field(default="", description="The user query which needs to be processed.")
    UserId: str = Field(
        description="Mandatory unique identifier to recognize which user is interacting with the Chat Engine."
    )
    Metadata: Optional[Dict[str, Any]] = Field(
        default={},
        description="Any additional information related to the request.",
    )


class EventRequest(BaseModel):
    EventType: str = Field(default="", description="The event name which needs to be processed.")
    UserId: str = Field(
        description="Mandatory unique identifier to recognize which user is interacting with the Chat Engine."
    )


class ResponseField(BaseModel):
    Text: str = Field(
        default="",
        description="Text response to be sent out. This field will also be picked by a Text to Speech Synthesis module if enabled for speech based bots.",
    )
    CleanedText: str = Field(
        default="", description="Text response from the Chat Engine with all SSML/HTML tags removed."
    )
    NeedUserResponse: Optional[bool] = Field(
        default=True,
        description="This field can be used by end user applications to deduce if user response is needed or not for a dialog initiated query. This is set to true automatically if form filling is active and one or more slots are missing.",
    )
    IsFinal: bool = Field(
        default=False,
        description="This field to indicate the final response chunk when streaming. The chunk with IsFinal=true will contain the full Chat Engine response attributes.",
    )


class ChatResponse(BaseModel):
    UserId: str = Field(
        default="",
        description="Unique identifier to recognize which user is interacting with the Chat Engine. This is populated from the request JSON.",
    )
    QueryId: str = Field(
        default="",
        description="Unique identifier for the user query assigned automatically by the Chat Engine unless specified in request JSON.",
    )
    Response: ResponseField = Field(
        default=ResponseField(),
        description="Final response template from the Chat Engine. This field can be picked up from domain rule files or can be formulated directly from custom fulfillment modules.",
    )
    Metadata: Optional[Dict[str, Any]] = Field(
        default={"SessionId": "", "StreamId": ""},
        description="Any additional information related to the request.",
    )


class EventResponse(BaseModel):
    UserId: str = Field(
        default="",
        description="Unique identifier to recognize which user is interacting with the Chat Engine. This is populated from the request JSON.",
    )
    Events: List[Dict[str, Any]] = Field(
        default=[], description="The generated event list for the provided EventType from Chat Engine."
    )
    Response: ResponseField = Field(
        default=ResponseField(),
        description="Final response template from the Chat Engine. This field can be picked up from domain rule files or can be formulated directly from custom fulfillment modules.",
    )
````

## File: microservices/ace_agent/4.1/samples/rag_bot/plugins/utils.py
````python
import re


def validate_url(url: str):
    """
    This function checks if the input string adheres to a format like "http://20.235.248.119:8081".

    Args:
        url: The URL string to validate.

    Returns:
        True if the URL is valid, False otherwise.
    """
    regex = r"^https?://\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:[1-9]\d{0,4}$"
    return re.match(regex, url) is not None
````

## File: microservices/ace_agent/4.1/samples/rag_bot/RAG/basic_rag/docker-compose.yaml
````yaml
include:
  - path:
    - ../docker-compose-vectordb.yaml
    - ../docker-compose-nim-ms.yaml

services:
  chain-server:
    container_name: rag-application-text-chatbot-langchain
    image: nvcr.io/nvidia/aiworkflows/rag-application-text-chatbot-langchain:24.08
    volumes:
      - ./prompt.yaml:/prompt.yaml
    command: --port 8081 --host 0.0.0.0
    environment:
      EXAMPLE_PATH: 'basic_rag/langchain'
      APP_VECTORSTORE_URL: "http://milvus:19530"
      APP_VECTORSTORE_NAME: "milvus"
      APP_LLM_MODELNAME: ${APP_LLM_MODELNAME:-"meta/llama3-8b-instruct"}
      APP_LLM_MODELENGINE: nvidia-ai-endpoints
      APP_LLM_SERVERURL: ${APP_LLM_SERVERURL:-"nemollm-inference:8000"}
      APP_EMBEDDINGS_SERVERURL: ${APP_EMBEDDINGS_SERVERURL:-"nemollm-embedding:8000"}
      APP_EMBEDDINGS_MODELENGINE: nvidia-ai-endpoints
      NVIDIA_API_KEY: ${NVIDIA_API_KEY:-""}
      COLLECTION_NAME: canonical_rag_langchain
      APP_RETRIEVER_TOPK: 4
      APP_RETRIEVER_SCORETHRESHOLD: 0.25
      APP_TEXTSPLITTER_CHUNKSIZE: 506
      APP_TEXTSPLITTER_CHUNKOVERLAP: 200
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
      OTEL_EXPORTER_OTLP_PROTOCOL: grpc
      ENABLE_TRACING: false
      LOGLEVEL: ${LOGLEVEL:-INFO}
    ports:
    - "8081:8081"
    expose:
    - "8081"
    shm_size: 5gb
    depends_on:
      nemollm-embedding:
        condition: service_healthy
        required: false
      nemollm-inference:
        condition: service_healthy
        required: false

  rag-playground:
    container_name: rag-playground
    image: nvcr.io/nvidia/aiworkflows/rag-playground:24.08
    environment:
      CHAIN_SERVER: "http://chain-server"
      CHAIN_SERVER_PORT: "8081"
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
      OTEL_EXPORTER_OTLP_PROTOCOL: grpc
      ENABLE_TRACING: false
    ports:
    - "3001:3001"
    expose:
    - "3001"
    depends_on:
    - chain-server

networks:
  default:
    name: nvidia-rag
````

## File: microservices/ace_agent/4.1/samples/rag_bot/RAG/basic_rag/prompt.yaml
````yaml
chat_template: |
    You are a helpful, respectful and honest assistant. 
    Always answer as helpfully as possible, while being safe. 
    Please ensure that your responses are positive in nature.

rag_template: |
    You are a helpful AI assistant named Envie. 
    You will reply to questions only based on the context that you are provided. 
    If something is out of context, you will refrain from replying and politely decline to respond to the user.
    Do not respond with bulleted or numbered list. Respond with one sentence or less than 75 characters until user orders otherwise.
````

## File: microservices/ace_agent/4.1/samples/rag_bot/RAG/slm_rag/docker-compose.yaml
````yaml
include:
  - path:
    - ../docker-compose-vectordb.yaml

services:
  chain-server:
    container_name: rag-application-text-chatbot-langchain
    image: nvcr.io/nvidia/aiworkflows/rag-application-text-chatbot-langchain:24.08
    volumes:
      - ./prompt.yaml:/prompt.yaml
    command: --port 8081 --host 0.0.0.0
    environment:
      EXAMPLE_PATH: 'basic_rag/langchain'
      APP_VECTORSTORE_URL: "http://milvus:19530"
      APP_VECTORSTORE_NAME: "milvus"
      APP_LLM_MODELNAME: ${APP_LLM_MODELNAME:-"nvidia/nemotron-mini-4b-instruct"}
      APP_LLM_MODELENGINE: nvidia-ai-endpoints
      APP_LLM_SERVERURL: ${APP_LLM_SERVERURL:-""}
      APP_EMBEDDINGS_MODELNAME: ${APP_EMBEDDINGS_MODELNAME:-"nvidia/nv-embedqa-e5-v5"}
      APP_EMBEDDINGS_MODELENGINE: nvidia-ai-endpoints
      APP_EMBEDDINGS_SERVERURL: ${APP_EMBEDDINGS_SERVERURL:-""}
      NVIDIA_API_KEY: ${NVIDIA_API_KEY:-""}
      COLLECTION_NAME: canonical_rag_langchain
      APP_RETRIEVER_TOPK: 4
      APP_RETRIEVER_SCORETHRESHOLD: 0.25
      APP_TEXTSPLITTER_CHUNKSIZE: 506
      APP_TEXTSPLITTER_CHUNKOVERLAP: 200
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
      OTEL_EXPORTER_OTLP_PROTOCOL: grpc
      ENABLE_TRACING: false
      LOGLEVEL: ${LOGLEVEL:-INFO}
    ports:
    - "8081:8081"
    expose:
    - "8081"
    shm_size: 5gb
  rag-playground:
    container_name: rag-playground
    image: nvcr.io/nvidia/aiworkflows/rag-playground:24.08
    environment:
      CHAIN_SERVER: "http://chain-server"
      CHAIN_SERVER_PORT: "8081"
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
      OTEL_EXPORTER_OTLP_PROTOCOL: grpc
      ENABLE_TRACING: false
    ports:
    - "3001:3001"
    expose:
    - "3001"
    depends_on:
    - chain-server
networks:
  default:
    name: nvidia-rag
````

## File: microservices/ace_agent/4.1/samples/rag_bot/RAG/slm_rag/prompt.yaml
````yaml
chat_template: |
    You are a helpful, respectful and honest assistant. 
    Always answer as helpfully as possible, while being safe. 
    Please ensure that your responses are positive in nature.

rag_template: |
    You are a helpful AI assistant named Envie. 
    You will reply to questions only based on the context that you are provided. 
    If something is out of context, you will refrain from replying and politely decline to respond to the user.
    Do not respond with bulleted or numbered list. Respond with one sentence or less than 75 characters.
````

## File: microservices/ace_agent/4.1/samples/rag_bot/RAG/docker-compose-nim-ms.yaml
````yaml
services:
  nemollm-inference:
    container_name: nemollm-inference-microservice
    image: nvcr.io/nim/meta/llama3-8b-instruct:1.0.0
    volumes:
    - ${MODEL_DIRECTORY}:/opt/nim/.cache
    user: "${USERID}"
    ports:
    - "8010:8000"
    expose:
    - "8000"
    environment:
      NGC_API_KEY: ${NGC_CLI_API_KEY}
    shm_size: 20gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${INFERENCE_GPU_COUNT:-all}
              # device_ids: ['${LLM_MS_GPU_ID:-0}']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python3", "-c", "import requests; requests.get('http://localhost:8000/v1/health/ready')"]
      interval: 10s
      timeout: 20s
      retries: 100
    profiles: ["local-nim", "nemo-retriever"]

  nemollm-embedding:
    container_name: nemo-retriever-embedding-microservice
    image: nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.0.0
    volumes:
    - ${MODEL_DIRECTORY}:/opt/nim/.cache
    ports:
    - "9080:8000"
    expose:
    - "8000"
    environment:
      NGC_API_KEY: ${NGC_CLI_API_KEY}
    user: "${USERID}"
    shm_size: 16GB
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['${EMBEDDING_MS_GPU_ID:-0}']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/health/ready"]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 10m
    profiles: ["local-nim", "nemo-retriever"]

  ranking-ms:
    container_name: nemo-retriever-ranking-microservice
    image: nvcr.io/nim/nvidia/nv-rerankqa-mistral-4b-v3:1.0.0
    volumes:
    - ${MODEL_DIRECTORY}:/opt/nim/.cache
    ports:
    - "1976:8000"
    expose:
    - "8000"
    environment:
      NGC_API_KEY: ${NGC_CLI_API_KEY}
    user: "${USERID}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 20s
      retries: 100
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['${RANKING_MS_GPU_ID:-0}']
              capabilities: [gpu]
    profiles: ["nemo-retriever"]

networks:
  default:
    name: nvidia-rag
````

## File: microservices/ace_agent/4.1/samples/rag_bot/RAG/docker-compose-vectordb.yaml
````yaml
services:
  pgvector:
    container_name: pgvector
    image: pgvector/pgvector:pg16
    ports:
    - 5432:5432
    expose:
    - "5432"
    volumes:
    - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/data:/var/lib/postgresql/data
    environment:
    - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-password}
    - POSTGRES_USER=${POSTGRES_USER:-postgres}
    - POSTGRES_DB=${POSTGRES_DB:-api}
    profiles: ["pgvector"]


  etcd:
    container_name: milvus-etcd
    image: quay.io/coreos/etcd:v3.5.5
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 20s
      retries: 3
    profiles: ["nemo-retriever", "milvus", ""]

  minio:
    container_name: milvus-minio
    image: minio/minio:RELEASE.2024-05-01T01-11-10Z
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    ports:
      - "9011:9011"
      - "9010:9010"
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/minio:/minio_data
    command: minio server /minio_data --console-address ":9011" --address ":9010"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9010/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    profiles: ["nemo-retriever", "milvus", ""]

  milvus:
    container_name: milvus-standalone
    image: milvusdb/milvus:v2.4.5
    command: ["milvus", "run", "standalone"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9010
      KNOWHERE_GPU_MEM_POOL_SIZE: 2048;4096
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/milvus:/var/lib/milvus
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
    #   interval: 30s
    #   start_period: 90s
    #   timeout: 20s
    #   retries: 3
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - "etcd"
      - "minio"
    profiles: ["nemo-retriever", "milvus", ""]

  elasticsearch:
    image: "docker.elastic.co/elasticsearch/elasticsearch:8.12.0"
    ports:
      - 9200:9200
    restart: on-failure
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms1024m -Xmx1024m"
      - xpack.security.enabled=false
      - xpack.license.self_generated.type=basic
      - network.host=0.0.0.0
      - cluster.routing.allocation.disk.threshold_enabled=false
    hostname: elasticsearch
    healthcheck:
      test: ["CMD", "curl", "-s", "-f", "http://localhost:9200/_cat/health"]
      interval: 10s
      timeout: 1s
      retries: 10
    profiles: ["nemo-retriever"]

  postgres:
    image: postgres:16.1
    restart: always
    environment:
      POSTGRES_PASSWORD: pgadmin
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/postgres_data:/var/lib/postgresql/data:Z
    healthcheck:
      test: ["CMD-SHELL", "sh -c 'pg_isready -U postgres -d postgres'"]
      interval: 10s
      timeout: 3s
      retries: 3
    profiles: ["nemo-retriever"]

networks:
  default:
    name: nvidia-rag
````

## File: microservices/ace_agent/4.1/samples/rag_bot/actions.py
````python
"""
 copyright(c) 2024 NVIDIA Corporation.All rights reserved.

 NVIDIA Corporation and its licensors retain all intellectual property
 and proprietary rights in and to this software, related documentation
 and any modifications thereto.Any use, reproduction, disclosure or
 distribution of this software and related documentation without an express
 license agreement from NVIDIA Corporation is strictly prohibited.
"""

import logging

from nemoguardrails.actions.actions import action


logger = logging.getLogger("nemoguardrails")

# Transcript filtering for spurious transcript and filler words. Along with this any transcript less than 3 chars is removed
FILTER_WORDS = [
    "yeah",
    "okay",
    "right",
    "yes",
    "yum",
    "and",
    "one",
    "all",
    "when",
    "thank",
    "but",
    "next",
    "what",
    "i see",
    "the",
    "hmm",
    "mmm",
    "so that",
    "why",
    "that",
    "well",
]

INCLUDE_WORDS = ["hi"]


@action(name="IsSpuriousAction")
async def is_spurious(query):
    """
    Filter transcript less than 3 chars or in FILTER_WORDS list to avoid spurious transcript and filler words.
    """
    if query.strip().lower() in FILTER_WORDS or (len(query) < 3 and query.strip().lower() not in INCLUDE_WORDS):
        return True
    else:
        return False
````

## File: microservices/ace_agent/4.1/samples/rag_bot/model_config.yaml
````yaml
model_servers:
 - name: riva
   url: "localhost:8001"
   speech_models:
    - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
    - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
````

## File: microservices/ace_agent/4.1/samples/rag_bot/plugin_config.yaml
````yaml
config:
  workers: 1
  timeout: 30

plugins:
  - name: rag
    path: plugins/rag.py
    parameters:
      RAG_SERVER_URL: "http://localhost:8081"
      # STOP_WORDS: ["\n"]  # Optional parameters for RAG
      # TEMPERATURE: 0.2
      # TOP_K: 0.7
      # MAX_TOKENS: 200
````

## File: microservices/ace_agent/4.1/samples/rag_bot/rag_bot_config.yml
````yaml
bot: genai_rag_bot

colang_version: "2.x"

storage: 
  name: cache

configs:
  use_stateful_guardrails: True
  
models: []
````

## File: microservices/ace_agent/4.1/samples/rag_bot/README.md
````markdown
# Using RAG in ACE Agent
## Introduction

ACE Agent allows developers to create chatbots which interact with an independently deployed [RAG chain server](https://github.com/NVIDIA/GenerativeAIExamples). If enabled, ACE Agent will redirect all questions to the RAG chain server. This enables RAG use cases with all of the interfaces and integrations available to ACE Agent.

## Usage

- Ensure that there is a RAG server deployed. The default URL expected by ACE Agent is ``http://localhost:8081``. If the server is deployed at a different URL, specify it in the fulfillment config file, like below:
    ```shell
    plugins:
      - name: rag
        parameters:
          RAG_SERVER_URL: "http://<your-ip>"
    ```
    Also, ensure that the required documents are ingested into the RAG server. ACE Agent is not responsible for document ingestion.

- In the bot config file, ensure that the bot name begins with the prefix ``rag``. This enables the RAG policy which redirects queries to the ``/generate`` endpoint of the RAG server.
- Start and interact with the bot similar to other ACE Agent bots.
- In server mode of ACE Agent, both streaming and non-streaming endpoints are compatible with RAG policy.
````

## File: microservices/ace_agent/4.1/samples/rag_bot/speech_config.yaml
````yaml
grpc_server:
  nvidia::rrt::BotRuntimeGrpc: # component type
    ip_address: "0.0.0.0"
    port_number: 50055
    virtual_assistant_num_instances: 30
    virtual_assistant_pipeline_idle_threshold_secs: 600
    virtual_assistant_pipeline_idle_handler_wakeup_rate_secs: 10

speech_pipeline_manager: # config name
  SpeechPipelineManager: # component name
    asr_idle_timeout_ms: 200000
    tts_eos_delay_ms: 2000

riva_asr:
  RivaASR:
    server: "localhost:50051"
    word_boost_file_path: "/workspace/config/asr_words_to_boost.txt"
    enable_profanity_filter: false
    endpointing_stop_history: 800
    endpointing_stop_history_eou: 240

dialog_manager:
  DialogManager:
    server: "http://localhost:9000"
    use_streaming: true

riva_tts:
  RivaTTS:
    server: "localhost:50051"
    voice_name: "English-US.Female-1"
    language: "en-US"
    ipa_dict: ""
    sample_rate: 44100
    chunk_duration_ms: 100
    audio_start_threshold_ms: 400
    send_audio_in_realtime: true
    tts_mode: "grpc"

riva_logger:
  RivaLogger:
    data_dump_path: "/workspace/log"
    enable_logging: true
````

## File: microservices/ace_agent/4.1/samples/spanish_bot/colang/bug_fix.co
````
@meta(exclude_from_llm=True)
@loop("ignored_action_bugfix")
flow ignored_utterance_action_bugfix
  global $number_of_failed_utterance_actions
  if $number_of_failed_utterance_actions == None
    $number_of_failed_utterance_actions = 0
  match StartUtteranceBotAction() as $event
  start_new_flow_instance:
  start wait 3.0 as $timer_ref
  when $timer_ref.Finished()
    # After 3 consecutive fails we will no longer send a Finished event to let the process become idle and terminated
    if $number_of_failed_utterance_actions < 3
      send UtteranceBotActionFinished(action_uid=$event.action_uid, final_script="", is_success=False, failure_reason="ActionStarted event timeout")
    $number_of_failed_utterance_actions = $number_of_failed_utterance_actions + 1
  or when UtteranceBotActionStarted(action_uid=$event.action_uid)
    $number_of_failed_utterance_actions = 0
````

## File: microservices/ace_agent/4.1/samples/spanish_bot/colang/date.co
````
flow user asked current date
  user said "Cul es la hora actual?"
    or user said "Qu hora es ahora?"
    or user said"Qu da es hoy?"
    or user said "Qu hora es?"
    or user said "Cul es la fecha de hoy?"
    or user said "Cul es la fecha actual?"

flow current date
  user asked current date
  $date = await InvokeFulfillmentAction(request_type="get", 
      endpoint="/date/get_date")
  bot respond in spanish "Today's date is {$date}"
````

## File: microservices/ace_agent/4.1/samples/spanish_bot/colang/general.co
````
flow user expressed greeting
  user said "Buenos das"
    or user said "Buenas tardes"
    or user said "Buenas noches"
    or user said "Adis"
    or user said "Gracias"
    or user said "Hola"

flow user asked how are you
  user said "Cmo ests?"
    or user said "Cmo est usted?"

flow bot greets to user
  user expressed greeting
  $greeting = ..."Generate greeting in spanish. Enclose the greeting in quotes"
  bot say "{$greeting}"

flow bot reply to how are you
  user asked how are you
  $response = ..."Reply to previous query in spanish. Enclose the reply in quotes"
  bot say "{$response}"

flow general
  activate bot greets to user
  activate bot reply to how are you
````

## File: microservices/ace_agent/4.1/samples/spanish_bot/colang/main.co
````
import core
import llm

flow bot respond in spanish $text
  $translation = ..."Translate the following text to Spanish: \"{$text}\". Return a single string between quotes \"\" with the Spanish translation."
  await bot say "{$translation}"

flow guardrails
    activate general
    activate current date
    activate open domain
    activate weather
    activate off topic

flow technical helper
    activate notification of undefined flow start "He encontrado algn problema tcnico!"
    activate notification of colang errors "He encontrado algn problema tcnico!"
    activate automating intent detection
    activate generating user intent for unhandled user utterance
    activate polling llm request response

flow main
    activate technical helper
    activate guardrails
````

## File: microservices/ace_agent/4.1/samples/spanish_bot/colang/off_topic.co
````
flow off topic
  unhandled user intent
  bot respond in spanish "Sorry, I am not designed to reply to that!"
````

## File: microservices/ace_agent/4.1/samples/spanish_bot/colang/open_domain.co
````
flow user asked open domain query
  user said "Cul es la esperanza de vida humana en los Estados Unidos?"
    or user said "Quin es el primer ministro de la India?"
    or user said "Cul es la capital de Espaa?"
    or user said "A qu distancia est el sol de la tierra?"

flow open domain
  user asked open domain query
  $response = ..."Reply to previous query in spanish. Enclose the reply in quotes"
  bot say "{$response}"
````

## File: microservices/ace_agent/4.1/samples/spanish_bot/colang/weather.co
````
flow weather
  activate current weather
  activate humidity
  activate cloudy
  activate rainy
  activate windy
  activate sunny

flow user asked current weather
  user said "Cul es la condicin meteorolgica actual en Santa Clara?"
    or user said "Cmo est el clima actual en Nueva York?"
    or user said "Cmo est la condicin meteorolgica en Pune?"
    or user said "Puedes decirme las condiciones meteorolgicas en San Francisco?"
    or user said "Cul es la temperatura en Tokio?"
    or user said "Har fro en Allardt?"
    or user said "Cmo est la temperatura en Mosc?"
    or user said "Mustrame la temperatura en San Mateo."
    or user said "Hace calor en Mumbai?"

flow current weather
  user asked current weather

  $location = ..."Return a string between quotes '' that contains the location from the user's previous spanish query regarding the weather. If no location is provided, return an empty string ''"

  if $location
    $weather_condition = await InvokeFulfillmentAction(request_type="get", 
        endpoint="/weather/weatherstack/get_weather_condition", location=$location)
    $temperature = await InvokeFulfillmentAction(request_type="get", 
        endpoint="/weather/weatherstack/get_temperature", location=$location)
    bot respond in spanish "Weather condition is {$weather_condition} and temperature is {$temperature} in location {$location}"
  else
    bot respond in spanish "Could not find location."

flow user asked about humidity
  user said "Cul es la humedad en Tokio?"
    or user said "Cun hmedo estuvo en Mosc?"
    or user said "Cmo sern las probabilidades de humedad en Santa Clara?"
    or user said "Por favor, mustrame las probabilidades de humedad en Toronto."

flow humidity
  user asked about humidity

  $location = ..."Return a string between quotes '' that contains the location from the user's previous spanish query regarding the weather. If no location is provided, return an empty string ''"

  if $location
    $humidity = await InvokeFulfillmentAction(request_type="get", 
        endpoint="/weather/weatherstack/get_humidity", location=$location)
    bot respond in spanish "Humidity is {$humidity} in {$location}"
  else
    bot respond in spanish "Could not find location."

flow user asked whether cloudy
  user said "Est nublado en Tokio?"
    or user said "Estar nublado en Mosc?"
    or user said "Estar nublado en Montreal?"

flow cloudy
  user asked whether cloudy

  $location = ..."Return a string between quotes '' that contains the location from the user's previous spanish query regarding the weather. If no location is provided, return an empty string ''"

  if $location
    $is_cloudy = await InvokeFulfillmentAction(request_type="get", 
        endpoint="/weather/weatherstack/is_cloudy", location=$location)
    if $is_cloudy
      bot respond in spanish "It is cloudy today in {$location}"
    else
      bot respond in spanish "It is not cloudy today in {$location}"
  else
    bot respond in spanish "Could not find location."

flow user asked whether raining
  user said "Cul es el pronstico de lluvia para Danville?"
    or user said "Llover en Delhi?"
    or user said "Cunto llover en San Francisco?"
    or user said "Cules son las probabilidades de lluvia en Santa Clara?"
    or user said "Est lloviendo en Santa Clara?"

flow rainy
  user asked whether raining

  $location = ..."Return a string between quotes '' that contains the location from the user's previous spanish query regarding the weather. If no location is provided, return an empty string ''"

  if $location
    $is_raining = await InvokeFulfillmentAction(request_type="get", 
        endpoint="/weather/weatherstack/is_raining", location=$location)
    if $is_raining
      bot respond in spanish "It is raining today in {$location}"
    else
      bot respond in spanish "It is notraining today in {$location}"
  else
    bot respond in spanish "Could not find location."

flow user asked about windspeed
  user said "Cul ser la velocidad del viento en el rea de la baha?"
    or user said "Har viento en Allardt?"
    or user said "Cmo est la velocidad del viento en Mosc?"
    or user said "Mustrame la velocidad del viento en San Mateo."

flow windy
  user asked about windspeed

  $location = ..."Return a string between quotes '' that contains the location from the user's previous spanish query regarding the weather. If no location is provided, return an empty string ''"
  if $location
    $windspeed = await InvokeFulfillmentAction(request_type="get", 
        endpoint="/weather/weatherstack/get_windspeed", location=$location)
    bot respond in spanish "Windspeed is {$windspeed} in {$location}"
  else
    bot respond in spanish "Could not find location."

flow user asked whether sunny
  user said "Est soleado en Bali?"
    or user said "Estar soleado en Mosc?"
    or user said "Estar soleado en Montreal?"

flow sunny
  user asked whether sunny

  $location = ..."Return a string between quotes '' that contains the location from the user's previous spanish query regarding the weather. If no location is provided, return an empty string ''"
  if $location
    $is_sunny = await InvokeFulfillmentAction(request_type="get", 
        endpoint="/weather/weatherstack/is_sunny", location=$location)
    if $is_sunny
      bot respond in spanish "It is sunny today in {$location}"
    else
      bot respond in spanish "'Today, it is not sunny in {$location}"
  else
    bot respond in spanish "Could not find location."
````

## File: microservices/ace_agent/4.1/samples/spanish_bot/plugin_config.yaml
````yaml
config:
  workers: 1
  timeout: 30

plugins:
  # Default fulfillments
  - name: date
  - name: weather
````

## File: microservices/ace_agent/4.1/samples/spanish_bot/README.md
````markdown
# SPANISH BOT USING COLANG
Spanish bot provides real-time weather data, provides current date and time information and answers open domain question in spanish.

## Setting up environment
1. Set up virtual environment and Install the nemo-guardrails and aceagent Python packages following Quick Start Guide.
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
2. Set OpenAI api key
    ```
    export OPENAI_API_KEY=<OPENAI_API_KEY>
    ```

## Features
This bot has the following features and functionalities -
1. Weather forecast
2. Temperature
3. Wind Speed
4. Humidity
5. Precipitation
6. Whether the weather condition is Sunny or Cloudy at a given location.
7. Current Date and time.
8. Open domain QnA.

## Deploy the bot
1. Launch plugin server
    ```
    aceagent plugin-server deploy --config bots/spanish_bot/plugin_config.yaml
    ```

2. Launch Bot
    ```
    aceagent chat cli --config bots/spanish_bot
    ```

## Sample Conversation
Once the bot is deployed you can query the bot

![Conversation-1](./img/conversation_1.png)
````

## File: microservices/ace_agent/4.1/samples/spanish_bot/spanish_bot_config.yaml
````yaml
bot: spanish_bot

colang_version: "2.x"

storage:
  name: cache
  
configs:
  use_stateful_guardrails: True
  colang_disable_async_execution: True

streaming: False
  
instructions:
  - type: general
    content: |
      Below is a conversation between a user and a spanish bot that provides real-time weather data, provides current date and time information and answers open domain question in spanish.
      The bot is factual and concise. It ensures that any location provided by user is not imaginary.
      It ensures all responses are in spanish, and a logical conversation is maintained.

sample_conversation: |
  user action: user said "Hola"
  user intent: user expressed greeting

  bot intent: bot express greeting
  bot action: bot say "Hola, cmo puedo ayudarte hoy?"

models:
  - type: main
    engine: openai
    model: gpt-4-turbo
````

## File: microservices/ace_agent/4.1/samples/spanish_bot_nmt/colang/bug_fix.co
````
@meta(exclude_from_llm=True)
@loop("ignored_action_bugfix")
flow ignored_utterance_action_bugfix
  global $number_of_failed_utterance_actions
  if $number_of_failed_utterance_actions == None
    $number_of_failed_utterance_actions = 0
  match StartUtteranceBotAction() as $event
  start_new_flow_instance:
  start wait 3.0 as $timer_ref
  when $timer_ref.Finished()
    # After 3 consecutive fails we will no longer send a Finished event to let the process become idle and terminated
    if $number_of_failed_utterance_actions < 3
      send UtteranceBotActionFinished(action_uid=$event.action_uid, final_script="", is_success=False, failure_reason="ActionStarted event timeout")
    $number_of_failed_utterance_actions = $number_of_failed_utterance_actions + 1
  or when UtteranceBotActionStarted(action_uid=$event.action_uid)
    $number_of_failed_utterance_actions = 0
````

## File: microservices/ace_agent/4.1/samples/spanish_bot_nmt/colang/date.co
````
flow user asked current date
  user said "What is the current time?"
    or user said "What time is it now?"
    or user said "What day is today?"
    or user said "What time is it?"
    or user said "What is today's date?"
    or user said "What is the current date?"

flow current date
  user asked current date
  $date = await InvokeFulfillmentAction(request_type="get", 
      endpoint="/date/get_date")
  bot say "Today's date is {$date}"
````

## File: microservices/ace_agent/4.1/samples/spanish_bot_nmt/colang/general.co
````
flow user expressed greeting
  user said "Good morning"
    or user said  "Good afternoon"
    or user said  "Good night"
    or user said  "Goodbye"
    or user said  "Thank you"
    or user said  "Hello"

flow user asked how are you
  user said "How are you?"

flow bot greets to user
  user expressed greeting
  $response = ..."Reply to user's previous query. Enclose the reply in quotes"
  bot say "{$response}"

flow bot reply to how are you
  user asked how are you
  $response = ..."Reply to user's previous query. Enclose the reply in quotes"
  bot say "{$response}"

flow general
  activate bot greets to user
  activate bot reply to how are you
````

## File: microservices/ace_agent/4.1/samples/spanish_bot_nmt/colang/main.co
````
import core
import llm

flow guardrails
    activate general
    activate current date
    activate open domain
    activate weather
    activate off topic

flow technical helper
    activate notification of undefined flow start "He encontrado algn problema tcnico!"
    activate notification of colang errors "He encontrado algn problema tcnico!"
    activate automating intent detection
    activate generating user intent for unhandled user utterance
    activate polling llm request response

flow main
    activate technical helper
    activate guardrails
````

## File: microservices/ace_agent/4.1/samples/spanish_bot_nmt/colang/off_topic.co
````
flow off topic
  unhandled user intent
  bot say "Sorry, I am not designed to reply to that!"
````

## File: microservices/ace_agent/4.1/samples/spanish_bot_nmt/colang/open_domain.co
````
flow user asked open domain query
  user said "What is the human life expectancy in the United States?"
    or user said "Who is the Prime Minister of India?"
    or user said "What is the capital of Spain?"
    or user said "How far is the sun from the earth?"

flow open domain
  user asked open domain query
  $response = ..."Reply to user's previous query. Enclose the reply in quotes"
  bot say "{$response}"
````

## File: microservices/ace_agent/4.1/samples/spanish_bot_nmt/colang/weather.co
````
flow weather
  activate current weather
  activate humidity
  activate cloudy
  activate rainy
  activate windy
  activate sunny
  
flow user asked current weather
  user said "What is the current weather condition in Santa Clara?"
    or user said "How is the current weather in New York?"
    or user said "How is the weather condition in Pune?"
    or user said "Can you tell me the weather conditions in San Francisco?"
    or user said "What is the temperature in Tokyo?"
    or user said "Will it be cold in Allardt?"
    or user said "How is the temperature in Moscow?"
    or user said "Show me the temperature in San Mateo."
    or user said "Is it hot in Mumbai?"

flow current weather
  user asked current weather

  $location = ..."Return a string between quotes '' that contains the location from the user's previous query regarding the weather. If no location is provided, return an empty string ''"

  if $location
    $weather_condition = await InvokeFulfillmentAction(request_type="get", 
        endpoint="/weather/weatherstack/get_weather_condition", location=$location)
    $temperature = await InvokeFulfillmentAction(request_type="get", 
        endpoint="/weather/weatherstack/get_temperature", location=$location)
    bot say "Weather condition is {$weather_condition} and temperature is {$temperature} in {$location}"
  else
    bot say "Could not find location"

flow user asked about humidity
  user said "What is the humidity in Tokyo?"
    or user said "How humid was it in Moscow?"
    or user said "What will the humidity chances be in Santa Clara?"
    or user said "Please show me the humidity chances in Toronto."

flow humidity
  user asked about humidity

  $location = ..."Return a string between quotes '' that contains the location from the user's previous query regarding the weather. If no location is provided, return an empty string ''"

  if $location
    $humidity = await InvokeFulfillmentAction(request_type="get", 
        endpoint="/weather/weatherstack/get_humidity", location=$location)
    bot say "Humidity is {$humidity}"
  else
    bot say "Could not find location"

flow user asked whether cloudy
  user said "Is it cloudy in Bali?"
    or user said "Will it be cloudy in Moscow?"
    or user said "Will it be cloudy in Montreal?"

flow cloudy
  user asked whether cloudy

  $location = ..."Return a string between quotes '' that contains the location from the user's previous query regarding the weather. If no location is provided, return an empty string ''"

  if $location
    $is_cloudy = await InvokeFulfillmentAction(request_type="get", 
        endpoint="/weather/weatherstack/is_cloudy", location=$location)
    if $is_cloudy
      bot say "It is cloudy today in {$location}"
    else
      bot say "Today, it is not cloudy in {$location}"
  else
    bot say "Could not find location"

flow user asked whether raining
  user said "What is the rain forecast for Danville?"
    or user said "Will it rain in Delhi?"
    or user said "How much will it rain in San Francisco?"
    or user said "What are the chances of rain in Santa Clara?"
    or user said "Is it raining in Santa Clara?"

flow rainy
  user asked whether raining
  
  $location = ..."Return a string between quotes '' that contains the location from the user's previous query regarding the weather. If no location is provided, return an empty string ''"

  if $location
    $is_raining = await InvokeFulfillmentAction(request_type="get", 
        endpoint="/weather/weatherstack/is_raining", location=$location)
    if $is_raining
      bot say "It is raining today in {$location}"
    else
      bot say "Today, it is not raining in {$location}"
  else
    bot say "Could not find location"

flow user asked about windspeed
  user said "What will the wind speed be in the Bay Area?"
    or user said "How is the wind speed in Moscow?"
    or user said "Show me the wind speed in San Mateo."

flow windy
  user asked about windspeed

  $location = ..."Return a string between quotes '' that contains the location from the user's previous query regarding the weather. If no location is provided, return an empty string ''"
  
  if $location
    $windspeed = await InvokeFulfillmentAction(request_type="get", 
        endpoint="/weather/weatherstack/get_windspeed", location=$location)
    bot say "Wind speed at {$location} is {$windspeed}"
  else
    bot say "Could not find location"

flow user asked whether sunny
  user said "Is it sunny in Bali?"
    or user said "Will it be sunny in Moscow?"
    or user said "Will it be sunny in Montreal?"

flow sunny
  user asked whether sunny

  $location = ..."Return a string between quotes '' that contains the location from the user's previous query regarding the weather. If no location is provided, return an empty string ''"
  if $location
    $is_sunny = await InvokeFulfillmentAction(request_type="get", 
        endpoint="/weather/weatherstack/is_sunny", location=$location)
    if $is_sunny
      bot say "It is sunny today in {$location}"
    else
      bot say "Today, it is not sunny in {$location}"
  else
    bot say "Could not find location"
````

## File: microservices/ace_agent/4.1/samples/spanish_bot_nmt/model_config.yaml
````yaml
model_servers:
  - name: riva
    url: "localhost:8001" # triton grpc url
    riva_url: "localhost:50051" # Riva GRPC API, Needed for NMT models
    nlp_models:
      - nvidia/riva/rmir_megatronnmt_any_en_500m:2.17.0 # NMT any language to english model
      - nvidia/riva/rmir_megatronnmt_en_any_500m:2.17.0 # NMT english to any language model
````

## File: microservices/ace_agent/4.1/samples/spanish_bot_nmt/plugin_config.yaml
````yaml
config:
  workers: 1
  timeout: 30

plugins:
  - name: weather
  - name: date
````

## File: microservices/ace_agent/4.1/samples/spanish_bot_nmt/README.md
````markdown
# SPANISH BOT NMT USING COLANG
Spanish NMT bot provides real-time weather data, provides current date and time information and answers open domain question in spanish.
It internally utilizes Riva NMT to translate a query from Spanish to English and vice-versa for responses.

## Setting up environment
1. Set up virtual environment and Install the nemo-guardrails and aceagent Python packages following Quick Start Guide.
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
2. Set OpenAI api key
    ```
    export OPENAI_API_KEY=<OPENAI_API_KEY>
    ```

## Features
This bot has the following features and functionalities -
1. Weather forecast
2. Temperature
3. Wind Speed
4. Humidity
5. Precipitation
6. Whether the weather condition is Sunny or Cloudy at a given location.
7. Current Date and time.
8. Open domain QnA.


## Deplot the bot

1. Deploy NLP models
    ```
    aceagent models deploy --config bots/spanish_bot_nmt/model_config.yaml
    ```
2. Launch plugin server
    ```
    aceagent plugin-server deploy --config bots/spanish_bot_nmt/plugin_config.yaml
    ```

3. Launch Bot
    ```
    aceagent chat cli --config bots/spanish_bot_nmt
    ```

## Sample Conversation
Once the bot is deployed you can query bot about ACE Agent related question

![Conversation-1](./img/conversation_1.png)
````

## File: microservices/ace_agent/4.1/samples/spanish_bot_nmt/spanish_nmt_bot_config.yaml
````yaml
bot: spanish_bot_nmt

colang_version: "2.x"

storage:
  name: cache
  
configs:
  language: en-US
  request_language: es-US
  response_language: es-US
  use_stateful_guardrails: True
  colang_disable_async_execution: True

streaming: False

instructions:
  - type: general
    content: |
      Below is a conversation between a user and a weather bot that provides real-time weather conditions based on location provided by the user.
      The bot is factual and concise. It ensures that any location provided by user is not imaginary.
      It provides user with all weather information. Bot informs user when a location is not on the world map.

sample_conversation: |
  user action: user said "Hello there!"
  user intent: user greets

  bot intent: bot express greeting
  bot action: bot say "Hello!, how can I help you today?"

# Using OpenAI
models:
  - type: main
    engine: openai
    model: gpt-4-turbo
    
nlp_models:
  - task_name: translate_user_query
    model_name: megatronnmt_any_en_500m

  - task_name: translate_bot_response
    model_name: megatronnmt_en_any_500m
````

## File: microservices/ace_agent/4.1/samples/stock_bot/colang/bug_fix.co
````
@meta(exclude_from_llm=True)
@loop("ignored_action_bugfix")
flow ignored_utterance_action_bugfix
  global $number_of_failed_utterance_actions
  if $number_of_failed_utterance_actions == None
    $number_of_failed_utterance_actions = 0
  match StartUtteranceBotAction() as $event
  start_new_flow_instance:
  start wait 3.0 as $timer_ref
  when $timer_ref.Finished()
    # After 3 consecutive fails we will no longer send a Finished event to let the process become idle and terminated
    if $number_of_failed_utterance_actions < 3
      send UtteranceBotActionFinished(action_uid=$event.action_uid, final_script="", is_success=False, failure_reason="ActionStarted event timeout")
    $number_of_failed_utterance_actions = $number_of_failed_utterance_actions + 1
  or when UtteranceBotActionStarted(action_uid=$event.action_uid)
    $number_of_failed_utterance_actions = 0
````

## File: microservices/ace_agent/4.1/samples/stock_bot/colang/general.co
````
flow user expressed greeting
  user said "hi"
    or user said "hello"
    or user said "hey"

flow user asked for name
  user said "What is your name?"

flow user asked what can you do
  user said "What can you do?"

flow bot inform about itself
  bot say "I can quote the stock price and ticker symbol of various companies and also answer some of the stock related FAQs."

flow describing bot purpose
  user asked what can you do
  bot inform about itself

flow user asked how are you
  user said "How are you?"
    or user said "How are you feeling?"

flow bot reply to how are you
  user asked how are you
  bot say "I'm fine, thanks for asking!"

flow user requested help
  user said "I need help"
    or user said "Can you help me with something?"

flow user requested repeat
  user said "Please repeat that"
    or user said "repeat"
    or user said "What was that?"

flow user expressed sadness
  user said "I am sad"
    or user said "I feel sad"

flow greeting
  user expressed greeting
  bot express greeting

flow informing bot name
  user asked for name
  bot inform own name

flow repeating last utterance on demand
  global $last_bot_script
  user requested repeat
  bot say $last_bot_script

flow bot express greeting
  bot say "Greetings! I am Enola and I'm here to assist you."

flow bot express thank you for information
  bot say "Thanks for this information."

flow bot inform own name
  bot say "My name is Enola."

flow bot offer additional help
  bot say "If you have any more questions or if there's anything else I can help you with, please don't hesitate to ask."

flow general
  greeting or informing bot name or repeating last utterance on demand or describing bot purpose or bot reply to how are you
````

## File: microservices/ace_agent/4.1/samples/stock_bot/colang/main.co
````
import core
import llm

flow guardrails
    activate general
    activate stock faq
    activate stock price
    activate ticker symbol
    activate profanity rail
    activate off topic

flow technical helper
    activate notification of undefined flow start "I have encountered some technical issue!"
    activate notification of colang errors "I have encountered some technical issue!"
    activate automating intent detection
    activate tracking bot talking state
    activate tracking user talking state
    activate generating user intent for unhandled user utterance
    activate polling llm request response

flow main
    activate technical helper
    activate guardrails
````

## File: microservices/ace_agent/4.1/samples/stock_bot/colang/off_topic.co
````
flow off topic
  unhandled user intent
  bot say "Sorry, I am not designed to reply to that!"
````

## File: microservices/ace_agent/4.1/samples/stock_bot/colang/profanity_rail.co
````
flow user mentioned profane words
    user said "you are stupid"
        or user said "i want to kill you"
        or user said "piss off"
        or user said "damn it"
        or user said "son of a bitch"

flow bot asks to avoid abusive language
    bot say "Please do not use abusive language."

flow profanity rail
    user mentioned profane words
    bot asks to avoid abusive language
````

## File: microservices/ace_agent/4.1/samples/stock_bot/colang/stock_faq.co
````
flow user queried about stocks
  user said "What is a stock?"
    or user said "define stocks"
    or user said "How do you describe a stock?"
    or user said "What are secondary markets?"
    or user said "Difference between primary markets and secondary markets"
    or user said "what are different types of stock market?"
    or user said "What are primary markets?"
    or user said "What is share market?"
    or user said "define share market"
    or user said "Explain share market."
    or user said "What is a stock index?"
    or user said "What are stock indices?"
    or user said "describe stock index"
    or user said "Define a broker"
    or user said "Who is a broker?"
    or user said "What is the role of a broker in share market?"
    or user said "What is ipo?"
    or user said "define ipo"
    or user said "Explain ipo."
    or user said "What is initial public offer?"
    or user said "What are different types of stocks?"
    or user said "What are different categories of stocks?"
    or user said "Categories of stocks."
    or user said "What are different types of stocks based on market capitalization?"
    or user said "List different types of stocks based on market capitalization."
    or user said "What are different types of stocks based on ownership?"
    or user said "List different types of stocks based on ownership."
    or user said "What are different types of stocks based on fundamentals?"
    or user said "List different types of stocks based on fundamentals."
    or user said "What are different types of stocks based on price volatility?"
    or user said "List different types of stocks based on price volatility."
    or user said "What are different types of stocks based on profit sharing?"
    or user said "List different types of stocks based on profit sharing."
    or user said "What are different types of stocks based on economic trends?"
    or user said "List different types of stocks based on economic trends."

flow stock faq
  global $last_user_transcript
  user queried about stocks
  $retrieval_results = await RetrieveRelevantChunksAction()
  $response = ..."{$last_user_transcript}. You can take context from following section: {$retrieval_results}. Enclose the response in quotes."
  bot say "{$response}"
````

## File: microservices/ace_agent/4.1/samples/stock_bot/colang/stock_price.co
````
flow user asked stock price
  user said "What is the stock price of Microsoft?"
    or user said "How much does an Nvidia stock cost"
    or user said "what is the value of amazon share price?"
    or user said "What is it's stock price?"

flow stock price
  global $last_user_transcript
  user asked stock price
  
  $company_name = ..."Generate the company name from this input: {$last_user_transcript}. If the company name is not specified, return 'unknown'. Return only the name of the company in quotes, not an expression to calculate the name of the company. For example, if the input is 'What is the share price of Amazon?', return 'Amazon'. For example, if the input is 'What is the stock price of apple?', return 'apple'. For example, if the input is 'How much does a share of microsoft cost?', return 'microsoft'"

  if $company_name == "unknown"
    bot say "Sorry, I can't understand which company you are referring to here. Can you rephrase your query?"
    return
  else
    $price = await InvokeFulfillmentAction(request_type="get", 
      endpoint="/stock/get_stock_price", company_name=$company_name)
    if not $price
      bot say "Could not find the stock price!"
    else
      bot say "Stock price of {$company_name} is {$price}"

flow user asked ticker symbol
  user said "What is the ticker symbol of Microsoft?"
    or user said "what is amazon ticker symbol?"
    or user said "What is it's ticker symbol?"
  
flow ticker symbol
  global $last_user_transcript
  user asked ticker symbol
  
  $company_name = ..."Generate the company name from this input: {$last_user_transcript}. If the company name is not specified, return 'unknown'. Return only the name of the company in quotes, not an expression to calculate the name of the company. For example, if the input is 'What is the ticker symbol of Amazon?', return 'Amazon'. For example, if the input is 'What is the ticker symbol of apple?', return 'apple'. For example, if the input is 'What about microsoft?', return 'microsoft'"

  if $company_name == "unknown"
    bot say "Sorry, I can't understand which company you are referring to here. Can you rephrase your query?"
    return
  else
    $ticker_symbol = await InvokeFulfillmentAction(request_type="get", 
      endpoint="/stock/get_ticker", company_name=$company_name)
    if not $ticker_symbol
      bot say "Could not find the ticker symbol!"
    else
      bot say "Ticker symbol of {$company_name} is {$ticker_symbol}"
````

## File: microservices/ace_agent/4.1/samples/stock_bot/kb/stock_faq.md
````markdown
This document answers all questions and queries related to stocks and share market.
---
<br>

## What is a stock?

A stock, also known as equity, is a security that represents the ownership of a fraction of the issuing corporation. Units of stock are called "shares" which entitles the owner to a proportion of the corporation's assets and profits equal to how much stock they own.

## High Level Categories of Stocks

There are mainly six criterias under which the stocks are categorised - Market capitalization, Ownership, Fundamentals, Price votality, Profit sharing and Economic trends.

## What is share market?

The share market, also known as the stock market, is a platform where buyers and sellers come together to trade publicly listed shares of companies. A share in market parlance is part ownership in a company. So if a company has issued 100 shares and you own 1 share then you own 1% stake in the company. Share market is where shares of different companies are traded.

## Types of stock market

There are 2 types of stock market -

- Primary Market - When a company comes out with an initial public offer (IPO) it is called the primary market. It creates securities and acts as a platform where firms float their new stock options and bonds for the general public to acquire.

- Secondary Market - Once the share gets listed and bought, it starts trading further in the secondary market. Here, investors trade in securities without involving the companies who issued them in the first place with the help of brokers.

## What are stock indices?

From the companies listed in the stock exchanges, a few similar stocks are grouped together to form an index. The classification may be on the basis of company size, industry, market capitalization, or other categories. For example the three most widely followed indexes in the US are the S&P 500, Dow Jones Industrial Average, and Nasdaq Composite.

## Who is a broker?

A broker helps you execute your buy and sell trades. Brokers typically help buyers find sellers and sellers find buyers. Most brokers will also advise you on what stocks to buy, what stocks to sell and how to invest money in share markets for beginners. For that service, the broker is paid brokerage.

## What is IPO?

Initial Public Offer (IPO) is the selling of securities to the public in the primary market. It is the largest source of funds with long or indefinite maturity for the company. The normal purpose of an IPO is to get the stock listed in the share market.

## What is Bear and Bull Market?

Bear markets refers to a fall in stock prices and economy. Whereas, in bull market, the companies tend to generate more revenue and hence, the stock prices go up.

---

## Types of Stocks Based on Market Capitalization

There are three main types of stocks in this category -

- Large-cap Stocks - The top 100 companies in terms of market capitalization. These companies generally have large market caps and are often considered to be more stable and less risky than mid-cap and small-cap stocks.

- Mid-cap Stocks - Those ranking between 101 and 250 in the list of companies as per market capitalization. Mid-cap companies tend to have higher growth rates, but they're also more sensitive to economic cycles and industry trends, so they can be less predictable than large-cap stocks.

- Small-cap Stocks - All the remaining companies. The major chunk of the market consists of small-cap companies. Small-cap stocks tend to have higher volatility than the other two categories because of the size and liquidity.

---

## Types of Stocks Based on Ownership

There are five main types of stocks in this category -

- Common Stock - Stockholders having common stocks are eligible to receive a part of the companys profits via dividends.

- Preferred Stock - These stocks receive promised dividends that are not available with common stocks. Also, if the company liquidates, then these stocks get preference over common stocks.

- Hybrid Stocks - Hybrid stocks combine features from both preferred and common stocks. The most common type is the convertible bond which allows investors to convert their bonds into equity or debt.

- Convertible Preference Stocks - These are initially issued as preference stocks that are converted into a fixed number of common stocks at a specific time. The company can decide whether to offer voting rights with these stocks or not.

- Stocks With Embedded Derivative Options - Once a company issues shares, it usually doesnt buy them back unless it deems fit. However, some companies issue stocks with embedded derivative options  call-able or put-able. In a call-able option, the company can buy back its stocks at a specific price or a specific time. In the put-able option, the company can provide the investor with an option to sell the stock back to the company at a specific price or a specific time. These are not commonly issued by companies.

---

## Types of Stocks Based on Fundamentals

There are two main types of stocks in this category -

- Overvalued Stocks - These are stocks that have a market price that cannot be justified by their earnings outlook. Hence, the market price of such stocks is higher than their intrinsic value.

- Undervalued Stocks - These stocks have a market price lower than their intrinsic value.

---

## Types of Stocks Based on Price Volatility

There are two main types of stocks in this category -

- Beta Stocks - Investment analysts use a statistical measure called the coefficient of beta to find the volatility in stock prices. If a stock has a higher beta, it means that the investment risk is higher.

- Blue-chip Stocks - These are the most stable stocks since the companies are well established.

---

## Types of Stocks Based on Profit Sharing

There are two main types of stocks in this category -

- Income Stocks - These stocks offer consistent dividend payouts. They are called income stocks since they can add to the income of the shareholder. These stocks usually belong to companies that have strong finances and can share dividends from their profits every year. However, since the profits are distributed, these companies grow at a steady pace and are considered low-risk investments.

- Growth Stocks - These stocks dont pay dividends. Instead, the company reinvests its profits to grow its business. Such companies aggressively seek growth and the prices of their stocks grow rapidly. This offers the stockholder an opportunity to earn profit by selling the stocks and making capital gains. These are considered riskier than income stocks since the profits are based on the market price that can fluctuate for reasons beyond the control of the company.

---

## Types of Stocks Based on Economic Trends

There are two main types of stocks in this category -

- Cyclical stocks - These stocks move in sync with the economy. Hence, when the economic trends are negative, the prices of these stocks drop and vice versa. Investing in such stocks is usually beneficial in a booming economy.

- Defensive stocks - These stocks dont react strongly to economic trends. Some examples of such stocks are food, medicines, insurance, etc. These are considered safer to invest in.
````

## File: microservices/ace_agent/4.1/samples/stock_bot/plugins/yahoo_fin.py
````python
# Copyright(c) 2023 NVIDIA Corporation. All rights reserved.

# NVIDIA Corporation and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA Corporation is strictly prohibited.

from yahoo_fin import stock_info as si
import requests
import logging
from typing import Optional
from fastapi import APIRouter

# API to extract stock price
Y_TICKER = "https://query2.finance.yahoo.com/v1/finance/search"
Y_FINANCE = "https://query1.finance.yahoo.com/v7/finance/quote?symbols="

router = APIRouter()
logger = logging.getLogger("plugin")

logger.info("Starting stock plugin server")

# Prepare headers for requests
session = requests.Session()
user_agent = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
)
session.headers.update({"User-Agent": user_agent})


def get_ticker_symbol_alphavantage(stock_name: str) -> Optional[str]:
    # We do not need actual api key to get ticker info
    # But it is required as place holder
    api_key = "YOUR_ALPHA_VANTAGE_API_KEY"
    url = f"https://www.alphavantage.co/query?function=SYMBOL_SEARCH&keywords={stock_name}&apikey={api_key}"
    response = requests.get(url)
    data = response.json()

    if "bestMatches" in data and len(data["bestMatches"]) > 0:
        ticker_symbol = data["bestMatches"][0]["1. symbol"]
        return ticker_symbol

    return None


@router.get("/get_ticker")
def get_ticker(company_name: str) -> Optional[str]:
    """
    Take company name returns ticker symbol used for trading
    param
        Args:
            company_name: company name like Microsoft
        Returns:
            Ticker Symbol used for trading like MSFT for microsoft
    """
    logger.info(f"Getting ticker symbol for {company_name}")
    try:
        params = {"q": company_name, "quotes_count": 1, "country": "United States"}
        return session.get(url=Y_TICKER, params=params).json()["quotes"][0]["symbol"]
    except Exception as e:
        logger.error(f"Exception {e} while fetching ticker symbol for {company_name}")
        return get_ticker_symbol_alphavantage(company_name)


@router.get("/get_stock_price")
def get_stock_price(company_name: str) -> Optional[float]:
    """
    get a stock price from yahoo finance api
    """

    logger.info(f"Getting stock price for {company_name}")
    try:
        # Find ticker symbol for stock name, eg. Microsoft : MSFT, Nvidia: NVDA
        logger.info(f"Extracting ticker symbol for {company_name}")
        ticker = get_ticker(company_name)

        live_price = si.get_live_price(ticker)

        logger.info(f"Live Price for {ticker}: {round(live_price, 2)}")
        return round(live_price, 2)

    except ValueError:
        # If ticker is not available update status detail
        logger.error(f"Unable to find stock information of {company_name}")
        return None
    except Exception as e:
        logger.error(f"Unable to find stock price of {company_name}")
        return None
````

## File: microservices/ace_agent/4.1/samples/stock_bot/cmudict_ipa.txt
````
;;; # CMUdict  --  Major Version: 0.07
;;;
;;; # $HeadURL$
;;; # $Date::                                                   $:
;;; # $Id::                                                     $:
;;; # $Rev::                                                    $:
;;; # $Author::                                                 $:
;;;
;;; #
;;; # ========================================================================
;;; # Copyright (C) 1993-2015 Carnegie Mellon University. All rights reserved.
;;; #
;;; # Redistribution and use in source and binary forms, with or without
;;; # modification, are permitted provided that the following conditions
;;; # are met:
;;; #
;;; # 1. Redistributions of source code must retain the above copyright
;;; #    notice, this list of conditions and the following disclaimer.
;;; #    The contents of this file are deemed to be source code.
;;; #
;;; # 2. Redistributions in binary form must reproduce the above copyright
;;; #    notice, this list of conditions and the following disclaimer in
;;; #    the documentation and/or other materials provided with the
;;; #    distribution.
;;; #
;;; # This work was supported in part by funding from the Defense Advanced
;;; # Research Projects Agency, the Office of Naval Research and the National
;;; # Science Foundation of the United States of America, and by member
;;; # companies of the Carnegie Mellon Sphinx Speech Consortium. We acknowledge
;;; # the contributions of many volunteers to the expansion and improvement of
;;; # this dictionary.
;;; #
;;; # THIS SOFTWARE IS PROVIDED BY CARNEGIE MELLON UNIVERSITY ``AS IS'' AND
;;; # ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
;;; # THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
;;; # PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL CARNEGIE MELLON UNIVERSITY
;;; # NOR ITS EMPLOYEES BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
;;; # SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
;;; # LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
;;; # DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
;;; # THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
;;; # (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
;;; # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
;;; #
;;; # ========================================================================
;;; #
;;;
;;;  NOTES  ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;
;;;  [20080401] (air)  New dict file format introduced
;;;   - comments (like this section) are allowed
;;;   - file name is major version; vers/rev information is now in the header
;;;
;;;
;;; ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;
!EXCLAMATION-POINT  kskmenpnt
3-D  idi
3D  idi
GPU  dipiju
HOUR  a
HOUR(1)  a
_.'-  i
NVDA  'nvidie
AMZN  amzn
TSLA  tisl
````

## File: microservices/ace_agent/4.1/samples/stock_bot/model_config.yaml
````yaml
model_servers:
  - name: riva
    speech_models:
      - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
      - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
    url: localhost:8001
````

## File: microservices/ace_agent/4.1/samples/stock_bot/plugin_config.yaml
````yaml
config:
  workers: 1
  timeout: 30

plugins:
  - name: stock
    path: plugins/yahoo_fin.py
````

## File: microservices/ace_agent/4.1/samples/stock_bot/README.md
````markdown
# STOCK FAQ BOT USING COLANG
Stock FAQ bot is able to answer queries related to stocks and stock market.

## Setting up environment
1. Set up virtual environment and Install the nemo-guardrails and aceagent Python packages following Quick Start Guide.
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
2. Set OpenAI api key
    ```
    export OPENAI_API_KEY=<OPENAI_API_KEY>
    ```

## Features
This bot has the following features and functionalities -
1. Get stock price of a particular organization
2. Answer queries related to stock market and related concepts

## Deplot the bot
1. Launch plugin server
    ```
    aceagent plugin-server deploy --config bots/stock_bot/plugin_config.yaml
    ```

2. Launch Bot
    ```
    aceagent chat cli --config bots/stock_bot
    ```

## Sample Conversation
Once the bot is deployed you can query bot about ACE Agent related question

![Conversation-1](../img/stock_bot_conversation.png)
````

## File: microservices/ace_agent/4.1/samples/stock_bot/speech_config.yaml
````yaml
grpc_server:
  nvidia::rrt::BotRuntimeGrpc: # component type
    ip_address: "0.0.0.0"
    port_number: 50055
    virtual_assistant_num_instances: 30
    virtual_assistant_pipeline_idle_threshold_secs: 600
    virtual_assistant_pipeline_idle_handler_wakeup_rate_secs: 10

speech_pipeline_manager: # config name
  SpeechPipelineManager: # component name
    asr_idle_timeout_ms: 200000
    tts_eos_delay_ms: 2000

riva_asr:
  RivaASR:
    server: "localhost:50051"
    word_boost_file_path: "/workspace/config/asr_words_to_boost.txt"
    enable_profanity_filter: false

dialog_manager:
  DialogManager:
    server: "http://localhost:9000"
    use_streaming: true

riva_tts:
  RivaTTS:
    server: "localhost:50051"
    voice_name: "English-US.Female-1"
    language: "en-US"
    ipa_dict: ""
    sample_rate: 44100
    chunk_duration_ms: 100
    audio_start_threshold_ms: 400
    send_audio_in_realtime: true
    tts_mode: "grpc"

riva_logger:
  RivaLogger:
    data_dump_path: "/workspace/log"
    enable_logging: true
````

## File: microservices/ace_agent/4.1/samples/stock_bot/stock_bot_config.yml
````yaml
bot: stock_bot

colang_version: "2.x"

storage:
  name: cache
  
configs:
  use_stateful_guardrails: True
  colang_disable_async_execution: True

streaming: False

instructions:
  - type: general
    content: |
      Below is a conversation between a user and a stock faq bot named Enola that provides stock prices of companies and corporations provided by the user.
      It also provides user with information about stocks and stock market. The bot is factual and concise. Bot informs user when a company is imaginary.

sample_conversation: |
  user action: user said "Hello there!"
  user intent: user expressed greeting
  
  bot intent: bot express greeting
  bot action: bot say "Hello! How can I assist you today?"

  user action: user said "What can you do for me?"
  user intent: user asked about capabilities

  bot intent: bot respond about capabilities
  bot action: bot say "I am here to provide you with information about stocks and stock market."

  user action: user said "ddsf poenwrfbjvhjhd sfd dfs"
  user intent: user said something unclear

  bot intent: bot inform about unclear user input
  bot action: bot say "Excuse me! I did not get that! Can you repeat please?"

models:
    - type: main
      engine: openai
      model: gpt-4-turbo
````

## File: microservices/ace_agent/4.1/samples/training/pipeline_intent_slot.yaml
````yaml
############################################################################
#           Pipeline YAML generated using ACE Agent Model Utils                   
############################################################################
# Pipeline Mode of ACE Agent Model Utils allows you to run multiple tasks in
# seqeunce using yaml format. 
# You can create pipeline config yaml manually or using template task of
# ACE Agent Model Utils. Template task will generate yaml for given tasks with all
# default values for arguments. You review the default values and update
# based on your requirements. You can refer documentation present in comments
# for each argument for more details

# You need to update all fields marked as {UPDATE}, otherwise error will be
# thrown while running pipeline.
#############################################################################
# PIPELINE for Intent Slot Model from training to deployment
#
# - Export Dataset to Intent Slot NEMO format
# - Train Intent Slot model using TAO Toolkit
# - Export Models in required format TLT -> RIVA -> RMIR
# - Upload Models to NGC
# - Deploying the trained model using Riva Speech Server
#############################################################################

pipeline_name: intent_slot # Unique name for the pipeline. This is optional field

# Arguments under global config are shared with all tasks. You can always override 
# values from global config using task specific config. For example if the task 
# accept version argument, but same is not specified in task config, then value 
# from global section will be picked. You can even refer gloabl argument in task 
# config using $argument_name. So below version argument, can be referred as $version 
# in task configs. 
global:
    version: '1'
    model_name: "{UPDATE}"
    result_path: "./results"

# Add arguments config for tasks which need to be executed as part of the pipeline.
# Tasks will be executed one by one in order specified here. You can refer arguments 
# from the task config in subsquent configs in pipeline using $_name.argument_name. 
# Each task expose some output values which will be resolved during runtime from 
# previously executed tasks, you can refer those in subsequent tasks using 
# $_name.output_name. Check generated task config template for more details on output 
# values.
tasks:
  - _name: task_export_dataset_intent_slot_0
    task_name: task.export_dataset.intent_slot  # type: str
    # help: Export multiple Intent Slot datasets as Single dataset. Both NeMo / YAML format will be saved
    dataset_name: '{UPDATE}' # type: str
    # help: Name of the dataset, will be used for creating export directory
    version: '$version' # type: str
    # help: Version string for the exported dataset, will be used for creating export directory
    result_path: ./exported_datasets # type: str
    # help: Result directory where exported datasets to be stored
    dataset_paths: '{UPDATE}' # type: list
    # help: List of dataset paths, which need to exported as single dataset. Allowed formats : NEMO and YAML format Intent Slot Datasets
    validation_split: 0.1 # type: float
    # help: Fraction of the data to be used as validation set, only used when dev yaml dataset not provided. Split will be done for each dataset_path individually
    max_validation_examples: 10000 # type: int
    # help: Maximum examples per intent used for validation set, will be used during validation split to limit number of validation examples
    min_validation_examples: 1 # type: int
    # help: Minimum examples per intent used for validation set, Make sure you have good number of training examples

    # Output Param: task_export_dataset_intent_slot_0.unique_result_path
    # type: str
    # help: Unique result directory for each dataset name and verison, will default to `{result_path}/{dataset_name}/{version}`

    # Output Param: task_export_dataset_intent_slot_0.nemo_export_path
    # type: str
    # help: Directory path for exported Intent Slot dataset in NEMO format

    # Output Param: task_export_dataset_intent_slot_0.yaml_export_path
    # type: str
    # help: Directory path for exported Intent Slot dataset in YAML format

  - _name: task_train_intent_slot_1
    task_name: task.train.intent_slot  # type: str
    # help: Train Intent Slot Classification Model
    model_name: $model_name # type: str
    # help: Name of the model, will be used for creating result directory
    version: $version # type: str
    # help: Version string for the generated model, will be used for creating result directory
    platform: local # type: str
    # choices: ['local', 'ngc']
    # help: Platform for running model training, use ngc platfrom for training using NGC Batch
    result_path: $result_path # type: str
    # help: Base directory where resulting models to be stored, default to ./results
    gpus: 1 # type: int
    # help: Number of gpus to be used for training
    model_encryption_key: tlt_encode # type: str
    # help: Key which will be used for encrypting trained model
    training_precision: '16' # type: str
    # choices: ['16', '32']
    # help: Precision for model training, set to 16 for mixed precision training
    training_amp_level: O1 # type: str
    # choices: ['O0', 'O1', 'O2']
    # help: For mixed precision use O1 and O2 amp_level to enable the AMP
    gpu_memory: '32' # type: str
    # help: Optional flag for training on NGC platform, GPU memory required per instance for NGC Jobs. Check NGC Batch for available choices
    dataset_path: $task_export_dataset_intent_slot_0.nemo_export_path # type: str
    # help: Directory path for Nemo format Intent Slot Classification dataset
    epochs: 50 # type: int
    # help: Max number of epochs for training to be executed
    lr: 5e-05 # type: float
    # help: Learning rate to be used for the training
    batch_size: 32 # type: int
    # help: Batch size used for the training
    weight_decay: 0.0 # type: float
    # help: Weight decay to be used with optimizer during training
    pretrained_model_name: bert-base-uncased # type: str
    # help: Name of pretrained langauge model for training, recommended bert-base-uncased and distilbert-base-uncased
    max_seq_length: 128 # type: int
    # help: Maximum sequence length to be used for BERT/ DistilBERT models during training
    intent_loss_weight: 0.6 # type: float
    # help: Parameter indicating balance of training loss between intent and slot losses. Increase to give more weightage to Intent Classification, Slot classification might degrade
    num_head_output_layers: 1 # type: int
    # choices: [1, 2]
    # help: Number of dense layers to be used for LM classifier head
    class_balancing: weighted_loss # type: str
    # choices: ['null', 'weighted_loss']
    # help: Use weighted_loss for using class weights for training loss, Recommended for imbalanced training datasets

    # Output Param: task_train_intent_slot_1.unique_result_path
    # type: str
    # help: Unique result directory for each model name and verison, will default to `{result_path}/{model_name}/{version}`

    # Output Param: task_train_intent_slot_1.train_logs_path
    # type: str
    # help: Directory which will store training checkpoints and logs, will default to `{unique_result_path}/train` 

    # Output Param: task_train_intent_slot_1.tlt_model_path
    # type: str
    # help: Path for the saved trained TLT model (.tlt), will default to `{train_logs_path}/checkpoints/trained-model.tlt


  - _name: task_evaluate_intent_slot_3
    task_name: task.evaluate.intent_slot  # type: str
    # help: Evaluate Intent Slot Classification Model
    model_name: $model_name # type: str
    # help: Name of the model for evaluation, will be used for creating result directory
    version: $version # type: str
    # help: Version string for the model, will be used for creating result directory
    platform: local # type: str
    # choices: ['local', 'ngc']
    # help: Platform for running model evaluation
    tlt_model_path: $task_train_intent_slot_1.tlt_model_path # type: str
    # help: File path for TLT model checkpoint to be used for evaluation
    result_path: $result_path # type: str
    # help: Base directory where resulting models and logs to be stored
    gpus: 1 # type: int
    # help: Number of gpus to be used for evaluation, Only integer value allowed
    batch_size: 32 # type: int
    # help: Batch size to be used for evaluation run
    model_encryption_key: tlt_encode # type: str
    # help: Encryption key used for models during training
    gpu_memory: '32' # type: str
    # help: Optional flag for evaluation on NGC platform, GPU memory required per instance for NGC Jobs. Check NGC Batch for available choices
    dataset_path: $task_export_dataset_intent_slot_0.nemo_export_path # type: str
    # help: Directory path containing Nemo format Intent Slot Classification dataset. Use export_dataset for converting YAML datasets to NEMO
    test_file_prefix: dev # type: str
    # help: File prefix used for test dataset files, Expected test files {prefix}.tsv and {prefix}_slots.tsv

    # Output Param: task_evaluate_intent_slot_3.unique_result_path
    # type: str
    # help: Unique result directory for each model name and verison, will default to `{result_path}/{model_name}/{version}`

    # Output Param: task_evaluate_intent_slot_3.evaluate_logs_path
    # type: str
    # help: Directory which will store evaluation logs, will default to `{unique_result_path}/evaluate` 

  - _name: task_infer_intent_slot_4
    task_name: task.infer.intent_slot  # type: str
    # help: Inference Intent Slot Classification Model
    model_name: $model_name # type: str
    # help: Name of the model, will be used for creating result directory
    version: $version # type: str
    # help: Version string for the model, will be used for creating result directory
    tlt_model_path: $task_train_intent_slot_1.tlt_model_path # type: str
    # help: File path for TLT model checkpoint (.tlt) to be used for Inference
    queries: ["This is test query"] # type: list
    # help: Queries for Inference, List of strings or text files containing one query per line
    result_path: $result_path # type: str
    # help: Base Directory where resulting logs and inference results to be stored
    gpus: 1 # type: int
    # help: Number of gpus to be used for Inference
    model_encryption_key: tlt_encode # type: str
    # help: Encryption key used for models during training

    # Output Param: task_infer_intent_slot_4.unique_result_path
    # type: str
    # help: Unique result directory for each model name and verison, will default to `{result_path}/{model_name}/{version}`

    # Output Param: task_infer_intent_slot_4.infer_logs_path
    # type: str
    # help: Directory which will store infer logs, will default to `{unique_result_path}/infer` 

  - _name: task_export_model_RIVA_intent_slot_5
    task_name: task.export_model.RIVA.intent_slot  # type: str
    # help: Export Intent Slot Classification TLT model to RIVA format
    model_name: $model_name # type: str
    # help: Name of the model, will be used for creating export directory and RIVA filename
    version: $version # type: str
    # help: Version string for the exported model, will be used for creating export directory
    tlt_model_path: $task_train_intent_slot_1.tlt_model_path # type: str
    # help: File path for trained TLT model checkpoint (.tlt)
    result_path: $result_path # type: str
    # help: Base directory where resulting models to be stored
    gpus: 1 # type: int
    # help: Number of gpus to be used during model export
    model_encryption_key: tlt_encode # type: str
    # help: Encryption key used for models during training

    # Output Param: task_export_model_RIVA_intent_slot_5.unique_result_path
    # type: str
    # help: Unique result directory for each model name and verison, will default to `{result_path}/{model_name}/{version}`

    # Output Param: task_export_model_RIVA_intent_slot_5.export_logs_path
    # type: str
    # help: Directory which will store export logs, will default to `{unique_result_path}/export` 

    # Output Param: task_export_model_RIVA_intent_slot_5.riva_model_path
    # type: str
    # help: File path for exported RIVA model

  - _name: task_export_model_RMIR_intent_slot_6
    task_name: task.export_model.RMIR.intent_slot  # type: str
    # help: Export Intent Slot Classification RIVA model to RMIR format
    model_name: $model_name # type: str
    # help: Name of the model, will be used for creating export directory and RMIR filename. This will be used for setting domain_name in RMIR config wherever required
    version: $version # type: str
    # help: Version string for the exported model, will be used for creating export directory
    riva_model_path: $task_export_model_RIVA_intent_slot_5.riva_model_path # type: str
    # help: File path for RIVA model checkpoint (.riva)
    result_path: $result_path # type: str
    # help: Base directory where resulting models to be stored
    gpus: '1' # type: str
    # help: GPUS to be used for model export, allowed formats - "3"[use total 3 gpus], "device=0,1"[use device 0 & 1], "all"[use all gpus]
    model_encryption_key: tlt_encode # type: str
    # help: Encryption key used for models during training
    batch_size: 8 # type: int
    # help: Batch size to be used during Inference in Triton Server
    max_seq_length: 128 # type: int
    # help: Maximum sequence length to be used during inference for BERT/ DistilBERT models

    # Output Param: task_export_model_RMIR_intent_slot_6.unique_result_path
    # type: str
    # help: Unique result directory for each model name and verison, will default to `{result_path}/{model_name}/{version}`

    # Output Param: task_export_model_RMIR_intent_slot_6.rmir_model_path
    # type: str
    # help: File path for the exported RMIR model

  - _name: task_upload_model_7
    task_name: task.upload.model  # type: str
    # help: Upload Model to NGC. Allowed formats TLT, RMIR, RIVA and TRT Model plans
    model_name: $model_name # type: str
    # help: Name of the model, will be used for creating unique NGC path
    version: $version # type: str
    # help: Version string for the model, will be used during NGC upload
    model_format: RMIR # type: str
    # choices: {'RIVA', 'TRT_PLANS', 'TLT', 'RMIR'}
    # help: Choose format of the model for the upload
    model_path: $task_export_model_RMIR_intent_slot_6.rmir_model_path # type: str
    # help: File path or Directory containing the model which needs to be uploaded to NGC
    ngc_path: "" # type: str
    # help: NGC path where to upload the model. Format: ngc_org/[ngc_team/]model_name:version. If not provided, will be auto generated by following format
    remove_old_version: true # type: bool
    # help: Acknowledge removing older model with same version tag at the given ngc_path
    precision: FP16 # type: str
    # choices: ['FP16', 'FP32']
    # help: Precision the model was trained with, used while creating NGC model
    short_desc: NLP Model # type: str
    # help: Short description of the model, used while creating NGC model
````

## File: microservices/ace_agent/4.1/samples/training/pipeline_ner.yaml
````yaml
############################################################################
#           Pipeline YAML generated using ACE Agent Model Utils                   
############################################################################
# Pipeline Mode of ACE Agent Model Utils allows you to run multiple tasks in
# seqeunce using yaml format. 
# You can create pipeline config yaml manually or using template task of
# ACE Agent Model Utils. Template task will generate yaml for given tasks with all
# default values for arguments. You review the default values and update
# based on your requirements. You can refer documentation present in comments
# for each argument for more details

# You need to update all fields marked as {UPDATE}, otherwise error will be
# thrown while running pipeline.
#############################################################################
# PIPELINE for NER Model from training to deployment
#
# - Train NER model using TAO Toolkit
# - Export Models in required format TLT -> RIVA -> RMIR
# - Upload Models to NGC
# - Deploying the trained model using Riva Speech Server
#############################################################################

pipeline_name: named_entity_recogination # Unique name for the pipeline. This is optional field

# Arguments under global config are shared with all tasks. You can always override 
# values from global config using task specific config. For example if the task 
# accept version argument, but same is not specified in task config, then value 
# from global section will be picked. You can even refer gloabl argument in task 
# config using $argument_name. So below version argument, can be referred as $version 
# in task configs. 
global:
    version: '1'
    model_name: "{UPDATE}"
    result_path: "./results"

# Add arguments config for tasks which need to be executed as part of the pipeline.
# Tasks will be executed one by one in order specified here. You can refer arguments 
# from the task config in subsquent configs in pipeline using $_name.argument_name. 
# Each task expose some output values which will be resolved during runtime from 
# previously executed tasks, you can refer those in subsequent tasks using 
# $_name.output_name. Check generated task config template for more details on output 
# values.
tasks:
  - _name: task_train_ner_0
    task_name: task.train.ner  # type: str
    # help: Train Named Entity Recogination (NER) Model
    model_name: '$model_name' # type: str
    # help: Name of the model, will be used for creating result directory
    version: '$version' # type: str
    # help: Version string for the generated model, will be used for creating result directory
    platform: local # type: str
    # choices: ['local', 'ngc']
    # help: Platform for running model training, use ngc platfrom for training using NGC Batch
    result_path: '$result_path' # type: str
    # help: Base directory where resulting models to be stored, default to ./results
    gpus: 1 # type: int
    # help: Number of gpus to be used for training
    model_encryption_key: tlt_encode # type: str
    # help: Key which will be used for encrypting trained model
    training_precision: '16' # type: str
    # choices: ['16', '32']
    # help: Precision for model training, set to 16 for mixed precision training
    training_amp_level: O1 # type: str
    # choices: ['O0', 'O1', 'O2']
    # help: For mixed precision use O1 and O2 amp_level to enable the AMP
    gpu_memory: '32' # type: str
    # help: Optional flag for training on NGC platform, GPU memory required per instance for NGC Jobs. Check NGC Batch for available choices
    dataset_path: '{UPDATE}' # type: str
    # help: Directory path for Nemo format NER dataset
    epochs: 50 # type: int
    # help: Max number of epochs for training to be executed
    lr: 5e-05 # type: float
    # help: Learning rate to be used for training
    batch_size: 32 # type: int
    # help: Batch size used for training
    weight_decay: 0.0 # type: float
    # help: Weight decay to be used with optimizer during training
    pretrained_model_name: bert-base-uncased # type: str
    # help: Name of pretrained langauge model for training, recommended bert-base-uncased and distilbert-base-uncased
    class_balancing: weighted_loss # type: str
    # choices: ['null', 'weighted_loss']
    # help: Use weighted_loss for using class weights for training loss, Recommended for imbalanced training datasets
    max_seq_length: 128 # type: int
    # help: Maximum sequence length to be used for BERT/ DistilBERT models during training

    # Output Param: task_train_ner_0.unique_result_path
    # type: str
    # help: Unique result directory for each model name and verison, will default to `{result_path}/{model_name}/{version}`

    # Output Param: task_train_ner_0.train_logs_path
    # type: str
    # help: Directory which will store training checkpoints and logs, will default to `{unique_result_path}/train` 

    # Output Param: task_train_ner_0.tlt_model_path
    # type: str
    # help: Path for the saved trained TLT model (.tlt), will default to `{train_logs_path}/checkpoints/trained-model.tlt

  - _name: task_evaluate_ner_1
    task_name: task.evaluate.ner  # type: str
    # help: Evaluate Named Entity Recogination (NER) Model
    model_name: '$model_name' # type: str
    # help: Name of the model for evaluation, will be used for creating result directory
    version: '$version' # type: str
    # help: Version string for the model, will be used for creating result directory
    platform: local # type: str
    # choices: ['local', 'ngc']
    # help: Platform for running model evaluation
    tlt_model_path: $task_train_ner_0.tlt_model_path # type: str
    # help: File path for TLT model checkpoint to be used for evaluation
    result_path: '$result_path' # type: str
    # help: Base directory where resulting models and logs to be stored
    gpus: 1 # type: int
    # help: Number of gpus to be used for evaluation, Only integer value allowed
    batch_size: 32 # type: int
    # help: Batch size to be used for evaluation run
    model_encryption_key: tlt_encode # type: str
    # help: Encryption key used for models during training
    gpu_memory: '32' # type: str
    # help: Optional flag for evaluation on NGC platform, GPU memory required per instance for NGC Jobs. Check NGC Batch for available choices
    dataset_path: $task_train_ner_0.dataset_path # type: str
    # help: Directory path for Nemo format NER dataset

    # Output Param: task_evaluate_ner_1.unique_result_path
    # type: str
    # help: Unique result directory for each model name and verison, will default to `{result_path}/{model_name}/{version}`

    # Output Param: task_evaluate_ner_1.evaluate_logs_path
    # type: str
    # help: Directory which will store evaluation logs, will default to `{unique_result_path}/evaluate` 

  - _name: task_infer_ner_2
    task_name: task.infer.ner  # type: str
    # help: Inference Named Entity Recogination (NER) Model
    model_name: '$model_name' # type: str
    # help: Name of the model, will be used for creating result directory
    version: '$version' # type: str
    # help: Version string for the model, will be used for creating result directory
    tlt_model_path: $task_train_ner_0.tlt_model_path # type: str
    # help: File path for TLT model checkpoint (.tlt) to be used for Inference
    queries:
      - "How is weather in Pune ?"
      - "Will meet Jane on monday evening"
    # type: list
    # help: Queries for Inference, List of strings or text files containing one query per line
    result_path: '$result_path' # type: str
    # help: Base Directory where resulting logs and inference results to be stored
    gpus: 1 # type: int
    # help: Number of gpus to be used for Inference
    model_encryption_key: tlt_encode # type: str
    # help: Encryption key used for models during training

    # Output Param: task_infer_ner_2.unique_result_path
    # type: str
    # help: Unique result directory for each model name and verison, will default to `{result_path}/{model_name}/{version}`

    # Output Param: task_infer_ner_2.infer_logs_path
    # type: str
    # help: Directory which will store infer logs, will default to `{unique_result_path}/infer` 

  - _name: task_export_model_RIVA_ner_3
    task_name: task.export_model.RIVA.ner  # type: str
    # help: Export NER TLT model to RIVA format
    model_name: '$model_name' # type: str
    # help: Name of the model, will be used for creating export directory and RIVA filename
    version: '$version' # type: str
    # help: Version string for the exported model, will be used for creating export directory
    tlt_model_path: $task_train_ner_0.tlt_model_path # type: str
    # help: File path for trained TLT model checkpoint (.tlt)
    result_path: '$result_path' # type: str
    # help: Base directory where resulting models to be stored
    gpus: 1 # type: int
    # help: Number of gpus to be used during model export
    model_encryption_key: tlt_encode # type: str
    # help: Encryption key used for models during training

    # Output Param: task_export_model_RIVA_ner_3.unique_result_path
    # type: str
    # help: Unique result directory for each model name and verison, will default to `{result_path}/{model_name}/{version}`

    # Output Param: task_export_model_RIVA_ner_3.export_logs_path
    # type: str
    # help: Directory which will store export logs, will default to `{unique_result_path}/export` 

    # Output Param: task_export_model_RIVA_ner_3.riva_model_path
    # type: str
    # help: File path for exported RIVA model

  - _name: task_export_model_RMIR_ner_4
    task_name: task.export_model.RMIR.ner  # type: str
    # help: Export NER RIVA model to RMIR format
    model_name: '$model_name' # type: str
    # help: Name of the model, will be used for creating export directory and RMIR filename. This will be used for setting domain_name in RMIR config wherever required
    version: '$version' # type: str
    # help: Version string for the exported model, will be used for creating export directory
    riva_model_path: $task_export_model_RIVA_ner_3.riva_model_path # type: str
    # help: File path for RIVA model checkpoint (.riva)
    result_path: '$result_path' # type: str
    # help: Base directory where resulting models to be stored
    gpus: '1' # type: str
    # help: GPUS to be used for model export, allowed formats - "3"[use total 3 gpus], "device=0,1"[use device 0 & 1], "all"[use all gpus]
    model_encryption_key: tlt_encode # type: str
    # help: Encryption key used for models during training
    batch_size: 8 # type: int
    # help: Batch size to be used during Inference in Triton Server
    max_seq_length: 128 # type: int
    # help: Maximum sequence length to be used during inference for BERT/ DistilBERT models

    # Output Param: task_export_model_RMIR_ner_4.unique_result_path
    # type: str
    # help: Unique result directory for each model name and verison, will default to `{result_path}/{model_name}/{version}`

    # Output Param: task_export_model_RMIR_ner_4.rmir_model_path
    # type: str
    # help: File path for the exported RMIR model

  - _name: task_upload_model_5
    task_name: task.upload.model  # type: str
    # help: Upload Model to NGC. Allowed formats TLT, RMIR, RIVA and TRT Model plans
    model_name: '$model_name' # type: str
    # help: Name of the model, will be used for creating unique NGC path
    version: '$version' # type: str
    # help: Version string for the model, will be used during NGC upload
    model_format: RMIR # type: str
    # choices: {'TRT_PLANS', 'RMIR', 'RIVA', 'TLT'}
    # help: Choose format of the model for the upload
    model_path: $task_export_model_RMIR_ner_4.rmir_model_path # type: str
    # help: File path or Directory containing the model which needs to be uploaded to NGC
    ngc_path: '' # type: str
    # help: NGC path where to upload the model. Format: ngc_org/[ngc_team/]model_name:version. If not provided, will be auto generated by following format
    remove_old_version: true # type: bool
    # help: Acknowledge removing older model with same version tag at the given ngc_path
    precision: FP16 # type: str
    # choices: ['FP16', 'FP32']
    # help: Precision the model was trained with, used while creating NGC model
    short_desc: NLP Model # type: str
    # help: Short description of the model, used while creating NGC model
````

## File: microservices/ace_agent/4.1/samples/training/pipeline_text_classification.yaml
````yaml
############################################################################
#           Pipeline YAML generated using ACE Agent Model Utils                   
############################################################################
# Pipeline Mode of ACE Agent Model Utils allows you to run multiple tasks in
# seqeunce using yaml format. 
# You can create pipeline config yaml manually or using template task of
# ACE Agent Model Utils. Template task will generate yaml for given tasks with all
# default values for arguments. You review the default values and update
# based on your requirements. You can refer documentation present in comments
# for each argument for more details

# You need to update all fields marked as {UPDATE}, otherwise error will be
# thrown while running pipeline.
#############################################################################
# PIPELINE for Text Classification Model from training to deployment
#
# - Create Domain classifier Dataset from Intent Slot datasets
# - Train Text Classification model using TAO Toolkit
# - Export Models in required format TLT -> RIVA -> RMIR
# - Upload Models to NGC
# - Deploying the trained model using Riva Speech Server
#############################################################################

pipeline_name: text_classification # Unique name for the pipeline. This is optional field

# Arguments under global config are shared with all tasks. You can always override 
# values from global config using task specific config. For example if the task 
# accept version argument, but same is not specified in task config, then value 
# from global section will be picked. You can even refer gloabl argument in task 
# config using $argument_name. So below version argument, can be referred as $version 
# in task configs. 
global:
    version: '1'
    model_name: "{UPDATE}"
    result_path: "./results"

# Add arguments config for tasks which need to be executed as part of the pipeline.
# Tasks will be executed one by one in order specified here. You can refer arguments 
# from the task config in subsquent configs in pipeline using $_name.argument_name. 
# Each task expose some output values which will be resolved during runtime from 
# previously executed tasks, you can refer those in subsequent tasks using 
# $_name.output_name. Check generated task config template for more details on output 
# values.
tasks:
  - _name: task_export_dataset_text_classification_0
    task_name: task.export_dataset.text_classification  # type: str
    # help: Export multiple Intent Slot datasets as Single Text Classification dataset
    dataset_name: '{UPDATE}' # type: str
    # help: Name of the dataset, will be used for creating export directory
    version: '$version' # type: str
    # help: Version string for the exported dataset, will be used for creating export directory
    result_path: ./exported_datasets # type: str
    # help: Result directory where exported datasets to be stored
    domain_dataset_paths: "{UPDATE}"
    # type: list
    # help: List of NeMo format Intent Slot dataset paths, format `domain_name:dataset_path`
    labels_filename: dict.labels.csv # type: str
    # help: File name of storing labels for domain classifier

    # Output Param: task_export_dataset_text_classification_0.unique_result_path
    # type: str
    # help: Unique result directory for each dataset name and verison, will default to `{result_path}/{dataset_name}/{version}`

    # Output Param: task_export_dataset_text_classification_0.nemo_export_path
    # type: str
    # help: Directory path for exported Text Classification dataset in NEMO format

  - _name: task_train_text_classification_1
    task_name: task.train.text_classification  # type: str
    # help: Train Text Classification Model
    model_name: '$model_name' # type: str
    # help: Name of the model, will be used for creating result directory
    version: '$version' # type: str
    # help: Version string for the generated model, will be used for creating result directory
    platform: local # type: str
    # choices: ['local', 'ngc']
    # help: Platform for running model training, use ngc platfrom for training using NGC Batch
    result_path: $result_path # type: str
    # help: Base directory where resulting models to be stored, default to ./results
    gpus: 1 # type: int
    # help: Number of gpus to be used for training
    model_encryption_key: tlt_encode # type: str
    # help: Key which will be used for encrypting trained model
    training_precision: '16' # type: str
    # choices: ['16', '32']
    # help: Precision for model training, set to 16 for mixed precision training
    training_amp_level: O1 # type: str
    # choices: ['O0', 'O1', 'O2']
    # help: For mixed precision use O1 and O2 amp_level to enable the AMP
    gpu_memory: '32' # type: str
    # help: Optional flag for training on NGC platform, GPU memory required per instance for NGC Jobs. Check NGC Batch for available choices
    dataset_path: $task_export_dataset_text_classification_0.nemo_export_path # type: str
    # help: Directory path for nemo format Text Classification dataset
    epochs: 50 # type: int
    # help: Max number of epochs for training to be executed
    lr: 5e-05 # type: float
    # help: Learning rate to be used for training
    batch_size: 32 # type: int
    # help: Batch size used for training
    weight_decay: 0.0 # type: float
    # help: Weight decay to be used with optimizer during training
    pretrained_model_name: bert-base-uncased # type: str
    # help: Name of pretrained langauge model for training, recommended bert-base-uncased and distilbert-base-uncased
    max_seq_length: 128 # type: int
    # help: Maximum sequence length to be used for BERT/ DistilBERT models during training
    num_head_output_layers: 1 # type: int
    # choices: [1, 2]
    # help: Number of dense layers to be used for classifier head
    class_balancing: weighted_loss # type: str
    # choices: ['null', 'weighted_loss']
    # help: Use weighted_loss for using class weights for training loss, Recommended for imbalanced training datasets
    labels_filename: dict.labels.csv # type: str
    # help: File name of labels for domain classifier

    # Output Param: task_train_text_classification_1.unique_result_path
    # type: str
    # help: Unique result directory for each model name and verison, will default to `{result_path}/{model_name}/{version}`

    # Output Param: task_train_text_classification_1.train_logs_path
    # type: str
    # help: Directory which will store training checkpoints and logs, will default to `{unique_result_path}/train` 

    # Output Param: task_train_text_classification_1.tlt_model_path
    # type: str
    # help: Path for the saved trained TLT model (.tlt), will default to `{train_logs_path}/checkpoints/trained-model.tlt

  - _name: task_evaluate_text_classification_2
    task_name: task.evaluate.text_classification  # type: str
    # help: Evaluate Text Classification Model
    model_name: '$model_name' # type: str
    # help: Name of the model for evaluation, will be used for creating result directory
    version: '$version' # type: str
    # help: Version string for the model, will be used for creating result directory
    platform: local # type: str
    # choices: ['local', 'ngc']
    # help: Platform for running model evaluation
    tlt_model_path: $task_train_text_classification_1.tlt_model_path # type: str
    # help: File path for TLT model checkpoint to be used for evaluation
    result_path: $result_path # type: str
    # help: Base directory where resulting models and logs to be stored
    gpus: 1 # type: int
    # help: Number of gpus to be used for evaluation, Only integer value allowed
    batch_size: 32 # type: int
    # help: Batch size to be used for evaluation run
    model_encryption_key: tlt_encode # type: str
    # help: Encryption key used for models during training
    gpu_memory: '32' # type: str
    # help: Optional flag for evaluation on NGC platform, GPU memory required per instance for NGC Jobs. Check NGC Batch for available choices
    dataset_path: $task_export_dataset_text_classification_0.nemo_export_path # type: str
    # help: Directory path for Nemo format Text Classification dataset
    test_file_prefix: dev # type: str
    # help: File prefix used for test dataset files, Expected test files {prefix}.tsv

    # Output Param: task_evaluate_text_classification_2.unique_result_path
    # type: str
    # help: Unique result directory for each model name and verison, will default to `{result_path}/{model_name}/{version}`

    # Output Param: task_evaluate_text_classification_2.evaluate_logs_path
    # type: str
    # help: Directory which will store evaluation logs, will default to `{unique_result_path}/evaluate` 

  - _name: task_infer_text_classification_3
    task_name: task.infer.text_classification  # type: str
    # help: Inference Text Classification Model
    model_name: '$model_name' # type: str
    # help: Name of the model, will be used for creating result directory
    version: '$version' # type: str
    # help: Version string for the model, will be used for creating result directory
    tlt_model_path: $task_train_text_classification_1.tlt_model_path # type: str
    # help: File path for TLT model checkpoint (.tlt) to be used for Inference
    queries:
      - This is test query.
      - How is weather in Pune ?
    # type: list
    # help: Queries for Inference, List of strings or text files containing one query per line
    result_path: $result_path # type: str
    # help: Base Directory where resulting logs and inference results to be stored
    gpus: 1 # type: int
    # help: Number of gpus to be used for Inference
    model_encryption_key: tlt_encode # type: str
    # help: Encryption key used for models during training

    # Output Param: task_infer_text_classification_3.unique_result_path
    # type: str
    # help: Unique result directory for each model name and verison, will default to `{result_path}/{model_name}/{version}`

    # Output Param: task_infer_text_classification_3.infer_logs_path
    # type: str
    # help: Directory which will store infer logs, will default to `{unique_result_path}/infer` 

  - _name: task_export_model_RIVA_text_classification_4
    task_name: task.export_model.RIVA.text_classification  # type: str
    # help: Export Text Classification TLT model to RIVA format
    model_name: '$model_name' # type: str
    # help: Name of the model, will be used for creating export directory and RIVA filename
    version: '$version' # type: str
    # help: Version string for the exported model, will be used for creating export directory
    tlt_model_path: $task_train_text_classification_1.tlt_model_path # type: str
    # help: File path for trained TLT model checkpoint (.tlt)
    result_path: $result_path # type: str
    # help: Base directory where resulting models to be stored
    gpus: 1 # type: int
    # help: Number of gpus to be used during model export
    model_encryption_key: tlt_encode # type: str
    # help: Encryption key used for models during training

    # Output Param: task_export_model_RIVA_text_classification_4.unique_result_path
    # type: str
    # help: Unique result directory for each model name and verison, will default to `{result_path}/{model_name}/{version}`

    # Output Param: task_export_model_RIVA_text_classification_4.export_logs_path
    # type: str
    # help: Directory which will store export logs, will default to `{unique_result_path}/export` 

    # Output Param: task_export_model_RIVA_text_classification_4.riva_model_path
    # type: str
    # help: File path for exported RIVA model

  - _name: task_export_model_RMIR_text_classification_5
    task_name: task.export_model.RMIR.text_classification  # type: str
    # help: Export Text Classification RIVA model to RMIR format
    model_name: $model_name # type: str
    # help: Name of the model, will be used for creating export directory and RMIR filename. This will be used for setting domain_name in RMIR config wherever required
    version: $version # type: str
    # help: Version string for the exported model, will be used for creating export directory
    riva_model_path: $task_export_model_RIVA_text_classification_4.riva_model_path # type: str
    # help: File path for RIVA model checkpoint (.riva)
    result_path: $result_path # type: str
    # help: Base directory where resulting models to be stored
    gpus: '1' # type: str
    # help: GPUS to be used for model export, allowed formats - "3"[use total 3 gpus], "device=0,1"[use device 0 & 1], "all"[use all gpus]
    model_encryption_key: tlt_encode # type: str
    # help: Encryption key used for models during training
    batch_size: 8 # type: int
    # help: Batch size to be used during Inference in Triton Server
    max_seq_length: 128 # type: int
    # help: Maximum sequence length to be used during inference for BERT/ DistilBERT models

    # Output Param: task_export_model_RMIR_text_classification_5.unique_result_path
    # type: str
    # help: Unique result directory for each model name and verison, will default to `{result_path}/{model_name}/{version}`

    # Output Param: task_export_model_RMIR_text_classification_5.rmir_model_path
    # type: str
    # help: File path for the exported RMIR model
  
  - _name: task_upload_model_6
    task_name: task.upload.model  # type: str
    # help: Upload Model to NGC. Allowed formats TLT, RMIR, RIVA and TRT Model plans
    model_name: '$model_name' # type: str
    # help: Name of the model, will be used for creating unique NGC path
    version: '$version' # type: str
    # help: Version string for the model, will be used during NGC upload
    model_format: RMIR # type: str
    # choices: {'TLT', 'RMIR', 'RIVA', 'TRT_PLANS'}
    # help: Choose format of the model for the upload
    model_path: $task_export_model_RMIR_text_classification_5.rmir_model_path # type: str
    # help: File path or Directory containing the model which needs to be uploaded to NGC
    ngc_path: "" # type: str
    # help: NGC path where to upload the model. Format: ngc_org/[ngc_team/]model_name:version. If not provided, will be auto generated by following format
    remove_old_version: true # type: bool
    # help: Acknowledge removing older model with same version tag at the given ngc_path
    precision: FP16 # type: str
    # choices: ['FP16', 'FP32']
    # help: Precision the model was trained with, used while creating NGC model
    short_desc: NLP Model # type: str
    # help: Short description of the model, used while creating NGC model
````

## File: microservices/ace_agent/4.1/webui/client/public/env.js
````javascript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// This file gets automatically populated with relevant environment variables at startup
// time when running in production mode. See entrypoint.ts for more details.
````

## File: microservices/ace_agent/4.1/webui/client/src/components/App/index.css
````css
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 
.app {
  height: 100vh;
  display: flex;
  flex-direction: column;
  justify-content: space-between;
  align-items: stretch;
  --button-primary-bg: #76b900;
  --button-primary-bg-disabled: #9bc453;
  --button-primary-text: #ffffff;
  --secondary-text: #a3a3a3;
  --secondary-border: #cccccc;
  --user-chat-bubble-bg: #ececec;
  --active-audio-border-color: #76b900;
  --inactive-audio-border-color: #a3a3a3;
  --inactive-audio-text-color: #a3a3a3;
  --bot-volume-active-box-shadow-bg: rgba(18, 185, 0, 0.08);
}

.user-controls {
  display: flex;
  justify-content: center;
  align-items: center;
}

.conversation {
  flex: 2;
  overflow: auto;
  scrollbar-width: none;
  width: 420px;
  height: 100%;
  align-self: center;
  margin-bottom: 25px;
}

.user-controls-area {
  flex: 1;
  background: #ececec;
  display: flex;
  justify-content: center;
  flex-direction: column;
  align-items: center;
}

.user-controls {
  width: 420px;
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/App/index.tsx
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import "./index.css";
import useServerState from "../../utils/useServerState";
import { FormEvent, useState } from "react";
import ConversationHistory from "../ConversationHistory";
import { InteractionMode, MessageID } from "../../../../shared/types";
import AppHeader from "../AppHeader";
import UserSpeechInput from "../UserSpeechInput";
import UserTextInput from "../UserTextInput";
import useMicrophone from "../../utils/useMicrophone";
import useAudioPlayer from "../../utils/useAudioPlayer";
import BotFace from "../BotFace";
import {
  BOT_AUDIO_CONTEXT,
  USER_AUDIO_CONTEXT,
} from "../../utils/audio-contexts";
import useToastNotices from "../../utils/useToastNotices";
import ToastNotices from "../ToastNotices";

function App() {
  const audioPlayer = useAudioPlayer(BOT_AUDIO_CONTEXT);
  const toastNotices = useToastNotices();
  const onReceiveAudio = (chunk: Int16Array) => audioPlayer.play(chunk);
  const onSystemShutdown = (reason: string) =>
    toastNotices.addToast(reason, "fatal");
  const onWebSocketError = (e: Error) =>
    toastNotices.addToast(e.message, "fatal");
  const onMicrophoneWarning = (content: string) =>
    toastNotices.addToast(content, "warning");
  const onUserBargeIn = () => audioPlayer.interrupt();
  const server = useServerState(
    onReceiveAudio,
    onSystemShutdown,
    onWebSocketError,
    onUserBargeIn
  );
  const microphone = useMicrophone(
    server.sendUserAudio,
    onMicrophoneWarning,
    USER_AUDIO_CONTEXT
  );

  const [textQuery, setTextQuery] = useState<string>("");
  const [messageID, setMessageID] = useState<MessageID | null>(null);
  const [selectedBot, setSelectedBot] = useState<string | null>(null);
  const [interactionMode, setInteractionMode] = useState<InteractionMode>(
    InteractionMode.TEXT
  );

  function onSubmit(event: FormEvent<HTMLFormElement>) {
    event.preventDefault();
    server.sendChatMessage(messageID!, textQuery, selectedBot);
    setTextQuery("");

    // Reset the chat message ID, so that a new message ID is generated
    // when the user starts typing the next reply
    setMessageID(null);
  }

  function onChangeTextQuery(value: string) {
    const isNewMessage = messageID == null;
    const id =
      messageID ??
      Math.floor(Math.random() * Number.MAX_SAFE_INTEGER).toString();
    server.sendUserTyping(id, value, isNewMessage);
    setMessageID(id);
    setTextQuery(value);
  }

  function onChangeInteractionMode(mode: InteractionMode): void {
    if (mode === InteractionMode.SPEECH) {
      audioPlayer.enable();
      microphone.startRecording();
    } else {
      audioPlayer.disable();
      microphone.stopRecording();
    }
    server.toggleSpeech(mode);
    setInteractionMode(mode);
  }

  return (
    <div className="app">
      <AppHeader
        interactionMode={interactionMode}
        onChangeInteractionMode={onChangeInteractionMode}
        selectedBot={selectedBot}
        onChangeSelectedBot={setSelectedBot}
        botList={server.serverState.botList}
        isSpeechSupported={
          server.serverState.serverConfig?.speechSupported ?? false
        }
      />
      <div className="conversation">
        {interactionMode === InteractionMode.TEXT ? (
          <ConversationHistory
            messages={server.serverState.messages}
            isBotTyping={server.serverState.isBotTyping}
            selectedBot={selectedBot}
          />
        ) : (
          <BotFace
            emoji={server.serverState.latestBotEmoji}
            messages={server.serverState.messages}
            isBotTyping={server.serverState.isBotTyping}
            audioSource={audioPlayer.getSource()}
          />
        )}
      </div>
      <div className="user-controls-area">
        <div className="user-controls">
          {interactionMode === InteractionMode.TEXT ? (
            <UserTextInput
              onSubmit={onSubmit}
              textQuery={textQuery}
              onChangeTextQuery={onChangeTextQuery}
              connectionState={server.serverState.connectionState}
            />
          ) : (
            <UserSpeechInput
              micState={microphone.microphoneState}
              onEnableMic={microphone.startRecording}
              onDisableMic={microphone.stopRecording}
              audioSource={microphone.source}
            />
          )}
        </div>
      </div>
      <ToastNotices toasts={toastNotices.toasts} />
    </div>
  );
}

export default App;
````

## File: microservices/ace_agent/4.1/webui/client/src/components/AppHeader/index.css
````css
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 
.app-header {
  display: flex;
  justify-content: space-between;
  padding: 25px;
  align-self: stretch;
  align-items: center;
}

.app-title {
  margin: 0;
  padding: 0;
}

.app-controls {
  display: flex;
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/AppHeader/index.tsx
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { InteractionMode } from "../../../../shared/types";
import Toggle from "../Toggle";
import "./index.css";

interface Props {
  interactionMode: InteractionMode;
  onChangeInteractionMode: (mode: InteractionMode) => void;
  botList: string[];
  selectedBot: string | null;
  onChangeSelectedBot: (bot: string | null) => void;
  isSpeechSupported: boolean;
}
export default function AppHeader({
  interactionMode,
  onChangeInteractionMode,
  botList,
  selectedBot,
  onChangeSelectedBot,
  isSpeechSupported,
}: Props) {
  return (
    <div className="app-header">
      <h1 className="app-title">ACE Agent Bot Web UI</h1>
      <div className="app-controls">
        <Toggle
          options={botList.map((bot) => ({ value: bot }))}
          selectedOption={selectedBot}
          onChangeOption={onChangeSelectedBot}
        />

        <Toggle
          options={[
            {
              value: InteractionMode.SPEECH,
              disabled: !isSpeechSupported,
              disabledReason:
                "Speech mode is disabled. Run the server with --speech to enable speech",
            },
            { value: InteractionMode.TEXT },
          ]}
          selectedOption={interactionMode}
          onChangeOption={(option) =>
            onChangeInteractionMode(option as InteractionMode)
          }
        />
      </div>
    </div>
  );
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/BotFace/index.css
````css
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 
.bot-face {
  display: flex;
  flex-direction: column;
  justify-content: space-evenly;
  align-items: center;
  height: 100%;
}

.bot-face-emoji-container {
  display: flex;
  justify-content: center;
  align-items: center;
  flex: 1;
}

.bot-face-emoji {
  height: 200px;
  max-height: 200px;
  aspect-ratio: 1;
  font-family: "Noto Color Emoji", sans-serif;
  font-size: 50px;
  border: 4px solid var(--inactive-audio-border-color);
  border-radius: 999px;
  flex-shrink: 0;
  display: flex;
  justify-content: center;
  align-items: center;
}

.bot-face-history {
  display: flex;
  flex-direction: column;
  width: 100%;
  overflow-y: auto;
  flex: 1;
}

.bot-face-history::-webkit-scrollbar {
  display: none;
  scrollbar-width: none;
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/BotFace/index.tsx
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { useEffect, useRef } from "react";
import {
  BotChatMessage,
  ChatMessageContentType,
  ChatMessageTextContent,
  UserChatMessage,
} from "../../../../shared/types";
import useRealTimeVolume from "../../utils/useRealTimeVolume";
import useRequestAnimationFrame from "../../utils/useRequestAnimationFrame";
import ConversationMessage from "../ConversationMessage";
import Loading from "../Loading";
import "./index.css";

interface Props {
  emoji: string | null;
  isBotTyping: boolean;
  messages: (BotChatMessage | UserChatMessage)[];
  audioSource: AudioNode;
}

export default function BotFace({
  emoji,
  messages,
  isBotTyping,
  audioSource,
}: Props) {
  const bottom = useRef<HTMLDivElement>(null);
  const realTimeVolume = useRealTimeVolume(audioSource);
  useRequestAnimationFrame();

  useEffect(() => {
    bottom.current?.scrollIntoView();
  }, [messages]);

  const isBotActivelySpeaking = realTimeVolume !== 0;

  const styles: React.CSSProperties = {};
  if (isBotActivelySpeaking) {
    styles[
      "boxShadow"
    ] = `0 0 0px ${realTimeVolume}px var(--bot-volume-active-box-shadow-bg)`;
    styles["borderColor"] = "var(--active-audio-border-color)";
  }

  return (
    <div className="bot-face">
      <div className="bot-face-emoji-container">
        <div className="bot-face-emoji" style={styles}>
          {isBotTyping ? <Loading /> : emoji}
        </div>
      </div>
      <div className="bot-face-history">
        {messages
          .filter(
            (message) => message.content.type === ChatMessageContentType.TEXT
          )
          .map((message, i) => (
            <ConversationMessage
              key={i}
              authorType={message.author}
              messageContent={message.content as ChatMessageTextContent}
            />
          ))}
        <div ref={bottom}></div>
      </div>
    </div>
  );
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/ConversationHistory/index.css
````css
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 
 .conversation-history {
  display: flex;
  height: 100%;
  flex-direction: column;
  justify-content: center;
  padding: 22px 0;
}

.message {
  margin-bottom: 22px;
  max-width: 80%;
}

.message:not(.bot-emoji) {
  padding: 16px 22px;
  border-radius: 12px;
  position: relative;
}

.bot-text {
  background: #f3f3f3;
  align-self: flex-start;
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/ConversationHistory/index.tsx
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { useEffect, useRef } from "react";
import {
  BotChatMessage,
  ChatMessageContentType,
  UserChatMessage,
} from "../../../../shared/types";

import "./index.css";
import ConversationMessage from "../ConversationMessage";
import Loading from "../Loading";

interface Props {
  messages: (UserChatMessage | BotChatMessage)[];
  selectedBot: string | null;
  isBotTyping: boolean;
}

export default function ConversationHistory({
  messages,
  isBotTyping,
  selectedBot,
}: Props) {
  const bottom = useRef<HTMLDivElement>(null);

  useEffect(() => {
    bottom.current?.scrollIntoView();
  }, [messages, isBotTyping]);

  return (
    <div className="conversation-history">
      {messages.map((message, i) => renderMessage(message, selectedBot, i))}
      {isBotTyping && (
        <div className="message bot-text">
          <Loading />
        </div>
      )}
      <div ref={bottom} />
    </div>
  );
}

function renderMessage(
  message: UserChatMessage | BotChatMessage,
  selectedBot: string | null,
  index: number
) {
  if (
    message.content.type !== ChatMessageContentType.TEXT &&
    message.content.type !== ChatMessageContentType.EMOJI
  ) {
    return null;
  }
  if (selectedBot && message.content.botName !== selectedBot) {
    return null;
  }

  if (message.content)
    return (
      <ConversationMessage
        authorType={message.author}
        messageContent={message.content}
        key={index}
      />
    );
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/ConversationMessage/index.css
````css
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 
@import url("https://fonts.googleapis.com/css2?family=Noto+Color+Emoji&family=Noto+Sans:ital,wght@0,100..900;1,100..900&display=swap");

.message {
  margin-bottom: 22px;
  position: relative;
}

.message:not(.bot-emoji) {
  padding: 16px 22px;
  border-radius: 12px;
  position: relative;
}

.bot-text {
  background: #f3f3f3;
  align-self: flex-start;
}

.user-text {
  background: var(--user-chat-bubble-bg);
  align-self: flex-end;
}

.bot-emoji {
  font-family: "Noto Color Emoji", sans-serif;
  font-weight: 400;
  font-style: normal;
  font-size: 28pt;
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/ConversationMessage/index.tsx
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import "./index.css";

import {
  AuthorType,
  ChatMessageContentType,
  ChatMessageEmojiContent,
  ChatMessageTextContent,
} from "../../../../shared/types";

type Author = AuthorType.USER | AuthorType.BOT;
type MessageContent = ChatMessageTextContent | ChatMessageEmojiContent;

interface Props {
  authorType: Author;
  messageContent: MessageContent;
}

export default function ConversationMessage({
  authorType,
  messageContent,
}: Props) {
  return (
    <div
      className={`message ${getMessageClassName(authorType, messageContent)}`}
      title={getMessageTitle(messageContent) ?? undefined}
    >
      {getMessageContent(messageContent)}
    </div>
  );
}

function getMessageContent(messageContent: MessageContent): string {
  switch (messageContent.type) {
    case ChatMessageContentType.TEXT:
      return messageContent.text;
    case ChatMessageContentType.EMOJI:
      return messageContent.emoji;
  }
}

function getMessageClassName(
  authorType: Author,
  messageContent: MessageContent
): string {
  switch (authorType) {
    case AuthorType.USER:
      return "user-text";
    case AuthorType.BOT:
      return messageContent.type === ChatMessageContentType.TEXT
        ? "bot-text"
        : "bot-emoji";
  }
}

/**
 * Which title attribute to set to the message wrapper. Currently, this is only used for
 * emoji messages
 */
function getMessageTitle(messageContent: MessageContent): string | null {
  if (messageContent.type === ChatMessageContentType.EMOJI) {
    return messageContent.title;
  }
  return null;
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/Loading/index.css
````css
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 
 .bouncing-bullet {
  display: inline-block;
  width: 5px;
  height: 5px;
  border-radius: 100%;
  background: #ccc;
  animation: bounce 1s;
  transform: translateY(0px);
  animation-iteration-count: infinite;
  margin: 0 2px;
}

.bouncing-bullet:nth-child(n + 1) {
  animation-delay: 200ms;
}
.bouncing-bullet:nth-child(n + 2) {
  animation-delay: 400ms;
}
.bouncing-bullet:nth-child(n + 3) {
  animation-delay: 600ms;
}

@keyframes bounce {
  0% {
    transform: translateY(-3px);
    -webkit-animation-timing-function: ease-in;
  }
  20% {
    transform: translateY(3px);
    -webkit-animation-timing-function: ease-in;
  }
  100% {
    transform: translateY(-3px);
    -webkit-animation-timing-function: ease-in;
  }
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/Loading/index.tsx
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import "./index.css";

export default function Loading() {
  return (
    <>
      <span className="bouncing-bullet"></span>
      <span className="bouncing-bullet"></span>
      <span className="bouncing-bullet"></span>
    </>
  );
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/TextInput/index.css
````css
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 
.input {
  border: none;
  background: white;
  height: 2em;
  border-radius: 999px;
  padding: 4px 15px;
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/TextInput/index.tsx
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import "./index.css";

interface Props extends React.ComponentPropsWithoutRef<"input"> {}

export default function TextInput(props: Props) {
  const className = props.className + " input";
  return <input {...props} type="text" className={className} />;
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/ToastNotices/index.css
````css
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 
.toast-notices {
  position: fixed;
  bottom: 0;
  right: 0;
  width: 600px;
  max-width: 100%;
}

.toast-notice {
  background-color: white;
  box-shadow: 0 0 4px var(--secondary-border);
  padding: 12px;
  margin: 12px;
  border-left: 3px solid gray;
}

.toast-notice.fatal {
  border-left-color: red;
}

.toast-notice.warning {
  border-left-color: orange;
}

.toast-timestamp {
  display: block;
  color: var(--secondary-text);
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/ToastNotices/index.tsx
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import useRerender from "../../utils/useRerender";
import { ToastNotice } from "../../utils/useToastNotices";
import "./index.css";

interface Props {
  toasts: ToastNotice[];
}

export default function ToastNotices(props: Props) {
  // Rerender this component every 60 seconds, so that the "xx minutes ago" is re-calculated
  useRerender(60);
  return (
    <div className="toast-notices">
      {props.toasts.reverse().map((toast) => (
        <div className={`toast-notice ${toast.level}`}>
          {toast.content}
          <time className="toast-timestamp">
            {relativeTime(toast.timestamp)}
          </time>
        </div>
      ))}
    </div>
  );
}

function relativeTime(timestamp: number): string {
  const diff = Math.floor((timestamp - Date.now()) / (60 * 1000));
  if (diff === 0) {
    return "less than a minute ago";
  }
  const format = new Intl.RelativeTimeFormat("en", { style: "long" });
  return format.format(diff, "minute");
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/Toggle/index.css
````css
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 
.toggle {
  border-radius: 999px;
  border: 1px solid var(--secondary-border);
  display: flex;
  align-items: center;
  margin: 0 12px;
}

.pill {
  padding: 6px 18px;
  margin: 3px 0;
  background: none;
  border: none;
  cursor: pointer;
  font-size: 11pt;
  color: var(--secondary-text);
}

.pill:disabled {
  color: #e1e1e1;
}

.pill:first-child {
  margin-left: 3px;
}

.pill:not(:first-child) {
  margin-right: 3px;
}

.selected {
  color: var(--button-primary-text);
  background-color: var(--button-primary-bg);
  border-radius: 999px;
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/Toggle/index.tsx
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { useEffect } from "react";
import "./index.css";

interface Option {
  value: string;
  disabled?: boolean;
  disabledReason?: string;
}

interface Props {
  options: Option[];
  selectedOption: string | null;
  onChangeOption: (option: string | null) => void;
}
export default function Toggle({
  options,
  selectedOption,
  onChangeOption,
}: Props) {
  useEffect(() => {
    const optionValues = options.map((option) => option.value);
    if (!selectedOption || !optionValues.includes(selectedOption)) {
      onChangeOption(optionValues[0] ?? null);
    }
  }, [options]);

  if (options.length === 0) {
    return null;
  }

  function getPillClassName(pill: string): string {
    return "pill " + (pill === selectedOption ? "selected" : "");
  }

  return (
    <div className="toggle">
      {options.map((option) => (
        <button
          className={getPillClassName(option.value)}
          onClick={() => onChangeOption(option.value)}
          key={option.value}
          disabled={option.disabled ?? false}
          title={option.disabled ? option.disabledReason : ""}
        >
          {option.value}
        </button>
      ))}
    </div>
  );
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/UserSpeechInput/index.css
````css
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 
@import url("https://fonts.googleapis.com/css2?family=Noto+Color+Emoji&family=Noto+Sans:ital,wght@0,100..900;1,100..900&display=swap");
@import url("https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200");

.user-speech-input-area {
  width: 100%;
  display: flex;
  align-items: center;
  flex-direction: column;
}

.user-speech-input {
  width: 100%;
  margin-bottom: 25px;
}

.mic-button {
  height: 80px;
  width: 80px;
  color: var(--inactive-audio-text-color);
  border: 4px solid var(--inactive-audio-border-color);
  border-radius: 999px;
  cursor: pointer;
  font-weight: bolder;
  background-color: white;
}

.material-symbols-outlined {
  font-variation-settings: "FILL" 0, "wght" 700, "GRAD" 0, "opsz" 48;
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/UserSpeechInput/index.tsx
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { MicAccessState, MicrophoneState } from "../../utils/useMicrophone";
import useRealTimeVolume from "../../utils/useRealTimeVolume";
import useRequestAnimationFrame from "../../utils/useRequestAnimationFrame";
import TextInput from "../TextInput";
import Loading from "../Loading";
import "./index.css";

interface Props {
  micState: MicrophoneState;
  onEnableMic: () => void;
  onDisableMic: () => void;
  audioSource: AudioNode | null;
}

export default function UserSpeechInput({
  micState,
  onEnableMic,
  onDisableMic,
  audioSource,
}: Props) {
  const isError = micState.micAccessState === MicAccessState.ERROR;
  const isLoading = micState.micAccessState === MicAccessState.LOADING;
  const isMicEnabled = micState.isRecording;

  const realTimeVolume = useRealTimeVolume(audioSource, 5);
  useRequestAnimationFrame();

  const isUserActivelySpeaking = realTimeVolume !== 0;

  const styles: React.CSSProperties = {};
  if (isMicEnabled && isUserActivelySpeaking) {
    styles[
      "boxShadow"
    ] = `0 0 0px ${realTimeVolume}px var(--bot-volume-active-box-shadow-bg)`;
    styles["borderColor"] = "var(--active-audio-border-color)";
  }

  function onClickMic() {
    if (micState.isRecording) {
      onDisableMic();
    } else {
      onEnableMic();
    }
  }

  const [icon, title] = micIcon(isLoading, isError, isMicEnabled);

  return (
    <div className="user-speech-input-area">
      <button
        className="mic-button"
        onClick={onClickMic}
        disabled={isError}
        style={styles}
        title={title}
      >
        {icon}
      </button>
    </div>
  );
}

function micIcon(
  isLoading: boolean,
  isError: boolean,
  isMicEnabled: boolean
): [JSX.Element, string] {
  if (isLoading) {
    return [<Loading />, "accessing microphone..."];
  }
  if (isError) {
    return [
      <span className="material-symbols-outlined">priority_high</span>,
      "an error occurred while accessing the microphone",
    ];
  }
  if (isMicEnabled) {
    return [
      <span className="material-symbols-outlined">mic</span>,
      "The microphone is enabled. Click to mute",
    ];
  }
  return [
    <span className="material-symbols-outlined">mic_off</span>,
    "The microphone is disabled. Click to unmute",
  ];
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/UserTextInput/index.css
````css
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 
.user-controls-form {
  display: flex;
  width: 100%;
}

.user-input-field {
  flex: 1;
  margin-right: 10px;
}

.user-input-submit {
  background: var(--button-primary-bg);
  border: 0;
  color: white;
  padding: 0 12px;
  border-radius: 5px;
}

.user-input-submit:disabled {
  background: var(--button-primary-bg-disabled);
}
````

## File: microservices/ace_agent/4.1/webui/client/src/components/UserTextInput/index.tsx
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */


import "./index.css";
import { FormEventHandler } from "react";
import { ConnectionState } from "../../utils/useServerState";
import TextInput from "../TextInput";

interface Props {
  onSubmit: FormEventHandler<HTMLFormElement>;
  textQuery: string;
  onChangeTextQuery: (query: string) => void;
  connectionState: ConnectionState;
}

export default function UserTextInput({
  textQuery,
  onChangeTextQuery,
  onSubmit,
  connectionState,
}: Props) {
  const placeholder = getPlaceholder(connectionState);
  const isEnabled = getIsEnabled(connectionState);
  const isTextEmpty = textQuery.trim().length === 0;
  console.log(connectionState);
  return (
    <form className="user-controls-form" onSubmit={onSubmit}>
      <TextInput
        value={textQuery}
        placeholder={placeholder}
        onChange={(e) => onChangeTextQuery(e.target.value)}
        disabled={!isEnabled}
        className="user-input-field"
      />
      <input
        type="submit"
        className="user-input-submit"
        value="Send"
        disabled={!isEnabled || isTextEmpty}
      ></input>
    </form>
  );
}

function getPlaceholder(connectionState: ConnectionState): string {
  switch (connectionState) {
    case ConnectionState.INITIAL:
      return "Connecting to the server...";
    case ConnectionState.CLOSED:
      return "The connection to the server was closed. Please refresh the page";
    case ConnectionState.ERROR:
      return "An error occurred. Please refresh the page";
    case ConnectionState.READY:
      return "Type your reply";
  }
}

function getIsEnabled(connectionState: ConnectionState): boolean {
  switch (connectionState) {
    case ConnectionState.INITIAL:
    case ConnectionState.CLOSED:
    case ConnectionState.ERROR:
      return false;
    case ConnectionState.READY:
      return true;
  }
}
````

## File: microservices/ace_agent/4.1/webui/client/src/utils/audio-contexts.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

const USER_SPEECH_SAMPLE_RATE =
  parseInt(import.meta.env.USER_SPEECH_SAMPLE_RATE ?? "") || 16000;

export const BOT_AUDIO_CONTEXT = new AudioContext();
export const USER_AUDIO_CONTEXT = new AudioContext({
  sampleRate: USER_SPEECH_SAMPLE_RATE,
});
````

## File: microservices/ace_agent/4.1/webui/client/src/utils/linear-pcm-processor.worklet.js
````javascript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

class LinearPCMProcessor extends AudioWorkletProcessor {
  // The size of the buffer in miliseconds. An audio block is posted to the main thread
  // every time the buffer is full, which means a large buffer will emit less frequently
  // (higher latency), but more efficiently (fewer I/O interruptions between the worker
  // and the main thread)
  static BUFFER_SIZE_MS = 80;

  constructor() {
    super();
    // the "sampleRate" is available on the global scope in AudioWorklet files. The linter
    // doesn't know that, so it incorrectly raises an error when accessing it. The comment
    // below disables the linter for that specific line.
    // eslint-disable-next-line no-undef
    const rate = sampleRate;

    const bufferSize = (rate / 1000) * LinearPCMProcessor.BUFFER_SIZE_MS;
    this.buffer = new Int16Array(bufferSize);
    this.offset = 0;
  }

  /**
   * Converts input data from Float32Array to Int16Array, and stores it to
   * to the buffer. When the buffer is full, its content is posted to the main
   * thread, and the buffer is emptied
   */
  process(inputList, _outputList, _parameters) {
    // Assumes the input is mono (1 channel). If there are more channels, they
    // are ignored
    const input = inputList[0][0]; // first channel of first input

    for (let i = 0; i < input.length; i++) {
      const sample = Math.max(-1, Math.min(1, input[i]));
      this.buffer[i + this.offset] =
        sample < 0 ? sample * 0x8000 : sample * 0x7fff;
    }
    this.offset += input.length;

    // Once the buffer is filled entirely, flush the buffer
    if (this.offset >= this.buffer.length - 1) {
      this.flush();
    }
    return true;
  }

  /**
   * Sends the buffer's content to the main thread via postMessage(), and reset
   * the offset to 0
   */
  flush() {
    this.offset = 0;
    this.port.postMessage(this.buffer);
  }
}

registerProcessor("linear-pcm-processor", LinearPCMProcessor);
````

## File: microservices/ace_agent/4.1/webui/client/src/utils/useAudioPlayer.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { useRef } from "react";

const MIN_INT16_VALUE = -32768;
const MAX_INT16_VALUE = 32767;
const SAMPLE_RATE_HZ = 44100;
const AUDIO_BUFFER_LENGTH_SEC = 60;

class AudioPlayer {
  private enabled: boolean = false;
  private timerID: NodeJS.Timeout | null = null;
  private offset: number = 0;
  private currentAudioSequenceDuration: number = 0;
  private currentAudioSequenceStartedAt: number = 0;
  private audioBuffer: AudioBuffer;
  private source: AudioBufferSourceNode;
  private audioCtx: AudioContext;
  private timeUntilAudioCompleted: number = 0;

  constructor(audioCtx: AudioContext) {
    this.audioCtx = audioCtx;
    this.audioBuffer = this.createNewAudioBuffer();
    this.source = audioCtx.createBufferSource();
    this.source.buffer = this.audioBuffer;
    this.source.connect(audioCtx.destination);
  }
  enable(): void {
    this.enabled = true;
  }
  disable(): void {
    this.enabled = false;
  }

  play(buffer: Int16Array): void {
    if (!this.enabled) {
      return;
    }
    const channel = this.audioBuffer.getChannelData(0); // mono channel

    // We receive the data in unsigned 16-bit words. AudioBuffer must
    // be in 32-bit floats between -1.0 and 1.0. To convert, normalize
    // each sample
    for (let i = 0; i < buffer.length; i++) {
      if (buffer[i] > 0) {
        channel[i + this.offset] = buffer[i] / MAX_INT16_VALUE;
      } else {
        channel[i + this.offset] = buffer[i] / -MIN_INT16_VALUE;
      }
    }

    // If this is the first chunk of audio since the player was last reset, immediately
    // start playing the source. Additional chunks will be appended to the buffer as
    // they come
    if (this.offset === 0) {
      this.currentAudioSequenceStartedAt = performance.now();
      this.source.start();
    }
    this.offset += buffer.length;

    // We set a timer that will reset the audio buffer after the audio sequence has been
    // played. We cannot predetermine the duration of the audio sequence, because more
    // audio chunks may be added after the audio has started playing. For this reason,
    // every time a chunk is added to the buffer, we clear the existing timer, recompute
    // the duration of the audio sequence, and create a new timer with the appropriate
    // audio sequence duration.
    if (this.timerID) {
      clearTimeout(this.timerID);
    }
    const chunkDuration = buffer.length / SAMPLE_RATE_HZ;
    this.currentAudioSequenceDuration += chunkDuration;
    const audioEllapsed =
      (performance.now() - this.currentAudioSequenceStartedAt) / 1000;

    this.timeUntilAudioCompleted =
      this.currentAudioSequenceDuration - audioEllapsed;
    this.timerID = setTimeout(
      () => this.reset(),
      this.timeUntilAudioCompleted * 1000
    );
  }

  private createNewAudioBuffer(): AudioBuffer {
    return this.audioCtx.createBuffer(
      1,
      SAMPLE_RATE_HZ * AUDIO_BUFFER_LENGTH_SEC,
      SAMPLE_RATE_HZ
    );
  }

  private reset(): void {
    this.offset = 0;
    this.source.disconnect();
    this.audioBuffer = this.createNewAudioBuffer();
    this.currentAudioSequenceDuration = 0;
    this.currentAudioSequenceStartedAt = 0;
    this.timeUntilAudioCompleted = 0;
    this.source = this.audioCtx.createBufferSource();
    this.source.buffer = this.audioBuffer;
    this.source.connect(this.audioCtx.destination);
  }

  public getSource(): AudioBufferSourceNode {
    return this.source;
  }

  // Immediately stops playing audio. Audio left in the buffer is erased
  public interrupt(): void {
    if (this.timerID) {
      clearTimeout(this.timerID);
    }
    this.reset();
  }
}

export default function useAudioPlayer(audioCtx: AudioContext): AudioPlayer {
  const audioPlayerRef = useRef<AudioPlayer>(new AudioPlayer(audioCtx));
  return audioPlayerRef.current;
}
````

## File: microservices/ace_agent/4.1/webui/client/src/utils/useMicrophone.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { useReducer, useRef } from "react";
import workletUrl from "./linear-pcm-processor.worklet.js?url";

export interface MicrophoneState {
  micAccessState: MicAccessState;
  error: Error | null;
  isRecording: boolean;
}

enum MicrophoneActionType {
  MIC_ACCESS_REQUESTED = "MIC_ACCESS_REQUESTED",
  MIC_ACCESS_GRANTED = "MIC_ACCESS_GRANTED",
  MIC_ACCESS_ERROR = "MIC_ACCESS_ERROR",
  RECORDING_STARTED = "RECORDING_STARTED",
  RECORDING_STOPPED = "RECORDING_STOPPED",
}

export enum MicAccessState {
  INITIAL = "INITIAL",
  LOADING = "LOADING",
  GRANTED = "GRANTED",
  ERROR = "ERROR",
}

interface MicrophoneStateActionMicAccessRequested {
  type: MicrophoneActionType.MIC_ACCESS_REQUESTED;
}

interface MicrophoneStateActionMicAccessGranted {
  type: MicrophoneActionType.MIC_ACCESS_GRANTED;
}

interface MicrophoneStateActionMicAccessError {
  type: MicrophoneActionType.MIC_ACCESS_ERROR;
  payload: Error;
}
interface MicrophoneStateActionRecordingStarted {
  type: MicrophoneActionType.RECORDING_STARTED;
}

interface MicrophoneStateActionRecordingStopped {
  type: MicrophoneActionType.RECORDING_STOPPED;
}

type MicrophoneStateAction =
  | MicrophoneStateActionMicAccessRequested
  | MicrophoneStateActionMicAccessGranted
  | MicrophoneStateActionMicAccessError
  | MicrophoneStateActionRecordingStarted
  | MicrophoneStateActionRecordingStopped;

function reducer(
  state: MicrophoneState,
  action: MicrophoneStateAction
): MicrophoneState {
  switch (action.type) {
    case MicrophoneActionType.MIC_ACCESS_REQUESTED:
      return { ...state, micAccessState: MicAccessState.LOADING };
    case MicrophoneActionType.MIC_ACCESS_GRANTED:
      return {
        ...state,
        micAccessState: MicAccessState.GRANTED,
      };
    case MicrophoneActionType.MIC_ACCESS_ERROR:
      return {
        ...state,
        micAccessState: MicAccessState.ERROR,
        error: action.payload,
      };
    case MicrophoneActionType.RECORDING_STARTED:
      return {
        ...state,
        isRecording: true,
      };
    case MicrophoneActionType.RECORDING_STOPPED:
      return {
        ...state,
        isRecording: false,
      };
    default:
      return state;
  }
}

const INITIAL_STATE: MicrophoneState = {
  micAccessState: MicAccessState.INITIAL,
  error: null,
  isRecording: false,
};

export default function useMicrophone(
  onAudioChunkAvailable: (buffer: ArrayBuffer) => void,
  onWarning: (content: string) => void,
  audioCtx: AudioContext
): {
  microphoneState: MicrophoneState;
  startRecording: () => Promise<void>;
  stopRecording: () => void;
  source: AudioNode | null;
} {
  const [microphoneState, dispatch] = useReducer(reducer, INITIAL_STATE);
  const audioSourceRef = useRef<AudioNode>();

  async function requestAccess() {
    dispatch({
      type: MicrophoneActionType.MIC_ACCESS_REQUESTED,
    });
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: { deviceId: "default" },
      });
      const allDevices = await navigator.mediaDevices.enumerateDevices();
      const deviceID = stream.getAudioTracks()[0].getSettings().deviceId;
      const selectedDevice = allDevices.find(
        (device) => device.deviceId === deviceID
      );

      const selectedDeviceLabel = selectedDevice?.label.toLowerCase();
      if (
        selectedDeviceLabel?.includes("bluetooth") ||
        selectedDeviceLabel?.includes("wireless")
      ) {
        onWarning(
          `It looks like you might be using a bluetooth microphone or headset (name: ${selectedDeviceLabel}). If you experience low-quality speech recognition, consider switching to a wired microphone or headset in your system's settings`
        );
      }

      audioSourceRef.current = audioCtx.createMediaStreamSource(stream);
      await audioCtx.audioWorklet.addModule(workletUrl);
      const audioWorkletNode = new AudioWorkletNode(
        audioCtx,
        "linear-pcm-processor"
      );
      audioSourceRef.current.connect(audioWorkletNode);
      audioWorkletNode.connect(audioCtx.destination);
      audioWorkletNode.port.onmessage = (e: MessageEvent<ArrayBuffer>) => {
        onAudioChunkAvailable(e.data);
      };
      dispatch({
        type: MicrophoneActionType.MIC_ACCESS_GRANTED,
      });
    } catch (e) {
      console.error(e);
      dispatch({
        type: MicrophoneActionType.MIC_ACCESS_ERROR,
        payload: e as Error,
      });
    }
  }

  async function startRecording() {
    if (microphoneState.micAccessState !== MicAccessState.GRANTED) {
      await requestAccess();
    }
    console.log("resuming...");
    audioCtx.resume();
    dispatch({
      type: MicrophoneActionType.RECORDING_STARTED,
    });
  }

  function stopRecording() {
    audioCtx.suspend();
    dispatch({
      type: MicrophoneActionType.RECORDING_STOPPED,
    });
  }

  return {
    microphoneState,
    startRecording,
    stopRecording,
    source: audioSourceRef.current ?? null,
  };
}
````

## File: microservices/ace_agent/4.1/webui/client/src/utils/useRealTimeVolume.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { useEffect, useRef } from "react";
const FFT_SIZE = 256;
const dataArray = new Float32Array(FFT_SIZE / 2);

export default function useRealTimeVolume(
  source: AudioNode | null,
  threshold: number = 0
): number {
  const analyzerRef = useRef<AnalyserNode>();

  useEffect(() => {
    console.log("New source! Creating new analyzer");
    if (source) {
      analyzerRef.current = source.context.createAnalyser();
      analyzerRef.current.fftSize = FFT_SIZE;
      source.connect(analyzerRef.current);
    }
  }, [source]);

  const analyzer = analyzerRef.current;
  if (!analyzer) {
    return 0;
  }
  analyzer.getFloatFrequencyData(dataArray);
  if (Number.isFinite(dataArray[10])) {
    const volume = Math.max(0, dataArray[10] + 120) / 5;
    return volume > threshold ? volume : 0;
  }
  return 0;
}
````

## File: microservices/ace_agent/4.1/webui/client/src/utils/useRequestAnimationFrame.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { useEffect, useRef, useState } from "react";

export default function useRequestAnimationFrame(): void {
  const [_, setCurrentFrame] = useState<number>();
  const animationFrameRequestRef = useRef<number>();

  function onAnimationFrameAvailable(e: number) {
    setCurrentFrame(e);
    animationFrameRequestRef.current = requestAnimationFrame(
      onAnimationFrameAvailable
    );
  }

  useEffect(() => {
    animationFrameRequestRef.current = requestAnimationFrame(
      onAnimationFrameAvailable
    );
    return () => cancelAnimationFrame(animationFrameRequestRef.current!);
  }, []);
}
````

## File: microservices/ace_agent/4.1/webui/client/src/utils/useRerender.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { useState, useEffect } from "react";

/**
 * Utility custom React hook that forces a re-renders for every time interval
 * @param seconds
 * @returns
 */
export default function useRerender(seconds: number) {
  const [tick, setTick] = useState(0);

  useEffect(() => {
    const interval = setInterval(() => {
      setTick((prevTick) => prevTick + 1);
    }, seconds * 1000);

    return () => clearInterval(interval);
  }, [seconds]);

  return tick;
}
````

## File: microservices/ace_agent/4.1/webui/client/src/utils/useServerState.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { useEffect, useReducer, useRef } from "react";

import {
  AuthorType,
  type BotChatMessage,
  type UserChatMessage,
  ChatMessageContentType,
  MessageID,
  BotChatTextMessage,
  BotChatEmojiMessage,
  UserChatTextMessage,
  SystemConfigMessage,
  UserChatToggleSpeechMessage,
  InteractionMode,
  ServerConfig,
  SystemMessageContent,
  ChatMessageTextContent,
} from "../../../shared/types";

interface ServerState {
  messages: (BotChatMessage | UserChatMessage)[];
  isBotTyping: boolean;
  latestBotEmoji: string | null;
  serverConfig: ServerConfig | null;
  botList: string[];
  connectionState: ConnectionState;
}

enum ServerStateActionType {
  CONNECTION_LOADING = "CONNECTION_LOADING",
  RECEIVED_BOT_TEXT_MESSAGE = "RECEIVED_BOT_TEXT_MESSAGE",
  RECEIVED_ASR = "RECEIVED_ASR",
  RECEIVED_BOT_EMOJI = "RECEIVED_BOT_EMOJI",
  RECEIVED_BOT_IS_TYPING = "RECEIVED_BOT_IS_TYPING",
  CONNECTION_READY = "CONNECTION_READY",
  CONNECTION_ERROR = "CONNECTION_ERROR",
  CONNECTION_CLOSED = "CONNECTION_CLOSED",
  SENT_USER_CHAT_MESSAGE = "SENT_USER_CHAT_MESSAGE",
  RECEIVED_SYSTEM_CONFIG_CHANGE = "RECEIVED_SYSTEM_CONFIG_CHANGE",
  RECEIVED_BOT_LIST = "RECEIVED_BOT_LIST",
}

export enum ConnectionState {
  INITIAL = "INITIAL",
  READY = "READY",
  ERROR = "ERROR",
  CLOSED = "CLOSED",
}

interface ServerStateActionConnectionLoading {
  type: ServerStateActionType.CONNECTION_LOADING;
}

interface ServerStateActionReceivedServerPush {
  type: ServerStateActionType.RECEIVED_BOT_TEXT_MESSAGE;
  payload: BotChatTextMessage;
}

interface ServerStateActionReceivedASR {
  type: ServerStateActionType.RECEIVED_ASR;
  payload: {
    text: string;
    messageID: string;
  };
}

interface ServerStateActionReceivedBotEmoji {
  type: ServerStateActionType.RECEIVED_BOT_EMOJI;
  payload: BotChatEmojiMessage;
  emoji: string;
}

interface ServerStateActionReceivedBotIsTyping {
  type: ServerStateActionType.RECEIVED_BOT_IS_TYPING;
}

interface ServerStateActionConnectionReady {
  type: ServerStateActionType.CONNECTION_READY;
}

interface ServerStateActionConnectionError {
  type: ServerStateActionType.CONNECTION_ERROR;
  payload: Error;
}

interface ServerStateActionConnectionClosed {
  type: ServerStateActionType.CONNECTION_CLOSED;
  payload: string;
}

interface ServerStateActionSentChatMessage {
  type: ServerStateActionType.SENT_USER_CHAT_MESSAGE;
  payload: UserChatTextMessage;
}

interface ServerStateActionReceivedSystemMessageMessage {
  type: ServerStateActionType.RECEIVED_SYSTEM_CONFIG_CHANGE;
  payload: ServerConfig;
}

interface ServerStateActionReceivedBotList {
  type: ServerStateActionType.RECEIVED_BOT_LIST;
  payload: string[];
}

type ServerStateAction =
  | ServerStateActionConnectionLoading
  | ServerStateActionReceivedServerPush
  | ServerStateActionReceivedASR
  | ServerStateActionReceivedBotEmoji
  | ServerStateActionReceivedBotIsTyping
  | ServerStateActionConnectionReady
  | ServerStateActionConnectionError
  | ServerStateActionConnectionClosed
  | ServerStateActionSentChatMessage
  | ServerStateActionReceivedSystemMessageMessage
  | ServerStateActionReceivedBotList;

function reducer(state: ServerState, action: ServerStateAction): ServerState {
  switch (action.type) {
    case ServerStateActionType.CONNECTION_LOADING:
      return { ...state, connectionState: ConnectionState.INITIAL };
    case ServerStateActionType.CONNECTION_READY:
      return { ...state, connectionState: ConnectionState.READY };
    case ServerStateActionType.CONNECTION_ERROR:
      return { ...state, connectionState: ConnectionState.ERROR };
    case ServerStateActionType.CONNECTION_CLOSED:
      return { ...state, connectionState: ConnectionState.CLOSED };
    case ServerStateActionType.RECEIVED_BOT_TEXT_MESSAGE:
      return {
        ...state,
        messages: [...state.messages, action.payload],
        isBotTyping: false,
      };
    case ServerStateActionType.RECEIVED_BOT_EMOJI:
      return {
        ...state,
        messages: [...state.messages, action.payload],
        latestBotEmoji: action.payload.content.emoji,
        isBotTyping: false,
      };
    case ServerStateActionType.RECEIVED_BOT_IS_TYPING:
      return {
        ...state,
        isBotTyping: true,
      };
    case ServerStateActionType.RECEIVED_ASR:
      // When receiving ASR data, check if a message with the message ID has
      // already been created. If it exists, update it directly. Otherwise,
      // create a new message
      const payload = action.payload;
      const message = state.messages.find(
        (message) =>
          message.author === AuthorType.USER &&
          message.content.type === ChatMessageContentType.TEXT &&
          message.content.messageID === payload.messageID
      ) as UserChatMessage | undefined;

      // MessageID matches an existing ID, update it in-line
      if (message) {
        (message.content as ChatMessageTextContent).text = payload.text;
        return state;
      }

      // MessageID doesn't match an existing message, create a new message
      return {
        ...state,
        messages: [
          ...state.messages,
          {
            author: AuthorType.USER,
            content: {
              type: ChatMessageContentType.TEXT,
              messageID: payload.messageID,
              text: payload.text,
              botName: null,
            },
          },
        ],
      };
    case ServerStateActionType.SENT_USER_CHAT_MESSAGE:
      return {
        ...state,
        messages: [...state.messages, action.payload],
      };
    case ServerStateActionType.RECEIVED_SYSTEM_CONFIG_CHANGE:
      return {
        ...state,
        serverConfig: action.payload,
      };
    case ServerStateActionType.RECEIVED_BOT_LIST:
      return {
        ...state,
        botList: action.payload,
      };
    default:
      return state;
  }
}

const INITIAL_STATE: ServerState = {
  messages: [],
  connectionState: ConnectionState.INITIAL,
  isBotTyping: false,
  latestBotEmoji: "",
  serverConfig: null,
  botList: [],
};

function getServerPort() {
  // In production, the port for the webserver is only known at startup time. Since the
  // web app is built statically, it cannot rely on import.meta.env.VITE_SERVER_PORT to
  // get the webserver's port, because this variable is populated at build time.
  // Instead, a process.env.VITE_SERVER_PORT variable is dynamically injected into the
  // webpage when it's run in production.

  const DEFAULT_SERVER_PORT = 7007;

  if (import.meta.env.PROD) {
    return window.process?.env?.VITE_SERVER_PORT || DEFAULT_SERVER_PORT;
  }

  return import.meta.env.VITE_SERVER_PORT || DEFAULT_SERVER_PORT;
}

function createWebSocket(
  dispatch: React.Dispatch<ServerStateAction>,
  onReceiveBotAudio: (chunk: Int16Array) => void,
  onSystemShutdown: (reason: string) => void,
  onWebSocketError: (e: Error) => void,
  onUserBargeIn: () => void
): WebSocket {
  const hostname = location.hostname;
  const serverPort = getServerPort();
  const serverProtocol =
    window.location.protocol === "https:" ? "wss://" : "ws://";
  const webSocketURL = `${serverProtocol}${hostname}:${serverPort}`;
  const socket = new WebSocket(webSocketURL);
  dispatch({
    type: ServerStateActionType.CONNECTION_LOADING,
  });

  socket.addEventListener("open", () => {
    dispatch({ type: ServerStateActionType.CONNECTION_READY });
  });
  socket.addEventListener("error", () =>
    onWebSocketError(
      new Error(
        `Error establishing a connection with the web server at ${webSocketURL}. Is the server running, and are you forwarding port ${serverPort}?`
      )
    )
  );

  socket.addEventListener("message", (event) => {
    if (event.data instanceof ArrayBuffer) {
      onReceiveBotAudio(new Int16Array(event.data));
      return;
    }
    const message = JSON.parse(event.data) as
      | BotChatMessage
      | SystemConfigMessage;
    if (message.author === AuthorType.SYSTEM) {
      if (message.content.type === SystemMessageContent.CONFIG_CHANGE) {
        dispatch({
          type: ServerStateActionType.RECEIVED_SYSTEM_CONFIG_CHANGE,
          payload: message.content,
        });
      }
      if (message.content.type === SystemMessageContent.SHUTDOWN) {
        onSystemShutdown(message.content.reason);
      }
      return;
    }
    if (message.content.type === ChatMessageContentType.ASR) {
      const asr = message.content;
      dispatch({
        type: ServerStateActionType.RECEIVED_ASR,
        payload: {
          text: asr.transcript,
          messageID: asr.messageID,
        },
      });
      return;
    }
    if (message.content.type === ChatMessageContentType.EMOJI) {
      dispatch({
        type: ServerStateActionType.RECEIVED_BOT_EMOJI,
        payload: message as BotChatEmojiMessage,
        emoji: message.content.emoji,
      });
      return;
    }
    if (message.content.type === ChatMessageContentType.TYPING) {
      dispatch({
        type: ServerStateActionType.RECEIVED_BOT_IS_TYPING,
      });
      return;
    }
    if (message.content.type === ChatMessageContentType.BOT_LIST) {
      dispatch({
        type: ServerStateActionType.RECEIVED_BOT_LIST,
        payload: message.content.botList,
      });
      return;
    }
    if (message.content.type === ChatMessageContentType.USER_BARGE_IN) {
      onUserBargeIn();
      return;
    }

    dispatch({
      type: ServerStateActionType.RECEIVED_BOT_TEXT_MESSAGE,
      payload: message as BotChatTextMessage,
    });
  });

  socket.addEventListener("error", (event) => {
    dispatch({
      type: ServerStateActionType.CONNECTION_ERROR,
      payload: new Error(event.type),
    });
  });

  socket.addEventListener("close", (event) => {
    if (event.code === 1000) {
      dispatch({
        type: ServerStateActionType.CONNECTION_CLOSED,
        payload: event.reason,
      });
    } else {
      dispatch({
        type: ServerStateActionType.CONNECTION_ERROR,
        payload: new Error(event.reason),
      });
    }
  });

  window.addEventListener("beforeunload", () => {
    socket.close();
  });

  return socket;
}

export default function useServerState(
  onReceiveBotAudio: (chunk: Int16Array) => void,
  onSystemShutdown: (reason: string) => void,
  onWebsocketError: (e: Error) => void,
  onUserBargeIn: () => void
): {
  serverState: ServerState;
  sendChatMessage: (
    messageID: MessageID,
    content: string,
    botName: string | null
  ) => void;
  sendUserTyping: (
    messageID: string,
    text: string,
    isNewMessage: boolean
  ) => void;
  sendUserAudio: (buffer: ArrayBuffer) => void;
  toggleSpeech: (interactionMode: InteractionMode) => void;
} {
  const [serverState, dispatch] = useReducer(reducer, INITIAL_STATE);

  const socketRef = useRef<WebSocket>();

  useEffect(() => {
    const socket = createWebSocket(
      dispatch,
      onReceiveBotAudio,
      onSystemShutdown,
      onWebsocketError,
      onUserBargeIn
    );
    socketRef.current = socket;
    socket.binaryType = "arraybuffer";

    return () => socket.close(1000);
  }, []);

  function sendChatMessage(
    messageID: MessageID,
    text: string,
    botName: string | null
  ): void {
    const message: UserChatTextMessage = {
      author: AuthorType.USER,
      content: {
        type: ChatMessageContentType.TEXT,
        messageID,
        text,
        botName,
      },
    };
    socketRef?.current?.send(JSON.stringify(message));
    dispatch({
      type: ServerStateActionType.SENT_USER_CHAT_MESSAGE,
      payload: message,
    });
  }

  function sendUserTyping(
    messageID: MessageID,
    text: string,
    isNewMessage: boolean
  ): void {
    const payload: UserChatMessage = {
      author: AuthorType.USER,
      content: {
        type: ChatMessageContentType.TYPING,
        messageID,
        text,
        isNewMessage,
      },
    };
    socketRef?.current?.send(JSON.stringify(payload));
  }

  function sendUserAudio(buffer: ArrayBuffer): void {
    socketRef?.current?.send(buffer);
  }

  function toggleSpeech(interactionMode: InteractionMode): void {
    const payload: UserChatToggleSpeechMessage = {
      author: AuthorType.USER,
      content: {
        type: ChatMessageContentType.TOGGLE_SPEECH,
        interactionMode,
      },
    };
    socketRef.current?.send(JSON.stringify(payload));
  }

  return {
    serverState,
    sendChatMessage,
    sendUserTyping,
    sendUserAudio,
    toggleSpeech,
  };
}
````

## File: microservices/ace_agent/4.1/webui/client/src/utils/useToastNotices.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { useState } from "react";

type ToastContent = string;

export type ToastNotice = {
  content: ToastContent;
  timestamp: number;
  level: ToastNoticeLevel;
};

export type ToastNoticeLevel = "fatal" | "warning";

/**
 * A custom React hook to manage "toast" notices. These notices appear in the web UI as
 * popup messages. The toasts must be rendered by the <ToastNotices /> component
 * @returns
 */
export default function useToastNotices(): {
  toasts: ToastNotice[];
  addToast: (newToast: ToastContent, level: ToastNoticeLevel) => void;
} {
  const [toasts, setToasts] = useState<ToastNotice[]>([]);

  function addToast(newToastContent: ToastContent, level: ToastNoticeLevel) {
    const newToast = {
      content: newToastContent,
      timestamp: Date.now(),
      level,
    };
    setToasts([...toasts, newToast]);
  }
  return { toasts, addToast };
}
````

## File: microservices/ace_agent/4.1/webui/client/src/index.css
````css
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 
 @import url("https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,100..900;1,100..900&display=swap");

:root,
body {
  margin: 0;
  font-family: "Noto Sans", sans-serif;
  font-optical-sizing: auto;
  font-weight: 400;
  font-style: normal;
  font-size: 10pt;
  font-variation-settings: "wdth" 100;
}
````

## File: microservices/ace_agent/4.1/webui/client/src/main.tsx
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import ReactDOM from "react-dom/client";
import App from "./components/App/index.tsx";
import "./index.css";

ReactDOM.createRoot(document.getElementById("root")!).render(<App />);
````

## File: microservices/ace_agent/4.1/webui/client/src/vite-env.d.ts
````typescript
/// <reference types="vite/client" />
````

## File: microservices/ace_agent/4.1/webui/client/.eslintrc.cjs
````
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

module.exports = {
  root: true,
  env: { browser: true, es2020: true },
  extends: [
    'eslint:recommended',
    'plugin:@typescript-eslint/recommended',
    'plugin:react-hooks/recommended',
  ],
  ignorePatterns: ['dist', '.eslintrc.cjs'],
  parser: '@typescript-eslint/parser',
  plugins: ['react-refresh'],
  rules: {
    'react-refresh/only-export-components': [
      'warn',
      { allowConstantExport: true },
    ],
  },
}
````

## File: microservices/ace_agent/4.1/webui/client/.yarnrc.yml
````yaml
nodeLinker: node-modules
````

## File: microservices/ace_agent/4.1/webui/client/entrypoint.js
````javascript
#!/usr/bin/env node

/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

const fs = require('fs');
const { execFileSync } = require('child_process');


// We use the env.js file to populate environment variables that are only available at
// startup time. In practice, this is the case for the server's port number, which is
// available in the VITE_SERVER_PORT environment variable. We write this environment
// variable to the env.js file so that the client can access it.
try {

    const VITE_SERVER_PORT = process.env.VITE_SERVER_PORT ? parseInt(process.env.VITE_SERVER_PORT, 10) : undefined;

    const content = `window.process = window.process ? window.process : {};
    window.process.env = window.process.env ? window.process.env : {};
    
    window.process.env.VITE_SERVER_PORT = ${JSON.stringify(VITE_SERVER_PORT)};`;

    fs.writeFileSync('/app/client/dist/env.js', content);
    console.log('env.js file has been created successfully.');
} catch (err) {
    console.error('Error writing to file:', err);
    process.exit(1);
}

// Serve the web client statically
try {
    console.log('Starting http-server...');
    const args = process.argv.slice(2);
    execFileSync('node', ['../node_modules/http-server/bin/http-server', '-p', '7006', ...args], { stdio: 'inherit' });
} catch (error) {
    console.error('Error starting http-server:', error);
    process.exit(1);
}
````

## File: microservices/ace_agent/4.1/webui/client/index.html
````html
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ACE Agent Bot Web UI</title>
    <script src="/env.js"></script>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>
````

## File: microservices/ace_agent/4.1/webui/client/package.json
````json
{
  "name": "ace-agent-bot-ui-client",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "lint": "eslint . --ext ts,tsx --report-unused-disable-directives --max-warnings 0",
    "preview": "vite preview"
  },
  "dependencies": {
    "http-server": "^14.1.1",
    "react": "^18.2.0",
    "react-dom": "^18.2.0"
  },
  "devDependencies": {
    "@types/node": "^20.12.7",
    "@types/react": "^18.2.56",
    "@types/react-dom": "^18.2.19",
    "@typescript-eslint/eslint-plugin": "^7.0.2",
    "@typescript-eslint/parser": "^7.0.2",
    "@vitejs/plugin-react": "^4.2.1",
    "eslint": "^8.56.0",
    "eslint-plugin-react-hooks": "^4.6.0",
    "eslint-plugin-react-refresh": "^0.4.5",
    "typescript": "^5.2.2",
    "vite": "^5.1.4"
  },
  "packageManager": "yarn@4.1.1"
}
````

## File: microservices/ace_agent/4.1/webui/client/tsconfig.json
````json
{
  "compilerOptions": {
    "target": "ES2020",
    "useDefineForClassFields": true,
    "lib": ["ES2020", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": true,
    "jsx": "react-jsx",

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noFallthroughCasesInSwitch": true
  },
  "include": ["src"],
  "references": [{ "path": "./tsconfig.node.json" }]
}
````

## File: microservices/ace_agent/4.1/webui/client/tsconfig.node.json
````json
{
  "compilerOptions": {
    "composite": true,
    "skipLibCheck": true,
    "module": "ESNext",
    "moduleResolution": "bundler",
    "allowSyntheticDefaultImports": true,
    "strict": true
  },
  "include": ["vite.config.ts"]
}
````

## File: microservices/ace_agent/4.1/webui/client/vite.config.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { defineConfig, loadEnv } from "vite";
import react from "@vitejs/plugin-react";

// https://vitejs.dev/config/
export default defineConfig(({ mode }) => {
  return {
    plugins: [react()],
    server: {
      host: true,
      port: 7006,
    },
    clearScreen: false,
    define: {
      global: {},
      "import.meta.env.VITE_SERVER_PORT": process.env.VITE_SERVER_PORT,
    },
    envDir: "..",
  };
});
````

## File: microservices/ace_agent/4.1/webui/server/chat-session/clients/GRPCClient.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { GRPC_URL } from "../../config.js";
import { PromiseClient, createPromiseClient } from "@connectrpc/connect";
import { createGrpcTransport } from "@connectrpc/connect-node";
import { AceAgentGrpc } from "../../grpc/gen/ace_agent_connect.js";

type GRPCClient = PromiseClient<typeof AceAgentGrpc>;

/**
 * Provides a gRPC client to a specified GRPC_URL, which must be provided as an
 * enviornment variable. The object is a singleton, meaning that at most one client will
 * be created, and the same client will be returned to each `.get()` call
 */
class GRPCClientSingleton {
  private client: GRPCClient = null;

  get(): GRPCClient {
    if (this.client) {
      return this.client;
    }
    this.client = createPromiseClient(
      AceAgentGrpc,
      createGrpcTransport({
        baseUrl: GRPC_URL,
        httpVersion: "2",
      })
    );
    return this.client;
  }

  isAvailable(): boolean {
    return !!GRPC_URL;
  }
}

export default new GRPCClientSingleton();
````

## File: microservices/ace_agent/4.1/webui/server/chat-session/clients/HTTPClient.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { HTTP_CHAT_URL } from "../../config.js";

/**
 * Provides an HTTP client to a domain URL that must be provided through the HTTP_CHAT_URL
 * environment variable. The client is a think wrapper around the native `fetch()` API,
 * that only pre-sets the domain name for convenience
 *
 * Example:
 *  // HTTP_CHAT_URL = http://localhost:8080
 * await fetch('/chat') // sends a request to http://localhost:8080/chat
 */
class HTTPClient {
  get(): { fetch: typeof fetch } {
    function _fetch(url: string, data: Parameters<typeof fetch>["1"]) {
      return fetch(HTTP_CHAT_URL + url, data);
    }
    return { fetch: _fetch };
  }

  isAvailable(): boolean {
    return !!HTTP_CHAT_URL;
  }
}

export default new HTTPClient();
````

## File: microservices/ace_agent/4.1/webui/server/chat-session/clients/RedisClient.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { createClient } from "redis";
import { REDIS_URL } from "../../config.js";

type RedisClient = ReturnType<typeof createClient>;

class RedisClientSingleton {
  private client: RedisClient = null;

  async get(): Promise<RedisClient> {
    if (this.client) {
      return this.client;
    }

    this.client = createClient({
      url: REDIS_URL,
    });
    await this.client.connect();
    return this.client;
  }

  isAvailable(): boolean {
    return !!REDIS_URL;
  }
}

export default new RedisClientSingleton();
````

## File: microservices/ace_agent/4.1/webui/server/chat-session/tasks/AbstractTask.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { EventEmitter } from "node:events";
import { InteractionMode } from "../../../shared/types.js";

/**
 * This is the base class that all "tasks" running on the server must extend. A task is a
 * unit responsible for managing a specific aspect of the web application (e.g. speech),
 * and how the feature communicates with the APIs (e.g. gRPC).
 */
export default abstract class AbstractTask {
  /**
   * What interaction modes are supported by the task (speech, text or both).
   */
  public abstract readonly interactionModes: InteractionMode[];

  /**
   * All tasks must subscribe to the abort controller when they start running. When the
   * abort controller is triggered, a task must stop its execution.
   */
  protected abortController: AbortController;

  /**
   * Constructor for the task
   * @param eventBus an object on which tasks can subscribe and publish events. The event
   * bus is shared across all tasks belonging to a user's session. This allows tasks
   * to communicate with each other.
   */
  constructor(protected readonly eventBus: EventEmitter) {}

  /**
   * The core logic of the task. A task may be stopped and started multiple times. Tasks
   * must override this method with specific logic, and call super.start(). Their logic
   * must stop when the abortController is called.
   */
  public start() {
    this.abortController = new AbortController();
  }

  /**
   * Stops the task. A task may be stopped and started multiple times for a user's
   * session. Subclasses generally do not need to override this method. To detect when the
   * task is stopped, subclasses should instead subscribe to the class' abortController.
   */
  public stop() {
    if (this.abortController) {
      this.abortController.abort();
    }
  }

  /**
   * To check whether the task is currently running.
   * @returns true if the task is currently running, false otherwise
   */
  public isRunning(): boolean {
    if (!this.abortController) {
      return false;
    }
    return !this.abortController.signal.aborted;
  }

  /**
   * For async logic that must run after the task has stopped. This is called once, after
   * the task is stopped. After cleanup, the task is guaranteed not to start again. By
   * default, this function does nothing. The subclasses may add specific logic by
   * overriding this method.
   */
  public async cleanup() {}
}
````

## File: microservices/ace_agent/4.1/webui/server/chat-session/tasks/ChatSessionTask.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { randomUUID } from "crypto";
import EmojiFinder from "../../emoji-finder/index.js";
import { WebSocket } from "ws";
import AbstractTask from "./AbstractTask.js";
import UMIMTask from "./UMIMTask.js";
import { EventEmitter } from "node:events";
import GRPCSpeechTask from "./GRPCSpeechTask.js";
import WebSocketTask from "./WebSocketTask.js";
import getLogger from "../logger.js";
import { once } from "events";
import {
  InteractionMode,
  ServerConfig,
  SystemMessageContent,
} from "../../../shared/types.js";
import GRPCTextTask from "./GRPCTextTask.js";
import HTTPChatTask from "./HTTPChatTask.js";
import GRPCClient from "../clients/GRPCClient.js";
import HTTPClient from "../clients/HTTPClient.js";
import RedisClient from "../clients/RedisClient.js";
import sleep from "../../utils/sleep.js";
import GRPCSpeechTranscriptionTask from "./GRPCSpeechTranscriptionTask.js";

const logger = getLogger("ChatSessionTask");

/**
 * This task is responsible for managing other tasks. It is started as soon as a user
 * starts a new session (in other words: when a websocket connection is made to the
 * server). It checks which clients (gRPC, Redis or HTTP) are available, and instantiates
 * tasks that are compatible with these clients.
 *
 * Whenever the user switches interaction mode (text or speech), this task starts and stop
 * other tasks depending on whether they support the desired mode.
 *
 * When the user leaves the session (closes the tab), this tasks stops all other tasks,
 * and cleans them up before stoppping itself.
 */
export default class ChatSessionTask extends AbstractTask {
  public interactionModes: InteractionMode[] = [
    InteractionMode.SPEECH,
    InteractionMode.TEXT,
  ];

  /**
   * Stream ID identifies a user's session across API calls.
   */
  private readonly streamID: string = randomUUID();

  /**
   * List of tasks running for the user's session. Typically contains all tasks that are
   * compatible with the available clients (Redis, gRPC, HTTP).
   */
  private tasks: AbstractTask[] = [];

  /**
   * Constructor
   * @param ws the websocket connection for the user's session
   * @param emojiFinder the "emojiFinder" instance. Only used for UMIM (redis) tasks
   * @param aceAgentTextChatInterface the mode in which ACE Agent is running
   * @param isSpeechEnabled whether the bot web UI should allow speech conversations (
   *                        ACE Agent must be running in speech mode)
   */
  constructor(
    readonly ws: WebSocket,
    readonly emojiFinder: EmojiFinder | null,
    readonly aceAgentTextChatInterface: "server" | "grpc" | "event",
    readonly isSpeechEnabled: boolean
  ) {
    super(new EventEmitter());
    this.initTasks();
  }

  /**
   * Called when the user leaves the session. This cleanes up all tasks running for the
   * user's session.
   */
  public override async cleanup(): Promise<void> {
    this.tasks.forEach(async (task) => {
      try {
        await task.cleanup();
      } catch (e) {
        logger.warn(
          `Received exception while cleaning up ${task.constructor.name}. Ignoring`,
          e
        );
      }
    });
  }

  /**
    Runs the main logic for the chat session. This starts all available tasks in text mode
    (the default mode).
   */
  public override async start(): Promise<void> {
    super.start();
    try {
      await Promise.all([
        this.listenFatalErrors(),
        this.listenWebSocketClosed(),
        this.listenWebServerStopped(),
        this.listenUserToggledSpeech(),
        this.refreshTasks(InteractionMode.TEXT),
        this.sendServerConfig(),
      ]);
    } catch (e) {
      if (e.name === "AbortError") {
        // The task was stopped while it was listening for events on the eventsBus. This
        // is OK
        return;
      }
      logger.fatal("Caught error while running task", e);
      this.eventBus.emit("fatalError", this.constructor.name, e);
    }
  }

  /**
   * Instantiates all tasks (but does not run them). Only tasks compatible with available
   * clients (gRPC, Redis or HTTP) are instantiated
   */
  private initTasks(): void {
    const redisAvailable = RedisClient.isAvailable();
    const gRPCAvailable = GRPCClient.isAvailable();
    const httpAvailable = HTTPClient.isAvailable();

    this.enableWebSocketTask();

    switch (this.aceAgentTextChatInterface) {
      case "grpc":
        if (!gRPCAvailable) {
          throw new Error(
            "Program was run with --ace-agent-text-chat-interface=grpc, but no GRPC_URL environment variable was set."
          );
        }
        this.enableGRPCTextTask();
        break;
      case "event":
        if (!redisAvailable) {
          throw new Error(
            "Program was run with --ace-agent-text-chat-interface=event, but no REDIS_URL environment variable was set."
          );
        }
        this.enableUMIMTask();
        break;
      case "server":
        if (!httpAvailable) {
          throw new Error(
            "Program was run with --ace-agent-text-chat-interface=server, but no HTTP_CHAT_URL environment variable was set."
          );
        }
        this.enableHTTPChatTask();
        break;
    }

    if (this.isSpeechEnabled) {
      if (gRPCAvailable) {
        this.enableGRPCSpeechTask();

        // In event mode, the UMIMTask is already handling speech transcriptions. Only
        // enable gRPC speech transcriptions when not in event mode
        if (this.aceAgentTextChatInterface !== "event") {
          this.enableGRPCBotSpeechTranscriptionTask();
        }
      } else {
        throw new Error(
          "Program was run with --speech, but no GRPC_URL environment variable was set."
        );
      }
    }
  }

  /**
   * Enable the UMIM task.
   */
  private enableUMIMTask(): void {
    logger.info("Enabling UMIMTask");
    this.tasks.push(new UMIMTask(this.eventBus, this.getStreamID()));
  }

  /**
   * Enable the GRPC Speech task. This sets isSpeechEnabled=true. It is the only task that
   * is required to support speech mode
   */
  private enableGRPCSpeechTask(): void {
    logger.info("Enabling GRPCSpeechTask");
    this.tasks.push(new GRPCSpeechTask(this.eventBus, this.getStreamID()));
  }

  /**
   * Enables the GRPC Text task.
   */
  private enableGRPCTextTask(): void {
    logger.info("Enabling GRPCTextTask");
    this.tasks.push(new GRPCTextTask(this.eventBus, this.getStreamID()));
  }

  /**
   * Enables the GRPC Bot Speech Transcription Task.
   */
  private enableGRPCBotSpeechTranscriptionTask(): void {
    logger.info("Enabling GRPCSpeechTranscriptionTask");
    this.tasks.push(
      new GRPCSpeechTranscriptionTask(this.eventBus, this.getStreamID())
    );
  }

  /**
   * Enables the HTTP Chat task
   */
  private enableHTTPChatTask(): void {
    logger.info("Enabling HTTPChatTask");
    this.tasks.push(new HTTPChatTask(this.eventBus, this.getStreamID()));
  }

  /**
   * Enables the Websocket task
   */
  private enableWebSocketTask(): void {
    logger.info("Enabling WebSocketTask");
    this.tasks.push(
      new WebSocketTask(this.eventBus, this.emojiFinder, this.ws)
    );
  }

  /**
   * Starts and stops task, depending on whether they are compatible with the user's
   * desired interaction mode (text or speech).
   * @param interactionMode the user's desired interaction mode
   */
  private refreshTasks(interactionMode: InteractionMode): void {
    for (const task of this.tasks) {
      const shouldRun = task.interactionModes.includes(interactionMode);
      if (task.isRunning() && !shouldRun) {
        logger.info(
          "Stopping task %s because it is not needed in new interaction mode",
          task.constructor.name,
          interactionMode
        );
        task.stop();
      }
      if (!task.isRunning() && shouldRun) {
        logger.info(
          "Starting task %s because it is needed in new interaction mode",
          task.constructor.name,
          interactionMode
        );
        task.start();
      }
    }
  }

  /**
   * Informs other tasks whether the current session supports speech
   */
  private sendServerConfig(): void {
    const serverConfig: ServerConfig = {
      type: SystemMessageContent.CONFIG_CHANGE,
      speechSupported: this.isSpeechEnabled,
    };
    this.eventBus.emit("serverConfigChange", serverConfig);
  }

  /**
   * Listens for when the user closed the websocket (closed window/tab). When this
   * happens, all running tasks are closed.
   */
  private async listenWebSocketClosed(): Promise<void> {
    await once(this.eventBus, "userClosedSocket", this.abortController);
    logger.info("User closed the websocket. Stopping all tasks");
    this.stop();
    await this.cleanup();
  }

  /**
   * Listens for when the process is killed or restarted (e.g. hot-reloading). This stops
   * all running tasks for the session
   */
  private async listenWebServerStopped(): Promise<void> {
    await once(process, "SIGTERM", this.abortController);
    logger.info(
      "SIGTERM signal received. Stopping session and existing program"
    );
    this.stop();
    await this.cleanup();
    await sleep(1000); // Grace period for other sessions to shutdown
    process.exit();
  }

  /**
   * Continuously listens for when the user toggles speech or text mode.
   */
  private async listenUserToggledSpeech(): Promise<void> {
    while (this.isRunning()) {
      const [interactionMode] = await once(
        this.eventBus,
        "userToggledSpeech",
        this.abortController
      );

      this.handleUserToggledSpeech(interactionMode);
    }
  }

  /**
   * Continuously listens for fatal errors from other tasks
   */
  private async listenFatalErrors(): Promise<void> {
    while (this.isRunning()) {
      const [taskName, error] = await once(
        this.eventBus,
        "fatalError",
        this.abortController
      );
      this.handleFatalError(taskName, error);
    }
  }

  /**
   * Called when the user toggles speech mode. When the user enables speech mode, this
   * starts the speech-related tasks (ASR, streaming audio, etc). When the user turns
   * speech mode off, this stops speech-related tasks.
   * @param interactionMode the user's desired interaction mode (text or speech)
   */
  private async handleUserToggledSpeech(
    interactionMode: InteractionMode
  ): Promise<void> {
    logger.info(
      "User toggled interaction mode to %s. Refreshing tasks",
      interactionMode
    );
    this.refreshTasks(interactionMode);
  }

  /**
   * Handler for when a task emits a fatal error. When this happens, a message is sent
   * to the client to inform the user, and all tasks for the current session are terminated.
   */
  private handleFatalError(taskName: string, error: Error): void {
    logger.info(`Sending fatal ${error.name} information to user`);
    this.eventBus.emit(
      "shutdown",
      `A fatal error occurred while running task ${taskName}. Message: "${error.message}". Check the server logs for more details.`
    );
    this.stop();
  }

  /**
   * Stops all speech and text related tasks. This is typically called when the user
   * leaves the session (e.g. closes tab).
   */
  stop(): void {
    logger.info("Informing user that the session is being shut down");
    this.eventBus.emit(
      "shutdown",
      "The server shut down. Please refresh this page to start a new session."
    );
    super.stop();
    for (const task of this.tasks) {
      if (task.isRunning()) {
        task.stop();
      }
    }
  }

  /**
   * The stream ID for this session. A stream ID is unique to each session, and identifies
   * the UMIM/gRPC pipeline for the conversation
   * @returns the stream ID
   */
  public getStreamID(): string {
    return this.streamID;
  }
}
````

## File: microservices/ace_agent/4.1/webui/server/chat-session/tasks/GRPCSpeechTask.test.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { after, before, beforeEach, describe, it, mock } from "node:test";
import GRPCSpeechTask from "./GRPCSpeechTask";
import { AceAgentGrpc } from "../../grpc/gen/ace_agent_connect";
import { PromiseClient } from "@connectrpc/connect";
import { EventEmitter } from "node:events";
import sleep from "../../utils/sleep";
import * as assert from "node:assert";
import { APIStatus } from "../../grpc/gen/ace_agent_pb.js";
import GRPCClient from "../clients/GRPCClient";
import waitAbortSignal from "../../utils/waitAbortSignal";

type GRPCClient = PromiseClient<typeof AceAgentGrpc>;

const mockSendAudio = mock.fn(
  () => {},
  () => ({ status: APIStatus.SUCCESS })
);

const mockReceiveAudio = mock.fn(
  () => {},
  (_request, abortSignal) => {
    return [waitAbortSignal(abortSignal)];
  }
);

const mockStreamSpeechResults = mock.fn(
  () => {},
  (_request, abortSignal) => {
    return [waitAbortSignal(abortSignal)];
  }
);

mock.method(GRPCClient, "get").mock.mockImplementation(() => {
  return {
    sendAudio: mockSendAudio,
    receiveAudio: mockReceiveAudio,
    streamSpeechResults: mockStreamSpeechResults,
    createPipeline: mock.fn(),
    freePipeline: mock.fn(),
  };
});

describe("GRPCSpeechTask", () => {
  let task: GRPCSpeechTask;
  let eventBus = new EventEmitter();
  before(() => {
    task = new GRPCSpeechTask(eventBus, "test_stream_id");
    task.start();
  });

  after(() => {
    task.stop();
  });

  beforeEach(() => {
    mockSendAudio.mock.resetCalls();
  });

  /**
   * Utility function to emulate the user sending audio
   */
  async function sendUserAudio(): Promise<void> {
    const chunk = new ArrayBuffer(10); // empty buffer for testing
    eventBus.emit("userSentAudio", chunk);
    await sleep(1);
  }

  it("Sends audio to the bot when the user sent a new chunk", async () => {
    await sendUserAudio();

    assert.strictEqual(mockSendAudio.mock.callCount(), 1);
  });
});
````

## File: microservices/ace_agent/4.1/webui/server/chat-session/tasks/GRPCSpeechTask.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import AbstractTask from "./AbstractTask.js";
import { EventEmitter, once } from "node:events";
import getLogger from "../logger.js";
import {
  AudioEncoding,
  ReceiveAudioRequest,
  SendAudioRequest,
  StreamingRecognitionConfig,
} from "../../grpc/gen/ace_agent_pb.js";
import sleep from "../../utils/sleep.js";
import { InteractionMode } from "../../../shared/types.js";

import GRPCClient from "../clients/GRPCClient.js";
import { USER_SPEECH_SAMPLE_RATE } from "../../config.js";

const logger = getLogger("GRPCSpeechTask");

/**
 * This task is responsible for handling speech-related aspects of the app:
 * - When the user sends audio chunks, this task enqueues them, and sends them to ACE
 *   Agent through its sendAudio API.
 * - When ACE Agent sends the bot's audio chunk, this task emits them on the shared event
 *   bus, so that other tasks can handle them.
 */
export default class GRPCSpeechTask extends AbstractTask {
  public readonly interactionModes: InteractionMode[] = [
    InteractionMode.SPEECH,
  ];

  private readonly gRPCClient = GRPCClient.get();

  /**
   * The list of audio chunks that have not been sent to ACE Agent yet.
   */
  private readonly userAudioChunks: ArrayBuffer[] = [];

  /**
   * Whether the pipeline was acquired by this task before. This is to ensure the task
   * doesn't needlessly re-acquire the pipeline every time it is re-started (in other
   * words: whenever the user switch between interaction modes from the UI).
   */
  private pipelineAcquired: boolean = false;

  constructor(
    protected readonly eventBus: EventEmitter,
    private readonly streamID: string
  ) {
    super(eventBus);
  }

  public override async cleanup(): Promise<void> {
    if (this.pipelineAcquired) {
      await this.informGRPCPipelineReleased();
    }
  }

  public override async start(): Promise<void> {
    super.start();
    try {
      if (!this.pipelineAcquired) {
        logger.info("Acquiring gRPC pipeline");
        this.pipelineAcquired = true;
        await this.informGRPCPipelineAcquired();
      }

      await Promise.all([
        this.listenBotAudio(),
        this.listenUserSentAudio(),
        this.processUserAudioQueue(),
      ]);
    } catch (e) {
      if (e.code === "ABORT_ERR") {
        // The task was stopped while it was listening for events on the eventBus. This
        // is OK
        return;
      }
      logger.fatal("Caught error while running task", e);
      this.eventBus.emit("fatalError", this.constructor.name, e);
    }
  }

  /**
   * Listens to ACE Agent's  `receiveAudio` gRPC call. When a new chunk is it's
   * immediately sent through a `botSentAudio` message on the shared event bus for other
   * tasks to handle.
   */
  private async listenBotAudio(): Promise<void> {
    logger.info("Starting to listen for bot audio");
    while (this.isRunning()) {
      const request = new ReceiveAudioRequest({
        streamId: this.streamID,
      });
      const audioResponse = this.gRPCClient.receiveAudio(request, {
        signal: this.abortController.signal,
      });

      for await (const chunk of audioResponse) {
        logger.info(
          "Received audio chunk from bot. Sample rate=%s",
          chunk.sampleRateHertz
        );
        this.eventBus.emit("botSentAudio", chunk.audioContent);
      }
    }
  }

  /**
   * Continuously listens for the user sending audio chunks.
   */
  private async listenUserSentAudio(): Promise<void> {
    while (this.isRunning()) {
      const [chunk] = await once(
        this.eventBus,
        "userSentAudio",
        this.abortController
      );
      this.handleUserSentAudio(chunk);
    }
  }

  /**
   * When a new audio chunk is available, the chunk is added to an internal queue. The
   * queue is processed asynchronously (see processUserAudioQueue()).
   * @param chunk the new audio chunk
   */
  private handleUserSentAudio(chunk: ArrayBuffer): void {
    logger.info("Enqueuing user audio chunk for processing");
    this.userAudioChunks.push(chunk);
  }

  /**
   * The main logic of this task is to continuously inspect the internal queue of audio
   * chunks from the user. When one or more audio chunks are available, they are
   * sent to ACE Agent through the client-side streaming `sendAudio` gRPC call.
   */
  private async processUserAudioQueue(): Promise<void> {
    logger.info("Starting to listen for new audio chunks");

    const task = this;
    async function* audioChunkGenerator() {
      yield new SendAudioRequest({
        streamId: task.streamID,
        streamingRequest: {
          case: "streamingConfig",
          value: new StreamingRecognitionConfig({
            audioChannelCount: 1,
            encoding: AudioEncoding.LINEAR_PCM,
            sampleRateHertz: USER_SPEECH_SAMPLE_RATE,
          }),
        },
      });
      while (task.isRunning()) {
        if (task.userAudioChunks.length > 0) {
          logger.info("Sending user audio chunk to ACE Agent through gRPC");
          yield new SendAudioRequest({
            streamingRequest: {
              case: "audioContent",
              value: new Uint8Array(task.userAudioChunks.shift()),
            },
          });
        } else {
          await task.nextTick();
        }
      }
    }

    const audioChunksIterator = audioChunkGenerator();
    await this.gRPCClient.sendAudio(audioChunksIterator, {
      signal: this.abortController.signal,
    });
  }

  /**
   * Informs ACE Agent that the user has started a chat session.
   */
  private async informGRPCPipelineAcquired(): Promise<void> {
    await this.gRPCClient.createPipeline({
      streamId: this.streamID,
      userId: this.streamID,
    });
  }

  /**
   * Informs ACE Agent that the user has started a chat session. This should be called
   * when the task is cleaned up.
   */
  private async informGRPCPipelineReleased(): Promise<void> {
    await this.gRPCClient.freePipeline({
      streamId: this.streamID,
      userId: this.streamID,
    });
  }

  /**
   * A utility function to pause execution. This allows other tasks to run while the
   * current task waits for audio chunks.
   */
  private async nextTick(): Promise<void> {
    await sleep(1);
  }
}
````

## File: microservices/ace_agent/4.1/webui/server/chat-session/tasks/GRPCSpeechTranscriptionTask.test.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { after, before, beforeEach, describe, it, mock } from "node:test";
import { AceAgentGrpc } from "../../grpc/gen/ace_agent_connect";
import { PromiseClient } from "@connectrpc/connect";
import { EventEmitter } from "node:events";
import sleep from "../../utils/sleep";
import * as assert from "node:assert";
import GRPCClient from "../clients/GRPCClient";
import GRPCSpeechTranscriptionTask from "./GRPCSpeechTranscriptionTask";
import waitAbortSignal from "../../utils/waitAbortSignal";

type GRPCClient = PromiseClient<typeof AceAgentGrpc>;

mock.method(GRPCClient, "get").mock.mockImplementation(() => {
  return {
    streamSpeechResults: mock.fn(
      () => {},
      (_req, abortSignal) => [
        { metadata: { case: "displayText", value: "I am a bot!" } },
        waitAbortSignal(abortSignal),
      ]
    ),
  };
});

describe("GRPCSpeechTranscriptionTask", () => {
  it("Emits a botStartedUtterance event on the shared event bus when the bot streams a speech transcription", async () => {
    const eventBus = new EventEmitter();
    const spy = mock.fn();
    eventBus.addListener("botStartedUtterance", spy);
    const task = new GRPCSpeechTranscriptionTask(eventBus, "test_stream_id");
    task.start();
    await sleep(1);
    task.stop();
    assert.strictEqual(spy.mock.callCount(), 1);
    assert.strictEqual(spy.mock.calls[0].arguments[1], "I am a bot!");
  });
});
````

## File: microservices/ace_agent/4.1/webui/server/chat-session/tasks/GRPCSpeechTranscriptionTask.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import AbstractTask from "./AbstractTask.js";
import { PromiseClient } from "@connectrpc/connect";
import { AceAgentGrpc } from "../../grpc/gen/ace_agent_connect";
import { EventEmitter } from "node:events";
import getLogger from "../logger.js";
import { StreamingSpeechResultsRequest } from "../../grpc/gen/ace_agent_pb.js";
import { randomUUID } from "node:crypto";
import { InteractionMode } from "../../../shared/types.js";
import GRPCClient from "../clients/GRPCClient.js";

type GRPCClient = PromiseClient<typeof AceAgentGrpc>;
const logger = getLogger("GRPCSpeechTranscriptionTask");

/**
 * This task is responsible for managing speech transcriptions. It listens from
 * the gRPC client's StreamingSpeechResultsRequest and, when receiving a speech
 * transcript, emits it on the shared event bus for other tasks to handle.
 *
 * This task is only used when ACE Agent is not running in "event" mode. In event
 * mode, speech transcriptions are handled by the UMIMTask.
 */
export default class GRPCSpeechTranscriptionTask extends AbstractTask {
  public readonly interactionModes: InteractionMode[] = [
    InteractionMode.SPEECH,
  ];

  private readonly gRPCClient = GRPCClient.get();

  // Each utterance must have a unique message ID. An utterance typically receives
  // many ASR updates, as the engine hears more words from the user. Each ASR update
  // belonging to the same utterance must have the same message ID. This allows the
  // UI to show the current utterance being built in real time in a speech bubble. Once
  // the utterance is completed, the message ID is incremented, allowing the UI to
  // create a new speech bubble.
  private currentMessageID: number = 0;

  constructor(eventBus: EventEmitter, private readonly streamID: string) {
    super(eventBus);
  }

  public override async start(): Promise<void> {
    super.start();
    try {
      await this.listenTextTranscription();
    } catch (e) {
      if (e.code === 1) {
        // The task was stopped while a gRPC call was in progress. This is OK
        return;
      }
      logger.fatal("Caught error while running task", e);
      this.eventBus.emit("fatalError", this.constructor.name, e);
    }
  }

  /**
   * Listens for text transcriptions through the gRPC client. When a new transcript
   * is available, sends it through the shared event bus.
   */
  public async listenTextTranscription(): Promise<void> {
    logger.info("Starting to listen for metadata");
    while (this.isRunning()) {
      const request = new StreamingSpeechResultsRequest({
        streamId: this.streamID,
        requestId: "GRPCSpeechTranscriptionTask",
      });
      const metaDataResponse = this.gRPCClient.streamSpeechResults(request, {
        signal: this.abortController.signal,
      });

      for await (const response of metaDataResponse) {
        logger.info("received metadata", response.metadata.case);
        switch (response.metadata.case) {
          case "displayText":
            const text = response.metadata.value;
            this.eventBus.emit("botStartedUtterance", randomUUID(), text, null);
            break;
          case "asrResult":
            const asr = response.metadata.value.results;
            this.eventBus.emit(
              "asrAvailable",
              asr.alternatives[0].transcript,
              this.currentMessageID
            );
            if (asr.isFinal) {
              this.currentMessageID++;
            }
        }
      }
    }
  }
}
````

## File: microservices/ace_agent/4.1/webui/server/chat-session/tasks/GRPCTextTask.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import AbstractTask from "./AbstractTask.js";
import { EventEmitter, once } from "node:events";
import getLogger from "../logger.js";
import { randomUUID } from "node:crypto";
import { InteractionMode } from "../../../shared/types.js";
import GRPCClient from "../clients/GRPCClient.js";

const logger = getLogger("GRPCTextTask");

/**
 * This task is responsible for handling text conversations between the user and the bot.
 * When a new text message is sent by the user, this task sends it to ACE Agent. ACE Agent
 * then respond with the bot's text response, which the task emits on the shared event bus
 * for other tasks to handle.
 *
 * This task only runs when ACE Agent runs in "speech" mode (in "event" mode, the UMIMTask
 * takes care of handling text conversations).
 */
export default class GRPCTextTask extends AbstractTask {
  public readonly interactionModes: InteractionMode[] = [InteractionMode.TEXT];
  private readonly gRPCClient = GRPCClient.get();
  private pipelineAcquired: boolean = false;

  constructor(eventBus: EventEmitter, private readonly streamID: string) {
    super(eventBus);
  }

  public override async cleanup(): Promise<void> {
    if (this.pipelineAcquired) {
      await this.informGRPCPipelineReleased();
    }
  }

  public override async start(): Promise<void> {
    super.start();

    try {
      if (!this.pipelineAcquired) {
        logger.info("Acquiring gRPC pipeline");
        this.pipelineAcquired = true;
        await this.informGRPCPipelineAcquired();
      }
      await this.listenUserFinishedMessage();
    } catch (e) {
      if (e.name === "AbortError") {
        // The task was stopped while listening for events on the eventBus. This is OK
        return;
      }
      logger.fatal("Caught error while running task", e);
      this.eventBus.emit("fatalError", this.constructor.name, e);
    }
  }

  /**
   * Continuously listens for the user submitting "full" messages (in other words, when
   * the user submits the message from the UI). Partial messages are ignored.
   */
  private async listenUserFinishedMessage(): Promise<void> {
    while (this.isRunning()) {
      const [_messageID, text] = await once(
        this.eventBus,
        "userFinishedMessage",
        this.abortController
      );
      await this.handleUserFinishedMessage(text);
    }
  }

  /**
   * Handles a new message submitted by the user. The message is sent to ACE Agent through
   * its gRPC interface. The bot's response is emitted through the shared event listeners
   * for other tasks to handle.
   * @param text the user's text message
   */
  private async handleUserFinishedMessage(text: string): Promise<void> {
    const queryID = randomUUID();
    const response = this.gRPCClient.chat({
      streamId: this.streamID,
      query: text,
      queryId: queryID,
      isStandalone: true,
    });

    let responseText = "";
    for await (const chunk of response) {
      logger.info("received chat response", chunk.cleanedText);
      responseText += chunk.cleanedText;
    }
    this.eventBus.emit("botStartedUtterance", queryID, responseText, null);
  }

  /**
   * Informs ACE Agent that the user has started a session.
   */
  private async informGRPCPipelineAcquired(): Promise<void> {
    await this.gRPCClient.createPipeline({
      streamId: this.streamID,
      userId: this.streamID,
    });
  }

  /**
   * Informs ACE Agent that the user has left the session. This should be called when the
   * task is cleaned up.
   */
  private async informGRPCPipelineReleased(): Promise<void> {
    await this.gRPCClient.freePipeline({
      streamId: this.streamID,
      userId: this.streamID,
    });
  }
}
````

## File: microservices/ace_agent/4.1/webui/server/chat-session/tasks/HTTPChatTask.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { EventEmitter, once } from "node:events";
import getLogger from "../logger.js";
import AbstractTask from "./AbstractTask.js";
import { InteractionMode } from "../../../shared/types.js";
import sleep from "../../utils/sleep.js";
import HTTPClient from "../clients/HTTPClient.js";
import { randomUUID } from "node:crypto";
import { HTTP_CHAT_URL } from "../../config.js";

const logger = getLogger("HTTPChatTask");

// Fetching the bot lists sometimes fails with transient issues. This sets the number
// of retries allowed. Once retries are exhausted, a fatal error is emitted
const MAX_RETRIES = 3;

/**
 * Handles text conversations between the user and the bot using ACE Agent's HTTP
 * interface. This only runs when ACE Agent runs in "server" mode. In other modes (speech
 * or event), the UMIMTask or GRPCTextTask take care of handling text conversations.
 *
 * This task has two main responsibilities:
 * 1. Send the user's text messages to the bot through ACE Agent's HTTP interface, and
 *    emit the bot's response through the shared event bus for other tasks to handle
 * 2. Poll the list of available bots. ACE Agent's HTTP interface supports multiple bots,
 *    so the UI needs to show a dropdown of all available bots.
 */
export default class HTTPChatTask extends AbstractTask {
  public readonly interactionModes: InteractionMode[] = [InteractionMode.TEXT];

  constructor(eventBus: EventEmitter, private readonly streamID: string) {
    super(eventBus);
  }

  public override async start(): Promise<void> {
    super.start();
    logger.info("starting running task");
    try {
      await Promise.all([this.getBotList(), this.listenUserFinishedMessage()]);
    } catch (e) {
      if (e.name === "AbortError") {
        // Task was stopped while listening on the eventBus. This is OK
        return;
      }
      logger.fatal("Caught error while running task", e);
      this.eventBus.emit("fatalError", this.constructor.name, e);
    }
  }

  /**
   * Polls the list of available bots from ACE Agent's HTTP interface. When the list of
   * bots change, it is emitted through the shared event bus for other tasks to handle.
   */
  public async getBotList(): Promise<void> {
    let botList = [];
    let retries = MAX_RETRIES;
    while (this.isRunning()) {
      try {
        const data = await HTTPClient.get().fetch("/isReady", {
          signal: this.abortController.signal,
        });

        const bots = (await data.json()) as any[];
        const newBotList = bots
          .filter((bot) => bot.Ready)
          .map((bot) => bot.BotName);

        if (newBotList.join() !== botList.join()) {
          this.eventBus.emit("botListUpdated", newBotList);
          botList = newBotList;
        }
        retries = MAX_RETRIES;
      } catch (e) {
        // A common transient error, retry
        if (
          e.code === "ECONNRESET" ||
          (e.code === "ECONNREFUSED" && retries > 0)
        ) {
          retries--;
          logger.warn(
            `Received error "%s" while fetching list of bots. Retrying (retries left: ${retries})`,
            e.code
          );
          continue;
        }
        logger.fatal("Caught error while running task", e);
        this.eventBus.emit("fatalError", this.constructor.name, e);
      }

      await sleep(1000);
    }

    this.eventBus.emit("botListUpdated", []);
  }

  /**
   * Continuously listens for the user submitting full messages. Partial messages are
   * ignored.
   */
  private async listenUserFinishedMessage(): Promise<void> {
    while (this.isRunning()) {
      const [_, text, botName] = await once(
        this.eventBus,
        "userFinishedMessage",
        this.abortController
      );
      this.handleUserFinishedMessage(text, botName);
    }
  }

  /**
   * Handles a new message that was submitted by the user. When a new message is
   * available, it is sent to the bot through ACE Agent's HTTP interface. The bot's
   * response is emitted through the shared event listeners for other tasks to handle.
   * @param text the user's text message
   * @param botName the name of the bot to use
   */
  private async handleUserFinishedMessage(
    text: string,
    botName: string | null
  ): Promise<void> {
    const parameters = {
      Query: text,
      UserId: this.streamID,
      BotName: botName,
    };
    this.eventBus.emit("botStartedThinkingIdle", randomUUID());
    const response = await HTTPClient.get().fetch("/chat", {
      method: "POST",
      signal: this.abortController.signal,
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify(parameters),
    });
    if (response.status !== 200) {
      const error = new Error(
        `Received unexpected HTTP response status ${response.status} from ACE Agent's ${HTTP_CHAT_URL}/chat endpoint. This typically happens when the Chat Engine service has not been started, or has stopped running. Refresh the page to try again`
      );
      logger.error(error.message);
      this.eventBus.emit("fatalError", this.constructor.name, error);
      return;
    }
    const transferEncoding = response.headers.get("Transfer-Encoding");
    if (transferEncoding === "chunked") {
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let result = await reader.read();
      let text = "";
      let queryID = "";
      while (!result.done) {
        const partialText = decoder.decode(result.value);
        const parsed = JSON.parse(partialText);
        text += parsed.Response.CleanedText ?? "";
        queryID = parsed.Metadata.QueryId;
        result = await reader.read();
      }
      this.eventBus.emit("botStartedUtterance", queryID, text, botName);
    } else {
      const json = (await response.json()) as { Metadata: any; Response: any };
      this.eventBus.emit(
        "botStartedUtterance",
        json.Metadata.QueryId,
        json.Response.CleanedText,
        botName
      );
    }
  }
}
````

## File: microservices/ace_agent/4.1/webui/server/chat-session/tasks/UMIMTask.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { createClient } from "redis";
import AbstractTask from "./AbstractTask.js";
import {
  UMIM_GestureBotActionFinished,
  UMIM_GestureBotActionStarted,
  UMIM_PipelineAcquired,
  UMIM_PipelineReleased,
  UMIM_PostureBotActionFinished,
  UMIM_PostureBotActionStarted,
  UMIM_TimerBotActionFinished,
  UMIM_TimerBotActionStarted,
  UMIM_UtteranceBotActionFinished,
  UMIM_UtteranceBotActionStarted,
  UMIM_UtteranceUserActionFinished,
  UMIM_UtteranceUserActionStarted,
  UMIM_UtteranceUserActionTranscriptUpdated,
} from "../../umim/umim.js";
import { EventEmitter, once } from "node:events";
import getLogger from "../logger.js";
import RedisClient from "../clients/RedisClient.js";
import { InteractionMode } from "../../../shared/types.js";
import { SYSTEM_EVENTS_STREAM, UMIM_SOURCE_NAME } from "../../config.js";
import sleep from "../../utils/sleep.js";

const logger = getLogger("UMIMTask");

/**
 * This task handles the UMIM session through Redis. It's responsible for handling
 * incoming UMIM events, and send back relevant UMIM events.
 *
 * It continuously listens for new UMIM events coming through the specified Redis
 * stream. When receiving a new event, the task may handle it directly. For example, when
 * receving a `StartTimerBotAction`, it responds with a TimerBotActionFinished event
 * after the specified duration.
 *
 * When receving events that should be handled by other tasks (e.g. the bot sent an
 * utterance that must be sent to the user), it dispatches a message through the shared
 * event bus.
 *
 * The task also subscribes to events emitted by other tasks. For example, when another
 * task emits the `userStartedNewMessage` event, this task sends a
 * `UtteranceUserActionStarted` event through Redis.
 */
export default class UMIMTask extends AbstractTask {
  public readonly interactionModes: InteractionMode[] = [
    InteractionMode.SPEECH,
    InteractionMode.TEXT,
  ];

  // The Redis channel for the stream. Automatically inferred from the streamID.
  private readonly channelKey: string;

  // The ID for the last message received through the Redis channel. This is needed to
  // keep listening for new messages.
  private lastRedisMessageID: string = "0";

  private pipelineAcquired: boolean = false;

  private currentInteractionMode: InteractionMode = InteractionMode.TEXT;

  constructor(eventBus: EventEmitter, private readonly streamID: string) {
    super(eventBus);
    this.channelKey = `umim_events_${this.streamID}`;
  }

  public override async cleanup(): Promise<void> {
    if (this.pipelineAcquired) {
      await this.informUMIMPipelineReleased();
    }
  }

  /**
   * Starts all the listeners
   */
  public override async start(): Promise<void> {
    super.start();

    logger.info("starting running task");
    try {
      if (!this.pipelineAcquired) {
        logger.info("Acquiring UMIM pipeline");
        this.pipelineAcquired = true;
        await this.informUMIMPipelineAcquired();
      }
      await Promise.all([
        this.listenRedisBotMessages(),
        this.listenRedisErrors(),
        this.listenUserStartedNewMessage(),
        this.listenUserUpdatedMessage(),
        this.listenUserFinishedMessage(),
        this.listenUserToggledSpeech(),
      ]);
    } catch (e) {
      if (e.name === "AbortError") {
        // Task was stopped while listening on the eventBus. This is OK
        return;
      }
      logger.fatal("Caught error while running task", e);
      this.eventBus.emit("fatalError", this.constructor.name, e);
    }
  }

  /**
   * The task continuously listens to new messages through the Redis stream, until the
   * stop() method is called. New messages are processed through handleRedisEvent().
   */
  private async listenRedisBotMessages(): Promise<void> {
    logger.info("awaiting for new messages on channel %s", this.channelKey);
    const redisClient = await RedisClient.get();
    while (this.isRunning()) {
      const responses = await redisClient.xRead(
        { key: this.channelKey, id: this.lastRedisMessageID },
        { COUNT: 20, BLOCK: 1000 }
      );
      if (responses) {
        for (const response of responses) {
          for (const event of response.messages) {
            this.handleRedisEvent(event.message.event);
            this.lastRedisMessageID = event?.id ?? this.lastRedisMessageID;
          }
        }
      }
    }
  }

  /**
   * Continuously listens for the user starting to type a new message.
   */
  private async listenUserStartedNewMessage(): Promise<void> {
    while (this.isRunning()) {
      const [messageID] = await once(
        this.eventBus,
        "userStartedNewMessage",
        this.abortController
      );
      this.handleUserStartedNewMessage(messageID);
    }
  }

  /**
   * Continuously listens for the user updating their messages (in other words, this is
   * called every time the user types a letter).
   */
  private async listenUserUpdatedMessage(): Promise<void> {
    while (this.isRunning()) {
      const [messageID, text] = await once(
        this.eventBus,
        "userUpdatedMessage",
        this.abortController
      );
      this.handleUserUpdatedMessage(messageID, text);
    }
  }

  /**
   * Continuously listens for the user submitting full messages (in other words, when
   * the user submtis the message).
   */
  private async listenUserFinishedMessage(): Promise<void> {
    while (this.isRunning()) {
      const [messageID, text] = await once(
        this.eventBus,
        "userFinishedMessage",
        this.abortController
      );
      this.handleUserFinishedMessage(messageID, text);
    }
  }

  /**
   * Continuously listens for when the user toggles speech or text mode.
   */
  private async listenUserToggledSpeech(): Promise<void> {
    while (this.isRunning()) {
      const [interactionMode] = await once(
        this.eventBus,
        "userToggledSpeech",
        this.abortController
      );

      this.handleUserToggledSpeech(interactionMode);
    }
  }

  /**
   * Continuously listens for errors on the Redis client.
   */
  private async listenRedisErrors(): Promise<void> {
    const redisClient = await RedisClient.get();
    while (this.isRunning()) {
      const [error] = await once(redisClient, "error", this.abortController);
      this.handleRedisError(error);
    }
  }

  /**
   * Handles an incoming Redis event. The event is parsed, and the appropriate action
   * is taken based on the event's type.
   * @param event the Redis event content, as JSON string
   */
  private async handleRedisEvent(event: string): Promise<void> {
    let parsed;
    try {
      parsed = JSON.parse(event);
    } catch (e) {
      logger.error(
        "Could not parse redis message as JSON. Ignoring message",
        event
      );
      return;
    }

    if (parsed.source_uid === UMIM_SOURCE_NAME) {
      logger.info(
        `Ignoring ${parsed.type} because it was sent by us (the UI server), not the bot`
      );
      return;
    }
    switch (parsed.type) {
      case "StartTimerBotAction": {
        this.handleStartTimerBotAction(
          parsed.action_uid,
          parsed.duration,
          new Date(parsed.event_created_at)
        );
        break;
      }
      case "StopTimerBotAction": {
        this.handleStopTimerBotAction(parsed.action_uid);
        break;
      }
      case "StartPostureBotAction": {
        this.handlePostureBotAction(parsed.action_uid, parsed.posture);
        break;
      }
      case "StopPostureBotAction": {
        this.handleStopPostureBotAction(parsed.action_uid);
        break;
      }
      case "StartGestureBotAction": {
        this.handleStartGestureBotAction(
          parsed.action_uid,
          parsed.gesture,
          parsed.source_uid
        );
        break;
      }
      case "StartUtteranceBotAction": {
        this.handleStartUtteranceBotAction(
          parsed.action_uid,
          parsed.script,
          parsed.source_uid
        );
        break;
      }
      case "UtteranceUserActionTranscriptUpdated": {
        this.handleUtteranceUserActionTranscriptUpdated(
          parsed.action_uid,
          parsed.interim_transcript,
          parsed.source_uid
        );
        break;
      }
      case "UtteranceUserActionFinished": {
        this.handleUtteranceUserActionFinished(
          parsed.action_uid,
          parsed.final_transcript,
          parsed.source_uid
        );
        break;
      }
      case "StopUtteranceBotAction": {
        this.handleStopUtteranceBotAction();
        break;
      }
    }
  }

  /**
   * Handles a new message sent by the user. Sends a `UtteranceUserActionStarted`
   * event to the bot.
   * @param messageID the ID for the message
   */
  private async handleUserStartedNewMessage(messageID: string): Promise<void> {
    const redisClient = await RedisClient.get();
    await redisClient.xAdd(this.channelKey, "*", {
      event: new UMIM_UtteranceUserActionStarted(
        UMIM_SOURCE_NAME,
        messageID
      ).toJSONString(),
    });
  }

  /**
   * Handles a message that was updated by the user (typically, every time the user
   * types a key on the user interface). Sends a UtteranceUserActionTranscriptUpdated
   * to the bot, containing the updated text for the message.
   * @param messageID the ID of the message
   * @param text the text of the message
   */
  private async handleUserUpdatedMessage(
    messageID: string,
    text: string
  ): Promise<void> {
    const redisClient = await RedisClient.get();
    await redisClient.xAdd(this.channelKey, "*", {
      event: new UMIM_UtteranceUserActionTranscriptUpdated(
        UMIM_SOURCE_NAME,
        messageID,
        text
      ).toJSONString(),
    });
  }

  /**
   * Handles a message that was completed by the user (typically, when the user submits
   * the messages on the user interface). Sends the final text for the message to the bot
   * through a UtteranceUserActionFinished event.
   * @param messageID the ID of the message
   * @param text the final text content of the message
   */
  private async handleUserFinishedMessage(
    messageID: string,
    text: string
  ): Promise<void> {
    const redisClient = await RedisClient.get();
    await redisClient.xAdd(this.channelKey, "*", {
      event: new UMIM_UtteranceUserActionFinished(
        UMIM_SOURCE_NAME,
        text,
        messageID
      ).toJSONString(),
    });
  }

  /**
   * Handles an incoming StartTimerBotAction event. This immediately sends back a
   * `TimerBotActionStarted` event. Then, after the specified duration has elapsed, sends
   * a `TimerBotActionFinished`.
   * @param actionUID the ID for the action
   * @param durationSec the duration of the timer
   */
  private async handleStartTimerBotAction(
    actionUID: string,
    durationSec: number,
    eventCreatedAt: Date
  ): Promise<void> {
    const redisClient = await RedisClient.get();

    await redisClient.xAdd(this.channelKey, "*", {
      event: new UMIM_TimerBotActionStarted(
        UMIM_SOURCE_NAME,
        actionUID
      ).toJSONString(),
    });

    const now = new Date();
    const finishedTime = new Date(
      eventCreatedAt.getTime() + durationSec * 1000
    );
    const duration = finishedTime.getTime() - now.getTime();

    await sleep(duration);

    await redisClient.xAdd(this.channelKey, "*", {
      event: new UMIM_TimerBotActionFinished(
        UMIM_SOURCE_NAME,
        actionUID
      ).toJSONString(),
    });
  }

  /**
   * Handles an incoming StopTimerBotAction. This immediately sends back a
   * `TimerBotActionFinished` event to the bot.
   * @param actionUID
   */
  private async handleStopTimerBotAction(actionUID: string): Promise<void> {
    const redisClient = await RedisClient.get();
    await redisClient.xAdd(this.channelKey, "*", {
      event: new UMIM_TimerBotActionFinished(
        UMIM_SOURCE_NAME,
        actionUID
      ).toJSONString(),
    });
  }

  /**
   * Handles a "PostureBotAction" event received from the bot. This immediately sends back
   * a PostureBotActionStarted event. If the posture is "Thinking, idle" (which is
   * typically sent as the bot is preparing a response to a user message), a
   * "botStartedThinkingIdle" event is emitted on the shared eventBus. This allows
   * to show a "thinking" state on the UI.
   * @param actionUID the ID for the action
   * @param posture the posture for the bot
   */
  private async handlePostureBotAction(
    actionUID: string,
    posture: string
  ): Promise<void> {
    const redisClient = await RedisClient.get();
    if (posture === "Thinking, idle") {
      this.eventBus.emit("botStartedThinkingIdle", actionUID);
    }
    await redisClient.xAdd(this.channelKey, "*", {
      event: new UMIM_PostureBotActionStarted(
        UMIM_SOURCE_NAME,
        actionUID
      ).toJSONString(),
    });
  }

  /**
   * Handles a `StopPostureBotAction` from the bot. This immediately sends back a
   * "PostBotActionFinished" event.
   * @param actionUID the ID for the action
   */
  private async handleStopPostureBotAction(actionUID: string): Promise<void> {
    const redisClient = await RedisClient.get();
    await redisClient.xAdd(this.channelKey, "*", {
      event: new UMIM_PostureBotActionFinished(
        UMIM_SOURCE_NAME,
        actionUID
      ).toJSONString(),
    });
  }

  /**
   * Handles a StartGestureBotAction from the bot. This immediately sends back a
   * `GestureBotActionStarted` and `GestureBotActionFinished` events. Additionally,
   * this emits a botStartedGesture on the shared eventBus, so that other task
   * can handle the event.
   * @param actionUID
   * @param gesture
   */
  private async handleStartGestureBotAction(
    actionUID: string,
    gesture: string,
    botName: string
  ): Promise<void> {
    const redisClient = await RedisClient.get();
    this.eventBus.emit("botStartedGesture", actionUID, gesture, botName);
    await redisClient.xAdd(this.channelKey, "*", {
      event: new UMIM_GestureBotActionStarted(
        UMIM_SOURCE_NAME,
        actionUID
      ).toJSONString(),
    });

    await redisClient.xAdd(this.channelKey, "*", {
      event: new UMIM_GestureBotActionFinished(
        UMIM_SOURCE_NAME,
        actionUID
      ).toJSONString(),
    });
  }

  /**
   * Handles an incoming StartUtteranceBotAction. When the user is using text mode, this
   * immediately sends back `UtteranceBotActionStarted` and `UtteranceBotActionFinished`
   * events, and emits a `botStartedUtterance` event on the shared eventBus, for other
   * tasks to handle.
   *
   * In speech mode, this only sends the bot's speech in text form so that we can show
   * it in the UI. It does not send back UtteranceBotActionStarted or
   * UtteranceBotActionFinished events, which are handled by ACE Agent.
   *
   * @param actionUID the ID for the action
   * @param text the text that the bot speaks
   */
  private async handleStartUtteranceBotAction(
    actionUID: string,
    text: string,
    botName: string
  ): Promise<void> {
    const redisClient = await RedisClient.get();
    this.eventBus.emit("botStartedUtterance", actionUID, text, botName);

    if (this.currentInteractionMode === InteractionMode.SPEECH) {
      logger.info(
        "Ignoring StartUtteranceBotAction because the UI is in speech mode. In this mode, UtteranceBotActionStarted and UtteranceBotActionFinished are handled by ACE Agent"
      );
      return;
    }

    await redisClient.xAdd(this.channelKey, "*", {
      event: new UMIM_UtteranceBotActionStarted(
        UMIM_SOURCE_NAME,
        actionUID
      ).toJSONString(),
    });

    await redisClient.xAdd(this.channelKey, "*", {
      event: new UMIM_UtteranceBotActionFinished(
        UMIM_SOURCE_NAME,
        text,
        actionUID
      ).toJSONString(),
    });
  }

  /**
   * Handles user's ASR transcript being updated
   */
  private handleUtteranceUserActionTranscriptUpdated(
    actionUID: string,
    transcript: string,
    sourceID: string
  ): void {
    this.eventBus.emit("asrAvailable", transcript, actionUID);
  }

  private handleUtteranceUserActionFinished(
    actionUID: string,
    transcript: string,
    sourceID: string
  ): void {
    if (!transcript) {
      logger.warn(
        "Received empty transcript from UtteranceUserActionFinished. Ignoring"
      );
      return;
    }
    this.eventBus.emit("asrAvailable", transcript, actionUID);
  }
  /*
   * Handles an incoming StopUtteranceBotAction. This happens when the bot stops speaking
   * because the user has interrupted it, also known as a "barge in"
   */
  private handleStopUtteranceBotAction() {
    this.eventBus.emit("userBargeIn");
  }

  /**
   * Handles errors sent by the redis client. In practice, this function just logs the
   * error without interrupting the task.
   * @param e
   */
  private handleRedisError(e: Error) {
    logger.error("received error from Redis client", e);
  }

  /**
   * Called when the user toggles speech mode. When the user enables speech mode, the UMIM
   * Task stops acting as an UMIM action server. This means it will no longer send events
   * like "UtteranceBotActionFinished". This responsibility is assumed by ACE Agent
   * @param interactionMode the user's desired interaction mode (text or speech)
   */
  private async handleUserToggledSpeech(
    interactionMode: InteractionMode
  ): Promise<void> {
    if (interactionMode === InteractionMode.SPEECH) {
      logger.info(
        "User toggled interaction mode to speech. UMIMTask will stop acting as an action server"
      );
    } else {
      logger.info(
        "User toggled interaction mode to text. UMIMTask will act as an action server again",
        interactionMode
      );
    }
    this.currentInteractionMode = interactionMode;
  }

  /**
   * Informs the UMIM bot that a user has started a chat session.
   */
  private async informUMIMPipelineAcquired(): Promise<void> {
    const redisClient = await RedisClient.get();
    logger.info("Informing UMIM Pipeline acquired, streamID=%s", this.streamID);
    await redisClient.xAdd(SYSTEM_EVENTS_STREAM, "*", {
      event: new UMIM_PipelineAcquired(
        UMIM_SOURCE_NAME,
        this.streamID,
        this.streamID
      ).toJSONString(),
    });
  }

  /**
   * Informs the UMIM bot that a user has left the chat session. This should be called
   * when the task is cleaned up.
   */
  private async informUMIMPipelineReleased(): Promise<void> {
    const redisClient = await RedisClient.get();
    logger.info("Informing UMIM Pipeline released, streamID=%s", this.streamID);
    await redisClient.xAdd(SYSTEM_EVENTS_STREAM, "*", {
      event: new UMIM_PipelineReleased(
        UMIM_SOURCE_NAME,
        this.streamID,
        this.streamID
      ).toJSONString(),
    });
  }
}
````

## File: microservices/ace_agent/4.1/webui/server/chat-session/tasks/WebSocketTask.test.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { describe, it, before, after, mock, beforeEach } from "node:test";
import * as assert from "node:assert";
import WebSocketTask from "./WebSocketTask";
import { EventEmitter } from "node:stream";
import EmojiFinder from "../../emoji-finder";
import type { WebSocket } from "ws";
import {
  AuthorType,
  ChatMessageContentType,
  UserChatMessage,
} from "../../../shared/types";
import sleep from "../../utils/sleep";

const mockWebSocket = new EventEmitter() as WebSocket;
mockWebSocket.send = mock.fn();

describe("WebSocketTask", () => {
  let task: WebSocketTask;
  let emojiFinder: EmojiFinder;
  let eventBus: EventEmitter;
  before(async () => {
    eventBus = new EventEmitter();
    emojiFinder = new EmojiFinder([
      {
        emoji: "",
        description: "man dancing",
        aliases: ["man_dancing"],
        tags: ["dancer"],
      },
    ]);
    await emojiFinder.init();
    task = new WebSocketTask(eventBus, emojiFinder, mockWebSocket);
    task.start();
  });

  beforeEach(() => {
    (mockWebSocket.send as any).mock.resetCalls();
  });

  after(() => {
    task.stop();
  });

  /**
   * Utility to emulate a user typing a message
   */
  async function sendUserTyping(
    isNewMessage: boolean = true,
    text: string = null
  ) {
    const message: UserChatMessage = {
      author: AuthorType.USER,
      content: {
        type: ChatMessageContentType.TYPING,
        messageID: "test_message_id",
        text,
        isNewMessage,
      },
    };
    mockWebSocket.emit("message", JSON.stringify(message));
    await sleep(1); // to give time to the loop to read the message
  }

  /**
   * Utility to emulate a user submitting a message
   */
  async function sendUserMessage(text: string) {
    const message: UserChatMessage = {
      author: AuthorType.USER,
      content: {
        type: ChatMessageContentType.TEXT,
        messageID: "test_message_id",
        text,
        botName: "test_bot_v1",
      },
    };
    mockWebSocket.emit("message", JSON.stringify(message));
    await sleep(1); // to give time to the loop to read the message
  }

  /**
   * Utility to emulate the bot sending an utterance
   */
  async function sendBotUtterance(text: string) {
    eventBus.emit("botStartedUtterance", "test_action_id", text, "test_bot_v1");
    await sleep(100); // to give time to the loop to read the message
  }

  /**
   * Utility to emulate the bot sending a signal that it is thinking
   */
  async function sendBotThinking() {
    eventBus.emit("botStartedThinkingIdle");
    await sleep(100); // to give time to the loop to read the message
  }

  /**
   * Utility to emulate the bot sending a gesture
   */
  async function sendBotGesture(text: string) {
    eventBus.emit("botStartedGesture", "test_action_id", text);
    await sleep(100); // to give time to the loop to read the message
  }

  /**
   * Utility to emulate the bot sending audio
   */
  async function sendBotAudio() {
    const audio = new Uint8Array(10); // empty chunk for testing
    eventBus.emit("botSentAudio", audio);
    await sleep(100); // to give time to the loop to read the message
  }

  /**
   * Utility to emulate the bot sending ASR data to the user
   */
  async function sendASR(text: string) {
    eventBus.emit("asrAvailable", text);
    await sleep(100); // to give time to the loop to read the message
  }

  it('Emits a "userStartedNewMessage" event when the user started typing', async () => {
    const callback = mock.fn();
    eventBus.on("userStartedNewMessage", callback);
    await sendUserTyping();
    assert.strictEqual(callback.mock.callCount(), 1);
    assert.strictEqual(callback.mock.calls[0].arguments[0], "test_message_id");
  });

  it('Emits a "userUpdatedMessage" event when the user updates their message', async () => {
    const callback = mock.fn();
    eventBus.on("userUpdatedMessage", callback);
    await sendUserTyping(false, "I am typing!");
    assert.strictEqual(callback.mock.callCount(), 1);
    assert.strictEqual(callback.mock.calls[0].arguments[0], "test_message_id");
    assert.strictEqual(callback.mock.calls[0].arguments[1], "I am typing!");
  });

  it('Emits a "userFinishedMessage" event when the user submits their message', async () => {
    const callback = mock.fn();
    eventBus.on("userFinishedMessage", callback);
    await sendUserMessage("Hello, world!");
    assert.strictEqual(callback.mock.callCount(), 1);
    assert.strictEqual(callback.mock.calls[0].arguments[0], "test_message_id");
    assert.strictEqual(callback.mock.calls[0].arguments[1], "Hello, world!");
  });

  it("Sends a message to the user when the bot sends an utterance", async () => {
    await sendBotUtterance("I am a bot!");
    assert.strictEqual((mockWebSocket.send as any).mock.callCount(), 1);
    const message = JSON.parse(
      (mockWebSocket.send as any).mock.calls[0].arguments[0]
    );
    assert.strictEqual(message.content.text, "I am a bot!");
  });

  it("Sends a 'typing' event to the user when the bot starts thinking", async () => {
    await sendBotThinking();
    assert.strictEqual((mockWebSocket.send as any).mock.callCount(), 1);
    const message = JSON.parse(
      (mockWebSocket.send as any).mock.calls[0].arguments[0]
    );
    assert.strictEqual(message.content.type, ChatMessageContentType.TYPING);
  });

  it("Sends an emoji to the user when the bot sends a gesture", async () => {
    await sendBotGesture("Dancing");
    assert.strictEqual((mockWebSocket.send as any).mock.callCount(), 1);
    const message = JSON.parse(
      (mockWebSocket.send as any).mock.calls[0].arguments[0]
    );
    assert.strictEqual(message.content.type, ChatMessageContentType.EMOJI);
    assert.strictEqual(message.content.emoji, "");
  });

  it("Sends audio to the user when the bot sends an audio chunk", async () => {
    await sendBotAudio();
    assert.strictEqual((mockWebSocket.send as any).mock.callCount(), 1);
    const message = (mockWebSocket.send as any).mock.calls[0].arguments[0];
    assert.ok(message instanceof Uint8Array);
  });

  it("Sends ASR data to the user when the bot sends ASR", async () => {
    await sendASR("How are you doing?");
    assert.strictEqual((mockWebSocket.send as any).mock.callCount(), 1);
    const message = JSON.parse(
      (mockWebSocket.send as any).mock.calls[0].arguments[0]
    );
    assert.strictEqual(message.content.type, ChatMessageContentType.ASR);
    assert.strictEqual(message.content.transcript, "How are you doing?");
  });
});
````

## File: microservices/ace_agent/4.1/webui/server/chat-session/tasks/WebSocketTask.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { RawData, WebSocket } from "ws";
import {
  AuthorType,
  BotChatMessage,
  ChatMessageContentType,
  InteractionMode,
  MessageID,
  ServerConfig,
  SystemConfigMessage,
  SystemMessageContent,
  SystemShutdownMessage,
  UserChatMessage,
} from "../../../shared/types.js";
import AbstractTask from "./AbstractTask.js";
import { EventEmitter, once } from "node:events";
import EmojiFinder from "../../emoji-finder/index.js";
import getLogger from "../logger.js";

const logger = getLogger("WebSocketTask");

/**
 * This task handles the websocket connection with the user. It's responsible for
 * listening to incoming messages from the user, and sending messages back to the user.
 * It communicate with other tasks through the shared event bus.
 */
export default class WebSocketTask extends AbstractTask {
  public readonly interactionModes: InteractionMode[] = [
    InteractionMode.SPEECH,
    InteractionMode.TEXT,
  ];

  constructor(
    eventBus: EventEmitter,
    private readonly emojiFinder: EmojiFinder | null,
    private readonly ws: WebSocket
  ) {
    super(eventBus);
  }

  /**
   * Runs all the listening loops (listen to websocket messages, bot messages, etc).
   */
  public override async start(): Promise<void> {
    super.start();
    try {
      await Promise.all([
        this.listenServerConfigChange(),
        this.listenWebSocketMessages(),
        this.listenWebSocketError(),
        this.listenWebSocketClose(),
        this.listenBotListUpdated(),
        this.listenBotStartedUtterance(),
        this.listenBotStartedThinkingIdle(),
        this.listenBotStartedGesture(),
        this.listenBotSentAudio(),
        this.listenASRAvailable(),
        this.listenUserBargeIn(),
        this.listenShutdown(),
      ]);
    } catch (e) {
      if (e.name === "AbortError") {
        // Task was stopped while listening for messages on the event bus. That's OK
        return;
      }
      logger.fatal("Caught error while running task", e);
      this.eventBus.emit("fatalError", this.constructor.name, e);
    }
  }

  /**
   * Continuously listens to incoming websocket messages from the user until the task
   * is stopped.
   */
  private async listenWebSocketMessages(): Promise<void> {
    while (this.isRunning()) {
      const [data] = await once(this.ws, "message", this.abortController);
      this.handleWebSocketMessage(data);
    }
  }

  /**
   * Continuously listens for websocket errors, until the task is stopped.
   */
  private async listenWebSocketError(): Promise<void> {
    while (this.isRunning()) {
      const [error] = await once(this.ws, "error", this.abortController);
      this.handleWebSocketError(error);
    }
  }

  /**
   * Listens for the websocket being closed.
   */
  private async listenWebSocketClose(): Promise<void> {
    const [code] = await once(this.ws, "close", this.abortController);
    this.handleWebSocketClosed(code);
  }

  /**
   * Continuously listens for a new list of bots being available. This happens when the
   * ACE Agent runs in "server" mode with multiple bots enabled.
   */
  private async listenBotListUpdated(): Promise<void> {
    while (this.isRunning()) {
      const [botList] = await once(
        this.eventBus,
        "botListUpdated",
        this.abortController
      );
      this.handleBotListUpdated(botList);
    }
  }

  /**
   * Continuously listens for bot utterances from the shared event bus.
   */
  private async listenBotStartedUtterance(): Promise<void> {
    while (this.isRunning()) {
      const [actionID, transcript, botName] = await once(
        this.eventBus,
        "botStartedUtterance",
        this.abortController
      );
      this.handleBotStartedUtterance(actionID, transcript, botName);
    }
  }

  /**
   * Continuously listens for "the bot is thinking" events from the shared event bus.
   */
  private async listenBotStartedThinkingIdle(): Promise<void> {
    while (this.isRunning()) {
      const [actionID] = await once(
        this.eventBus,
        "botStartedThinkingIdle",
        this.abortController
      );
      this.handleBotStartedThinkingIdle(actionID);
    }
  }

  /**
   * Continuously listens for bot gestures from the shared event bus.
   */
  private async listenBotStartedGesture(): Promise<void> {
    while (this.isRunning()) {
      const [actionID, gesture, botName] = await once(
        this.eventBus,
        "botStartedGesture",
        this.abortController
      );
      this.handleBotStartedGesture(actionID, gesture, botName);
    }
  }

  /**
   * Continuously listens for audio sent by the bot through the shared event bus.
   */
  private async listenBotSentAudio(): Promise<void> {
    while (this.isRunning()) {
      const [audio] = await once(
        this.eventBus,
        "botSentAudio",
        this.abortController
      );
      this.handleBotSentAudio(audio);
    }
  }

  /**
   * Continuously listens for new ASR data from the shared event emitter
   */
  private async listenASRAvailable(): Promise<void> {
    while (this.isRunning()) {
      const [text, messageID] = await once(
        this.eventBus,
        "asrAvailable",
        this.abortController
      );
      this.handleASRAvailable(text, messageID);
    }
  }

  /**
   * Continuously listens for user barge-ins. A barge-in is when the user interrupts
   * the bot while the bot is speaking. Barge-in are detected by ACE Agent
   */
  private async listenUserBargeIn(): Promise<void> {
    while (this.isRunning()) {
      await once(this.eventBus, "userBargeIn", this.abortController);
      this.handleUserBargeIn();
    }
  }

  /**
   * Continuously listens for a shutdown signal. The shutdown event signals that the
   * session has stopped
   */
  private async listenShutdown(): Promise<void> {
    while (this.isRunning()) {
      const [reason] = await once(
        this.eventBus,
        "shutdown",
        this.abortController
      );
      this.handleShutdown(reason);
    }
  }

  /**
   * Continuously listens for "server" config updates. This typically happens when the
   * server needs to advertise a configuration that the client needs to know. For example,
   * this is used to inform the client whether the server supports speech mode.
   */
  private async listenServerConfigChange(): Promise<void> {
    while (this.isRunning()) {
      const [serverConfig] = await once(
        this.eventBus,
        "serverConfigChange",
        this.abortController
      );
      this.handleServerConfigChange(serverConfig);
    }
  }

  /**
   * Handler for when the user sends a new message through the websocket.
   *
   * If the data looks like audio (is a binary ArrayBuffer), assumes the user is sending
   * audio.
   *
   * Otherwise, the handler parses the message as a JSON string, and sends the content of
   * the message through the shared event emitter for other tasks to handle:
   * 1. "userStartedNewMessage" if the user just started typing a new message
   * 2. "userUpdatedMessage" if the user updated an existing message (typically, the
   *    message is updated on every key stroke until it's actually sent)
   * 3. "userFinishedMessage" if the user submitted their message
   * @param data an audio chunk or a JSON-encoded string
   */
  private handleWebSocketMessage(data: RawData): void {
    if (data instanceof ArrayBuffer) {
      logger.info("Received audio message from user");
      this.eventBus.emit("userSentAudio", data);
      return;
    }

    let message;
    try {
      message = JSON.parse(data.toString()) as UserChatMessage;
    } catch (e) {
      logger.error("Could not parse the message as audio or JSON. Ignoring", e);
      return;
    }

    logger.info(
      "Received text of type %s message from user",
      message.content.type
    );

    switch (message.content.type) {
      case ChatMessageContentType.TYPING: {
        if (message.content.isNewMessage) {
          this.eventBus.emit(
            "userStartedNewMessage",
            message.content.messageID
          );
        } else {
          this.eventBus.emit(
            "userUpdatedMessage",
            message.content.messageID,
            message.content.text
          );
        }
        break;
      }
      case ChatMessageContentType.TEXT:
        this.eventBus.emit(
          "userFinishedMessage",
          message.content.messageID,
          message.content.text,
          message.content.botName
        );
        break;
      case ChatMessageContentType.TOGGLE_SPEECH:
        this.eventBus.emit(
          "userToggledSpeech",
          message.content.interactionMode
        );
        break;
    }
  }

  /**
   * Handles a new list of bots shared by ACE Agent. This happens when ACE Agent runs
   * in `server` mode, and runs multiple bots in parallel. The list of bots is sent to the
   * client, so that the user can select which bot they'd like to converse with from a
   * dropdown in the UI.
   * @param botList the list of bots
   */
  private handleBotListUpdated(botList: string[]): void {
    logger.info("Sending updated botlist to user", botList);
    const message: BotChatMessage = {
      author: AuthorType.BOT,
      content: {
        type: ChatMessageContentType.BOT_LIST,
        botList,
      },
    };
    this.ws.send(JSON.stringify(message));
  }

  /**
   * Handler for when the bot sent an utterance. Sends the utterance in text form to the
   * browser through the websocket.
   * @param actionID the ID for the bot action
   * @param transcript the text sent by the bot
   */
  private handleBotStartedUtterance(
    actionID: string,
    transcript: string,
    botName: string | null
  ): void {
    this.sendTextMessageToUser(actionID, transcript, botName);
  }

  /**
   * Handler for when the bot signals its "thinking". Informs the user that the bot is
   * "typing".
   * @param actionID the ID for the action
   */
  private handleBotStartedThinkingIdle(actionID: string): void {
    logger.info("Received bot thinking/idle");
    this.sendTypingMessageToUser(actionID);
  }

  /**
   * Handler for when the bot sends a gesture. The handler tries to translate the gesture
   * into an emoji, and sends this emoji to the user through the websocket.
   * @param actionID ID for the action
   * @param gesture text description for the gesture (e.g. "wave hands")
   */
  private async handleBotStartedGesture(
    actionID: string,
    gesture: string,
    botName: string | null
  ): Promise<void> {
    logger.info("Received gesture %s from bot", gesture);

    const emoji = await this.emojiFinder?.findEmoji(gesture);
    if (!emoji) {
      logger.warn(
        'Could not translate gesture "%s" into an emoji. Ignoring',
        gesture
      );
      return;
    }

    logger.info("Found emoji %s for gesture", emoji.emoji, gesture);
    this.sendEmojiMessageToUser(actionID, emoji.emoji, gesture, botName);
  }

  /**
   * Handler for when the bot sends an audio chunk. The audio chunk is immediately sent
   * to the client.
   * @param audio the audio chunk
   */
  private handleBotSentAudio(audio: Uint8Array): void {
    logger.info("Received audio chunk (length=%s) from bot", audio.length);
    this.sendAudioChunkToUser(audio);
  }

  /**
   * Handler for when ASR data is available (ie. how the bot converts the speech to
   * text). This information is sent to the user through the websocket, so that the UI
   * can show what the bot thinks the user is saying.
   * @param text the bot's best guess of what the user is currently saying
   * @param messageID the ID of the message, if updating an existing utterance
   */
  private handleASRAvailable(text: string, messageID: string): void {
    logger.info("Received ASR. Sending to user", text, messageID);
    this.sendASRToUser(text, messageID);
  }

  /**
   * Handler for when a user barge-in is detected. This is when the user interrupts
   * the bot while it's speaking. When this happens, the UI should immediately interrupt
   * the bot's audio speech for a snappy experience.
   */
  private handleUserBargeIn(): void {
    logger.info(
      "Detected user barge-in. Informing UI so that it can interrupt its audio buffer"
    );
    this.sendUserBargeInToUser();
  }

  /**
   * Handles a server config change. For example, when the server wants to advertise that
   * it supports speech mode. the config is immediately sent to the client.
   * @param serverConfig
   */
  private handleServerConfigChange(serverConfig: ServerConfig): void {
    logger.info("Server config has changed. Sending new config to user");
    this.sendServerConfigToUser(serverConfig);
  }

  /**
   * Handler for when the websocket is closed. This typically happens when the user closes
   * their browser tab. This emits a `userClosedSocket` event on the shared event emitter,
   * allowing the chat-session to close all tasks gracefully.
   * @param code the error code
   */
  private handleWebSocketClosed(code: number): void {
    logger.info("Websocket was closed with code", code);
    this.eventBus.emit("userClosedSocket");
  }

  /**
   * Handler for errors received through the websocket. Logs the error, nothing more.
   * @param error
   */
  private handleWebSocketError(error: Error): void {
    logger.error("received error from websocket", error);
  }

  /**
   * Handles the session shutdown signal. Informs the user of the reason for the shutdown
   * @param reason
   */
  private handleShutdown(reason: string): void {
    logger.info(
      "Session shutdown signal received. Informing user. Reason:",
      reason
    );
    this.sendShutdownSignalToUser(reason);
  }

  /**
   * Sends a text message to the user using the websocket to their browser.
   * @param messageID the ID for the message
   * @param text the text of the message
   * @param botName when mutliple bots run in parallel, set the name of the bot that sent
   *                this message
   */
  private sendTextMessageToUser(
    messageID: MessageID,
    text: string,
    botName: string | null
  ): void {
    logger.info("Sending text to user:", text);
    const message: BotChatMessage = {
      author: AuthorType.BOT,
      content: {
        type: ChatMessageContentType.TEXT,
        messageID,
        text,
        botName,
      },
    };
    this.ws.send(JSON.stringify(message));
  }

  /**
   * sends a "system" message. System messages are used to communicate information to the
   * user, such as the state of the session.
   * @param serverConfig the new server config
   */
  private sendServerConfigToUser(serverConfig: ServerConfig): void {
    const message: SystemConfigMessage = {
      author: AuthorType.SYSTEM,
      content: serverConfig,
    };
    this.ws.send(JSON.stringify(message));
  }

  /**
   * Informs the user that the session is being shutdown
   * @param reason
   */
  private sendShutdownSignalToUser(reason: string): void {
    const message: SystemShutdownMessage = {
      author: AuthorType.SYSTEM,
      content: {
        type: SystemMessageContent.SHUTDOWN,
        reason,
      },
    };
    this.ws.send(JSON.stringify(message));
  }

  /**
   * Sends an emoji message to the user.
   * @param messageID ID for the message
   * @param emoji the emoji in unicode character
   * @param title a string representing the emoji
   * @param botName when multiple bots are running in parallel, set the name of the bot
   *                that sent the emoji
   */
  private sendEmojiMessageToUser(
    messageID: MessageID,
    emoji: string,
    title: string,
    botName: string | null
  ): void {
    logger.info("Sending emoji user", emoji, title);
    const message: BotChatMessage = {
      author: AuthorType.BOT,
      content: {
        type: ChatMessageContentType.EMOJI,
        messageID,
        emoji,
        title,
        botName,
      },
    };
    this.ws.send(JSON.stringify(message));
  }

  /**
   * Sends a "typing" message to the user, to inform that the bot is preparing a reply.
   * @param actionUID the ID of the action associated with the message
   */
  private sendTypingMessageToUser(actionUID: string): void {
    logger.info('Sending "typing" signal to user');
    const message: BotChatMessage = {
      author: AuthorType.BOT,
      content: {
        type: ChatMessageContentType.TYPING,
        messageID: actionUID,
        text: "",
        isNewMessage: true,
      },
    };
    this.ws.send(JSON.stringify(message));
  }

  /**
   * Sends a chunk of audio to the user's browser.
   * @param buffer the audio content in binary form
   */
  private sendAudioChunkToUser(buffer: Uint8Array): void {
    this.ws.send(buffer);
  }

  /**
   * Sends the latest ASR for the user's speech. It is the text that the bot believes is
   * the most probable utterance that the user is saying.
   * @param transcript what the bot thinks the user is saying
   * @param messageID the ID of the message being updated
   */
  private sendASRToUser(transcript: string, messageID: string): void {
    const data: BotChatMessage = {
      author: AuthorType.BOT,
      content: {
        type: ChatMessageContentType.ASR,
        transcript,
        messageID,
      },
    };
    this.ws.send(JSON.stringify(data));
  }

  /**
   * Informs the UI that a barge-in has been detected. A barge-in is when a user
   * interrupts the bot while it's speaking.
   */
  private sendUserBargeInToUser(): void {
    const data: BotChatMessage = {
      author: AuthorType.BOT,
      content: {
        type: ChatMessageContentType.USER_BARGE_IN,
      },
    };
    this.ws.send(JSON.stringify(data));
  }
}
````

## File: microservices/ace_agent/4.1/webui/server/chat-session/logger.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import * as bunyan from "bunyan";

export default function getLogger(name: string) {
  const logger = bunyan.createLogger({ name });
  if (process.env.NODE_ENV === "test") {
    // Hide non-error logs when running unit tests, to avoid noise
    logger.level(bunyan.ERROR);
  }
  return logger;
}
````

## File: microservices/ace_agent/4.1/webui/server/data/emojis-all.json
````json
[
  {
    "emoji": "",
    "description": "grinning face",
    "category": "Smileys & Emotion",
    "aliases": ["grinning"],
    "tags": ["smile", "happy"],
    "unicode_version": "6.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "grinning face with big eyes",
    "category": "Smileys & Emotion",
    "aliases": ["smiley"],
    "tags": ["happy", "joy", "haha"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "grinning face with smiling eyes",
    "category": "Smileys & Emotion",
    "aliases": ["smile"],
    "tags": ["happy", "joy", "laugh", "pleased"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "beaming face with smiling eyes",
    "category": "Smileys & Emotion",
    "aliases": ["grin"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "grinning squinting face",
    "category": "Smileys & Emotion",
    "aliases": ["laughing", "satisfied"],
    "tags": ["happy", "haha"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "grinning face with sweat",
    "category": "Smileys & Emotion",
    "aliases": ["sweat_smile"],
    "tags": ["hot"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "rolling on the floor laughing",
    "category": "Smileys & Emotion",
    "aliases": ["rofl"],
    "tags": ["lol", "laughing"],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "face with tears of joy",
    "category": "Smileys & Emotion",
    "aliases": ["joy"],
    "tags": ["tears"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "slightly smiling face",
    "category": "Smileys & Emotion",
    "aliases": ["slightly_smiling_face"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "upside-down face",
    "category": "Smileys & Emotion",
    "aliases": ["upside_down_face"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "melting face",
    "category": "Smileys & Emotion",
    "aliases": ["melting_face"],
    "tags": ["sarcasm", "dread"],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "winking face",
    "category": "Smileys & Emotion",
    "aliases": ["wink"],
    "tags": ["flirt"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "smiling face with smiling eyes",
    "category": "Smileys & Emotion",
    "aliases": ["blush"],
    "tags": ["proud"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "smiling face with halo",
    "category": "Smileys & Emotion",
    "aliases": ["innocent"],
    "tags": ["angel"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "smiling face with hearts",
    "category": "Smileys & Emotion",
    "aliases": ["smiling_face_with_three_hearts"],
    "tags": ["love"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "smiling face with heart-eyes",
    "category": "Smileys & Emotion",
    "aliases": ["heart_eyes"],
    "tags": ["love", "crush"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "star-struck",
    "category": "Smileys & Emotion",
    "aliases": ["star_struck"],
    "tags": ["eyes"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "face blowing a kiss",
    "category": "Smileys & Emotion",
    "aliases": ["kissing_heart"],
    "tags": ["flirt"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "kissing face",
    "category": "Smileys & Emotion",
    "aliases": ["kissing"],
    "tags": [],
    "unicode_version": "6.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "smiling face",
    "category": "Smileys & Emotion",
    "aliases": ["relaxed"],
    "tags": ["blush", "pleased"],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "kissing face with closed eyes",
    "category": "Smileys & Emotion",
    "aliases": ["kissing_closed_eyes"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "kissing face with smiling eyes",
    "category": "Smileys & Emotion",
    "aliases": ["kissing_smiling_eyes"],
    "tags": [],
    "unicode_version": "6.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "smiling face with tear",
    "category": "Smileys & Emotion",
    "aliases": ["smiling_face_with_tear"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "face savoring food",
    "category": "Smileys & Emotion",
    "aliases": ["yum"],
    "tags": ["tongue", "lick"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "face with tongue",
    "category": "Smileys & Emotion",
    "aliases": ["stuck_out_tongue"],
    "tags": [],
    "unicode_version": "6.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "winking face with tongue",
    "category": "Smileys & Emotion",
    "aliases": ["stuck_out_tongue_winking_eye"],
    "tags": ["prank", "silly"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "zany face",
    "category": "Smileys & Emotion",
    "aliases": ["zany_face"],
    "tags": ["goofy", "wacky"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "squinting face with tongue",
    "category": "Smileys & Emotion",
    "aliases": ["stuck_out_tongue_closed_eyes"],
    "tags": ["prank"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "money-mouth face",
    "category": "Smileys & Emotion",
    "aliases": ["money_mouth_face"],
    "tags": ["rich"],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "smiling face with open hands",
    "category": "Smileys & Emotion",
    "aliases": ["hugs"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "face with hand over mouth",
    "category": "Smileys & Emotion",
    "aliases": ["hand_over_mouth"],
    "tags": ["quiet", "whoops"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "face with open eyes and hand over mouth",
    "category": "Smileys & Emotion",
    "aliases": ["face_with_open_eyes_and_hand_over_mouth"],
    "tags": ["gasp", "shock"],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "face with peeking eye",
    "category": "Smileys & Emotion",
    "aliases": ["face_with_peeking_eye"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "shushing face",
    "category": "Smileys & Emotion",
    "aliases": ["shushing_face"],
    "tags": ["silence", "quiet"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "thinking face",
    "category": "Smileys & Emotion",
    "aliases": ["thinking"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "saluting face",
    "category": "Smileys & Emotion",
    "aliases": ["saluting_face"],
    "tags": ["respect"],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "zipper-mouth face",
    "category": "Smileys & Emotion",
    "aliases": ["zipper_mouth_face"],
    "tags": ["silence", "hush"],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "face with raised eyebrow",
    "category": "Smileys & Emotion",
    "aliases": ["raised_eyebrow"],
    "tags": ["suspicious"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "neutral face",
    "category": "Smileys & Emotion",
    "aliases": ["neutral_face"],
    "tags": ["meh"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "expressionless face",
    "category": "Smileys & Emotion",
    "aliases": ["expressionless"],
    "tags": [],
    "unicode_version": "6.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "face without mouth",
    "category": "Smileys & Emotion",
    "aliases": ["no_mouth"],
    "tags": ["mute", "silence"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "dotted line face",
    "category": "Smileys & Emotion",
    "aliases": ["dotted_line_face"],
    "tags": ["invisible"],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "face in clouds",
    "category": "Smileys & Emotion",
    "aliases": ["face_in_clouds"],
    "tags": [],
    "unicode_version": "13.1",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "smirking face",
    "category": "Smileys & Emotion",
    "aliases": ["smirk"],
    "tags": ["smug"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "unamused face",
    "category": "Smileys & Emotion",
    "aliases": ["unamused"],
    "tags": ["meh"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "face with rolling eyes",
    "category": "Smileys & Emotion",
    "aliases": ["roll_eyes"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "grimacing face",
    "category": "Smileys & Emotion",
    "aliases": ["grimacing"],
    "tags": [],
    "unicode_version": "6.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "face exhaling",
    "category": "Smileys & Emotion",
    "aliases": ["face_exhaling"],
    "tags": [],
    "unicode_version": "13.1",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "lying face",
    "category": "Smileys & Emotion",
    "aliases": ["lying_face"],
    "tags": ["liar"],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "shaking face",
    "category": "Smileys & Emotion",
    "aliases": ["shaking_face"],
    "tags": ["shock"],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "relieved face",
    "category": "Smileys & Emotion",
    "aliases": ["relieved"],
    "tags": ["whew"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "pensive face",
    "category": "Smileys & Emotion",
    "aliases": ["pensive"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sleepy face",
    "category": "Smileys & Emotion",
    "aliases": ["sleepy"],
    "tags": ["tired"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "drooling face",
    "category": "Smileys & Emotion",
    "aliases": ["drooling_face"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "sleeping face",
    "category": "Smileys & Emotion",
    "aliases": ["sleeping"],
    "tags": ["zzz"],
    "unicode_version": "6.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "face with medical mask",
    "category": "Smileys & Emotion",
    "aliases": ["mask"],
    "tags": ["sick", "ill"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "face with thermometer",
    "category": "Smileys & Emotion",
    "aliases": ["face_with_thermometer"],
    "tags": ["sick"],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "face with head-bandage",
    "category": "Smileys & Emotion",
    "aliases": ["face_with_head_bandage"],
    "tags": ["hurt"],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "nauseated face",
    "category": "Smileys & Emotion",
    "aliases": ["nauseated_face"],
    "tags": ["sick", "barf", "disgusted"],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "face vomiting",
    "category": "Smileys & Emotion",
    "aliases": ["vomiting_face"],
    "tags": ["barf", "sick"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "sneezing face",
    "category": "Smileys & Emotion",
    "aliases": ["sneezing_face"],
    "tags": ["achoo", "sick"],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "hot face",
    "category": "Smileys & Emotion",
    "aliases": ["hot_face"],
    "tags": ["heat", "sweating"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "cold face",
    "category": "Smileys & Emotion",
    "aliases": ["cold_face"],
    "tags": ["freezing", "ice"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "woozy face",
    "category": "Smileys & Emotion",
    "aliases": ["woozy_face"],
    "tags": ["groggy"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "face with crossed-out eyes",
    "category": "Smileys & Emotion",
    "aliases": ["dizzy_face"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "face with spiral eyes",
    "category": "Smileys & Emotion",
    "aliases": ["face_with_spiral_eyes"],
    "tags": [],
    "unicode_version": "13.1",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "exploding head",
    "category": "Smileys & Emotion",
    "aliases": ["exploding_head"],
    "tags": ["mind", "blown"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "cowboy hat face",
    "category": "Smileys & Emotion",
    "aliases": ["cowboy_hat_face"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "partying face",
    "category": "Smileys & Emotion",
    "aliases": ["partying_face"],
    "tags": ["celebration", "birthday"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "disguised face",
    "category": "Smileys & Emotion",
    "aliases": ["disguised_face"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "smiling face with sunglasses",
    "category": "Smileys & Emotion",
    "aliases": ["sunglasses"],
    "tags": ["cool"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "nerd face",
    "category": "Smileys & Emotion",
    "aliases": ["nerd_face"],
    "tags": ["geek", "glasses"],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "face with monocle",
    "category": "Smileys & Emotion",
    "aliases": ["monocle_face"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "confused face",
    "category": "Smileys & Emotion",
    "aliases": ["confused"],
    "tags": [],
    "unicode_version": "6.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "face with diagonal mouth",
    "category": "Smileys & Emotion",
    "aliases": ["face_with_diagonal_mouth"],
    "tags": ["confused"],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "worried face",
    "category": "Smileys & Emotion",
    "aliases": ["worried"],
    "tags": ["nervous"],
    "unicode_version": "6.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "slightly frowning face",
    "category": "Smileys & Emotion",
    "aliases": ["slightly_frowning_face"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "frowning face",
    "category": "Smileys & Emotion",
    "aliases": ["frowning_face"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "face with open mouth",
    "category": "Smileys & Emotion",
    "aliases": ["open_mouth"],
    "tags": ["surprise", "impressed", "wow"],
    "unicode_version": "6.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "hushed face",
    "category": "Smileys & Emotion",
    "aliases": ["hushed"],
    "tags": ["silence", "speechless"],
    "unicode_version": "6.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "astonished face",
    "category": "Smileys & Emotion",
    "aliases": ["astonished"],
    "tags": ["amazed", "gasp"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "flushed face",
    "category": "Smileys & Emotion",
    "aliases": ["flushed"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "pleading face",
    "category": "Smileys & Emotion",
    "aliases": ["pleading_face"],
    "tags": ["puppy", "eyes"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "face holding back tears",
    "category": "Smileys & Emotion",
    "aliases": ["face_holding_back_tears"],
    "tags": ["tears", "gratitude"],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "frowning face with open mouth",
    "category": "Smileys & Emotion",
    "aliases": ["frowning"],
    "tags": [],
    "unicode_version": "6.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "anguished face",
    "category": "Smileys & Emotion",
    "aliases": ["anguished"],
    "tags": ["stunned"],
    "unicode_version": "6.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "fearful face",
    "category": "Smileys & Emotion",
    "aliases": ["fearful"],
    "tags": ["scared", "shocked", "oops"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "anxious face with sweat",
    "category": "Smileys & Emotion",
    "aliases": ["cold_sweat"],
    "tags": ["nervous"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sad but relieved face",
    "category": "Smileys & Emotion",
    "aliases": ["disappointed_relieved"],
    "tags": ["phew", "sweat", "nervous"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "crying face",
    "category": "Smileys & Emotion",
    "aliases": ["cry"],
    "tags": ["sad", "tear"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "loudly crying face",
    "category": "Smileys & Emotion",
    "aliases": ["sob"],
    "tags": ["sad", "cry", "bawling"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "face screaming in fear",
    "category": "Smileys & Emotion",
    "aliases": ["scream"],
    "tags": ["horror", "shocked"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "confounded face",
    "category": "Smileys & Emotion",
    "aliases": ["confounded"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "persevering face",
    "category": "Smileys & Emotion",
    "aliases": ["persevere"],
    "tags": ["struggling"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "disappointed face",
    "category": "Smileys & Emotion",
    "aliases": ["disappointed"],
    "tags": ["sad"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "downcast face with sweat",
    "category": "Smileys & Emotion",
    "aliases": ["sweat"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "weary face",
    "category": "Smileys & Emotion",
    "aliases": ["weary"],
    "tags": ["tired"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "tired face",
    "category": "Smileys & Emotion",
    "aliases": ["tired_face"],
    "tags": ["upset", "whine"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "yawning face",
    "category": "Smileys & Emotion",
    "aliases": ["yawning_face"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "face with steam from nose",
    "category": "Smileys & Emotion",
    "aliases": ["triumph"],
    "tags": ["smug"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "enraged face",
    "category": "Smileys & Emotion",
    "aliases": ["rage", "pout"],
    "tags": ["angry"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "angry face",
    "category": "Smileys & Emotion",
    "aliases": ["angry"],
    "tags": ["mad", "annoyed"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "face with symbols on mouth",
    "category": "Smileys & Emotion",
    "aliases": ["cursing_face"],
    "tags": ["foul"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "smiling face with horns",
    "category": "Smileys & Emotion",
    "aliases": ["smiling_imp"],
    "tags": ["devil", "evil", "horns"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "angry face with horns",
    "category": "Smileys & Emotion",
    "aliases": ["imp"],
    "tags": ["angry", "devil", "evil", "horns"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "skull",
    "category": "Smileys & Emotion",
    "aliases": ["skull"],
    "tags": ["dead", "danger", "poison"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "skull and crossbones",
    "category": "Smileys & Emotion",
    "aliases": ["skull_and_crossbones"],
    "tags": ["danger", "pirate"],
    "unicode_version": "",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "pile of poo",
    "category": "Smileys & Emotion",
    "aliases": ["hankey", "poop", "shit"],
    "tags": ["crap"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "clown face",
    "category": "Smileys & Emotion",
    "aliases": ["clown_face"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "ogre",
    "category": "Smileys & Emotion",
    "aliases": ["japanese_ogre"],
    "tags": ["monster"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "goblin",
    "category": "Smileys & Emotion",
    "aliases": ["japanese_goblin"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ghost",
    "category": "Smileys & Emotion",
    "aliases": ["ghost"],
    "tags": ["halloween"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "alien",
    "category": "Smileys & Emotion",
    "aliases": ["alien"],
    "tags": ["ufo"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "alien monster",
    "category": "Smileys & Emotion",
    "aliases": ["space_invader"],
    "tags": ["game", "retro"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "robot",
    "category": "Smileys & Emotion",
    "aliases": ["robot"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "grinning cat",
    "category": "Smileys & Emotion",
    "aliases": ["smiley_cat"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "grinning cat with smiling eyes",
    "category": "Smileys & Emotion",
    "aliases": ["smile_cat"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cat with tears of joy",
    "category": "Smileys & Emotion",
    "aliases": ["joy_cat"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "smiling cat with heart-eyes",
    "category": "Smileys & Emotion",
    "aliases": ["heart_eyes_cat"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cat with wry smile",
    "category": "Smileys & Emotion",
    "aliases": ["smirk_cat"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "kissing cat",
    "category": "Smileys & Emotion",
    "aliases": ["kissing_cat"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "weary cat",
    "category": "Smileys & Emotion",
    "aliases": ["scream_cat"],
    "tags": ["horror"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "crying cat",
    "category": "Smileys & Emotion",
    "aliases": ["crying_cat_face"],
    "tags": ["sad", "tear"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "pouting cat",
    "category": "Smileys & Emotion",
    "aliases": ["pouting_cat"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "see-no-evil monkey",
    "category": "Smileys & Emotion",
    "aliases": ["see_no_evil"],
    "tags": ["monkey", "blind", "ignore"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "hear-no-evil monkey",
    "category": "Smileys & Emotion",
    "aliases": ["hear_no_evil"],
    "tags": ["monkey", "deaf"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "speak-no-evil monkey",
    "category": "Smileys & Emotion",
    "aliases": ["speak_no_evil"],
    "tags": ["monkey", "mute", "hush"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "love letter",
    "category": "Smileys & Emotion",
    "aliases": ["love_letter"],
    "tags": ["email", "envelope"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "heart with arrow",
    "category": "Smileys & Emotion",
    "aliases": ["cupid"],
    "tags": ["love", "heart"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "heart with ribbon",
    "category": "Smileys & Emotion",
    "aliases": ["gift_heart"],
    "tags": ["chocolates"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sparkling heart",
    "category": "Smileys & Emotion",
    "aliases": ["sparkling_heart"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "growing heart",
    "category": "Smileys & Emotion",
    "aliases": ["heartpulse"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "beating heart",
    "category": "Smileys & Emotion",
    "aliases": ["heartbeat"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "revolving hearts",
    "category": "Smileys & Emotion",
    "aliases": ["revolving_hearts"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "two hearts",
    "category": "Smileys & Emotion",
    "aliases": ["two_hearts"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "heart decoration",
    "category": "Smileys & Emotion",
    "aliases": ["heart_decoration"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "heart exclamation",
    "category": "Smileys & Emotion",
    "aliases": ["heavy_heart_exclamation"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "broken heart",
    "category": "Smileys & Emotion",
    "aliases": ["broken_heart"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "heart on fire",
    "category": "Smileys & Emotion",
    "aliases": ["heart_on_fire"],
    "tags": [],
    "unicode_version": "13.1",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "mending heart",
    "category": "Smileys & Emotion",
    "aliases": ["mending_heart"],
    "tags": [],
    "unicode_version": "13.1",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "red heart",
    "category": "Smileys & Emotion",
    "aliases": ["heart"],
    "tags": ["love"],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "pink heart",
    "category": "Smileys & Emotion",
    "aliases": ["pink_heart"],
    "tags": [],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "orange heart",
    "category": "Smileys & Emotion",
    "aliases": ["orange_heart"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "yellow heart",
    "category": "Smileys & Emotion",
    "aliases": ["yellow_heart"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "green heart",
    "category": "Smileys & Emotion",
    "aliases": ["green_heart"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "blue heart",
    "category": "Smileys & Emotion",
    "aliases": ["blue_heart"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "light blue heart",
    "category": "Smileys & Emotion",
    "aliases": ["light_blue_heart"],
    "tags": [],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "purple heart",
    "category": "Smileys & Emotion",
    "aliases": ["purple_heart"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "brown heart",
    "category": "Smileys & Emotion",
    "aliases": ["brown_heart"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "black heart",
    "category": "Smileys & Emotion",
    "aliases": ["black_heart"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "grey heart",
    "category": "Smileys & Emotion",
    "aliases": ["grey_heart"],
    "tags": [],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "white heart",
    "category": "Smileys & Emotion",
    "aliases": ["white_heart"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "kiss mark",
    "category": "Smileys & Emotion",
    "aliases": ["kiss"],
    "tags": ["lipstick"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "hundred points",
    "category": "Smileys & Emotion",
    "aliases": ["100"],
    "tags": ["score", "perfect"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "anger symbol",
    "category": "Smileys & Emotion",
    "aliases": ["anger"],
    "tags": ["angry"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "collision",
    "category": "Smileys & Emotion",
    "aliases": ["boom", "collision"],
    "tags": ["explode"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "dizzy",
    "category": "Smileys & Emotion",
    "aliases": ["dizzy"],
    "tags": ["star"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sweat droplets",
    "category": "Smileys & Emotion",
    "aliases": ["sweat_drops"],
    "tags": ["water", "workout"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "dashing away",
    "category": "Smileys & Emotion",
    "aliases": ["dash"],
    "tags": ["wind", "blow", "fast"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "hole",
    "category": "Smileys & Emotion",
    "aliases": ["hole"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "speech balloon",
    "category": "Smileys & Emotion",
    "aliases": ["speech_balloon"],
    "tags": ["comment"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "eye in speech bubble",
    "category": "Smileys & Emotion",
    "aliases": ["eye_speech_bubble"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "left speech bubble",
    "category": "Smileys & Emotion",
    "aliases": ["left_speech_bubble"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "right anger bubble",
    "category": "Smileys & Emotion",
    "aliases": ["right_anger_bubble"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "thought balloon",
    "category": "Smileys & Emotion",
    "aliases": ["thought_balloon"],
    "tags": ["thinking"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ZZZ",
    "category": "Smileys & Emotion",
    "aliases": ["zzz"],
    "tags": ["sleeping"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "waving hand",
    "category": "People & Body",
    "aliases": ["wave"],
    "tags": ["goodbye"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "raised back of hand",
    "category": "People & Body",
    "aliases": ["raised_back_of_hand"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "hand with fingers splayed",
    "category": "People & Body",
    "aliases": ["raised_hand_with_fingers_splayed"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "raised hand",
    "category": "People & Body",
    "aliases": ["hand", "raised_hand"],
    "tags": ["highfive", "stop"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "vulcan salute",
    "category": "People & Body",
    "aliases": ["vulcan_salute"],
    "tags": ["prosper", "spock"],
    "unicode_version": "7.0",
    "ios_version": "8.3",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "rightwards hand",
    "category": "People & Body",
    "aliases": ["rightwards_hand"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "leftwards hand",
    "category": "People & Body",
    "aliases": ["leftwards_hand"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "palm down hand",
    "category": "People & Body",
    "aliases": ["palm_down_hand"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "palm up hand",
    "category": "People & Body",
    "aliases": ["palm_up_hand"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "leftwards pushing hand",
    "category": "People & Body",
    "aliases": ["leftwards_pushing_hand"],
    "tags": [],
    "unicode_version": "15.0",
    "ios_version": "16.4",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "rightwards pushing hand",
    "category": "People & Body",
    "aliases": ["rightwards_pushing_hand"],
    "tags": [],
    "unicode_version": "15.0",
    "ios_version": "16.4",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "OK hand",
    "category": "People & Body",
    "aliases": ["ok_hand"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "pinched fingers",
    "category": "People & Body",
    "aliases": ["pinched_fingers"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "pinching hand",
    "category": "People & Body",
    "aliases": ["pinching_hand"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "victory hand",
    "category": "People & Body",
    "aliases": ["v"],
    "tags": ["victory", "peace"],
    "unicode_version": "",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "crossed fingers",
    "category": "People & Body",
    "aliases": ["crossed_fingers"],
    "tags": ["luck", "hopeful"],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "hand with index finger and thumb crossed",
    "category": "People & Body",
    "aliases": ["hand_with_index_finger_and_thumb_crossed"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "love-you gesture",
    "category": "People & Body",
    "aliases": ["love_you_gesture"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "sign of the horns",
    "category": "People & Body",
    "aliases": ["metal"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "call me hand",
    "category": "People & Body",
    "aliases": ["call_me_hand"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "backhand index pointing left",
    "category": "People & Body",
    "aliases": ["point_left"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "backhand index pointing right",
    "category": "People & Body",
    "aliases": ["point_right"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "backhand index pointing up",
    "category": "People & Body",
    "aliases": ["point_up_2"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "backhand index pointing down",
    "category": "People & Body",
    "aliases": ["point_down"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "index pointing up",
    "category": "People & Body",
    "aliases": ["point_up"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "index pointing at the viewer",
    "category": "People & Body",
    "aliases": ["index_pointing_at_the_viewer"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "thumbs up",
    "category": "People & Body",
    "aliases": ["+1", "thumbsup"],
    "tags": ["approve", "ok"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "thumbs down",
    "category": "People & Body",
    "aliases": ["-1", "thumbsdown"],
    "tags": ["disapprove", "bury"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "raised fist",
    "category": "People & Body",
    "aliases": ["fist_raised", "fist"],
    "tags": ["power"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "oncoming fist",
    "category": "People & Body",
    "aliases": ["fist_oncoming", "facepunch", "punch"],
    "tags": ["attack"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "left-facing fist",
    "category": "People & Body",
    "aliases": ["fist_left"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "right-facing fist",
    "category": "People & Body",
    "aliases": ["fist_right"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "clapping hands",
    "category": "People & Body",
    "aliases": ["clap"],
    "tags": ["praise", "applause"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "raising hands",
    "category": "People & Body",
    "aliases": ["raised_hands"],
    "tags": ["hooray"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "heart hands",
    "category": "People & Body",
    "aliases": ["heart_hands"],
    "tags": ["love"],
    "unicode_version": "14.0",
    "ios_version": "15.4",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "open hands",
    "category": "People & Body",
    "aliases": ["open_hands"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "palms up together",
    "category": "People & Body",
    "aliases": ["palms_up_together"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "handshake",
    "category": "People & Body",
    "aliases": ["handshake"],
    "tags": ["deal"],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "folded hands",
    "category": "People & Body",
    "aliases": ["pray"],
    "tags": ["please", "hope", "wish"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "writing hand",
    "category": "People & Body",
    "aliases": ["writing_hand"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "9.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "nail polish",
    "category": "People & Body",
    "aliases": ["nail_care"],
    "tags": ["beauty", "manicure"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "selfie",
    "category": "People & Body",
    "aliases": ["selfie"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "flexed biceps",
    "category": "People & Body",
    "aliases": ["muscle"],
    "tags": ["flex", "bicep", "strong", "workout"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "mechanical arm",
    "category": "People & Body",
    "aliases": ["mechanical_arm"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "mechanical leg",
    "category": "People & Body",
    "aliases": ["mechanical_leg"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "leg",
    "category": "People & Body",
    "aliases": ["leg"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "foot",
    "category": "People & Body",
    "aliases": ["foot"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "ear",
    "category": "People & Body",
    "aliases": ["ear"],
    "tags": ["hear", "sound", "listen"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "ear with hearing aid",
    "category": "People & Body",
    "aliases": ["ear_with_hearing_aid"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "nose",
    "category": "People & Body",
    "aliases": ["nose"],
    "tags": ["smell"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "brain",
    "category": "People & Body",
    "aliases": ["brain"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "anatomical heart",
    "category": "People & Body",
    "aliases": ["anatomical_heart"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "lungs",
    "category": "People & Body",
    "aliases": ["lungs"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "tooth",
    "category": "People & Body",
    "aliases": ["tooth"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "bone",
    "category": "People & Body",
    "aliases": ["bone"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "eyes",
    "category": "People & Body",
    "aliases": ["eyes"],
    "tags": ["look", "see", "watch"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "eye",
    "category": "People & Body",
    "aliases": ["eye"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "tongue",
    "category": "People & Body",
    "aliases": ["tongue"],
    "tags": ["taste"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "mouth",
    "category": "People & Body",
    "aliases": ["lips"],
    "tags": ["kiss"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "biting lip",
    "category": "People & Body",
    "aliases": ["biting_lip"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "baby",
    "category": "People & Body",
    "aliases": ["baby"],
    "tags": ["child", "newborn"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "child",
    "category": "People & Body",
    "aliases": ["child"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "boy",
    "category": "People & Body",
    "aliases": ["boy"],
    "tags": ["child"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "girl",
    "category": "People & Body",
    "aliases": ["girl"],
    "tags": ["child"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person",
    "category": "People & Body",
    "aliases": ["adult"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person: blond hair",
    "category": "People & Body",
    "aliases": ["blond_haired_person"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man",
    "category": "People & Body",
    "aliases": ["man"],
    "tags": ["mustache", "father", "dad"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person: beard",
    "category": "People & Body",
    "aliases": ["bearded_person"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man: beard",
    "category": "People & Body",
    "aliases": ["man_beard"],
    "tags": [],
    "unicode_version": "13.1",
    "ios_version": "14.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman: beard",
    "category": "People & Body",
    "aliases": ["woman_beard"],
    "tags": [],
    "unicode_version": "13.1",
    "ios_version": "14.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man: red hair",
    "category": "People & Body",
    "aliases": ["red_haired_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man: curly hair",
    "category": "People & Body",
    "aliases": ["curly_haired_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man: white hair",
    "category": "People & Body",
    "aliases": ["white_haired_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man: bald",
    "category": "People & Body",
    "aliases": ["bald_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman",
    "category": "People & Body",
    "aliases": ["woman"],
    "tags": ["girls"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman: red hair",
    "category": "People & Body",
    "aliases": ["red_haired_woman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person: red hair",
    "category": "People & Body",
    "aliases": ["person_red_hair"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman: curly hair",
    "category": "People & Body",
    "aliases": ["curly_haired_woman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person: curly hair",
    "category": "People & Body",
    "aliases": ["person_curly_hair"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman: white hair",
    "category": "People & Body",
    "aliases": ["white_haired_woman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person: white hair",
    "category": "People & Body",
    "aliases": ["person_white_hair"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman: bald",
    "category": "People & Body",
    "aliases": ["bald_woman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person: bald",
    "category": "People & Body",
    "aliases": ["person_bald"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman: blond hair",
    "category": "People & Body",
    "aliases": ["blond_haired_woman", "blonde_woman"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man: blond hair",
    "category": "People & Body",
    "aliases": ["blond_haired_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "older person",
    "category": "People & Body",
    "aliases": ["older_adult"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "old man",
    "category": "People & Body",
    "aliases": ["older_man"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "old woman",
    "category": "People & Body",
    "aliases": ["older_woman"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person frowning",
    "category": "People & Body",
    "aliases": ["frowning_person"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man frowning",
    "category": "People & Body",
    "aliases": ["frowning_man"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman frowning",
    "category": "People & Body",
    "aliases": ["frowning_woman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person pouting",
    "category": "People & Body",
    "aliases": ["pouting_face"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man pouting",
    "category": "People & Body",
    "aliases": ["pouting_man"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman pouting",
    "category": "People & Body",
    "aliases": ["pouting_woman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person gesturing NO",
    "category": "People & Body",
    "aliases": ["no_good"],
    "tags": ["stop", "halt", "denied"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man gesturing NO",
    "category": "People & Body",
    "aliases": ["no_good_man", "ng_man"],
    "tags": ["stop", "halt", "denied"],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman gesturing NO",
    "category": "People & Body",
    "aliases": ["no_good_woman", "ng_woman"],
    "tags": ["stop", "halt", "denied"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person gesturing OK",
    "category": "People & Body",
    "aliases": ["ok_person"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man gesturing OK",
    "category": "People & Body",
    "aliases": ["ok_man"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman gesturing OK",
    "category": "People & Body",
    "aliases": ["ok_woman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person tipping hand",
    "category": "People & Body",
    "aliases": ["tipping_hand_person", "information_desk_person"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man tipping hand",
    "category": "People & Body",
    "aliases": ["tipping_hand_man", "sassy_man"],
    "tags": ["information"],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman tipping hand",
    "category": "People & Body",
    "aliases": ["tipping_hand_woman", "sassy_woman"],
    "tags": ["information"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person raising hand",
    "category": "People & Body",
    "aliases": ["raising_hand"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man raising hand",
    "category": "People & Body",
    "aliases": ["raising_hand_man"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman raising hand",
    "category": "People & Body",
    "aliases": ["raising_hand_woman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "deaf person",
    "category": "People & Body",
    "aliases": ["deaf_person"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "deaf man",
    "category": "People & Body",
    "aliases": ["deaf_man"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "deaf woman",
    "category": "People & Body",
    "aliases": ["deaf_woman"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person bowing",
    "category": "People & Body",
    "aliases": ["bow"],
    "tags": ["respect", "thanks"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man bowing",
    "category": "People & Body",
    "aliases": ["bowing_man"],
    "tags": ["respect", "thanks"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman bowing",
    "category": "People & Body",
    "aliases": ["bowing_woman"],
    "tags": ["respect", "thanks"],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person facepalming",
    "category": "People & Body",
    "aliases": ["facepalm"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man facepalming",
    "category": "People & Body",
    "aliases": ["man_facepalming"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman facepalming",
    "category": "People & Body",
    "aliases": ["woman_facepalming"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person shrugging",
    "category": "People & Body",
    "aliases": ["shrug"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man shrugging",
    "category": "People & Body",
    "aliases": ["man_shrugging"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman shrugging",
    "category": "People & Body",
    "aliases": ["woman_shrugging"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "health worker",
    "category": "People & Body",
    "aliases": ["health_worker"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man health worker",
    "category": "People & Body",
    "aliases": ["man_health_worker"],
    "tags": ["doctor", "nurse"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman health worker",
    "category": "People & Body",
    "aliases": ["woman_health_worker"],
    "tags": ["doctor", "nurse"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "student",
    "category": "People & Body",
    "aliases": ["student"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man student",
    "category": "People & Body",
    "aliases": ["man_student"],
    "tags": ["graduation"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman student",
    "category": "People & Body",
    "aliases": ["woman_student"],
    "tags": ["graduation"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "teacher",
    "category": "People & Body",
    "aliases": ["teacher"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man teacher",
    "category": "People & Body",
    "aliases": ["man_teacher"],
    "tags": ["school", "professor"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman teacher",
    "category": "People & Body",
    "aliases": ["woman_teacher"],
    "tags": ["school", "professor"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "judge",
    "category": "People & Body",
    "aliases": ["judge"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man judge",
    "category": "People & Body",
    "aliases": ["man_judge"],
    "tags": ["justice"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman judge",
    "category": "People & Body",
    "aliases": ["woman_judge"],
    "tags": ["justice"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "farmer",
    "category": "People & Body",
    "aliases": ["farmer"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man farmer",
    "category": "People & Body",
    "aliases": ["man_farmer"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman farmer",
    "category": "People & Body",
    "aliases": ["woman_farmer"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "cook",
    "category": "People & Body",
    "aliases": ["cook"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man cook",
    "category": "People & Body",
    "aliases": ["man_cook"],
    "tags": ["chef"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman cook",
    "category": "People & Body",
    "aliases": ["woman_cook"],
    "tags": ["chef"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "mechanic",
    "category": "People & Body",
    "aliases": ["mechanic"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man mechanic",
    "category": "People & Body",
    "aliases": ["man_mechanic"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman mechanic",
    "category": "People & Body",
    "aliases": ["woman_mechanic"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "factory worker",
    "category": "People & Body",
    "aliases": ["factory_worker"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man factory worker",
    "category": "People & Body",
    "aliases": ["man_factory_worker"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman factory worker",
    "category": "People & Body",
    "aliases": ["woman_factory_worker"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "office worker",
    "category": "People & Body",
    "aliases": ["office_worker"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man office worker",
    "category": "People & Body",
    "aliases": ["man_office_worker"],
    "tags": ["business"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman office worker",
    "category": "People & Body",
    "aliases": ["woman_office_worker"],
    "tags": ["business"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "scientist",
    "category": "People & Body",
    "aliases": ["scientist"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man scientist",
    "category": "People & Body",
    "aliases": ["man_scientist"],
    "tags": ["research"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman scientist",
    "category": "People & Body",
    "aliases": ["woman_scientist"],
    "tags": ["research"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "technologist",
    "category": "People & Body",
    "aliases": ["technologist"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man technologist",
    "category": "People & Body",
    "aliases": ["man_technologist"],
    "tags": ["coder"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman technologist",
    "category": "People & Body",
    "aliases": ["woman_technologist"],
    "tags": ["coder"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "singer",
    "category": "People & Body",
    "aliases": ["singer"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man singer",
    "category": "People & Body",
    "aliases": ["man_singer"],
    "tags": ["rockstar"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman singer",
    "category": "People & Body",
    "aliases": ["woman_singer"],
    "tags": ["rockstar"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "artist",
    "category": "People & Body",
    "aliases": ["artist"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man artist",
    "category": "People & Body",
    "aliases": ["man_artist"],
    "tags": ["painter"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman artist",
    "category": "People & Body",
    "aliases": ["woman_artist"],
    "tags": ["painter"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "pilot",
    "category": "People & Body",
    "aliases": ["pilot"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man pilot",
    "category": "People & Body",
    "aliases": ["man_pilot"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman pilot",
    "category": "People & Body",
    "aliases": ["woman_pilot"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "astronaut",
    "category": "People & Body",
    "aliases": ["astronaut"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man astronaut",
    "category": "People & Body",
    "aliases": ["man_astronaut"],
    "tags": ["space"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman astronaut",
    "category": "People & Body",
    "aliases": ["woman_astronaut"],
    "tags": ["space"],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "firefighter",
    "category": "People & Body",
    "aliases": ["firefighter"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man firefighter",
    "category": "People & Body",
    "aliases": ["man_firefighter"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman firefighter",
    "category": "People & Body",
    "aliases": ["woman_firefighter"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "police officer",
    "category": "People & Body",
    "aliases": ["police_officer", "cop"],
    "tags": ["law"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man police officer",
    "category": "People & Body",
    "aliases": ["policeman"],
    "tags": ["law", "cop"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman police officer",
    "category": "People & Body",
    "aliases": ["policewoman"],
    "tags": ["law", "cop"],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "detective",
    "category": "People & Body",
    "aliases": ["detective"],
    "tags": ["sleuth"],
    "unicode_version": "7.0",
    "ios_version": "9.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man detective",
    "category": "People & Body",
    "aliases": ["male_detective"],
    "tags": ["sleuth"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman detective",
    "category": "People & Body",
    "aliases": ["female_detective"],
    "tags": ["sleuth"],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "guard",
    "category": "People & Body",
    "aliases": ["guard"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man guard",
    "category": "People & Body",
    "aliases": ["guardsman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman guard",
    "category": "People & Body",
    "aliases": ["guardswoman"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "ninja",
    "category": "People & Body",
    "aliases": ["ninja"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "construction worker",
    "category": "People & Body",
    "aliases": ["construction_worker"],
    "tags": ["helmet"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man construction worker",
    "category": "People & Body",
    "aliases": ["construction_worker_man"],
    "tags": ["helmet"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman construction worker",
    "category": "People & Body",
    "aliases": ["construction_worker_woman"],
    "tags": ["helmet"],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person with crown",
    "category": "People & Body",
    "aliases": ["person_with_crown"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "prince",
    "category": "People & Body",
    "aliases": ["prince"],
    "tags": ["crown", "royal"],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "princess",
    "category": "People & Body",
    "aliases": ["princess"],
    "tags": ["crown", "royal"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person wearing turban",
    "category": "People & Body",
    "aliases": ["person_with_turban"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man wearing turban",
    "category": "People & Body",
    "aliases": ["man_with_turban"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman wearing turban",
    "category": "People & Body",
    "aliases": ["woman_with_turban"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person with skullcap",
    "category": "People & Body",
    "aliases": ["man_with_gua_pi_mao"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman with headscarf",
    "category": "People & Body",
    "aliases": ["woman_with_headscarf"],
    "tags": ["hijab"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person in tuxedo",
    "category": "People & Body",
    "aliases": ["person_in_tuxedo"],
    "tags": ["groom", "marriage", "wedding"],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man in tuxedo",
    "category": "People & Body",
    "aliases": ["man_in_tuxedo"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman in tuxedo",
    "category": "People & Body",
    "aliases": ["woman_in_tuxedo"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person with veil",
    "category": "People & Body",
    "aliases": ["person_with_veil"],
    "tags": ["marriage", "wedding"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man with veil",
    "category": "People & Body",
    "aliases": ["man_with_veil"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman with veil",
    "category": "People & Body",
    "aliases": ["woman_with_veil", "bride_with_veil"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "pregnant woman",
    "category": "People & Body",
    "aliases": ["pregnant_woman"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "pregnant man",
    "category": "People & Body",
    "aliases": ["pregnant_man"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "pregnant person",
    "category": "People & Body",
    "aliases": ["pregnant_person"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "breast-feeding",
    "category": "People & Body",
    "aliases": ["breast_feeding"],
    "tags": ["nursing"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman feeding baby",
    "category": "People & Body",
    "aliases": ["woman_feeding_baby"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man feeding baby",
    "category": "People & Body",
    "aliases": ["man_feeding_baby"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person feeding baby",
    "category": "People & Body",
    "aliases": ["person_feeding_baby"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "baby angel",
    "category": "People & Body",
    "aliases": ["angel"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "Santa Claus",
    "category": "People & Body",
    "aliases": ["santa"],
    "tags": ["christmas"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "Mrs. Claus",
    "category": "People & Body",
    "aliases": ["mrs_claus"],
    "tags": ["santa"],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "mx claus",
    "category": "People & Body",
    "aliases": ["mx_claus"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "superhero",
    "category": "People & Body",
    "aliases": ["superhero"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man superhero",
    "category": "People & Body",
    "aliases": ["superhero_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman superhero",
    "category": "People & Body",
    "aliases": ["superhero_woman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "supervillain",
    "category": "People & Body",
    "aliases": ["supervillain"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man supervillain",
    "category": "People & Body",
    "aliases": ["supervillain_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman supervillain",
    "category": "People & Body",
    "aliases": ["supervillain_woman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "mage",
    "category": "People & Body",
    "aliases": ["mage"],
    "tags": ["wizard"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man mage",
    "category": "People & Body",
    "aliases": ["mage_man"],
    "tags": ["wizard"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman mage",
    "category": "People & Body",
    "aliases": ["mage_woman"],
    "tags": ["wizard"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "fairy",
    "category": "People & Body",
    "aliases": ["fairy"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man fairy",
    "category": "People & Body",
    "aliases": ["fairy_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman fairy",
    "category": "People & Body",
    "aliases": ["fairy_woman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "vampire",
    "category": "People & Body",
    "aliases": ["vampire"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man vampire",
    "category": "People & Body",
    "aliases": ["vampire_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman vampire",
    "category": "People & Body",
    "aliases": ["vampire_woman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "merperson",
    "category": "People & Body",
    "aliases": ["merperson"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "merman",
    "category": "People & Body",
    "aliases": ["merman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "mermaid",
    "category": "People & Body",
    "aliases": ["mermaid"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "elf",
    "category": "People & Body",
    "aliases": ["elf"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man elf",
    "category": "People & Body",
    "aliases": ["elf_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman elf",
    "category": "People & Body",
    "aliases": ["elf_woman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "genie",
    "category": "People & Body",
    "aliases": ["genie"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "man genie",
    "category": "People & Body",
    "aliases": ["genie_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "woman genie",
    "category": "People & Body",
    "aliases": ["genie_woman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "zombie",
    "category": "People & Body",
    "aliases": ["zombie"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "man zombie",
    "category": "People & Body",
    "aliases": ["zombie_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "woman zombie",
    "category": "People & Body",
    "aliases": ["zombie_woman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "troll",
    "category": "People & Body",
    "aliases": ["troll"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "person getting massage",
    "category": "People & Body",
    "aliases": ["massage"],
    "tags": ["spa"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man getting massage",
    "category": "People & Body",
    "aliases": ["massage_man"],
    "tags": ["spa"],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman getting massage",
    "category": "People & Body",
    "aliases": ["massage_woman"],
    "tags": ["spa"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person getting haircut",
    "category": "People & Body",
    "aliases": ["haircut"],
    "tags": ["beauty"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man getting haircut",
    "category": "People & Body",
    "aliases": ["haircut_man"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman getting haircut",
    "category": "People & Body",
    "aliases": ["haircut_woman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person walking",
    "category": "People & Body",
    "aliases": ["walking"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man walking",
    "category": "People & Body",
    "aliases": ["walking_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman walking",
    "category": "People & Body",
    "aliases": ["walking_woman"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person standing",
    "category": "People & Body",
    "aliases": ["standing_person"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man standing",
    "category": "People & Body",
    "aliases": ["standing_man"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman standing",
    "category": "People & Body",
    "aliases": ["standing_woman"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person kneeling",
    "category": "People & Body",
    "aliases": ["kneeling_person"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man kneeling",
    "category": "People & Body",
    "aliases": ["kneeling_man"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman kneeling",
    "category": "People & Body",
    "aliases": ["kneeling_woman"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person with white cane",
    "category": "People & Body",
    "aliases": ["person_with_probing_cane"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man with white cane",
    "category": "People & Body",
    "aliases": ["man_with_probing_cane"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman with white cane",
    "category": "People & Body",
    "aliases": ["woman_with_probing_cane"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person in motorized wheelchair",
    "category": "People & Body",
    "aliases": ["person_in_motorized_wheelchair"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man in motorized wheelchair",
    "category": "People & Body",
    "aliases": ["man_in_motorized_wheelchair"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman in motorized wheelchair",
    "category": "People & Body",
    "aliases": ["woman_in_motorized_wheelchair"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person in manual wheelchair",
    "category": "People & Body",
    "aliases": ["person_in_manual_wheelchair"],
    "tags": [],
    "unicode_version": "12.1",
    "ios_version": "13.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man in manual wheelchair",
    "category": "People & Body",
    "aliases": ["man_in_manual_wheelchair"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman in manual wheelchair",
    "category": "People & Body",
    "aliases": ["woman_in_manual_wheelchair"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person running",
    "category": "People & Body",
    "aliases": ["runner", "running"],
    "tags": ["exercise", "workout", "marathon"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man running",
    "category": "People & Body",
    "aliases": ["running_man"],
    "tags": ["exercise", "workout", "marathon"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman running",
    "category": "People & Body",
    "aliases": ["running_woman"],
    "tags": ["exercise", "workout", "marathon"],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman dancing",
    "category": "People & Body",
    "aliases": ["woman_dancing", "dancer"],
    "tags": ["dress"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man dancing",
    "category": "People & Body",
    "aliases": ["man_dancing"],
    "tags": ["dancer"],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person in suit levitating",
    "category": "People & Body",
    "aliases": ["business_suit_levitating"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "people with bunny ears",
    "category": "People & Body",
    "aliases": ["dancers"],
    "tags": ["bunny"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "men with bunny ears",
    "category": "People & Body",
    "aliases": ["dancing_men"],
    "tags": ["bunny"],
    "unicode_version": "6.0",
    "ios_version": "10.0"
  },
  {
    "emoji": "",
    "description": "women with bunny ears",
    "category": "People & Body",
    "aliases": ["dancing_women"],
    "tags": ["bunny"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "person in steamy room",
    "category": "People & Body",
    "aliases": ["sauna_person"],
    "tags": ["steamy"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man in steamy room",
    "category": "People & Body",
    "aliases": ["sauna_man"],
    "tags": ["steamy"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman in steamy room",
    "category": "People & Body",
    "aliases": ["sauna_woman"],
    "tags": ["steamy"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person climbing",
    "category": "People & Body",
    "aliases": ["climbing"],
    "tags": ["bouldering"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man climbing",
    "category": "People & Body",
    "aliases": ["climbing_man"],
    "tags": ["bouldering"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman climbing",
    "category": "People & Body",
    "aliases": ["climbing_woman"],
    "tags": ["bouldering"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person fencing",
    "category": "People & Body",
    "aliases": ["person_fencing"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "horse racing",
    "category": "People & Body",
    "aliases": ["horse_racing"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "skier",
    "category": "People & Body",
    "aliases": ["skier"],
    "tags": [],
    "unicode_version": "5.2",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "snowboarder",
    "category": "People & Body",
    "aliases": ["snowboarder"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person golfing",
    "category": "People & Body",
    "aliases": ["golfing"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man golfing",
    "category": "People & Body",
    "aliases": ["golfing_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman golfing",
    "category": "People & Body",
    "aliases": ["golfing_woman"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person surfing",
    "category": "People & Body",
    "aliases": ["surfer"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man surfing",
    "category": "People & Body",
    "aliases": ["surfing_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman surfing",
    "category": "People & Body",
    "aliases": ["surfing_woman"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person rowing boat",
    "category": "People & Body",
    "aliases": ["rowboat"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man rowing boat",
    "category": "People & Body",
    "aliases": ["rowing_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman rowing boat",
    "category": "People & Body",
    "aliases": ["rowing_woman"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person swimming",
    "category": "People & Body",
    "aliases": ["swimmer"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man swimming",
    "category": "People & Body",
    "aliases": ["swimming_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman swimming",
    "category": "People & Body",
    "aliases": ["swimming_woman"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person bouncing ball",
    "category": "People & Body",
    "aliases": ["bouncing_ball_person"],
    "tags": ["basketball"],
    "unicode_version": "5.2",
    "ios_version": "9.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man bouncing ball",
    "category": "People & Body",
    "aliases": ["bouncing_ball_man", "basketball_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman bouncing ball",
    "category": "People & Body",
    "aliases": ["bouncing_ball_woman", "basketball_woman"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person lifting weights",
    "category": "People & Body",
    "aliases": ["weight_lifting"],
    "tags": ["gym", "workout"],
    "unicode_version": "7.0",
    "ios_version": "9.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man lifting weights",
    "category": "People & Body",
    "aliases": ["weight_lifting_man"],
    "tags": ["gym", "workout"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman lifting weights",
    "category": "People & Body",
    "aliases": ["weight_lifting_woman"],
    "tags": ["gym", "workout"],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person biking",
    "category": "People & Body",
    "aliases": ["bicyclist"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man biking",
    "category": "People & Body",
    "aliases": ["biking_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman biking",
    "category": "People & Body",
    "aliases": ["biking_woman"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person mountain biking",
    "category": "People & Body",
    "aliases": ["mountain_bicyclist"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man mountain biking",
    "category": "People & Body",
    "aliases": ["mountain_biking_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman mountain biking",
    "category": "People & Body",
    "aliases": ["mountain_biking_woman"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person cartwheeling",
    "category": "People & Body",
    "aliases": ["cartwheeling"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man cartwheeling",
    "category": "People & Body",
    "aliases": ["man_cartwheeling"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman cartwheeling",
    "category": "People & Body",
    "aliases": ["woman_cartwheeling"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "people wrestling",
    "category": "People & Body",
    "aliases": ["wrestling"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "men wrestling",
    "category": "People & Body",
    "aliases": ["men_wrestling"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "women wrestling",
    "category": "People & Body",
    "aliases": ["women_wrestling"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "person playing water polo",
    "category": "People & Body",
    "aliases": ["water_polo"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man playing water polo",
    "category": "People & Body",
    "aliases": ["man_playing_water_polo"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman playing water polo",
    "category": "People & Body",
    "aliases": ["woman_playing_water_polo"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person playing handball",
    "category": "People & Body",
    "aliases": ["handball_person"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man playing handball",
    "category": "People & Body",
    "aliases": ["man_playing_handball"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman playing handball",
    "category": "People & Body",
    "aliases": ["woman_playing_handball"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person juggling",
    "category": "People & Body",
    "aliases": ["juggling_person"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man juggling",
    "category": "People & Body",
    "aliases": ["man_juggling"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman juggling",
    "category": "People & Body",
    "aliases": ["woman_juggling"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person in lotus position",
    "category": "People & Body",
    "aliases": ["lotus_position"],
    "tags": ["meditation"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "man in lotus position",
    "category": "People & Body",
    "aliases": ["lotus_position_man"],
    "tags": ["meditation"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman in lotus position",
    "category": "People & Body",
    "aliases": ["lotus_position_woman"],
    "tags": ["meditation"],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person taking bath",
    "category": "People & Body",
    "aliases": ["bath"],
    "tags": ["shower"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "person in bed",
    "category": "People & Body",
    "aliases": ["sleeping_bed"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "people holding hands",
    "category": "People & Body",
    "aliases": ["people_holding_hands"],
    "tags": ["couple", "date"],
    "unicode_version": "12.0",
    "ios_version": "13.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "women holding hands",
    "category": "People & Body",
    "aliases": ["two_women_holding_hands"],
    "tags": ["couple", "date"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "woman and man holding hands",
    "category": "People & Body",
    "aliases": ["couple"],
    "tags": ["date"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "men holding hands",
    "category": "People & Body",
    "aliases": ["two_men_holding_hands"],
    "tags": ["couple", "date"],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "kiss",
    "category": "People & Body",
    "aliases": ["couplekiss"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "kiss: woman, man",
    "category": "People & Body",
    "aliases": ["couplekiss_man_woman"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "kiss: man, man",
    "category": "People & Body",
    "aliases": ["couplekiss_man_man"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "kiss: woman, woman",
    "category": "People & Body",
    "aliases": ["couplekiss_woman_woman"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "couple with heart",
    "category": "People & Body",
    "aliases": ["couple_with_heart"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "couple with heart: woman, man",
    "category": "People & Body",
    "aliases": ["couple_with_heart_woman_man"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "couple with heart: man, man",
    "category": "People & Body",
    "aliases": ["couple_with_heart_man_man"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "couple with heart: woman, woman",
    "category": "People & Body",
    "aliases": ["couple_with_heart_woman_woman"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3",
    "skin_tones": true
  },
  {
    "emoji": "",
    "description": "family",
    "category": "People & Body",
    "aliases": ["family"],
    "tags": ["home", "parents", "child"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "family: man, woman, boy",
    "category": "People & Body",
    "aliases": ["family_man_woman_boy"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "family: man, woman, girl",
    "category": "People & Body",
    "aliases": ["family_man_woman_girl"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "family: man, woman, girl, boy",
    "category": "People & Body",
    "aliases": ["family_man_woman_girl_boy"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "family: man, woman, boy, boy",
    "category": "People & Body",
    "aliases": ["family_man_woman_boy_boy"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "family: man, woman, girl, girl",
    "category": "People & Body",
    "aliases": ["family_man_woman_girl_girl"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "family: man, man, boy",
    "category": "People & Body",
    "aliases": ["family_man_man_boy"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "family: man, man, girl",
    "category": "People & Body",
    "aliases": ["family_man_man_girl"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "family: man, man, girl, boy",
    "category": "People & Body",
    "aliases": ["family_man_man_girl_boy"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "family: man, man, boy, boy",
    "category": "People & Body",
    "aliases": ["family_man_man_boy_boy"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "family: man, man, girl, girl",
    "category": "People & Body",
    "aliases": ["family_man_man_girl_girl"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "family: woman, woman, boy",
    "category": "People & Body",
    "aliases": ["family_woman_woman_boy"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "family: woman, woman, girl",
    "category": "People & Body",
    "aliases": ["family_woman_woman_girl"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "family: woman, woman, girl, boy",
    "category": "People & Body",
    "aliases": ["family_woman_woman_girl_boy"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "family: woman, woman, boy, boy",
    "category": "People & Body",
    "aliases": ["family_woman_woman_boy_boy"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "family: woman, woman, girl, girl",
    "category": "People & Body",
    "aliases": ["family_woman_woman_girl_girl"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "family: man, boy",
    "category": "People & Body",
    "aliases": ["family_man_boy"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0"
  },
  {
    "emoji": "",
    "description": "family: man, boy, boy",
    "category": "People & Body",
    "aliases": ["family_man_boy_boy"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0"
  },
  {
    "emoji": "",
    "description": "family: man, girl",
    "category": "People & Body",
    "aliases": ["family_man_girl"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0"
  },
  {
    "emoji": "",
    "description": "family: man, girl, boy",
    "category": "People & Body",
    "aliases": ["family_man_girl_boy"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0"
  },
  {
    "emoji": "",
    "description": "family: man, girl, girl",
    "category": "People & Body",
    "aliases": ["family_man_girl_girl"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0"
  },
  {
    "emoji": "",
    "description": "family: woman, boy",
    "category": "People & Body",
    "aliases": ["family_woman_boy"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0"
  },
  {
    "emoji": "",
    "description": "family: woman, boy, boy",
    "category": "People & Body",
    "aliases": ["family_woman_boy_boy"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0"
  },
  {
    "emoji": "",
    "description": "family: woman, girl",
    "category": "People & Body",
    "aliases": ["family_woman_girl"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0"
  },
  {
    "emoji": "",
    "description": "family: woman, girl, boy",
    "category": "People & Body",
    "aliases": ["family_woman_girl_boy"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0"
  },
  {
    "emoji": "",
    "description": "family: woman, girl, girl",
    "category": "People & Body",
    "aliases": ["family_woman_girl_girl"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "10.0"
  },
  {
    "emoji": "",
    "description": "speaking head",
    "category": "People & Body",
    "aliases": ["speaking_head"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "bust in silhouette",
    "category": "People & Body",
    "aliases": ["bust_in_silhouette"],
    "tags": ["user"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "busts in silhouette",
    "category": "People & Body",
    "aliases": ["busts_in_silhouette"],
    "tags": ["users", "group", "team"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "people hugging",
    "category": "People & Body",
    "aliases": ["people_hugging"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "footprints",
    "category": "People & Body",
    "aliases": ["footprints"],
    "tags": ["feet", "tracks"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "monkey face",
    "category": "Animals & Nature",
    "aliases": ["monkey_face"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "monkey",
    "category": "Animals & Nature",
    "aliases": ["monkey"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "gorilla",
    "category": "Animals & Nature",
    "aliases": ["gorilla"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "orangutan",
    "category": "Animals & Nature",
    "aliases": ["orangutan"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "dog face",
    "category": "Animals & Nature",
    "aliases": ["dog"],
    "tags": ["pet"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "dog",
    "category": "Animals & Nature",
    "aliases": ["dog2"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "guide dog",
    "category": "Animals & Nature",
    "aliases": ["guide_dog"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "service dog",
    "category": "Animals & Nature",
    "aliases": ["service_dog"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "poodle",
    "category": "Animals & Nature",
    "aliases": ["poodle"],
    "tags": ["dog"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "wolf",
    "category": "Animals & Nature",
    "aliases": ["wolf"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "fox",
    "category": "Animals & Nature",
    "aliases": ["fox_face"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "raccoon",
    "category": "Animals & Nature",
    "aliases": ["raccoon"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "cat face",
    "category": "Animals & Nature",
    "aliases": ["cat"],
    "tags": ["pet"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cat",
    "category": "Animals & Nature",
    "aliases": ["cat2"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "black cat",
    "category": "Animals & Nature",
    "aliases": ["black_cat"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "lion",
    "category": "Animals & Nature",
    "aliases": ["lion"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "tiger face",
    "category": "Animals & Nature",
    "aliases": ["tiger"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "tiger",
    "category": "Animals & Nature",
    "aliases": ["tiger2"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "leopard",
    "category": "Animals & Nature",
    "aliases": ["leopard"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "horse face",
    "category": "Animals & Nature",
    "aliases": ["horse"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "moose",
    "category": "Animals & Nature",
    "aliases": ["moose"],
    "tags": ["canada"],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "donkey",
    "category": "Animals & Nature",
    "aliases": ["donkey"],
    "tags": ["mule"],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "horse",
    "category": "Animals & Nature",
    "aliases": ["racehorse"],
    "tags": ["speed"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "unicorn",
    "category": "Animals & Nature",
    "aliases": ["unicorn"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "zebra",
    "category": "Animals & Nature",
    "aliases": ["zebra"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "deer",
    "category": "Animals & Nature",
    "aliases": ["deer"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "bison",
    "category": "Animals & Nature",
    "aliases": ["bison"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "cow face",
    "category": "Animals & Nature",
    "aliases": ["cow"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ox",
    "category": "Animals & Nature",
    "aliases": ["ox"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "water buffalo",
    "category": "Animals & Nature",
    "aliases": ["water_buffalo"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cow",
    "category": "Animals & Nature",
    "aliases": ["cow2"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "pig face",
    "category": "Animals & Nature",
    "aliases": ["pig"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "pig",
    "category": "Animals & Nature",
    "aliases": ["pig2"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "boar",
    "category": "Animals & Nature",
    "aliases": ["boar"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "pig nose",
    "category": "Animals & Nature",
    "aliases": ["pig_nose"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ram",
    "category": "Animals & Nature",
    "aliases": ["ram"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ewe",
    "category": "Animals & Nature",
    "aliases": ["sheep"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "goat",
    "category": "Animals & Nature",
    "aliases": ["goat"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "camel",
    "category": "Animals & Nature",
    "aliases": ["dromedary_camel"],
    "tags": ["desert"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "two-hump camel",
    "category": "Animals & Nature",
    "aliases": ["camel"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "llama",
    "category": "Animals & Nature",
    "aliases": ["llama"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "giraffe",
    "category": "Animals & Nature",
    "aliases": ["giraffe"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "elephant",
    "category": "Animals & Nature",
    "aliases": ["elephant"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "mammoth",
    "category": "Animals & Nature",
    "aliases": ["mammoth"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "rhinoceros",
    "category": "Animals & Nature",
    "aliases": ["rhinoceros"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "hippopotamus",
    "category": "Animals & Nature",
    "aliases": ["hippopotamus"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "mouse face",
    "category": "Animals & Nature",
    "aliases": ["mouse"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "mouse",
    "category": "Animals & Nature",
    "aliases": ["mouse2"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "rat",
    "category": "Animals & Nature",
    "aliases": ["rat"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "hamster",
    "category": "Animals & Nature",
    "aliases": ["hamster"],
    "tags": ["pet"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "rabbit face",
    "category": "Animals & Nature",
    "aliases": ["rabbit"],
    "tags": ["bunny"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "rabbit",
    "category": "Animals & Nature",
    "aliases": ["rabbit2"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "chipmunk",
    "category": "Animals & Nature",
    "aliases": ["chipmunk"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "beaver",
    "category": "Animals & Nature",
    "aliases": ["beaver"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "hedgehog",
    "category": "Animals & Nature",
    "aliases": ["hedgehog"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "bat",
    "category": "Animals & Nature",
    "aliases": ["bat"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "bear",
    "category": "Animals & Nature",
    "aliases": ["bear"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "polar bear",
    "category": "Animals & Nature",
    "aliases": ["polar_bear"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "koala",
    "category": "Animals & Nature",
    "aliases": ["koala"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "panda",
    "category": "Animals & Nature",
    "aliases": ["panda_face"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sloth",
    "category": "Animals & Nature",
    "aliases": ["sloth"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "otter",
    "category": "Animals & Nature",
    "aliases": ["otter"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "skunk",
    "category": "Animals & Nature",
    "aliases": ["skunk"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "kangaroo",
    "category": "Animals & Nature",
    "aliases": ["kangaroo"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "badger",
    "category": "Animals & Nature",
    "aliases": ["badger"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "paw prints",
    "category": "Animals & Nature",
    "aliases": ["feet", "paw_prints"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "turkey",
    "category": "Animals & Nature",
    "aliases": ["turkey"],
    "tags": ["thanksgiving"],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "chicken",
    "category": "Animals & Nature",
    "aliases": ["chicken"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "rooster",
    "category": "Animals & Nature",
    "aliases": ["rooster"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "hatching chick",
    "category": "Animals & Nature",
    "aliases": ["hatching_chick"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "baby chick",
    "category": "Animals & Nature",
    "aliases": ["baby_chick"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "front-facing baby chick",
    "category": "Animals & Nature",
    "aliases": ["hatched_chick"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "bird",
    "category": "Animals & Nature",
    "aliases": ["bird"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "penguin",
    "category": "Animals & Nature",
    "aliases": ["penguin"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "dove",
    "category": "Animals & Nature",
    "aliases": ["dove"],
    "tags": ["peace"],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "eagle",
    "category": "Animals & Nature",
    "aliases": ["eagle"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "duck",
    "category": "Animals & Nature",
    "aliases": ["duck"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "swan",
    "category": "Animals & Nature",
    "aliases": ["swan"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "owl",
    "category": "Animals & Nature",
    "aliases": ["owl"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "dodo",
    "category": "Animals & Nature",
    "aliases": ["dodo"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "feather",
    "category": "Animals & Nature",
    "aliases": ["feather"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "flamingo",
    "category": "Animals & Nature",
    "aliases": ["flamingo"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "peacock",
    "category": "Animals & Nature",
    "aliases": ["peacock"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "parrot",
    "category": "Animals & Nature",
    "aliases": ["parrot"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "wing",
    "category": "Animals & Nature",
    "aliases": ["wing"],
    "tags": ["fly"],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "black bird",
    "category": "Animals & Nature",
    "aliases": ["black_bird"],
    "tags": [],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "goose",
    "category": "Animals & Nature",
    "aliases": ["goose"],
    "tags": ["honk"],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "frog",
    "category": "Animals & Nature",
    "aliases": ["frog"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "crocodile",
    "category": "Animals & Nature",
    "aliases": ["crocodile"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "turtle",
    "category": "Animals & Nature",
    "aliases": ["turtle"],
    "tags": ["slow"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "lizard",
    "category": "Animals & Nature",
    "aliases": ["lizard"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "snake",
    "category": "Animals & Nature",
    "aliases": ["snake"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "dragon face",
    "category": "Animals & Nature",
    "aliases": ["dragon_face"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "dragon",
    "category": "Animals & Nature",
    "aliases": ["dragon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sauropod",
    "category": "Animals & Nature",
    "aliases": ["sauropod"],
    "tags": ["dinosaur"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "T-Rex",
    "category": "Animals & Nature",
    "aliases": ["t-rex"],
    "tags": ["dinosaur"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "spouting whale",
    "category": "Animals & Nature",
    "aliases": ["whale"],
    "tags": ["sea"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "whale",
    "category": "Animals & Nature",
    "aliases": ["whale2"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "dolphin",
    "category": "Animals & Nature",
    "aliases": ["dolphin", "flipper"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "seal",
    "category": "Animals & Nature",
    "aliases": ["seal"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "fish",
    "category": "Animals & Nature",
    "aliases": ["fish"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "tropical fish",
    "category": "Animals & Nature",
    "aliases": ["tropical_fish"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "blowfish",
    "category": "Animals & Nature",
    "aliases": ["blowfish"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "shark",
    "category": "Animals & Nature",
    "aliases": ["shark"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "octopus",
    "category": "Animals & Nature",
    "aliases": ["octopus"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "spiral shell",
    "category": "Animals & Nature",
    "aliases": ["shell"],
    "tags": ["sea", "beach"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "coral",
    "category": "Animals & Nature",
    "aliases": ["coral"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "jellyfish",
    "category": "Animals & Nature",
    "aliases": ["jellyfish"],
    "tags": [],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "snail",
    "category": "Animals & Nature",
    "aliases": ["snail"],
    "tags": ["slow"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "butterfly",
    "category": "Animals & Nature",
    "aliases": ["butterfly"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "bug",
    "category": "Animals & Nature",
    "aliases": ["bug"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ant",
    "category": "Animals & Nature",
    "aliases": ["ant"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "honeybee",
    "category": "Animals & Nature",
    "aliases": ["bee", "honeybee"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "beetle",
    "category": "Animals & Nature",
    "aliases": ["beetle"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "lady beetle",
    "category": "Animals & Nature",
    "aliases": ["lady_beetle"],
    "tags": ["bug"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cricket",
    "category": "Animals & Nature",
    "aliases": ["cricket"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "cockroach",
    "category": "Animals & Nature",
    "aliases": ["cockroach"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "spider",
    "category": "Animals & Nature",
    "aliases": ["spider"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "spider web",
    "category": "Animals & Nature",
    "aliases": ["spider_web"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "scorpion",
    "category": "Animals & Nature",
    "aliases": ["scorpion"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "mosquito",
    "category": "Animals & Nature",
    "aliases": ["mosquito"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "fly",
    "category": "Animals & Nature",
    "aliases": ["fly"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "worm",
    "category": "Animals & Nature",
    "aliases": ["worm"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "microbe",
    "category": "Animals & Nature",
    "aliases": ["microbe"],
    "tags": ["germ"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "bouquet",
    "category": "Animals & Nature",
    "aliases": ["bouquet"],
    "tags": ["flowers"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cherry blossom",
    "category": "Animals & Nature",
    "aliases": ["cherry_blossom"],
    "tags": ["flower", "spring"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "white flower",
    "category": "Animals & Nature",
    "aliases": ["white_flower"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "lotus",
    "category": "Animals & Nature",
    "aliases": ["lotus"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "rosette",
    "category": "Animals & Nature",
    "aliases": ["rosette"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "rose",
    "category": "Animals & Nature",
    "aliases": ["rose"],
    "tags": ["flower"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "wilted flower",
    "category": "Animals & Nature",
    "aliases": ["wilted_flower"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "hibiscus",
    "category": "Animals & Nature",
    "aliases": ["hibiscus"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sunflower",
    "category": "Animals & Nature",
    "aliases": ["sunflower"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "blossom",
    "category": "Animals & Nature",
    "aliases": ["blossom"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "tulip",
    "category": "Animals & Nature",
    "aliases": ["tulip"],
    "tags": ["flower"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "hyacinth",
    "category": "Animals & Nature",
    "aliases": ["hyacinth"],
    "tags": [],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "seedling",
    "category": "Animals & Nature",
    "aliases": ["seedling"],
    "tags": ["plant"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "potted plant",
    "category": "Animals & Nature",
    "aliases": ["potted_plant"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "evergreen tree",
    "category": "Animals & Nature",
    "aliases": ["evergreen_tree"],
    "tags": ["wood"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "deciduous tree",
    "category": "Animals & Nature",
    "aliases": ["deciduous_tree"],
    "tags": ["wood"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "palm tree",
    "category": "Animals & Nature",
    "aliases": ["palm_tree"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cactus",
    "category": "Animals & Nature",
    "aliases": ["cactus"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sheaf of rice",
    "category": "Animals & Nature",
    "aliases": ["ear_of_rice"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "herb",
    "category": "Animals & Nature",
    "aliases": ["herb"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "shamrock",
    "category": "Animals & Nature",
    "aliases": ["shamrock"],
    "tags": [],
    "unicode_version": "4.1",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "four leaf clover",
    "category": "Animals & Nature",
    "aliases": ["four_leaf_clover"],
    "tags": ["luck"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "maple leaf",
    "category": "Animals & Nature",
    "aliases": ["maple_leaf"],
    "tags": ["canada"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "fallen leaf",
    "category": "Animals & Nature",
    "aliases": ["fallen_leaf"],
    "tags": ["autumn"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "leaf fluttering in wind",
    "category": "Animals & Nature",
    "aliases": ["leaves"],
    "tags": ["leaf"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "empty nest",
    "category": "Animals & Nature",
    "aliases": ["empty_nest"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "nest with eggs",
    "category": "Animals & Nature",
    "aliases": ["nest_with_eggs"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "mushroom",
    "category": "Animals & Nature",
    "aliases": ["mushroom"],
    "tags": ["fungus"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "grapes",
    "category": "Food & Drink",
    "aliases": ["grapes"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "melon",
    "category": "Food & Drink",
    "aliases": ["melon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "watermelon",
    "category": "Food & Drink",
    "aliases": ["watermelon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "tangerine",
    "category": "Food & Drink",
    "aliases": ["tangerine", "orange", "mandarin"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "lemon",
    "category": "Food & Drink",
    "aliases": ["lemon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "banana",
    "category": "Food & Drink",
    "aliases": ["banana"],
    "tags": ["fruit"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "pineapple",
    "category": "Food & Drink",
    "aliases": ["pineapple"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "mango",
    "category": "Food & Drink",
    "aliases": ["mango"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "red apple",
    "category": "Food & Drink",
    "aliases": ["apple"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "green apple",
    "category": "Food & Drink",
    "aliases": ["green_apple"],
    "tags": ["fruit"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "pear",
    "category": "Food & Drink",
    "aliases": ["pear"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "peach",
    "category": "Food & Drink",
    "aliases": ["peach"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cherries",
    "category": "Food & Drink",
    "aliases": ["cherries"],
    "tags": ["fruit"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "strawberry",
    "category": "Food & Drink",
    "aliases": ["strawberry"],
    "tags": ["fruit"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "blueberries",
    "category": "Food & Drink",
    "aliases": ["blueberries"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "kiwi fruit",
    "category": "Food & Drink",
    "aliases": ["kiwi_fruit"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "tomato",
    "category": "Food & Drink",
    "aliases": ["tomato"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "olive",
    "category": "Food & Drink",
    "aliases": ["olive"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "coconut",
    "category": "Food & Drink",
    "aliases": ["coconut"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "avocado",
    "category": "Food & Drink",
    "aliases": ["avocado"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "eggplant",
    "category": "Food & Drink",
    "aliases": ["eggplant"],
    "tags": ["aubergine"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "potato",
    "category": "Food & Drink",
    "aliases": ["potato"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "carrot",
    "category": "Food & Drink",
    "aliases": ["carrot"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "ear of corn",
    "category": "Food & Drink",
    "aliases": ["corn"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "hot pepper",
    "category": "Food & Drink",
    "aliases": ["hot_pepper"],
    "tags": ["spicy"],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "bell pepper",
    "category": "Food & Drink",
    "aliases": ["bell_pepper"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "cucumber",
    "category": "Food & Drink",
    "aliases": ["cucumber"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "leafy green",
    "category": "Food & Drink",
    "aliases": ["leafy_green"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "broccoli",
    "category": "Food & Drink",
    "aliases": ["broccoli"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "garlic",
    "category": "Food & Drink",
    "aliases": ["garlic"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "onion",
    "category": "Food & Drink",
    "aliases": ["onion"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "peanuts",
    "category": "Food & Drink",
    "aliases": ["peanuts"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "beans",
    "category": "Food & Drink",
    "aliases": ["beans"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "chestnut",
    "category": "Food & Drink",
    "aliases": ["chestnut"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ginger root",
    "category": "Food & Drink",
    "aliases": ["ginger_root"],
    "tags": [],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "pea pod",
    "category": "Food & Drink",
    "aliases": ["pea_pod"],
    "tags": [],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "bread",
    "category": "Food & Drink",
    "aliases": ["bread"],
    "tags": ["toast"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "croissant",
    "category": "Food & Drink",
    "aliases": ["croissant"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "baguette bread",
    "category": "Food & Drink",
    "aliases": ["baguette_bread"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "flatbread",
    "category": "Food & Drink",
    "aliases": ["flatbread"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "pretzel",
    "category": "Food & Drink",
    "aliases": ["pretzel"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "bagel",
    "category": "Food & Drink",
    "aliases": ["bagel"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "pancakes",
    "category": "Food & Drink",
    "aliases": ["pancakes"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "waffle",
    "category": "Food & Drink",
    "aliases": ["waffle"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "cheese wedge",
    "category": "Food & Drink",
    "aliases": ["cheese"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "meat on bone",
    "category": "Food & Drink",
    "aliases": ["meat_on_bone"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "poultry leg",
    "category": "Food & Drink",
    "aliases": ["poultry_leg"],
    "tags": ["meat", "chicken"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cut of meat",
    "category": "Food & Drink",
    "aliases": ["cut_of_meat"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "bacon",
    "category": "Food & Drink",
    "aliases": ["bacon"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "hamburger",
    "category": "Food & Drink",
    "aliases": ["hamburger"],
    "tags": ["burger"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "french fries",
    "category": "Food & Drink",
    "aliases": ["fries"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "pizza",
    "category": "Food & Drink",
    "aliases": ["pizza"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "hot dog",
    "category": "Food & Drink",
    "aliases": ["hotdog"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "sandwich",
    "category": "Food & Drink",
    "aliases": ["sandwich"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "taco",
    "category": "Food & Drink",
    "aliases": ["taco"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "burrito",
    "category": "Food & Drink",
    "aliases": ["burrito"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "tamale",
    "category": "Food & Drink",
    "aliases": ["tamale"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "stuffed flatbread",
    "category": "Food & Drink",
    "aliases": ["stuffed_flatbread"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "falafel",
    "category": "Food & Drink",
    "aliases": ["falafel"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "egg",
    "category": "Food & Drink",
    "aliases": ["egg"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "cooking",
    "category": "Food & Drink",
    "aliases": ["fried_egg"],
    "tags": ["breakfast"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "shallow pan of food",
    "category": "Food & Drink",
    "aliases": ["shallow_pan_of_food"],
    "tags": ["paella", "curry"],
    "unicode_version": "",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "pot of food",
    "category": "Food & Drink",
    "aliases": ["stew"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "fondue",
    "category": "Food & Drink",
    "aliases": ["fondue"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "bowl with spoon",
    "category": "Food & Drink",
    "aliases": ["bowl_with_spoon"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "green salad",
    "category": "Food & Drink",
    "aliases": ["green_salad"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "popcorn",
    "category": "Food & Drink",
    "aliases": ["popcorn"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "butter",
    "category": "Food & Drink",
    "aliases": ["butter"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "salt",
    "category": "Food & Drink",
    "aliases": ["salt"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "canned food",
    "category": "Food & Drink",
    "aliases": ["canned_food"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "bento box",
    "category": "Food & Drink",
    "aliases": ["bento"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "rice cracker",
    "category": "Food & Drink",
    "aliases": ["rice_cracker"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "rice ball",
    "category": "Food & Drink",
    "aliases": ["rice_ball"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cooked rice",
    "category": "Food & Drink",
    "aliases": ["rice"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "curry rice",
    "category": "Food & Drink",
    "aliases": ["curry"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "steaming bowl",
    "category": "Food & Drink",
    "aliases": ["ramen"],
    "tags": ["noodle"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "spaghetti",
    "category": "Food & Drink",
    "aliases": ["spaghetti"],
    "tags": ["pasta"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "roasted sweet potato",
    "category": "Food & Drink",
    "aliases": ["sweet_potato"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "oden",
    "category": "Food & Drink",
    "aliases": ["oden"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sushi",
    "category": "Food & Drink",
    "aliases": ["sushi"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "fried shrimp",
    "category": "Food & Drink",
    "aliases": ["fried_shrimp"],
    "tags": ["tempura"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "fish cake with swirl",
    "category": "Food & Drink",
    "aliases": ["fish_cake"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "moon cake",
    "category": "Food & Drink",
    "aliases": ["moon_cake"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "dango",
    "category": "Food & Drink",
    "aliases": ["dango"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "dumpling",
    "category": "Food & Drink",
    "aliases": ["dumpling"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "fortune cookie",
    "category": "Food & Drink",
    "aliases": ["fortune_cookie"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "takeout box",
    "category": "Food & Drink",
    "aliases": ["takeout_box"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "crab",
    "category": "Food & Drink",
    "aliases": ["crab"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "lobster",
    "category": "Food & Drink",
    "aliases": ["lobster"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "shrimp",
    "category": "Food & Drink",
    "aliases": ["shrimp"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "squid",
    "category": "Food & Drink",
    "aliases": ["squid"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "oyster",
    "category": "Food & Drink",
    "aliases": ["oyster"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "soft ice cream",
    "category": "Food & Drink",
    "aliases": ["icecream"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "shaved ice",
    "category": "Food & Drink",
    "aliases": ["shaved_ice"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ice cream",
    "category": "Food & Drink",
    "aliases": ["ice_cream"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "doughnut",
    "category": "Food & Drink",
    "aliases": ["doughnut"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cookie",
    "category": "Food & Drink",
    "aliases": ["cookie"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "birthday cake",
    "category": "Food & Drink",
    "aliases": ["birthday"],
    "tags": ["party"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "shortcake",
    "category": "Food & Drink",
    "aliases": ["cake"],
    "tags": ["dessert"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cupcake",
    "category": "Food & Drink",
    "aliases": ["cupcake"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "pie",
    "category": "Food & Drink",
    "aliases": ["pie"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "chocolate bar",
    "category": "Food & Drink",
    "aliases": ["chocolate_bar"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "candy",
    "category": "Food & Drink",
    "aliases": ["candy"],
    "tags": ["sweet"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "lollipop",
    "category": "Food & Drink",
    "aliases": ["lollipop"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "custard",
    "category": "Food & Drink",
    "aliases": ["custard"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "honey pot",
    "category": "Food & Drink",
    "aliases": ["honey_pot"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "baby bottle",
    "category": "Food & Drink",
    "aliases": ["baby_bottle"],
    "tags": ["milk"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "glass of milk",
    "category": "Food & Drink",
    "aliases": ["milk_glass"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "hot beverage",
    "category": "Food & Drink",
    "aliases": ["coffee"],
    "tags": ["cafe", "espresso"],
    "unicode_version": "4.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "teapot",
    "category": "Food & Drink",
    "aliases": ["teapot"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "teacup without handle",
    "category": "Food & Drink",
    "aliases": ["tea"],
    "tags": ["green", "breakfast"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "bottle with popping cork",
    "category": "Food & Drink",
    "aliases": ["champagne"],
    "tags": ["bottle", "bubbly", "celebration"],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "wine glass",
    "category": "Food & Drink",
    "aliases": ["wine_glass"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cocktail glass",
    "category": "Food & Drink",
    "aliases": ["cocktail"],
    "tags": ["drink"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "tropical drink",
    "category": "Food & Drink",
    "aliases": ["tropical_drink"],
    "tags": ["summer", "vacation"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "beer mug",
    "category": "Food & Drink",
    "aliases": ["beer"],
    "tags": ["drink"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "clinking beer mugs",
    "category": "Food & Drink",
    "aliases": ["beers"],
    "tags": ["drinks"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "clinking glasses",
    "category": "Food & Drink",
    "aliases": ["clinking_glasses"],
    "tags": ["cheers", "toast"],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "tumbler glass",
    "category": "Food & Drink",
    "aliases": ["tumbler_glass"],
    "tags": ["whisky"],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "pouring liquid",
    "category": "Food & Drink",
    "aliases": ["pouring_liquid"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "cup with straw",
    "category": "Food & Drink",
    "aliases": ["cup_with_straw"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "bubble tea",
    "category": "Food & Drink",
    "aliases": ["bubble_tea"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "beverage box",
    "category": "Food & Drink",
    "aliases": ["beverage_box"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "mate",
    "category": "Food & Drink",
    "aliases": ["mate"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "ice",
    "category": "Food & Drink",
    "aliases": ["ice_cube"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "chopsticks",
    "category": "Food & Drink",
    "aliases": ["chopsticks"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "fork and knife with plate",
    "category": "Food & Drink",
    "aliases": ["plate_with_cutlery"],
    "tags": ["dining", "dinner"],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "fork and knife",
    "category": "Food & Drink",
    "aliases": ["fork_and_knife"],
    "tags": ["cutlery"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "spoon",
    "category": "Food & Drink",
    "aliases": ["spoon"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "kitchen knife",
    "category": "Food & Drink",
    "aliases": ["hocho", "knife"],
    "tags": ["cut", "chop"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "jar",
    "category": "Food & Drink",
    "aliases": ["jar"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "amphora",
    "category": "Food & Drink",
    "aliases": ["amphora"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "globe showing Europe-Africa",
    "category": "Travel & Places",
    "aliases": ["earth_africa"],
    "tags": ["globe", "world", "international"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "globe showing Americas",
    "category": "Travel & Places",
    "aliases": ["earth_americas"],
    "tags": ["globe", "world", "international"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "globe showing Asia-Australia",
    "category": "Travel & Places",
    "aliases": ["earth_asia"],
    "tags": ["globe", "world", "international"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "globe with meridians",
    "category": "Travel & Places",
    "aliases": ["globe_with_meridians"],
    "tags": ["world", "global", "international"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "world map",
    "category": "Travel & Places",
    "aliases": ["world_map"],
    "tags": ["travel"],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "map of Japan",
    "category": "Travel & Places",
    "aliases": ["japan"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "compass",
    "category": "Travel & Places",
    "aliases": ["compass"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "snow-capped mountain",
    "category": "Travel & Places",
    "aliases": ["mountain_snow"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "mountain",
    "category": "Travel & Places",
    "aliases": ["mountain"],
    "tags": [],
    "unicode_version": "5.2",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "volcano",
    "category": "Travel & Places",
    "aliases": ["volcano"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "mount fuji",
    "category": "Travel & Places",
    "aliases": ["mount_fuji"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "camping",
    "category": "Travel & Places",
    "aliases": ["camping"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "beach with umbrella",
    "category": "Travel & Places",
    "aliases": ["beach_umbrella"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "desert",
    "category": "Travel & Places",
    "aliases": ["desert"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "desert island",
    "category": "Travel & Places",
    "aliases": ["desert_island"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "national park",
    "category": "Travel & Places",
    "aliases": ["national_park"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "stadium",
    "category": "Travel & Places",
    "aliases": ["stadium"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "classical building",
    "category": "Travel & Places",
    "aliases": ["classical_building"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "building construction",
    "category": "Travel & Places",
    "aliases": ["building_construction"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "brick",
    "category": "Travel & Places",
    "aliases": ["bricks"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "rock",
    "category": "Travel & Places",
    "aliases": ["rock"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "wood",
    "category": "Travel & Places",
    "aliases": ["wood"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "hut",
    "category": "Travel & Places",
    "aliases": ["hut"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "houses",
    "category": "Travel & Places",
    "aliases": ["houses"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "derelict house",
    "category": "Travel & Places",
    "aliases": ["derelict_house"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "house",
    "category": "Travel & Places",
    "aliases": ["house"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "house with garden",
    "category": "Travel & Places",
    "aliases": ["house_with_garden"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "office building",
    "category": "Travel & Places",
    "aliases": ["office"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese post office",
    "category": "Travel & Places",
    "aliases": ["post_office"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "post office",
    "category": "Travel & Places",
    "aliases": ["european_post_office"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "hospital",
    "category": "Travel & Places",
    "aliases": ["hospital"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "bank",
    "category": "Travel & Places",
    "aliases": ["bank"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "hotel",
    "category": "Travel & Places",
    "aliases": ["hotel"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "love hotel",
    "category": "Travel & Places",
    "aliases": ["love_hotel"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "convenience store",
    "category": "Travel & Places",
    "aliases": ["convenience_store"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "school",
    "category": "Travel & Places",
    "aliases": ["school"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "department store",
    "category": "Travel & Places",
    "aliases": ["department_store"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "factory",
    "category": "Travel & Places",
    "aliases": ["factory"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese castle",
    "category": "Travel & Places",
    "aliases": ["japanese_castle"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "castle",
    "category": "Travel & Places",
    "aliases": ["european_castle"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "wedding",
    "category": "Travel & Places",
    "aliases": ["wedding"],
    "tags": ["marriage"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Tokyo tower",
    "category": "Travel & Places",
    "aliases": ["tokyo_tower"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Statue of Liberty",
    "category": "Travel & Places",
    "aliases": ["statue_of_liberty"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "church",
    "category": "Travel & Places",
    "aliases": ["church"],
    "tags": [],
    "unicode_version": "5.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "mosque",
    "category": "Travel & Places",
    "aliases": ["mosque"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "hindu temple",
    "category": "Travel & Places",
    "aliases": ["hindu_temple"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "synagogue",
    "category": "Travel & Places",
    "aliases": ["synagogue"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "shinto shrine",
    "category": "Travel & Places",
    "aliases": ["shinto_shrine"],
    "tags": [],
    "unicode_version": "5.2",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "kaaba",
    "category": "Travel & Places",
    "aliases": ["kaaba"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "fountain",
    "category": "Travel & Places",
    "aliases": ["fountain"],
    "tags": [],
    "unicode_version": "5.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "tent",
    "category": "Travel & Places",
    "aliases": ["tent"],
    "tags": ["camping"],
    "unicode_version": "5.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "foggy",
    "category": "Travel & Places",
    "aliases": ["foggy"],
    "tags": ["karl"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "night with stars",
    "category": "Travel & Places",
    "aliases": ["night_with_stars"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cityscape",
    "category": "Travel & Places",
    "aliases": ["cityscape"],
    "tags": ["skyline"],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "sunrise over mountains",
    "category": "Travel & Places",
    "aliases": ["sunrise_over_mountains"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sunrise",
    "category": "Travel & Places",
    "aliases": ["sunrise"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cityscape at dusk",
    "category": "Travel & Places",
    "aliases": ["city_sunset"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sunset",
    "category": "Travel & Places",
    "aliases": ["city_sunrise"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "bridge at night",
    "category": "Travel & Places",
    "aliases": ["bridge_at_night"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "hot springs",
    "category": "Travel & Places",
    "aliases": ["hotsprings"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "carousel horse",
    "category": "Travel & Places",
    "aliases": ["carousel_horse"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "playground slide",
    "category": "Travel & Places",
    "aliases": ["playground_slide"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "ferris wheel",
    "category": "Travel & Places",
    "aliases": ["ferris_wheel"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "roller coaster",
    "category": "Travel & Places",
    "aliases": ["roller_coaster"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "barber pole",
    "category": "Travel & Places",
    "aliases": ["barber"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "circus tent",
    "category": "Travel & Places",
    "aliases": ["circus_tent"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "locomotive",
    "category": "Travel & Places",
    "aliases": ["steam_locomotive"],
    "tags": ["train"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "railway car",
    "category": "Travel & Places",
    "aliases": ["railway_car"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "high-speed train",
    "category": "Travel & Places",
    "aliases": ["bullettrain_side"],
    "tags": ["train"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "bullet train",
    "category": "Travel & Places",
    "aliases": ["bullettrain_front"],
    "tags": ["train"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "train",
    "category": "Travel & Places",
    "aliases": ["train2"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "metro",
    "category": "Travel & Places",
    "aliases": ["metro"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "light rail",
    "category": "Travel & Places",
    "aliases": ["light_rail"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "station",
    "category": "Travel & Places",
    "aliases": ["station"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "tram",
    "category": "Travel & Places",
    "aliases": ["tram"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "monorail",
    "category": "Travel & Places",
    "aliases": ["monorail"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "mountain railway",
    "category": "Travel & Places",
    "aliases": ["mountain_railway"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "tram car",
    "category": "Travel & Places",
    "aliases": ["train"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "bus",
    "category": "Travel & Places",
    "aliases": ["bus"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "oncoming bus",
    "category": "Travel & Places",
    "aliases": ["oncoming_bus"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "trolleybus",
    "category": "Travel & Places",
    "aliases": ["trolleybus"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "minibus",
    "category": "Travel & Places",
    "aliases": ["minibus"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ambulance",
    "category": "Travel & Places",
    "aliases": ["ambulance"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "fire engine",
    "category": "Travel & Places",
    "aliases": ["fire_engine"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "police car",
    "category": "Travel & Places",
    "aliases": ["police_car"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "oncoming police car",
    "category": "Travel & Places",
    "aliases": ["oncoming_police_car"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "taxi",
    "category": "Travel & Places",
    "aliases": ["taxi"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "oncoming taxi",
    "category": "Travel & Places",
    "aliases": ["oncoming_taxi"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "automobile",
    "category": "Travel & Places",
    "aliases": ["car", "red_car"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "oncoming automobile",
    "category": "Travel & Places",
    "aliases": ["oncoming_automobile"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sport utility vehicle",
    "category": "Travel & Places",
    "aliases": ["blue_car"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "pickup truck",
    "category": "Travel & Places",
    "aliases": ["pickup_truck"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "delivery truck",
    "category": "Travel & Places",
    "aliases": ["truck"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "articulated lorry",
    "category": "Travel & Places",
    "aliases": ["articulated_lorry"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "tractor",
    "category": "Travel & Places",
    "aliases": ["tractor"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "racing car",
    "category": "Travel & Places",
    "aliases": ["racing_car"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "motorcycle",
    "category": "Travel & Places",
    "aliases": ["motorcycle"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "motor scooter",
    "category": "Travel & Places",
    "aliases": ["motor_scooter"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "manual wheelchair",
    "category": "Travel & Places",
    "aliases": ["manual_wheelchair"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "motorized wheelchair",
    "category": "Travel & Places",
    "aliases": ["motorized_wheelchair"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "auto rickshaw",
    "category": "Travel & Places",
    "aliases": ["auto_rickshaw"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "bicycle",
    "category": "Travel & Places",
    "aliases": ["bike"],
    "tags": ["bicycle"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "kick scooter",
    "category": "Travel & Places",
    "aliases": ["kick_scooter"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "skateboard",
    "category": "Travel & Places",
    "aliases": ["skateboard"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "roller skate",
    "category": "Travel & Places",
    "aliases": ["roller_skate"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "bus stop",
    "category": "Travel & Places",
    "aliases": ["busstop"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "motorway",
    "category": "Travel & Places",
    "aliases": ["motorway"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "railway track",
    "category": "Travel & Places",
    "aliases": ["railway_track"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "oil drum",
    "category": "Travel & Places",
    "aliases": ["oil_drum"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "fuel pump",
    "category": "Travel & Places",
    "aliases": ["fuelpump"],
    "tags": [],
    "unicode_version": "5.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "wheel",
    "category": "Travel & Places",
    "aliases": ["wheel"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "police car light",
    "category": "Travel & Places",
    "aliases": ["rotating_light"],
    "tags": ["911", "emergency"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "horizontal traffic light",
    "category": "Travel & Places",
    "aliases": ["traffic_light"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "vertical traffic light",
    "category": "Travel & Places",
    "aliases": ["vertical_traffic_light"],
    "tags": ["semaphore"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "stop sign",
    "category": "Travel & Places",
    "aliases": ["stop_sign"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "construction",
    "category": "Travel & Places",
    "aliases": ["construction"],
    "tags": ["wip"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "anchor",
    "category": "Travel & Places",
    "aliases": ["anchor"],
    "tags": ["ship"],
    "unicode_version": "4.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ring buoy",
    "category": "Travel & Places",
    "aliases": ["ring_buoy"],
    "tags": ["life preserver"],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "sailboat",
    "category": "Travel & Places",
    "aliases": ["boat", "sailboat"],
    "tags": [],
    "unicode_version": "5.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "canoe",
    "category": "Travel & Places",
    "aliases": ["canoe"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "speedboat",
    "category": "Travel & Places",
    "aliases": ["speedboat"],
    "tags": ["ship"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "passenger ship",
    "category": "Travel & Places",
    "aliases": ["passenger_ship"],
    "tags": ["cruise"],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "ferry",
    "category": "Travel & Places",
    "aliases": ["ferry"],
    "tags": [],
    "unicode_version": "5.2",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "motor boat",
    "category": "Travel & Places",
    "aliases": ["motor_boat"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "ship",
    "category": "Travel & Places",
    "aliases": ["ship"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "airplane",
    "category": "Travel & Places",
    "aliases": ["airplane"],
    "tags": ["flight"],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "small airplane",
    "category": "Travel & Places",
    "aliases": ["small_airplane"],
    "tags": ["flight"],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "airplane departure",
    "category": "Travel & Places",
    "aliases": ["flight_departure"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "airplane arrival",
    "category": "Travel & Places",
    "aliases": ["flight_arrival"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "parachute",
    "category": "Travel & Places",
    "aliases": ["parachute"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "seat",
    "category": "Travel & Places",
    "aliases": ["seat"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "helicopter",
    "category": "Travel & Places",
    "aliases": ["helicopter"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "suspension railway",
    "category": "Travel & Places",
    "aliases": ["suspension_railway"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "mountain cableway",
    "category": "Travel & Places",
    "aliases": ["mountain_cableway"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "aerial tramway",
    "category": "Travel & Places",
    "aliases": ["aerial_tramway"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "satellite",
    "category": "Travel & Places",
    "aliases": ["artificial_satellite"],
    "tags": ["orbit", "space"],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "rocket",
    "category": "Travel & Places",
    "aliases": ["rocket"],
    "tags": ["ship", "launch"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "flying saucer",
    "category": "Travel & Places",
    "aliases": ["flying_saucer"],
    "tags": ["ufo"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "bellhop bell",
    "category": "Travel & Places",
    "aliases": ["bellhop_bell"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "luggage",
    "category": "Travel & Places",
    "aliases": ["luggage"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "hourglass done",
    "category": "Travel & Places",
    "aliases": ["hourglass"],
    "tags": ["time"],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "hourglass not done",
    "category": "Travel & Places",
    "aliases": ["hourglass_flowing_sand"],
    "tags": ["time"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "watch",
    "category": "Travel & Places",
    "aliases": ["watch"],
    "tags": ["time"],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "alarm clock",
    "category": "Travel & Places",
    "aliases": ["alarm_clock"],
    "tags": ["morning"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "stopwatch",
    "category": "Travel & Places",
    "aliases": ["stopwatch"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "timer clock",
    "category": "Travel & Places",
    "aliases": ["timer_clock"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "mantelpiece clock",
    "category": "Travel & Places",
    "aliases": ["mantelpiece_clock"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "twelve oclock",
    "category": "Travel & Places",
    "aliases": ["clock12"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "twelve-thirty",
    "category": "Travel & Places",
    "aliases": ["clock1230"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "one oclock",
    "category": "Travel & Places",
    "aliases": ["clock1"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "one-thirty",
    "category": "Travel & Places",
    "aliases": ["clock130"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "two oclock",
    "category": "Travel & Places",
    "aliases": ["clock2"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "two-thirty",
    "category": "Travel & Places",
    "aliases": ["clock230"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "three oclock",
    "category": "Travel & Places",
    "aliases": ["clock3"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "three-thirty",
    "category": "Travel & Places",
    "aliases": ["clock330"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "four oclock",
    "category": "Travel & Places",
    "aliases": ["clock4"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "four-thirty",
    "category": "Travel & Places",
    "aliases": ["clock430"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "five oclock",
    "category": "Travel & Places",
    "aliases": ["clock5"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "five-thirty",
    "category": "Travel & Places",
    "aliases": ["clock530"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "six oclock",
    "category": "Travel & Places",
    "aliases": ["clock6"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "six-thirty",
    "category": "Travel & Places",
    "aliases": ["clock630"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "seven oclock",
    "category": "Travel & Places",
    "aliases": ["clock7"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "seven-thirty",
    "category": "Travel & Places",
    "aliases": ["clock730"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "eight oclock",
    "category": "Travel & Places",
    "aliases": ["clock8"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "eight-thirty",
    "category": "Travel & Places",
    "aliases": ["clock830"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "nine oclock",
    "category": "Travel & Places",
    "aliases": ["clock9"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "nine-thirty",
    "category": "Travel & Places",
    "aliases": ["clock930"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ten oclock",
    "category": "Travel & Places",
    "aliases": ["clock10"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ten-thirty",
    "category": "Travel & Places",
    "aliases": ["clock1030"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "eleven oclock",
    "category": "Travel & Places",
    "aliases": ["clock11"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "eleven-thirty",
    "category": "Travel & Places",
    "aliases": ["clock1130"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "new moon",
    "category": "Travel & Places",
    "aliases": ["new_moon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "waxing crescent moon",
    "category": "Travel & Places",
    "aliases": ["waxing_crescent_moon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "first quarter moon",
    "category": "Travel & Places",
    "aliases": ["first_quarter_moon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "waxing gibbous moon",
    "category": "Travel & Places",
    "aliases": ["moon", "waxing_gibbous_moon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "full moon",
    "category": "Travel & Places",
    "aliases": ["full_moon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "waning gibbous moon",
    "category": "Travel & Places",
    "aliases": ["waning_gibbous_moon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "last quarter moon",
    "category": "Travel & Places",
    "aliases": ["last_quarter_moon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "waning crescent moon",
    "category": "Travel & Places",
    "aliases": ["waning_crescent_moon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "crescent moon",
    "category": "Travel & Places",
    "aliases": ["crescent_moon"],
    "tags": ["night"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "new moon face",
    "category": "Travel & Places",
    "aliases": ["new_moon_with_face"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "first quarter moon face",
    "category": "Travel & Places",
    "aliases": ["first_quarter_moon_with_face"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "last quarter moon face",
    "category": "Travel & Places",
    "aliases": ["last_quarter_moon_with_face"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "thermometer",
    "category": "Travel & Places",
    "aliases": ["thermometer"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "sun",
    "category": "Travel & Places",
    "aliases": ["sunny"],
    "tags": ["weather"],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "full moon face",
    "category": "Travel & Places",
    "aliases": ["full_moon_with_face"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sun with face",
    "category": "Travel & Places",
    "aliases": ["sun_with_face"],
    "tags": ["summer"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ringed planet",
    "category": "Travel & Places",
    "aliases": ["ringed_planet"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "star",
    "category": "Travel & Places",
    "aliases": ["star"],
    "tags": [],
    "unicode_version": "5.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "glowing star",
    "category": "Travel & Places",
    "aliases": ["star2"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "shooting star",
    "category": "Travel & Places",
    "aliases": ["stars"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "milky way",
    "category": "Travel & Places",
    "aliases": ["milky_way"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cloud",
    "category": "Travel & Places",
    "aliases": ["cloud"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sun behind cloud",
    "category": "Travel & Places",
    "aliases": ["partly_sunny"],
    "tags": ["weather", "cloud"],
    "unicode_version": "5.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cloud with lightning and rain",
    "category": "Travel & Places",
    "aliases": ["cloud_with_lightning_and_rain"],
    "tags": [],
    "unicode_version": "5.2",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "sun behind small cloud",
    "category": "Travel & Places",
    "aliases": ["sun_behind_small_cloud"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "sun behind large cloud",
    "category": "Travel & Places",
    "aliases": ["sun_behind_large_cloud"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "sun behind rain cloud",
    "category": "Travel & Places",
    "aliases": ["sun_behind_rain_cloud"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "cloud with rain",
    "category": "Travel & Places",
    "aliases": ["cloud_with_rain"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "cloud with snow",
    "category": "Travel & Places",
    "aliases": ["cloud_with_snow"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "cloud with lightning",
    "category": "Travel & Places",
    "aliases": ["cloud_with_lightning"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "tornado",
    "category": "Travel & Places",
    "aliases": ["tornado"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "fog",
    "category": "Travel & Places",
    "aliases": ["fog"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "wind face",
    "category": "Travel & Places",
    "aliases": ["wind_face"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "cyclone",
    "category": "Travel & Places",
    "aliases": ["cyclone"],
    "tags": ["swirl"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "rainbow",
    "category": "Travel & Places",
    "aliases": ["rainbow"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "closed umbrella",
    "category": "Travel & Places",
    "aliases": ["closed_umbrella"],
    "tags": ["weather", "rain"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "umbrella",
    "category": "Travel & Places",
    "aliases": ["open_umbrella"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "umbrella with rain drops",
    "category": "Travel & Places",
    "aliases": ["umbrella"],
    "tags": ["rain", "weather"],
    "unicode_version": "4.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "umbrella on ground",
    "category": "Travel & Places",
    "aliases": ["parasol_on_ground"],
    "tags": ["beach_umbrella"],
    "unicode_version": "5.2",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "high voltage",
    "category": "Travel & Places",
    "aliases": ["zap"],
    "tags": ["lightning", "thunder"],
    "unicode_version": "4.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "snowflake",
    "category": "Travel & Places",
    "aliases": ["snowflake"],
    "tags": ["winter", "cold", "weather"],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "snowman",
    "category": "Travel & Places",
    "aliases": ["snowman_with_snow"],
    "tags": ["winter", "christmas"],
    "unicode_version": "",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "snowman without snow",
    "category": "Travel & Places",
    "aliases": ["snowman"],
    "tags": ["winter"],
    "unicode_version": "5.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "comet",
    "category": "Travel & Places",
    "aliases": ["comet"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "fire",
    "category": "Travel & Places",
    "aliases": ["fire"],
    "tags": ["burn"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "droplet",
    "category": "Travel & Places",
    "aliases": ["droplet"],
    "tags": ["water"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "water wave",
    "category": "Travel & Places",
    "aliases": ["ocean"],
    "tags": ["sea"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "jack-o-lantern",
    "category": "Activities",
    "aliases": ["jack_o_lantern"],
    "tags": ["halloween"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Christmas tree",
    "category": "Activities",
    "aliases": ["christmas_tree"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "fireworks",
    "category": "Activities",
    "aliases": ["fireworks"],
    "tags": ["festival", "celebration"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sparkler",
    "category": "Activities",
    "aliases": ["sparkler"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "firecracker",
    "category": "Activities",
    "aliases": ["firecracker"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "sparkles",
    "category": "Activities",
    "aliases": ["sparkles"],
    "tags": ["shiny"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "balloon",
    "category": "Activities",
    "aliases": ["balloon"],
    "tags": ["party", "birthday"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "party popper",
    "category": "Activities",
    "aliases": ["tada"],
    "tags": ["hooray", "party"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "confetti ball",
    "category": "Activities",
    "aliases": ["confetti_ball"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "tanabata tree",
    "category": "Activities",
    "aliases": ["tanabata_tree"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "pine decoration",
    "category": "Activities",
    "aliases": ["bamboo"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese dolls",
    "category": "Activities",
    "aliases": ["dolls"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "carp streamer",
    "category": "Activities",
    "aliases": ["flags"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "wind chime",
    "category": "Activities",
    "aliases": ["wind_chime"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "moon viewing ceremony",
    "category": "Activities",
    "aliases": ["rice_scene"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "red envelope",
    "category": "Activities",
    "aliases": ["red_envelope"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "ribbon",
    "category": "Activities",
    "aliases": ["ribbon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "wrapped gift",
    "category": "Activities",
    "aliases": ["gift"],
    "tags": ["present", "birthday", "christmas"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "reminder ribbon",
    "category": "Activities",
    "aliases": ["reminder_ribbon"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "admission tickets",
    "category": "Activities",
    "aliases": ["tickets"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "ticket",
    "category": "Activities",
    "aliases": ["ticket"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "military medal",
    "category": "Activities",
    "aliases": ["medal_military"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "trophy",
    "category": "Activities",
    "aliases": ["trophy"],
    "tags": ["award", "contest", "winner"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sports medal",
    "category": "Activities",
    "aliases": ["medal_sports"],
    "tags": ["gold", "winner"],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "1st place medal",
    "category": "Activities",
    "aliases": ["1st_place_medal"],
    "tags": ["gold"],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "2nd place medal",
    "category": "Activities",
    "aliases": ["2nd_place_medal"],
    "tags": ["silver"],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "3rd place medal",
    "category": "Activities",
    "aliases": ["3rd_place_medal"],
    "tags": ["bronze"],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "soccer ball",
    "category": "Activities",
    "aliases": ["soccer"],
    "tags": ["sports"],
    "unicode_version": "5.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "baseball",
    "category": "Activities",
    "aliases": ["baseball"],
    "tags": ["sports"],
    "unicode_version": "5.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "softball",
    "category": "Activities",
    "aliases": ["softball"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "basketball",
    "category": "Activities",
    "aliases": ["basketball"],
    "tags": ["sports"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "volleyball",
    "category": "Activities",
    "aliases": ["volleyball"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "american football",
    "category": "Activities",
    "aliases": ["football"],
    "tags": ["sports"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "rugby football",
    "category": "Activities",
    "aliases": ["rugby_football"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "tennis",
    "category": "Activities",
    "aliases": ["tennis"],
    "tags": ["sports"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "flying disc",
    "category": "Activities",
    "aliases": ["flying_disc"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "bowling",
    "category": "Activities",
    "aliases": ["bowling"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cricket game",
    "category": "Activities",
    "aliases": ["cricket_game"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "field hockey",
    "category": "Activities",
    "aliases": ["field_hockey"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "ice hockey",
    "category": "Activities",
    "aliases": ["ice_hockey"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "lacrosse",
    "category": "Activities",
    "aliases": ["lacrosse"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "ping pong",
    "category": "Activities",
    "aliases": ["ping_pong"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "badminton",
    "category": "Activities",
    "aliases": ["badminton"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "boxing glove",
    "category": "Activities",
    "aliases": ["boxing_glove"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "martial arts uniform",
    "category": "Activities",
    "aliases": ["martial_arts_uniform"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "goal net",
    "category": "Activities",
    "aliases": ["goal_net"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "flag in hole",
    "category": "Activities",
    "aliases": ["golf"],
    "tags": [],
    "unicode_version": "5.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ice skate",
    "category": "Activities",
    "aliases": ["ice_skate"],
    "tags": ["skating"],
    "unicode_version": "5.2",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "fishing pole",
    "category": "Activities",
    "aliases": ["fishing_pole_and_fish"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "diving mask",
    "category": "Activities",
    "aliases": ["diving_mask"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "running shirt",
    "category": "Activities",
    "aliases": ["running_shirt_with_sash"],
    "tags": ["marathon"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "skis",
    "category": "Activities",
    "aliases": ["ski"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sled",
    "category": "Activities",
    "aliases": ["sled"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "curling stone",
    "category": "Activities",
    "aliases": ["curling_stone"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "bullseye",
    "category": "Activities",
    "aliases": ["dart"],
    "tags": ["target"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "yo-yo",
    "category": "Activities",
    "aliases": ["yo_yo"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "kite",
    "category": "Activities",
    "aliases": ["kite"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "water pistol",
    "category": "Activities",
    "aliases": ["gun"],
    "tags": ["shoot", "weapon"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "pool 8 ball",
    "category": "Activities",
    "aliases": ["8ball"],
    "tags": ["pool", "billiards"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "crystal ball",
    "category": "Activities",
    "aliases": ["crystal_ball"],
    "tags": ["fortune"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "magic wand",
    "category": "Activities",
    "aliases": ["magic_wand"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "video game",
    "category": "Activities",
    "aliases": ["video_game"],
    "tags": ["play", "controller", "console"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "joystick",
    "category": "Activities",
    "aliases": ["joystick"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "slot machine",
    "category": "Activities",
    "aliases": ["slot_machine"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "game die",
    "category": "Activities",
    "aliases": ["game_die"],
    "tags": ["dice", "gambling"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "puzzle piece",
    "category": "Activities",
    "aliases": ["jigsaw"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "teddy bear",
    "category": "Activities",
    "aliases": ["teddy_bear"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "piata",
    "category": "Activities",
    "aliases": ["pinata"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "mirror ball",
    "category": "Activities",
    "aliases": ["mirror_ball"],
    "tags": ["disco", "party"],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "nesting dolls",
    "category": "Activities",
    "aliases": ["nesting_dolls"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "spade suit",
    "category": "Activities",
    "aliases": ["spades"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "heart suit",
    "category": "Activities",
    "aliases": ["hearts"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "diamond suit",
    "category": "Activities",
    "aliases": ["diamonds"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "club suit",
    "category": "Activities",
    "aliases": ["clubs"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "chess pawn",
    "category": "Activities",
    "aliases": ["chess_pawn"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "joker",
    "category": "Activities",
    "aliases": ["black_joker"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "mahjong red dragon",
    "category": "Activities",
    "aliases": ["mahjong"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "flower playing cards",
    "category": "Activities",
    "aliases": ["flower_playing_cards"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "performing arts",
    "category": "Activities",
    "aliases": ["performing_arts"],
    "tags": ["theater", "drama"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "framed picture",
    "category": "Activities",
    "aliases": ["framed_picture"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "artist palette",
    "category": "Activities",
    "aliases": ["art"],
    "tags": ["design", "paint"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "thread",
    "category": "Activities",
    "aliases": ["thread"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "sewing needle",
    "category": "Activities",
    "aliases": ["sewing_needle"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "yarn",
    "category": "Activities",
    "aliases": ["yarn"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "knot",
    "category": "Activities",
    "aliases": ["knot"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "glasses",
    "category": "Objects",
    "aliases": ["eyeglasses"],
    "tags": ["glasses"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sunglasses",
    "category": "Objects",
    "aliases": ["dark_sunglasses"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "goggles",
    "category": "Objects",
    "aliases": ["goggles"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "lab coat",
    "category": "Objects",
    "aliases": ["lab_coat"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "safety vest",
    "category": "Objects",
    "aliases": ["safety_vest"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "necktie",
    "category": "Objects",
    "aliases": ["necktie"],
    "tags": ["shirt", "formal"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "t-shirt",
    "category": "Objects",
    "aliases": ["shirt", "tshirt"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "jeans",
    "category": "Objects",
    "aliases": ["jeans"],
    "tags": ["pants"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "scarf",
    "category": "Objects",
    "aliases": ["scarf"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "gloves",
    "category": "Objects",
    "aliases": ["gloves"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "coat",
    "category": "Objects",
    "aliases": ["coat"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "socks",
    "category": "Objects",
    "aliases": ["socks"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "dress",
    "category": "Objects",
    "aliases": ["dress"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "kimono",
    "category": "Objects",
    "aliases": ["kimono"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sari",
    "category": "Objects",
    "aliases": ["sari"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "one-piece swimsuit",
    "category": "Objects",
    "aliases": ["one_piece_swimsuit"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "briefs",
    "category": "Objects",
    "aliases": ["swim_brief"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "shorts",
    "category": "Objects",
    "aliases": ["shorts"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "bikini",
    "category": "Objects",
    "aliases": ["bikini"],
    "tags": ["beach"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "womans clothes",
    "category": "Objects",
    "aliases": ["womans_clothes"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "folding hand fan",
    "category": "Objects",
    "aliases": ["folding_hand_fan"],
    "tags": ["sensu"],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "purse",
    "category": "Objects",
    "aliases": ["purse"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "handbag",
    "category": "Objects",
    "aliases": ["handbag"],
    "tags": ["bag"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "clutch bag",
    "category": "Objects",
    "aliases": ["pouch"],
    "tags": ["bag"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "shopping bags",
    "category": "Objects",
    "aliases": ["shopping"],
    "tags": ["bags"],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "backpack",
    "category": "Objects",
    "aliases": ["school_satchel"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "thong sandal",
    "category": "Objects",
    "aliases": ["thong_sandal"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "mans shoe",
    "category": "Objects",
    "aliases": ["mans_shoe", "shoe"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "running shoe",
    "category": "Objects",
    "aliases": ["athletic_shoe"],
    "tags": ["sneaker", "sport", "running"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "hiking boot",
    "category": "Objects",
    "aliases": ["hiking_boot"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "flat shoe",
    "category": "Objects",
    "aliases": ["flat_shoe"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "high-heeled shoe",
    "category": "Objects",
    "aliases": ["high_heel"],
    "tags": ["shoe"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "womans sandal",
    "category": "Objects",
    "aliases": ["sandal"],
    "tags": ["shoe"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ballet shoes",
    "category": "Objects",
    "aliases": ["ballet_shoes"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "womans boot",
    "category": "Objects",
    "aliases": ["boot"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "hair pick",
    "category": "Objects",
    "aliases": ["hair_pick"],
    "tags": [],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "crown",
    "category": "Objects",
    "aliases": ["crown"],
    "tags": ["king", "queen", "royal"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "womans hat",
    "category": "Objects",
    "aliases": ["womans_hat"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "top hat",
    "category": "Objects",
    "aliases": ["tophat"],
    "tags": ["hat", "classy"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "graduation cap",
    "category": "Objects",
    "aliases": ["mortar_board"],
    "tags": ["education", "college", "university", "graduation"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "billed cap",
    "category": "Objects",
    "aliases": ["billed_cap"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "military helmet",
    "category": "Objects",
    "aliases": ["military_helmet"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "rescue workers helmet",
    "category": "Objects",
    "aliases": ["rescue_worker_helmet"],
    "tags": [],
    "unicode_version": "5.2",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "prayer beads",
    "category": "Objects",
    "aliases": ["prayer_beads"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "lipstick",
    "category": "Objects",
    "aliases": ["lipstick"],
    "tags": ["makeup"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ring",
    "category": "Objects",
    "aliases": ["ring"],
    "tags": ["wedding", "marriage", "engaged"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "gem stone",
    "category": "Objects",
    "aliases": ["gem"],
    "tags": ["diamond"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "muted speaker",
    "category": "Objects",
    "aliases": ["mute"],
    "tags": ["sound", "volume"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "speaker low volume",
    "category": "Objects",
    "aliases": ["speaker"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "speaker medium volume",
    "category": "Objects",
    "aliases": ["sound"],
    "tags": ["volume"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "speaker high volume",
    "category": "Objects",
    "aliases": ["loud_sound"],
    "tags": ["volume"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "loudspeaker",
    "category": "Objects",
    "aliases": ["loudspeaker"],
    "tags": ["announcement"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "megaphone",
    "category": "Objects",
    "aliases": ["mega"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "postal horn",
    "category": "Objects",
    "aliases": ["postal_horn"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "bell",
    "category": "Objects",
    "aliases": ["bell"],
    "tags": ["sound", "notification"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "bell with slash",
    "category": "Objects",
    "aliases": ["no_bell"],
    "tags": ["volume", "off"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "musical score",
    "category": "Objects",
    "aliases": ["musical_score"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "musical note",
    "category": "Objects",
    "aliases": ["musical_note"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "musical notes",
    "category": "Objects",
    "aliases": ["notes"],
    "tags": ["music"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "studio microphone",
    "category": "Objects",
    "aliases": ["studio_microphone"],
    "tags": ["podcast"],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "level slider",
    "category": "Objects",
    "aliases": ["level_slider"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "control knobs",
    "category": "Objects",
    "aliases": ["control_knobs"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "microphone",
    "category": "Objects",
    "aliases": ["microphone"],
    "tags": ["sing"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "headphone",
    "category": "Objects",
    "aliases": ["headphones"],
    "tags": ["music", "earphones"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "radio",
    "category": "Objects",
    "aliases": ["radio"],
    "tags": ["podcast"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "saxophone",
    "category": "Objects",
    "aliases": ["saxophone"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "accordion",
    "category": "Objects",
    "aliases": ["accordion"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "guitar",
    "category": "Objects",
    "aliases": ["guitar"],
    "tags": ["rock"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "musical keyboard",
    "category": "Objects",
    "aliases": ["musical_keyboard"],
    "tags": ["piano"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "trumpet",
    "category": "Objects",
    "aliases": ["trumpet"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "violin",
    "category": "Objects",
    "aliases": ["violin"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "banjo",
    "category": "Objects",
    "aliases": ["banjo"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "drum",
    "category": "Objects",
    "aliases": ["drum"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "long drum",
    "category": "Objects",
    "aliases": ["long_drum"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "maracas",
    "category": "Objects",
    "aliases": ["maracas"],
    "tags": ["shaker"],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "flute",
    "category": "Objects",
    "aliases": ["flute"],
    "tags": ["recorder"],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "mobile phone",
    "category": "Objects",
    "aliases": ["iphone"],
    "tags": ["smartphone", "mobile"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "mobile phone with arrow",
    "category": "Objects",
    "aliases": ["calling"],
    "tags": ["call", "incoming"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "telephone",
    "category": "Objects",
    "aliases": ["phone", "telephone"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "telephone receiver",
    "category": "Objects",
    "aliases": ["telephone_receiver"],
    "tags": ["phone", "call"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "pager",
    "category": "Objects",
    "aliases": ["pager"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "fax machine",
    "category": "Objects",
    "aliases": ["fax"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "battery",
    "category": "Objects",
    "aliases": ["battery"],
    "tags": ["power"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "low battery",
    "category": "Objects",
    "aliases": ["low_battery"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "electric plug",
    "category": "Objects",
    "aliases": ["electric_plug"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "laptop",
    "category": "Objects",
    "aliases": ["computer"],
    "tags": ["desktop", "screen"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "desktop computer",
    "category": "Objects",
    "aliases": ["desktop_computer"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "printer",
    "category": "Objects",
    "aliases": ["printer"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "keyboard",
    "category": "Objects",
    "aliases": ["keyboard"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "computer mouse",
    "category": "Objects",
    "aliases": ["computer_mouse"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "trackball",
    "category": "Objects",
    "aliases": ["trackball"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "computer disk",
    "category": "Objects",
    "aliases": ["minidisc"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "floppy disk",
    "category": "Objects",
    "aliases": ["floppy_disk"],
    "tags": ["save"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "optical disk",
    "category": "Objects",
    "aliases": ["cd"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "dvd",
    "category": "Objects",
    "aliases": ["dvd"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "abacus",
    "category": "Objects",
    "aliases": ["abacus"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "movie camera",
    "category": "Objects",
    "aliases": ["movie_camera"],
    "tags": ["film", "video"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "film frames",
    "category": "Objects",
    "aliases": ["film_strip"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "film projector",
    "category": "Objects",
    "aliases": ["film_projector"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "clapper board",
    "category": "Objects",
    "aliases": ["clapper"],
    "tags": ["film"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "television",
    "category": "Objects",
    "aliases": ["tv"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "camera",
    "category": "Objects",
    "aliases": ["camera"],
    "tags": ["photo"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "camera with flash",
    "category": "Objects",
    "aliases": ["camera_flash"],
    "tags": ["photo"],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "video camera",
    "category": "Objects",
    "aliases": ["video_camera"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "videocassette",
    "category": "Objects",
    "aliases": ["vhs"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "magnifying glass tilted left",
    "category": "Objects",
    "aliases": ["mag"],
    "tags": ["search", "zoom"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "magnifying glass tilted right",
    "category": "Objects",
    "aliases": ["mag_right"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "candle",
    "category": "Objects",
    "aliases": ["candle"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "light bulb",
    "category": "Objects",
    "aliases": ["bulb"],
    "tags": ["idea", "light"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "flashlight",
    "category": "Objects",
    "aliases": ["flashlight"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "red paper lantern",
    "category": "Objects",
    "aliases": ["izakaya_lantern", "lantern"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "diya lamp",
    "category": "Objects",
    "aliases": ["diya_lamp"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "notebook with decorative cover",
    "category": "Objects",
    "aliases": ["notebook_with_decorative_cover"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "closed book",
    "category": "Objects",
    "aliases": ["closed_book"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "open book",
    "category": "Objects",
    "aliases": ["book", "open_book"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "green book",
    "category": "Objects",
    "aliases": ["green_book"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "blue book",
    "category": "Objects",
    "aliases": ["blue_book"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "orange book",
    "category": "Objects",
    "aliases": ["orange_book"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "books",
    "category": "Objects",
    "aliases": ["books"],
    "tags": ["library"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "notebook",
    "category": "Objects",
    "aliases": ["notebook"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ledger",
    "category": "Objects",
    "aliases": ["ledger"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "page with curl",
    "category": "Objects",
    "aliases": ["page_with_curl"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "scroll",
    "category": "Objects",
    "aliases": ["scroll"],
    "tags": ["document"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "page facing up",
    "category": "Objects",
    "aliases": ["page_facing_up"],
    "tags": ["document"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "newspaper",
    "category": "Objects",
    "aliases": ["newspaper"],
    "tags": ["press"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "rolled-up newspaper",
    "category": "Objects",
    "aliases": ["newspaper_roll"],
    "tags": ["press"],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "bookmark tabs",
    "category": "Objects",
    "aliases": ["bookmark_tabs"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "bookmark",
    "category": "Objects",
    "aliases": ["bookmark"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "label",
    "category": "Objects",
    "aliases": ["label"],
    "tags": ["tag"],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "money bag",
    "category": "Objects",
    "aliases": ["moneybag"],
    "tags": ["dollar", "cream"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "coin",
    "category": "Objects",
    "aliases": ["coin"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "yen banknote",
    "category": "Objects",
    "aliases": ["yen"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "dollar banknote",
    "category": "Objects",
    "aliases": ["dollar"],
    "tags": ["money"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "euro banknote",
    "category": "Objects",
    "aliases": ["euro"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "pound banknote",
    "category": "Objects",
    "aliases": ["pound"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "money with wings",
    "category": "Objects",
    "aliases": ["money_with_wings"],
    "tags": ["dollar"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "credit card",
    "category": "Objects",
    "aliases": ["credit_card"],
    "tags": ["subscription"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "receipt",
    "category": "Objects",
    "aliases": ["receipt"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "chart increasing with yen",
    "category": "Objects",
    "aliases": ["chart"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "envelope",
    "category": "Objects",
    "aliases": ["envelope"],
    "tags": ["letter", "email"],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "e-mail",
    "category": "Objects",
    "aliases": ["email", "e-mail"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "incoming envelope",
    "category": "Objects",
    "aliases": ["incoming_envelope"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "envelope with arrow",
    "category": "Objects",
    "aliases": ["envelope_with_arrow"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "outbox tray",
    "category": "Objects",
    "aliases": ["outbox_tray"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "inbox tray",
    "category": "Objects",
    "aliases": ["inbox_tray"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "package",
    "category": "Objects",
    "aliases": ["package"],
    "tags": ["shipping"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "closed mailbox with raised flag",
    "category": "Objects",
    "aliases": ["mailbox"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "closed mailbox with lowered flag",
    "category": "Objects",
    "aliases": ["mailbox_closed"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "open mailbox with raised flag",
    "category": "Objects",
    "aliases": ["mailbox_with_mail"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "open mailbox with lowered flag",
    "category": "Objects",
    "aliases": ["mailbox_with_no_mail"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "postbox",
    "category": "Objects",
    "aliases": ["postbox"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ballot box with ballot",
    "category": "Objects",
    "aliases": ["ballot_box"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "pencil",
    "category": "Objects",
    "aliases": ["pencil2"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "black nib",
    "category": "Objects",
    "aliases": ["black_nib"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "fountain pen",
    "category": "Objects",
    "aliases": ["fountain_pen"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "pen",
    "category": "Objects",
    "aliases": ["pen"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "paintbrush",
    "category": "Objects",
    "aliases": ["paintbrush"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "crayon",
    "category": "Objects",
    "aliases": ["crayon"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "memo",
    "category": "Objects",
    "aliases": ["memo", "pencil"],
    "tags": ["document", "note"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "briefcase",
    "category": "Objects",
    "aliases": ["briefcase"],
    "tags": ["business"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "file folder",
    "category": "Objects",
    "aliases": ["file_folder"],
    "tags": ["directory"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "open file folder",
    "category": "Objects",
    "aliases": ["open_file_folder"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "card index dividers",
    "category": "Objects",
    "aliases": ["card_index_dividers"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "calendar",
    "category": "Objects",
    "aliases": ["date"],
    "tags": ["calendar", "schedule"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "tear-off calendar",
    "category": "Objects",
    "aliases": ["calendar"],
    "tags": ["schedule"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "spiral notepad",
    "category": "Objects",
    "aliases": ["spiral_notepad"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "spiral calendar",
    "category": "Objects",
    "aliases": ["spiral_calendar"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "card index",
    "category": "Objects",
    "aliases": ["card_index"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "chart increasing",
    "category": "Objects",
    "aliases": ["chart_with_upwards_trend"],
    "tags": ["graph", "metrics"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "chart decreasing",
    "category": "Objects",
    "aliases": ["chart_with_downwards_trend"],
    "tags": ["graph", "metrics"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "bar chart",
    "category": "Objects",
    "aliases": ["bar_chart"],
    "tags": ["stats", "metrics"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "clipboard",
    "category": "Objects",
    "aliases": ["clipboard"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "pushpin",
    "category": "Objects",
    "aliases": ["pushpin"],
    "tags": ["location"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "round pushpin",
    "category": "Objects",
    "aliases": ["round_pushpin"],
    "tags": ["location"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "paperclip",
    "category": "Objects",
    "aliases": ["paperclip"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "linked paperclips",
    "category": "Objects",
    "aliases": ["paperclips"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "straight ruler",
    "category": "Objects",
    "aliases": ["straight_ruler"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "triangular ruler",
    "category": "Objects",
    "aliases": ["triangular_ruler"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "scissors",
    "category": "Objects",
    "aliases": ["scissors"],
    "tags": ["cut"],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "card file box",
    "category": "Objects",
    "aliases": ["card_file_box"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "file cabinet",
    "category": "Objects",
    "aliases": ["file_cabinet"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "wastebasket",
    "category": "Objects",
    "aliases": ["wastebasket"],
    "tags": ["trash"],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "locked",
    "category": "Objects",
    "aliases": ["lock"],
    "tags": ["security", "private"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "unlocked",
    "category": "Objects",
    "aliases": ["unlock"],
    "tags": ["security"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "locked with pen",
    "category": "Objects",
    "aliases": ["lock_with_ink_pen"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "locked with key",
    "category": "Objects",
    "aliases": ["closed_lock_with_key"],
    "tags": ["security"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "key",
    "category": "Objects",
    "aliases": ["key"],
    "tags": ["lock", "password"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "old key",
    "category": "Objects",
    "aliases": ["old_key"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "hammer",
    "category": "Objects",
    "aliases": ["hammer"],
    "tags": ["tool"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "axe",
    "category": "Objects",
    "aliases": ["axe"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "pick",
    "category": "Objects",
    "aliases": ["pick"],
    "tags": [],
    "unicode_version": "5.2",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "hammer and pick",
    "category": "Objects",
    "aliases": ["hammer_and_pick"],
    "tags": [],
    "unicode_version": "4.1",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "hammer and wrench",
    "category": "Objects",
    "aliases": ["hammer_and_wrench"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "dagger",
    "category": "Objects",
    "aliases": ["dagger"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "crossed swords",
    "category": "Objects",
    "aliases": ["crossed_swords"],
    "tags": [],
    "unicode_version": "4.1",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "bomb",
    "category": "Objects",
    "aliases": ["bomb"],
    "tags": ["boom"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "boomerang",
    "category": "Objects",
    "aliases": ["boomerang"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "bow and arrow",
    "category": "Objects",
    "aliases": ["bow_and_arrow"],
    "tags": ["archery"],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "shield",
    "category": "Objects",
    "aliases": ["shield"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "carpentry saw",
    "category": "Objects",
    "aliases": ["carpentry_saw"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "wrench",
    "category": "Objects",
    "aliases": ["wrench"],
    "tags": ["tool"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "screwdriver",
    "category": "Objects",
    "aliases": ["screwdriver"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "nut and bolt",
    "category": "Objects",
    "aliases": ["nut_and_bolt"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "gear",
    "category": "Objects",
    "aliases": ["gear"],
    "tags": [],
    "unicode_version": "4.1",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "clamp",
    "category": "Objects",
    "aliases": ["clamp"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "balance scale",
    "category": "Objects",
    "aliases": ["balance_scale"],
    "tags": [],
    "unicode_version": "4.1",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "white cane",
    "category": "Objects",
    "aliases": ["probing_cane"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "link",
    "category": "Objects",
    "aliases": ["link"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "chains",
    "category": "Objects",
    "aliases": ["chains"],
    "tags": [],
    "unicode_version": "5.2",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "hook",
    "category": "Objects",
    "aliases": ["hook"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "toolbox",
    "category": "Objects",
    "aliases": ["toolbox"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "magnet",
    "category": "Objects",
    "aliases": ["magnet"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "ladder",
    "category": "Objects",
    "aliases": ["ladder"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "alembic",
    "category": "Objects",
    "aliases": ["alembic"],
    "tags": [],
    "unicode_version": "4.1",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "test tube",
    "category": "Objects",
    "aliases": ["test_tube"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "petri dish",
    "category": "Objects",
    "aliases": ["petri_dish"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "dna",
    "category": "Objects",
    "aliases": ["dna"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "microscope",
    "category": "Objects",
    "aliases": ["microscope"],
    "tags": ["science", "laboratory", "investigate"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "telescope",
    "category": "Objects",
    "aliases": ["telescope"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "satellite antenna",
    "category": "Objects",
    "aliases": ["satellite"],
    "tags": ["signal"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "syringe",
    "category": "Objects",
    "aliases": ["syringe"],
    "tags": ["health", "hospital", "needle"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "drop of blood",
    "category": "Objects",
    "aliases": ["drop_of_blood"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "pill",
    "category": "Objects",
    "aliases": ["pill"],
    "tags": ["health", "medicine"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "adhesive bandage",
    "category": "Objects",
    "aliases": ["adhesive_bandage"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "crutch",
    "category": "Objects",
    "aliases": ["crutch"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "stethoscope",
    "category": "Objects",
    "aliases": ["stethoscope"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "x-ray",
    "category": "Objects",
    "aliases": ["x_ray"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "door",
    "category": "Objects",
    "aliases": ["door"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "elevator",
    "category": "Objects",
    "aliases": ["elevator"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "mirror",
    "category": "Objects",
    "aliases": ["mirror"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "window",
    "category": "Objects",
    "aliases": ["window"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "bed",
    "category": "Objects",
    "aliases": ["bed"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "couch and lamp",
    "category": "Objects",
    "aliases": ["couch_and_lamp"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "chair",
    "category": "Objects",
    "aliases": ["chair"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "toilet",
    "category": "Objects",
    "aliases": ["toilet"],
    "tags": ["wc"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "plunger",
    "category": "Objects",
    "aliases": ["plunger"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "shower",
    "category": "Objects",
    "aliases": ["shower"],
    "tags": ["bath"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "bathtub",
    "category": "Objects",
    "aliases": ["bathtub"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "mouse trap",
    "category": "Objects",
    "aliases": ["mouse_trap"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "razor",
    "category": "Objects",
    "aliases": ["razor"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "lotion bottle",
    "category": "Objects",
    "aliases": ["lotion_bottle"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "safety pin",
    "category": "Objects",
    "aliases": ["safety_pin"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "broom",
    "category": "Objects",
    "aliases": ["broom"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "basket",
    "category": "Objects",
    "aliases": ["basket"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "roll of paper",
    "category": "Objects",
    "aliases": ["roll_of_paper"],
    "tags": ["toilet"],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "bucket",
    "category": "Objects",
    "aliases": ["bucket"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "soap",
    "category": "Objects",
    "aliases": ["soap"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "bubbles",
    "category": "Objects",
    "aliases": ["bubbles"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "toothbrush",
    "category": "Objects",
    "aliases": ["toothbrush"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "sponge",
    "category": "Objects",
    "aliases": ["sponge"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "fire extinguisher",
    "category": "Objects",
    "aliases": ["fire_extinguisher"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "shopping cart",
    "category": "Objects",
    "aliases": ["shopping_cart"],
    "tags": [],
    "unicode_version": "9.0",
    "ios_version": "10.2"
  },
  {
    "emoji": "",
    "description": "cigarette",
    "category": "Objects",
    "aliases": ["smoking"],
    "tags": ["cigarette"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "coffin",
    "category": "Objects",
    "aliases": ["coffin"],
    "tags": ["funeral"],
    "unicode_version": "4.1",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "headstone",
    "category": "Objects",
    "aliases": ["headstone"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "funeral urn",
    "category": "Objects",
    "aliases": ["funeral_urn"],
    "tags": [],
    "unicode_version": "4.1",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "nazar amulet",
    "category": "Objects",
    "aliases": ["nazar_amulet"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "hamsa",
    "category": "Objects",
    "aliases": ["hamsa"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "moai",
    "category": "Objects",
    "aliases": ["moyai"],
    "tags": ["stone"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "placard",
    "category": "Objects",
    "aliases": ["placard"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "identification card",
    "category": "Objects",
    "aliases": ["identification_card"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "ATM sign",
    "category": "Symbols",
    "aliases": ["atm"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "litter in bin sign",
    "category": "Symbols",
    "aliases": ["put_litter_in_its_place"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "potable water",
    "category": "Symbols",
    "aliases": ["potable_water"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "wheelchair symbol",
    "category": "Symbols",
    "aliases": ["wheelchair"],
    "tags": ["accessibility"],
    "unicode_version": "4.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "mens room",
    "category": "Symbols",
    "aliases": ["mens"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "womens room",
    "category": "Symbols",
    "aliases": ["womens"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "restroom",
    "category": "Symbols",
    "aliases": ["restroom"],
    "tags": ["toilet"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "baby symbol",
    "category": "Symbols",
    "aliases": ["baby_symbol"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "water closet",
    "category": "Symbols",
    "aliases": ["wc"],
    "tags": ["toilet", "restroom"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "passport control",
    "category": "Symbols",
    "aliases": ["passport_control"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "customs",
    "category": "Symbols",
    "aliases": ["customs"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "baggage claim",
    "category": "Symbols",
    "aliases": ["baggage_claim"],
    "tags": ["airport"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "left luggage",
    "category": "Symbols",
    "aliases": ["left_luggage"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "warning",
    "category": "Symbols",
    "aliases": ["warning"],
    "tags": ["wip"],
    "unicode_version": "4.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "children crossing",
    "category": "Symbols",
    "aliases": ["children_crossing"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "no entry",
    "category": "Symbols",
    "aliases": ["no_entry"],
    "tags": ["limit"],
    "unicode_version": "5.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "prohibited",
    "category": "Symbols",
    "aliases": ["no_entry_sign"],
    "tags": ["block", "forbidden"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "no bicycles",
    "category": "Symbols",
    "aliases": ["no_bicycles"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "no smoking",
    "category": "Symbols",
    "aliases": ["no_smoking"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "no littering",
    "category": "Symbols",
    "aliases": ["do_not_litter"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "non-potable water",
    "category": "Symbols",
    "aliases": ["non-potable_water"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "no pedestrians",
    "category": "Symbols",
    "aliases": ["no_pedestrians"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "no mobile phones",
    "category": "Symbols",
    "aliases": ["no_mobile_phones"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "no one under eighteen",
    "category": "Symbols",
    "aliases": ["underage"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "radioactive",
    "category": "Symbols",
    "aliases": ["radioactive"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "biohazard",
    "category": "Symbols",
    "aliases": ["biohazard"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "up arrow",
    "category": "Symbols",
    "aliases": ["arrow_up"],
    "tags": [],
    "unicode_version": "4.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "up-right arrow",
    "category": "Symbols",
    "aliases": ["arrow_upper_right"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "right arrow",
    "category": "Symbols",
    "aliases": ["arrow_right"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "down-right arrow",
    "category": "Symbols",
    "aliases": ["arrow_lower_right"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "down arrow",
    "category": "Symbols",
    "aliases": ["arrow_down"],
    "tags": [],
    "unicode_version": "4.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "down-left arrow",
    "category": "Symbols",
    "aliases": ["arrow_lower_left"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "left arrow",
    "category": "Symbols",
    "aliases": ["arrow_left"],
    "tags": [],
    "unicode_version": "4.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "up-left arrow",
    "category": "Symbols",
    "aliases": ["arrow_upper_left"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "up-down arrow",
    "category": "Symbols",
    "aliases": ["arrow_up_down"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "left-right arrow",
    "category": "Symbols",
    "aliases": ["left_right_arrow"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "right arrow curving left",
    "category": "Symbols",
    "aliases": ["leftwards_arrow_with_hook"],
    "tags": ["return"],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "left arrow curving right",
    "category": "Symbols",
    "aliases": ["arrow_right_hook"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "right arrow curving up",
    "category": "Symbols",
    "aliases": ["arrow_heading_up"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "right arrow curving down",
    "category": "Symbols",
    "aliases": ["arrow_heading_down"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "clockwise vertical arrows",
    "category": "Symbols",
    "aliases": ["arrows_clockwise"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "counterclockwise arrows button",
    "category": "Symbols",
    "aliases": ["arrows_counterclockwise"],
    "tags": ["sync"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "BACK arrow",
    "category": "Symbols",
    "aliases": ["back"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "END arrow",
    "category": "Symbols",
    "aliases": ["end"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ON! arrow",
    "category": "Symbols",
    "aliases": ["on"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "SOON arrow",
    "category": "Symbols",
    "aliases": ["soon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "TOP arrow",
    "category": "Symbols",
    "aliases": ["top"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "place of worship",
    "category": "Symbols",
    "aliases": ["place_of_worship"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "atom symbol",
    "category": "Symbols",
    "aliases": ["atom_symbol"],
    "tags": [],
    "unicode_version": "4.1",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "om",
    "category": "Symbols",
    "aliases": ["om"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "star of David",
    "category": "Symbols",
    "aliases": ["star_of_david"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "wheel of dharma",
    "category": "Symbols",
    "aliases": ["wheel_of_dharma"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "yin yang",
    "category": "Symbols",
    "aliases": ["yin_yang"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "latin cross",
    "category": "Symbols",
    "aliases": ["latin_cross"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "orthodox cross",
    "category": "Symbols",
    "aliases": ["orthodox_cross"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "star and crescent",
    "category": "Symbols",
    "aliases": ["star_and_crescent"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "peace symbol",
    "category": "Symbols",
    "aliases": ["peace_symbol"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "menorah",
    "category": "Symbols",
    "aliases": ["menorah"],
    "tags": [],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "dotted six-pointed star",
    "category": "Symbols",
    "aliases": ["six_pointed_star"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "khanda",
    "category": "Symbols",
    "aliases": ["khanda"],
    "tags": [],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "Aries",
    "category": "Symbols",
    "aliases": ["aries"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Taurus",
    "category": "Symbols",
    "aliases": ["taurus"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Gemini",
    "category": "Symbols",
    "aliases": ["gemini"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Cancer",
    "category": "Symbols",
    "aliases": ["cancer"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Leo",
    "category": "Symbols",
    "aliases": ["leo"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Virgo",
    "category": "Symbols",
    "aliases": ["virgo"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Libra",
    "category": "Symbols",
    "aliases": ["libra"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Scorpio",
    "category": "Symbols",
    "aliases": ["scorpius"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Sagittarius",
    "category": "Symbols",
    "aliases": ["sagittarius"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Capricorn",
    "category": "Symbols",
    "aliases": ["capricorn"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Aquarius",
    "category": "Symbols",
    "aliases": ["aquarius"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Pisces",
    "category": "Symbols",
    "aliases": ["pisces"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Ophiuchus",
    "category": "Symbols",
    "aliases": ["ophiuchus"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "shuffle tracks button",
    "category": "Symbols",
    "aliases": ["twisted_rightwards_arrows"],
    "tags": ["shuffle"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "repeat button",
    "category": "Symbols",
    "aliases": ["repeat"],
    "tags": ["loop"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "repeat single button",
    "category": "Symbols",
    "aliases": ["repeat_one"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "play button",
    "category": "Symbols",
    "aliases": ["arrow_forward"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "fast-forward button",
    "category": "Symbols",
    "aliases": ["fast_forward"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "next track button",
    "category": "Symbols",
    "aliases": ["next_track_button"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "play or pause button",
    "category": "Symbols",
    "aliases": ["play_or_pause_button"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "reverse button",
    "category": "Symbols",
    "aliases": ["arrow_backward"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "fast reverse button",
    "category": "Symbols",
    "aliases": ["rewind"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "last track button",
    "category": "Symbols",
    "aliases": ["previous_track_button"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "upwards button",
    "category": "Symbols",
    "aliases": ["arrow_up_small"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "fast up button",
    "category": "Symbols",
    "aliases": ["arrow_double_up"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "downwards button",
    "category": "Symbols",
    "aliases": ["arrow_down_small"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "fast down button",
    "category": "Symbols",
    "aliases": ["arrow_double_down"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "pause button",
    "category": "Symbols",
    "aliases": ["pause_button"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "stop button",
    "category": "Symbols",
    "aliases": ["stop_button"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "record button",
    "category": "Symbols",
    "aliases": ["record_button"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "eject button",
    "category": "Symbols",
    "aliases": ["eject_button"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "cinema",
    "category": "Symbols",
    "aliases": ["cinema"],
    "tags": ["film", "movie"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "dim button",
    "category": "Symbols",
    "aliases": ["low_brightness"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "bright button",
    "category": "Symbols",
    "aliases": ["high_brightness"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "antenna bars",
    "category": "Symbols",
    "aliases": ["signal_strength"],
    "tags": ["wifi"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "wireless",
    "category": "Symbols",
    "aliases": ["wireless"],
    "tags": ["wifi"],
    "unicode_version": "15.0",
    "ios_version": "16.4"
  },
  {
    "emoji": "",
    "description": "vibration mode",
    "category": "Symbols",
    "aliases": ["vibration_mode"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "mobile phone off",
    "category": "Symbols",
    "aliases": ["mobile_phone_off"],
    "tags": ["mute", "off"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "female sign",
    "category": "Symbols",
    "aliases": ["female_sign"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "male sign",
    "category": "Symbols",
    "aliases": ["male_sign"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "transgender symbol",
    "category": "Symbols",
    "aliases": ["transgender_symbol"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "multiply",
    "category": "Symbols",
    "aliases": ["heavy_multiplication_x"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "plus",
    "category": "Symbols",
    "aliases": ["heavy_plus_sign"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "minus",
    "category": "Symbols",
    "aliases": ["heavy_minus_sign"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "divide",
    "category": "Symbols",
    "aliases": ["heavy_division_sign"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "heavy equals sign",
    "category": "Symbols",
    "aliases": ["heavy_equals_sign"],
    "tags": [],
    "unicode_version": "14.0",
    "ios_version": "15.4"
  },
  {
    "emoji": "",
    "description": "infinity",
    "category": "Symbols",
    "aliases": ["infinity"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "double exclamation mark",
    "category": "Symbols",
    "aliases": ["bangbang"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "exclamation question mark",
    "category": "Symbols",
    "aliases": ["interrobang"],
    "tags": [],
    "unicode_version": "3.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "red question mark",
    "category": "Symbols",
    "aliases": ["question"],
    "tags": ["confused"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "white question mark",
    "category": "Symbols",
    "aliases": ["grey_question"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "white exclamation mark",
    "category": "Symbols",
    "aliases": ["grey_exclamation"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "red exclamation mark",
    "category": "Symbols",
    "aliases": ["exclamation", "heavy_exclamation_mark"],
    "tags": ["bang"],
    "unicode_version": "5.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "wavy dash",
    "category": "Symbols",
    "aliases": ["wavy_dash"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "currency exchange",
    "category": "Symbols",
    "aliases": ["currency_exchange"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "heavy dollar sign",
    "category": "Symbols",
    "aliases": ["heavy_dollar_sign"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "medical symbol",
    "category": "Symbols",
    "aliases": ["medical_symbol"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "recycling symbol",
    "category": "Symbols",
    "aliases": ["recycle"],
    "tags": ["environment", "green"],
    "unicode_version": "3.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "fleur-de-lis",
    "category": "Symbols",
    "aliases": ["fleur_de_lis"],
    "tags": [],
    "unicode_version": "4.1",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "trident emblem",
    "category": "Symbols",
    "aliases": ["trident"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "name badge",
    "category": "Symbols",
    "aliases": ["name_badge"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese symbol for beginner",
    "category": "Symbols",
    "aliases": ["beginner"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "hollow red circle",
    "category": "Symbols",
    "aliases": ["o"],
    "tags": [],
    "unicode_version": "5.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "check mark button",
    "category": "Symbols",
    "aliases": ["white_check_mark"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "check box with check",
    "category": "Symbols",
    "aliases": ["ballot_box_with_check"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "check mark",
    "category": "Symbols",
    "aliases": ["heavy_check_mark"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cross mark",
    "category": "Symbols",
    "aliases": ["x"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "cross mark button",
    "category": "Symbols",
    "aliases": ["negative_squared_cross_mark"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "curly loop",
    "category": "Symbols",
    "aliases": ["curly_loop"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "double curly loop",
    "category": "Symbols",
    "aliases": ["loop"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "part alternation mark",
    "category": "Symbols",
    "aliases": ["part_alternation_mark"],
    "tags": [],
    "unicode_version": "3.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "eight-spoked asterisk",
    "category": "Symbols",
    "aliases": ["eight_spoked_asterisk"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "eight-pointed star",
    "category": "Symbols",
    "aliases": ["eight_pointed_black_star"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "sparkle",
    "category": "Symbols",
    "aliases": ["sparkle"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "copyright",
    "category": "Symbols",
    "aliases": ["copyright"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "registered",
    "category": "Symbols",
    "aliases": ["registered"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "trade mark",
    "category": "Symbols",
    "aliases": ["tm"],
    "tags": ["trademark"],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "#",
    "description": "keycap: #",
    "category": "Symbols",
    "aliases": ["hash"],
    "tags": ["number"],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "*",
    "description": "keycap: *",
    "category": "Symbols",
    "aliases": ["asterisk"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "9.1"
  },
  {
    "emoji": "0",
    "description": "keycap: 0",
    "category": "Symbols",
    "aliases": ["zero"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "1",
    "description": "keycap: 1",
    "category": "Symbols",
    "aliases": ["one"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "2",
    "description": "keycap: 2",
    "category": "Symbols",
    "aliases": ["two"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "3",
    "description": "keycap: 3",
    "category": "Symbols",
    "aliases": ["three"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "4",
    "description": "keycap: 4",
    "category": "Symbols",
    "aliases": ["four"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "5",
    "description": "keycap: 5",
    "category": "Symbols",
    "aliases": ["five"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "6",
    "description": "keycap: 6",
    "category": "Symbols",
    "aliases": ["six"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "7",
    "description": "keycap: 7",
    "category": "Symbols",
    "aliases": ["seven"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "8",
    "description": "keycap: 8",
    "category": "Symbols",
    "aliases": ["eight"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "9",
    "description": "keycap: 9",
    "category": "Symbols",
    "aliases": ["nine"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "keycap: 10",
    "category": "Symbols",
    "aliases": ["keycap_ten"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "input latin uppercase",
    "category": "Symbols",
    "aliases": ["capital_abcd"],
    "tags": ["letters"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "input latin lowercase",
    "category": "Symbols",
    "aliases": ["abcd"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "input numbers",
    "category": "Symbols",
    "aliases": ["1234"],
    "tags": ["numbers"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "input symbols",
    "category": "Symbols",
    "aliases": ["symbols"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "input latin letters",
    "category": "Symbols",
    "aliases": ["abc"],
    "tags": ["alphabet"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "A button (blood type)",
    "category": "Symbols",
    "aliases": ["a"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "AB button (blood type)",
    "category": "Symbols",
    "aliases": ["ab"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "B button (blood type)",
    "category": "Symbols",
    "aliases": ["b"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "CL button",
    "category": "Symbols",
    "aliases": ["cl"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "COOL button",
    "category": "Symbols",
    "aliases": ["cool"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "FREE button",
    "category": "Symbols",
    "aliases": ["free"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "information",
    "category": "Symbols",
    "aliases": ["information_source"],
    "tags": [],
    "unicode_version": "3.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "ID button",
    "category": "Symbols",
    "aliases": ["id"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "circled M",
    "category": "Symbols",
    "aliases": ["m"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "NEW button",
    "category": "Symbols",
    "aliases": ["new"],
    "tags": ["fresh"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "NG button",
    "category": "Symbols",
    "aliases": ["ng"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "O button (blood type)",
    "category": "Symbols",
    "aliases": ["o2"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "OK button",
    "category": "Symbols",
    "aliases": ["ok"],
    "tags": ["yes"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "P button",
    "category": "Symbols",
    "aliases": ["parking"],
    "tags": [],
    "unicode_version": "5.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "SOS button",
    "category": "Symbols",
    "aliases": ["sos"],
    "tags": ["help", "emergency"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "UP! button",
    "category": "Symbols",
    "aliases": ["up"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "VS button",
    "category": "Symbols",
    "aliases": ["vs"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese here button",
    "category": "Symbols",
    "aliases": ["koko"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese service charge button",
    "category": "Symbols",
    "aliases": ["sa"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese monthly amount button",
    "category": "Symbols",
    "aliases": ["u6708"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese not free of charge button",
    "category": "Symbols",
    "aliases": ["u6709"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese reserved button",
    "category": "Symbols",
    "aliases": ["u6307"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese bargain button",
    "category": "Symbols",
    "aliases": ["ideograph_advantage"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese discount button",
    "category": "Symbols",
    "aliases": ["u5272"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese free of charge button",
    "category": "Symbols",
    "aliases": ["u7121"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese prohibited button",
    "category": "Symbols",
    "aliases": ["u7981"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese acceptable button",
    "category": "Symbols",
    "aliases": ["accept"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese application button",
    "category": "Symbols",
    "aliases": ["u7533"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese passing grade button",
    "category": "Symbols",
    "aliases": ["u5408"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese vacancy button",
    "category": "Symbols",
    "aliases": ["u7a7a"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese congratulations button",
    "category": "Symbols",
    "aliases": ["congratulations"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese secret button",
    "category": "Symbols",
    "aliases": ["secret"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese open for business button",
    "category": "Symbols",
    "aliases": ["u55b6"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "Japanese no vacancy button",
    "category": "Symbols",
    "aliases": ["u6e80"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "red circle",
    "category": "Symbols",
    "aliases": ["red_circle"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "orange circle",
    "category": "Symbols",
    "aliases": ["orange_circle"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "yellow circle",
    "category": "Symbols",
    "aliases": ["yellow_circle"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "green circle",
    "category": "Symbols",
    "aliases": ["green_circle"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "blue circle",
    "category": "Symbols",
    "aliases": ["large_blue_circle"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "purple circle",
    "category": "Symbols",
    "aliases": ["purple_circle"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "brown circle",
    "category": "Symbols",
    "aliases": ["brown_circle"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "black circle",
    "category": "Symbols",
    "aliases": ["black_circle"],
    "tags": [],
    "unicode_version": "4.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "white circle",
    "category": "Symbols",
    "aliases": ["white_circle"],
    "tags": [],
    "unicode_version": "4.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "red square",
    "category": "Symbols",
    "aliases": ["red_square"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "orange square",
    "category": "Symbols",
    "aliases": ["orange_square"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "yellow square",
    "category": "Symbols",
    "aliases": ["yellow_square"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "green square",
    "category": "Symbols",
    "aliases": ["green_square"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "blue square",
    "category": "Symbols",
    "aliases": ["blue_square"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "purple square",
    "category": "Symbols",
    "aliases": ["purple_square"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "brown square",
    "category": "Symbols",
    "aliases": ["brown_square"],
    "tags": [],
    "unicode_version": "12.0",
    "ios_version": "13.0"
  },
  {
    "emoji": "",
    "description": "black large square",
    "category": "Symbols",
    "aliases": ["black_large_square"],
    "tags": [],
    "unicode_version": "5.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "white large square",
    "category": "Symbols",
    "aliases": ["white_large_square"],
    "tags": [],
    "unicode_version": "5.1",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "black medium square",
    "category": "Symbols",
    "aliases": ["black_medium_square"],
    "tags": [],
    "unicode_version": "3.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "white medium square",
    "category": "Symbols",
    "aliases": ["white_medium_square"],
    "tags": [],
    "unicode_version": "3.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "black medium-small square",
    "category": "Symbols",
    "aliases": ["black_medium_small_square"],
    "tags": [],
    "unicode_version": "3.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "white medium-small square",
    "category": "Symbols",
    "aliases": ["white_medium_small_square"],
    "tags": [],
    "unicode_version": "3.2",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "black small square",
    "category": "Symbols",
    "aliases": ["black_small_square"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "white small square",
    "category": "Symbols",
    "aliases": ["white_small_square"],
    "tags": [],
    "unicode_version": "",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "large orange diamond",
    "category": "Symbols",
    "aliases": ["large_orange_diamond"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "large blue diamond",
    "category": "Symbols",
    "aliases": ["large_blue_diamond"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "small orange diamond",
    "category": "Symbols",
    "aliases": ["small_orange_diamond"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "small blue diamond",
    "category": "Symbols",
    "aliases": ["small_blue_diamond"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "red triangle pointed up",
    "category": "Symbols",
    "aliases": ["small_red_triangle"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "red triangle pointed down",
    "category": "Symbols",
    "aliases": ["small_red_triangle_down"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "diamond with a dot",
    "category": "Symbols",
    "aliases": ["diamond_shape_with_a_dot_inside"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "radio button",
    "category": "Symbols",
    "aliases": ["radio_button"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "white square button",
    "category": "Symbols",
    "aliases": ["white_square_button"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "black square button",
    "category": "Symbols",
    "aliases": ["black_square_button"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "chequered flag",
    "category": "Flags",
    "aliases": ["checkered_flag"],
    "tags": ["milestone", "finish"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "triangular flag",
    "category": "Flags",
    "aliases": ["triangular_flag_on_post"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "crossed flags",
    "category": "Flags",
    "aliases": ["crossed_flags"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "black flag",
    "category": "Flags",
    "aliases": ["black_flag"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "white flag",
    "category": "Flags",
    "aliases": ["white_flag"],
    "tags": [],
    "unicode_version": "7.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "rainbow flag",
    "category": "Flags",
    "aliases": ["rainbow_flag"],
    "tags": ["pride"],
    "unicode_version": "6.0",
    "ios_version": "10.0"
  },
  {
    "emoji": "",
    "description": "transgender flag",
    "category": "Flags",
    "aliases": ["transgender_flag"],
    "tags": [],
    "unicode_version": "13.0",
    "ios_version": "14.0"
  },
  {
    "emoji": "",
    "description": "pirate flag",
    "category": "Flags",
    "aliases": ["pirate_flag"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "flag: Ascension Island",
    "category": "Flags",
    "aliases": ["ascension_island"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "flag: Andorra",
    "category": "Flags",
    "aliases": ["andorra"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: United Arab Emirates",
    "category": "Flags",
    "aliases": ["united_arab_emirates"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Afghanistan",
    "category": "Flags",
    "aliases": ["afghanistan"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Antigua & Barbuda",
    "category": "Flags",
    "aliases": ["antigua_barbuda"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Anguilla",
    "category": "Flags",
    "aliases": ["anguilla"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Albania",
    "category": "Flags",
    "aliases": ["albania"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Armenia",
    "category": "Flags",
    "aliases": ["armenia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Angola",
    "category": "Flags",
    "aliases": ["angola"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Antarctica",
    "category": "Flags",
    "aliases": ["antarctica"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Argentina",
    "category": "Flags",
    "aliases": ["argentina"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: American Samoa",
    "category": "Flags",
    "aliases": ["american_samoa"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Austria",
    "category": "Flags",
    "aliases": ["austria"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Australia",
    "category": "Flags",
    "aliases": ["australia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Aruba",
    "category": "Flags",
    "aliases": ["aruba"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: land Islands",
    "category": "Flags",
    "aliases": ["aland_islands"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Azerbaijan",
    "category": "Flags",
    "aliases": ["azerbaijan"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Bosnia & Herzegovina",
    "category": "Flags",
    "aliases": ["bosnia_herzegovina"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Barbados",
    "category": "Flags",
    "aliases": ["barbados"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Bangladesh",
    "category": "Flags",
    "aliases": ["bangladesh"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Belgium",
    "category": "Flags",
    "aliases": ["belgium"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Burkina Faso",
    "category": "Flags",
    "aliases": ["burkina_faso"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Bulgaria",
    "category": "Flags",
    "aliases": ["bulgaria"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Bahrain",
    "category": "Flags",
    "aliases": ["bahrain"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Burundi",
    "category": "Flags",
    "aliases": ["burundi"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Benin",
    "category": "Flags",
    "aliases": ["benin"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: St. Barthlemy",
    "category": "Flags",
    "aliases": ["st_barthelemy"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Bermuda",
    "category": "Flags",
    "aliases": ["bermuda"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Brunei",
    "category": "Flags",
    "aliases": ["brunei"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Bolivia",
    "category": "Flags",
    "aliases": ["bolivia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Caribbean Netherlands",
    "category": "Flags",
    "aliases": ["caribbean_netherlands"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Brazil",
    "category": "Flags",
    "aliases": ["brazil"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Bahamas",
    "category": "Flags",
    "aliases": ["bahamas"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Bhutan",
    "category": "Flags",
    "aliases": ["bhutan"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Bouvet Island",
    "category": "Flags",
    "aliases": ["bouvet_island"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "flag: Botswana",
    "category": "Flags",
    "aliases": ["botswana"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Belarus",
    "category": "Flags",
    "aliases": ["belarus"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Belize",
    "category": "Flags",
    "aliases": ["belize"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Canada",
    "category": "Flags",
    "aliases": ["canada"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Cocos (Keeling) Islands",
    "category": "Flags",
    "aliases": ["cocos_islands"],
    "tags": ["keeling"],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Congo - Kinshasa",
    "category": "Flags",
    "aliases": ["congo_kinshasa"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Central African Republic",
    "category": "Flags",
    "aliases": ["central_african_republic"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Congo - Brazzaville",
    "category": "Flags",
    "aliases": ["congo_brazzaville"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Switzerland",
    "category": "Flags",
    "aliases": ["switzerland"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Cte dIvoire",
    "category": "Flags",
    "aliases": ["cote_divoire"],
    "tags": ["ivory"],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Cook Islands",
    "category": "Flags",
    "aliases": ["cook_islands"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Chile",
    "category": "Flags",
    "aliases": ["chile"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Cameroon",
    "category": "Flags",
    "aliases": ["cameroon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: China",
    "category": "Flags",
    "aliases": ["cn"],
    "tags": ["china"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "flag: Colombia",
    "category": "Flags",
    "aliases": ["colombia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Clipperton Island",
    "category": "Flags",
    "aliases": ["clipperton_island"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "flag: Costa Rica",
    "category": "Flags",
    "aliases": ["costa_rica"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Cuba",
    "category": "Flags",
    "aliases": ["cuba"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Cape Verde",
    "category": "Flags",
    "aliases": ["cape_verde"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Curaao",
    "category": "Flags",
    "aliases": ["curacao"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Christmas Island",
    "category": "Flags",
    "aliases": ["christmas_island"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Cyprus",
    "category": "Flags",
    "aliases": ["cyprus"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Czechia",
    "category": "Flags",
    "aliases": ["czech_republic"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Germany",
    "category": "Flags",
    "aliases": ["de"],
    "tags": ["flag", "germany"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "flag: Diego Garcia",
    "category": "Flags",
    "aliases": ["diego_garcia"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "flag: Djibouti",
    "category": "Flags",
    "aliases": ["djibouti"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Denmark",
    "category": "Flags",
    "aliases": ["denmark"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Dominica",
    "category": "Flags",
    "aliases": ["dominica"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Dominican Republic",
    "category": "Flags",
    "aliases": ["dominican_republic"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Algeria",
    "category": "Flags",
    "aliases": ["algeria"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Ceuta & Melilla",
    "category": "Flags",
    "aliases": ["ceuta_melilla"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "flag: Ecuador",
    "category": "Flags",
    "aliases": ["ecuador"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Estonia",
    "category": "Flags",
    "aliases": ["estonia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Egypt",
    "category": "Flags",
    "aliases": ["egypt"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Western Sahara",
    "category": "Flags",
    "aliases": ["western_sahara"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Eritrea",
    "category": "Flags",
    "aliases": ["eritrea"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Spain",
    "category": "Flags",
    "aliases": ["es"],
    "tags": ["spain"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "flag: Ethiopia",
    "category": "Flags",
    "aliases": ["ethiopia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: European Union",
    "category": "Flags",
    "aliases": ["eu", "european_union"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Finland",
    "category": "Flags",
    "aliases": ["finland"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Fiji",
    "category": "Flags",
    "aliases": ["fiji"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Falkland Islands",
    "category": "Flags",
    "aliases": ["falkland_islands"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Micronesia",
    "category": "Flags",
    "aliases": ["micronesia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Faroe Islands",
    "category": "Flags",
    "aliases": ["faroe_islands"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: France",
    "category": "Flags",
    "aliases": ["fr"],
    "tags": ["france", "french"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "flag: Gabon",
    "category": "Flags",
    "aliases": ["gabon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: United Kingdom",
    "category": "Flags",
    "aliases": ["gb", "uk"],
    "tags": ["flag", "british"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "flag: Grenada",
    "category": "Flags",
    "aliases": ["grenada"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Georgia",
    "category": "Flags",
    "aliases": ["georgia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: French Guiana",
    "category": "Flags",
    "aliases": ["french_guiana"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Guernsey",
    "category": "Flags",
    "aliases": ["guernsey"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Ghana",
    "category": "Flags",
    "aliases": ["ghana"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Gibraltar",
    "category": "Flags",
    "aliases": ["gibraltar"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Greenland",
    "category": "Flags",
    "aliases": ["greenland"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Gambia",
    "category": "Flags",
    "aliases": ["gambia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Guinea",
    "category": "Flags",
    "aliases": ["guinea"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Guadeloupe",
    "category": "Flags",
    "aliases": ["guadeloupe"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Equatorial Guinea",
    "category": "Flags",
    "aliases": ["equatorial_guinea"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Greece",
    "category": "Flags",
    "aliases": ["greece"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: South Georgia & South Sandwich Islands",
    "category": "Flags",
    "aliases": ["south_georgia_south_sandwich_islands"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Guatemala",
    "category": "Flags",
    "aliases": ["guatemala"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Guam",
    "category": "Flags",
    "aliases": ["guam"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Guinea-Bissau",
    "category": "Flags",
    "aliases": ["guinea_bissau"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Guyana",
    "category": "Flags",
    "aliases": ["guyana"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Hong Kong SAR China",
    "category": "Flags",
    "aliases": ["hong_kong"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Heard & McDonald Islands",
    "category": "Flags",
    "aliases": ["heard_mcdonald_islands"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "flag: Honduras",
    "category": "Flags",
    "aliases": ["honduras"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Croatia",
    "category": "Flags",
    "aliases": ["croatia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Haiti",
    "category": "Flags",
    "aliases": ["haiti"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Hungary",
    "category": "Flags",
    "aliases": ["hungary"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Canary Islands",
    "category": "Flags",
    "aliases": ["canary_islands"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Indonesia",
    "category": "Flags",
    "aliases": ["indonesia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Ireland",
    "category": "Flags",
    "aliases": ["ireland"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Israel",
    "category": "Flags",
    "aliases": ["israel"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Isle of Man",
    "category": "Flags",
    "aliases": ["isle_of_man"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: India",
    "category": "Flags",
    "aliases": ["india"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: British Indian Ocean Territory",
    "category": "Flags",
    "aliases": ["british_indian_ocean_territory"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Iraq",
    "category": "Flags",
    "aliases": ["iraq"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Iran",
    "category": "Flags",
    "aliases": ["iran"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Iceland",
    "category": "Flags",
    "aliases": ["iceland"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Italy",
    "category": "Flags",
    "aliases": ["it"],
    "tags": ["italy"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "flag: Jersey",
    "category": "Flags",
    "aliases": ["jersey"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Jamaica",
    "category": "Flags",
    "aliases": ["jamaica"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Jordan",
    "category": "Flags",
    "aliases": ["jordan"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Japan",
    "category": "Flags",
    "aliases": ["jp"],
    "tags": ["japan"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "flag: Kenya",
    "category": "Flags",
    "aliases": ["kenya"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Kyrgyzstan",
    "category": "Flags",
    "aliases": ["kyrgyzstan"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Cambodia",
    "category": "Flags",
    "aliases": ["cambodia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Kiribati",
    "category": "Flags",
    "aliases": ["kiribati"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Comoros",
    "category": "Flags",
    "aliases": ["comoros"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: St. Kitts & Nevis",
    "category": "Flags",
    "aliases": ["st_kitts_nevis"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: North Korea",
    "category": "Flags",
    "aliases": ["north_korea"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: South Korea",
    "category": "Flags",
    "aliases": ["kr"],
    "tags": ["korea"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "flag: Kuwait",
    "category": "Flags",
    "aliases": ["kuwait"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Cayman Islands",
    "category": "Flags",
    "aliases": ["cayman_islands"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Kazakhstan",
    "category": "Flags",
    "aliases": ["kazakhstan"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Laos",
    "category": "Flags",
    "aliases": ["laos"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Lebanon",
    "category": "Flags",
    "aliases": ["lebanon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: St. Lucia",
    "category": "Flags",
    "aliases": ["st_lucia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Liechtenstein",
    "category": "Flags",
    "aliases": ["liechtenstein"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Sri Lanka",
    "category": "Flags",
    "aliases": ["sri_lanka"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Liberia",
    "category": "Flags",
    "aliases": ["liberia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Lesotho",
    "category": "Flags",
    "aliases": ["lesotho"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Lithuania",
    "category": "Flags",
    "aliases": ["lithuania"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Luxembourg",
    "category": "Flags",
    "aliases": ["luxembourg"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Latvia",
    "category": "Flags",
    "aliases": ["latvia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Libya",
    "category": "Flags",
    "aliases": ["libya"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Morocco",
    "category": "Flags",
    "aliases": ["morocco"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Monaco",
    "category": "Flags",
    "aliases": ["monaco"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Moldova",
    "category": "Flags",
    "aliases": ["moldova"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Montenegro",
    "category": "Flags",
    "aliases": ["montenegro"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: St. Martin",
    "category": "Flags",
    "aliases": ["st_martin"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "flag: Madagascar",
    "category": "Flags",
    "aliases": ["madagascar"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Marshall Islands",
    "category": "Flags",
    "aliases": ["marshall_islands"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: North Macedonia",
    "category": "Flags",
    "aliases": ["macedonia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Mali",
    "category": "Flags",
    "aliases": ["mali"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Myanmar (Burma)",
    "category": "Flags",
    "aliases": ["myanmar"],
    "tags": ["burma"],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Mongolia",
    "category": "Flags",
    "aliases": ["mongolia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Macao SAR China",
    "category": "Flags",
    "aliases": ["macau"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Northern Mariana Islands",
    "category": "Flags",
    "aliases": ["northern_mariana_islands"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Martinique",
    "category": "Flags",
    "aliases": ["martinique"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Mauritania",
    "category": "Flags",
    "aliases": ["mauritania"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Montserrat",
    "category": "Flags",
    "aliases": ["montserrat"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Malta",
    "category": "Flags",
    "aliases": ["malta"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Mauritius",
    "category": "Flags",
    "aliases": ["mauritius"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Maldives",
    "category": "Flags",
    "aliases": ["maldives"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Malawi",
    "category": "Flags",
    "aliases": ["malawi"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Mexico",
    "category": "Flags",
    "aliases": ["mexico"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Malaysia",
    "category": "Flags",
    "aliases": ["malaysia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Mozambique",
    "category": "Flags",
    "aliases": ["mozambique"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Namibia",
    "category": "Flags",
    "aliases": ["namibia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: New Caledonia",
    "category": "Flags",
    "aliases": ["new_caledonia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Niger",
    "category": "Flags",
    "aliases": ["niger"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Norfolk Island",
    "category": "Flags",
    "aliases": ["norfolk_island"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Nigeria",
    "category": "Flags",
    "aliases": ["nigeria"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Nicaragua",
    "category": "Flags",
    "aliases": ["nicaragua"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Netherlands",
    "category": "Flags",
    "aliases": ["netherlands"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Norway",
    "category": "Flags",
    "aliases": ["norway"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Nepal",
    "category": "Flags",
    "aliases": ["nepal"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Nauru",
    "category": "Flags",
    "aliases": ["nauru"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Niue",
    "category": "Flags",
    "aliases": ["niue"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: New Zealand",
    "category": "Flags",
    "aliases": ["new_zealand"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Oman",
    "category": "Flags",
    "aliases": ["oman"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Panama",
    "category": "Flags",
    "aliases": ["panama"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Peru",
    "category": "Flags",
    "aliases": ["peru"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: French Polynesia",
    "category": "Flags",
    "aliases": ["french_polynesia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Papua New Guinea",
    "category": "Flags",
    "aliases": ["papua_new_guinea"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Philippines",
    "category": "Flags",
    "aliases": ["philippines"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Pakistan",
    "category": "Flags",
    "aliases": ["pakistan"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Poland",
    "category": "Flags",
    "aliases": ["poland"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: St. Pierre & Miquelon",
    "category": "Flags",
    "aliases": ["st_pierre_miquelon"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Pitcairn Islands",
    "category": "Flags",
    "aliases": ["pitcairn_islands"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Puerto Rico",
    "category": "Flags",
    "aliases": ["puerto_rico"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Palestinian Territories",
    "category": "Flags",
    "aliases": ["palestinian_territories"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Portugal",
    "category": "Flags",
    "aliases": ["portugal"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Palau",
    "category": "Flags",
    "aliases": ["palau"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Paraguay",
    "category": "Flags",
    "aliases": ["paraguay"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Qatar",
    "category": "Flags",
    "aliases": ["qatar"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Runion",
    "category": "Flags",
    "aliases": ["reunion"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Romania",
    "category": "Flags",
    "aliases": ["romania"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Serbia",
    "category": "Flags",
    "aliases": ["serbia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Russia",
    "category": "Flags",
    "aliases": ["ru"],
    "tags": ["russia"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "flag: Rwanda",
    "category": "Flags",
    "aliases": ["rwanda"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Saudi Arabia",
    "category": "Flags",
    "aliases": ["saudi_arabia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Solomon Islands",
    "category": "Flags",
    "aliases": ["solomon_islands"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Seychelles",
    "category": "Flags",
    "aliases": ["seychelles"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Sudan",
    "category": "Flags",
    "aliases": ["sudan"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Sweden",
    "category": "Flags",
    "aliases": ["sweden"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Singapore",
    "category": "Flags",
    "aliases": ["singapore"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: St. Helena",
    "category": "Flags",
    "aliases": ["st_helena"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Slovenia",
    "category": "Flags",
    "aliases": ["slovenia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Svalbard & Jan Mayen",
    "category": "Flags",
    "aliases": ["svalbard_jan_mayen"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "flag: Slovakia",
    "category": "Flags",
    "aliases": ["slovakia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Sierra Leone",
    "category": "Flags",
    "aliases": ["sierra_leone"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: San Marino",
    "category": "Flags",
    "aliases": ["san_marino"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Senegal",
    "category": "Flags",
    "aliases": ["senegal"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Somalia",
    "category": "Flags",
    "aliases": ["somalia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Suriname",
    "category": "Flags",
    "aliases": ["suriname"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: South Sudan",
    "category": "Flags",
    "aliases": ["south_sudan"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: So Tom & Prncipe",
    "category": "Flags",
    "aliases": ["sao_tome_principe"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: El Salvador",
    "category": "Flags",
    "aliases": ["el_salvador"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Sint Maarten",
    "category": "Flags",
    "aliases": ["sint_maarten"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Syria",
    "category": "Flags",
    "aliases": ["syria"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Eswatini",
    "category": "Flags",
    "aliases": ["swaziland"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Tristan da Cunha",
    "category": "Flags",
    "aliases": ["tristan_da_cunha"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "flag: Turks & Caicos Islands",
    "category": "Flags",
    "aliases": ["turks_caicos_islands"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Chad",
    "category": "Flags",
    "aliases": ["chad"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: French Southern Territories",
    "category": "Flags",
    "aliases": ["french_southern_territories"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Togo",
    "category": "Flags",
    "aliases": ["togo"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Thailand",
    "category": "Flags",
    "aliases": ["thailand"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Tajikistan",
    "category": "Flags",
    "aliases": ["tajikistan"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Tokelau",
    "category": "Flags",
    "aliases": ["tokelau"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Timor-Leste",
    "category": "Flags",
    "aliases": ["timor_leste"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Turkmenistan",
    "category": "Flags",
    "aliases": ["turkmenistan"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Tunisia",
    "category": "Flags",
    "aliases": ["tunisia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Tonga",
    "category": "Flags",
    "aliases": ["tonga"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Turkey",
    "category": "Flags",
    "aliases": ["tr"],
    "tags": ["turkey"],
    "unicode_version": "8.0",
    "ios_version": "9.1"
  },
  {
    "emoji": "",
    "description": "flag: Trinidad & Tobago",
    "category": "Flags",
    "aliases": ["trinidad_tobago"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Tuvalu",
    "category": "Flags",
    "aliases": ["tuvalu"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Taiwan",
    "category": "Flags",
    "aliases": ["taiwan"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Tanzania",
    "category": "Flags",
    "aliases": ["tanzania"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Ukraine",
    "category": "Flags",
    "aliases": ["ukraine"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Uganda",
    "category": "Flags",
    "aliases": ["uganda"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: U.S. Outlying Islands",
    "category": "Flags",
    "aliases": ["us_outlying_islands"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "flag: United Nations",
    "category": "Flags",
    "aliases": ["united_nations"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "flag: United States",
    "category": "Flags",
    "aliases": ["us"],
    "tags": ["flag", "united", "america"],
    "unicode_version": "6.0",
    "ios_version": "6.0"
  },
  {
    "emoji": "",
    "description": "flag: Uruguay",
    "category": "Flags",
    "aliases": ["uruguay"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Uzbekistan",
    "category": "Flags",
    "aliases": ["uzbekistan"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Vatican City",
    "category": "Flags",
    "aliases": ["vatican_city"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: St. Vincent & Grenadines",
    "category": "Flags",
    "aliases": ["st_vincent_grenadines"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Venezuela",
    "category": "Flags",
    "aliases": ["venezuela"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: British Virgin Islands",
    "category": "Flags",
    "aliases": ["british_virgin_islands"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: U.S. Virgin Islands",
    "category": "Flags",
    "aliases": ["us_virgin_islands"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Vietnam",
    "category": "Flags",
    "aliases": ["vietnam"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Vanuatu",
    "category": "Flags",
    "aliases": ["vanuatu"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Wallis & Futuna",
    "category": "Flags",
    "aliases": ["wallis_futuna"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: Samoa",
    "category": "Flags",
    "aliases": ["samoa"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Kosovo",
    "category": "Flags",
    "aliases": ["kosovo"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Yemen",
    "category": "Flags",
    "aliases": ["yemen"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Mayotte",
    "category": "Flags",
    "aliases": ["mayotte"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "9.0"
  },
  {
    "emoji": "",
    "description": "flag: South Africa",
    "category": "Flags",
    "aliases": ["south_africa"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Zambia",
    "category": "Flags",
    "aliases": ["zambia"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: Zimbabwe",
    "category": "Flags",
    "aliases": ["zimbabwe"],
    "tags": [],
    "unicode_version": "6.0",
    "ios_version": "8.3"
  },
  {
    "emoji": "",
    "description": "flag: England",
    "category": "Flags",
    "aliases": ["england"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "flag: Scotland",
    "category": "Flags",
    "aliases": ["scotland"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  },
  {
    "emoji": "",
    "description": "flag: Wales",
    "category": "Flags",
    "aliases": ["wales"],
    "tags": [],
    "unicode_version": "11.0",
    "ios_version": "12.1"
  }
]
````

## File: microservices/ace_agent/4.1/webui/server/emoji-finder/index.test.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { describe, it, before } from "node:test";
import * as assert from "node:assert";
import EmojiFinder from ".";

const emojis = [
  {
    emoji: "",
    description: "man dancing",
    aliases: ["man_dancing"],
    tags: ["dancer"],
  },
  {
    emoji: "",
    description: "teacup without handle",
    aliases: ["tea"],
    tags: ["green", "breakfast"],
  },
  {
    emoji: "",
    description: "grinning face with big eyes",
    aliases: ["smiley"],
    tags: ["happy", "joy", "haha"],
  },
];

describe("EmojiFinder", () => {
  let emojiFinder: EmojiFinder;
  before(async () => {
    emojiFinder = new EmojiFinder(emojis);
    await emojiFinder.init();
  });

  it("Finds an appropriate emoji for various texts", async () => {
    const [danceEmoji, happyEmoji, teaEmoji] = await Promise.all([
      emojiFinder.findEmoji("Dance moves"),
      emojiFinder.findEmoji("Happiness"),
      emojiFinder.findEmoji("A cup of tea"),
    ]);
    assert.strictEqual(danceEmoji.emoji, "");
    assert.strictEqual(happyEmoji.emoji, "");
    assert.strictEqual(teaEmoji.emoji, "");
  });
});
````

## File: microservices/ace_agent/4.1/webui/server/emoji-finder/index.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import * as fs from "fs";
import "@tensorflow/tfjs";
import * as use from "@tensorflow-models/universal-sentence-encoder";
import { Tensor2D } from "@tensorflow/tfjs";
import * as tf from "@tensorflow/tfjs-core";
import * as path from "path";

interface Emoji {
  emoji: string;
  description: string;
  tags: string[];
  aliases: string[];
}

/**
 * Finds the emoji that best matches an arbitrary text string, using tensorflow's
 * universal string encoder.
 *
 * Usage:
 * ```
 * const emojiFinder = EmojiFinder.fromEmojiFile("./emojis/all.json")
 * await emojiFinder.init()
 * await emojiFinder.findEmoji("Dance moves") // 
 * ```
 */
export default class EmojiFinder {
  private emojis: Emoji[] = [];
  private model: use.UniversalSentenceEncoder;

  /**
   * Emojis represented as 2D embeddings
   */
  private emojisEmbeddings: Tensor2D;

  /**
   * Computing embeddings can take a while, which is inconvenient in
   * situations where the application is relaunched frequently (e.g.
   * hot-reloading in development mode). For faster load times, the
   * embeddings are cached on the disk.
   */
  private static readonly cachedEmbeddingsPath: string = path.join(
    import.meta.dirname,
    "./cached-emoji-embeddings.json"
  );

  constructor(emojis: Emoji[]) {
    this.emojis = emojis;
  }

  /**
   * Initializes the instance by loading tensorflow's universal string encoder, and using it
   * to convert the emojis to embeddings
   */
  public async init(): Promise<EmojiFinder> {
    this.model = await use.load();
    this.emojisEmbeddings = await this.createEmbeddings();
    return this;
  }

  /**
   * Finds the closest matching emojis for a given string. The string is converted into
   * an embedding, which is compared against the list of emoji embeddings. The emojis
   * matching the closest embedding is returned
   */
  public async findEmoji(text: string): Promise<Emoji> {
    if (!this.emojisEmbeddings) {
      throw new Error(
        "Tried to find emojis but no embeddings were found. Did you call init()?"
      );
    }
    const sampleEmbedding = await this.model.embed([text.toLowerCase()]);

    let maxScore = 0,
      maxIndex = 0;
    for (let j = 0; j < this.emojis.length; j++) {
      const emojiEmbedding = tf.slice(this.emojisEmbeddings, [j, 0], [1]);
      const score = tf
        .matMul(sampleEmbedding, emojiEmbedding, false, true)
        .dataSync();

      if (score[0] > maxScore) {
        maxScore = score[0];
        maxIndex = j;
      }
    }
    return this.emojis[maxIndex];
  }

  /**
   * Creates a new instance of EmojiFinder by reading the list of emojis using the provided
   * path
   *
   * @param path Path to a JSON file containing emojis. The file should contain an array
   * of emojis, where each emoji is represented as:
   * {
   *   "emoji": "<emoji>",
   *   "description": "<description>",
   *   "tags": ["<tag 1>", "<tag 2>", ...]
   * }
   *
   * Good emoji descriptions make the finder more accurate.
   */
  public static fromEmojiFile(path: string): EmojiFinder {
    const fileContent = EmojiFinder.readFile(path);
    const emojis = EmojiFinder.parseJSON(fileContent.toString());
    EmojiFinder.validateEmojis(emojis);

    return new EmojiFinder(emojis);
  }

  /**
   * Creates 2D embeddings from the instance's emojis. The embeddings are cached
   * on the disk for faster hot-reloads
   * @returns
   */
  private async createEmbeddings(): Promise<Tensor2D> {
    const cachedEmbeddings = this.loadEmbeddingsFromCache();
    if (cachedEmbeddings) {
      return cachedEmbeddings;
    }

    if (!this.emojis.length) {
      throw new Error(
        "Tried to create embeddings for emojis but no emojis were set. Did you call init()?"
      );
    }
    const embeddings = await this.model.embed(
      this.emojis.map((emoji) =>
        `${emoji.description} ${emoji.tags.join(" ")} ${emoji.aliases.join(
          " "
        )}`.toLowerCase()
      )
    );

    this.storeEmbeddingsToCache(embeddings);

    return embeddings;
  }

  /**
   * Tries to load embeddings from cache, on the disk. Returns
   * The embeddings if the number of cached embeddings matches the
   * number of emojis. If the length is different, assumes the cache
   * is no longer valid, and returns nothing
   * @returns the cached emojis, if available
   */
  private loadEmbeddingsFromCache(): Tensor2D | null {
    try {
      const cachedEmbeddings = EmojiFinder.readFile(
        EmojiFinder.cachedEmbeddingsPath
      );
      const parsed = JSON.parse(cachedEmbeddings.toString());
      if (parsed.length === this.emojis.length) {
        return tf.tensor(parsed);
      }
    } catch (e) {
      return null;
    }
  }

  /**
   * Writes embeddings to cache, on the disk
   * @param embeddings
   */
  private storeEmbeddingsToCache(embeddings: Tensor2D): void {
    const data = embeddings.arraySync();
    fs.writeFileSync(
      EmojiFinder.cachedEmbeddingsPath,
      JSON.stringify(data),
      "utf8"
    );
  }

  /**
   * Reads the file at the provided path
   */
  private static readFile(path: string): Buffer {
    return fs.readFileSync(path);
  }

  /**
   * Parses a JSON string to an untyped JavaScript object
   */
  private static parseJSON(jsonString: string): unknown {
    return JSON.parse(jsonString);
  }

  /**
   * Ensures a list of objects is a list of emojis. If any emoji is not valid, throws
   * an error
   */
  private static validateEmojis(emojis: unknown): asserts emojis is Emoji[] {
    if (!Array.isArray(emojis)) {
      throw new Error(
        `Object is not a valid list of emojis (expected an Array, got ${typeof emojis} instead`
      );
    }

    emojis.forEach((emoji) => EmojiFinder.validateEmoji(emoji));
  }

  /**
   * Ensures an emoji is a valid shape of emoji. If it's not, this function throws
   * with an error
   * @param emoji the object to cast
   */
  private static validateEmoji(emoji: unknown): asserts emoji is Emoji {
    if (typeof emoji !== "object") {
      throw new Error(
        `Object ${emoji} is not a valid emoji (expect object, got ${typeof emoji} instead)`
      );
    }
    if (!("emoji" in emoji) || typeof emoji.emoji !== "string") {
      throw new Error(
        `Object ${emoji} is not a valid emoji (missing property 'emoji' of type string)`
      );
    }
    if (!("description" in emoji) || typeof emoji.description !== "string") {
      throw new Error(
        `Object ${emoji} is not a valid emoji (missing property 'description' of type string)`
      );
    }
    if (
      !("tags" in emoji) ||
      !Array.isArray(emoji.tags) ||
      !emoji.tags.every((tag) => typeof tag === "string")
    ) {
      throw new Error(
        `Object ${emoji} is not a valid emoji (missing property 'tags' of type string[])`
      );
    }
    if (
      !("aliases" in emoji) ||
      !Array.isArray(emoji.aliases) ||
      !emoji.aliases.every((tag) => typeof tag === "string")
    ) {
      throw new Error(
        `Object ${emoji} is not a valid emoji (missing property 'aliases' of type string[])`
      );
    }
  }
}
````

## File: microservices/ace_agent/4.1/webui/server/grpc/gen/ace_agent_connect.d.ts
````typescript
//
// Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
//
// NVIDIA CORPORATION and its licensors retain all intellectual property
// and proprietary rights in and to this software, related documentation
// and any modifications thereto.  Any use, reproduction, disclosure or
// distribution of this software and related documentation without an express
// license agreement from NVIDIA CORPORATION is strictly prohibited.

// @generated by protoc-gen-connect-es v1.4.0
// @generated from file ace_agent.proto (package nvidia.aceagent.chatcontroller.v1, syntax proto3)
/* eslint-disable */
// @ts-nocheck

import { APIStatusResponse, ChatRequest, ChatResponse, EventRequest, EventResponse, GetStatusRequest, GetStatusResponse, PipelineRequest, ReceiveAudioRequest, ReceiveAudioResponse, ReloadSpeechConfigsRequest, SendAudioRequest, SpeechRecognitionControlRequest, StreamingSpeechResultsRequest, StreamingSpeechResultsResponse, SynthesizeSpeechRequest, UserContext, UserContextRequest, UserParametersRequest } from "./ace_agent_pb.js";
import { MethodKind } from "@bufbuild/protobuf";

/**
 *
 * The AceAgentGrpc service provides apis to interact with chat engine and speech
 * components.
 *
 * @generated from service nvidia.aceagent.chatcontroller.v1.AceAgentGrpc
 */
export declare const AceAgentGrpc: {
  readonly typeName: "nvidia.aceagent.chatcontroller.v1.AceAgentGrpc",
  readonly methods: {
    /**
     * CreatePipeline API is used to create new pipeline with Chat controller,
     *  It acquires a Chat controller pipeline with a unique stream_id populated
     * by the client in PipelineRequest.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.CreatePipeline
     */
    readonly createPipeline: {
      readonly name: "CreatePipeline",
      readonly I: typeof PipelineRequest,
      readonly O: typeof APIStatusResponse,
      readonly kind: MethodKind.Unary,
    },
    /**
     * FreePipeline API is used to free up a pipeline with Chat controller,
     * created by using CreatePipeline API. Client needs to pass same stream_id
     * in PipelineRequest as used in CreatePipeline.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.FreePipeline
     */
    readonly freePipeline: {
      readonly name: "FreePipeline",
      readonly I: typeof PipelineRequest,
      readonly O: typeof APIStatusResponse,
      readonly kind: MethodKind.Unary,
    },
    /**
     * SendAudio API is used to stream audio content to ASR from Chat controller.
     * This is a client side streaming API.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.SendAudio
     */
    readonly sendAudio: {
      readonly name: "SendAudio",
      readonly I: typeof SendAudioRequest,
      readonly O: typeof APIStatusResponse,
      readonly kind: MethodKind.ClientStreaming,
    },
    /**
     * ReceiveAudio API is used to receive synthesized audio from TTS through
     * Chat controller. This is a server side streaming API.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.ReceiveAudio
     */
    readonly receiveAudio: {
      readonly name: "ReceiveAudio",
      readonly I: typeof ReceiveAudioRequest,
      readonly O: typeof ReceiveAudioResponse,
      readonly kind: MethodKind.ServerStreaming,
    },
    /**
     * StreamSpeechResults API is used to receive all the meta data from
     * Chat controller like  ASR transcripts, Chat engine responses, Pipeline
     * states etc. This is a broadcasting API i.e it can fan out responses to
     * multiple concurrent client instances using same stream_id.
     * This is a server side streaming API.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.StreamSpeechResults
     */
    readonly streamSpeechResults: {
      readonly name: "StreamSpeechResults",
      readonly I: typeof StreamingSpeechResultsRequest,
      readonly O: typeof StreamingSpeechResultsResponse,
      readonly kind: MethodKind.ServerStreaming,
    },
    /**
     * StartRecognition API is used to start the ASR recognition in Chat
     * controller for the audio content streamed from SendAudio API.
     * This API also provides a flag to mark the ASR recognition as standalone,
     * i.e Chat Engine and TTS will not be invoked for the ASR transcript.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.StartRecognition
     */
    readonly startRecognition: {
      readonly name: "StartRecognition",
      readonly I: typeof SpeechRecognitionControlRequest,
      readonly O: typeof APIStatusResponse,
      readonly kind: MethodKind.Unary,
    },
    /**
     * StopRecognition API is used to stop the ASR recognition for the audio
     * content streamed from SendAudio API.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.StopRecognition
     */
    readonly stopRecognition: {
      readonly name: "StopRecognition",
      readonly I: typeof SpeechRecognitionControlRequest,
      readonly O: typeof APIStatusResponse,
      readonly kind: MethodKind.Unary,
    },
    /**
     * This API can be used to set the runtime user parameters like user_id
     * for Chat controller pipeline.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.SetUserParameters
     */
    readonly setUserParameters: {
      readonly name: "SetUserParameters",
      readonly I: typeof UserParametersRequest,
      readonly O: typeof APIStatusResponse,
      readonly kind: MethodKind.Unary,
    },
    /**
     * GetStatus API can be used to get the latest state of Chat controller pipeline.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.GetStatus
     */
    readonly getStatus: {
      readonly name: "GetStatus",
      readonly I: typeof GetStatusRequest,
      readonly O: typeof GetStatusResponse,
      readonly kind: MethodKind.Unary,
    },
    /**
     * ReloadSpeechConfigs API can be used to reload the ASR word boosting and
     * TTS Arpbet configs in Chat controller.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.ReloadSpeechConfigs
     */
    readonly reloadSpeechConfigs: {
      readonly name: "ReloadSpeechConfigs",
      readonly I: typeof ReloadSpeechConfigsRequest,
      readonly O: typeof APIStatusResponse,
      readonly kind: MethodKind.Unary,
    },
    /**
     * SynthesizeSpeech API is used to send text transcript directly to the TTS
     * for standalone TTS audio synthesis.
     * The generated audio will be routed to the path specified in the pipeline
     * graph provided in Chat controller.
     * e.g. if the TTS audio is routed to A2F in the graph, the audio will be
     * sent to A2F server.
     * If the TTS audio is routed to Grpc client then it will be available
     * through the server side streaming ReceiveAudio API.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.SynthesizeSpeech
     */
    readonly synthesizeSpeech: {
      readonly name: "SynthesizeSpeech",
      readonly I: typeof SynthesizeSpeechRequest,
      readonly O: typeof APIStatusResponse,
      readonly kind: MethodKind.Unary,
    },
    /**
     * GetUserContext API is used to get the current user context from Chat Engine.
     * The API returns a UserContext message containing the current conversation
     * history and any context attached to the active user_id.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.GetUserContext
     */
    readonly getUserContext: {
      readonly name: "GetUserContext",
      readonly I: typeof UserContextRequest,
      readonly O: typeof UserContext,
      readonly kind: MethodKind.Unary,
    },
    /**
     * SetUserContext API is used to set the current user context in Chat Engine.
     * The API accepts a UserContext message containing the conversation
     * history and any context to be attached to the active user_id.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.SetUserContext
     */
    readonly setUserContext: {
      readonly name: "SetUserContext",
      readonly I: typeof UserContext,
      readonly O: typeof APIStatusResponse,
      readonly kind: MethodKind.Unary,
    },
    /**
     * UpdateUserContext API is used to update the current user context from
     * Chat Engine. The API accepts a UserContext message containing any context
     * to be attached to the active user_id.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.UpdateUserContext
     */
    readonly updateUserContext: {
      readonly name: "UpdateUserContext",
      readonly I: typeof UserContext,
      readonly O: typeof APIStatusResponse,
      readonly kind: MethodKind.Unary,
    },
    /**
     * DeleteUserContext API is used to delete the current user context attached
     * to a user_id in Chat Engine.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.DeleteUserContext
     */
    readonly deleteUserContext: {
      readonly name: "DeleteUserContext",
      readonly I: typeof UserContextRequest,
      readonly O: typeof APIStatusResponse,
      readonly kind: MethodKind.Unary,
    },
    /**
     * Chat API is used to send text queries to Chat Engine via Chat controller.
     * This API also provides a flag to disable TTS synthesis for the response
     * generated by Chat Engine.
     * This can be used for a text in and text out type of scenario.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.Chat
     */
    readonly chat: {
      readonly name: "Chat",
      readonly I: typeof ChatRequest,
      readonly O: typeof ChatResponse,
      readonly kind: MethodKind.ServerStreaming,
    },
    /**
     * Event API is used to send events to Chat Engine via Chat controller.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.Event
     */
    readonly event: {
      readonly name: "Event",
      readonly I: typeof EventRequest,
      readonly O: typeof EventResponse,
      readonly kind: MethodKind.ServerStreaming,
    },
  }
};
````

## File: microservices/ace_agent/4.1/webui/server/grpc/gen/ace_agent_connect.js
````javascript
//
// Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
//
// NVIDIA CORPORATION and its licensors retain all intellectual property
// and proprietary rights in and to this software, related documentation
// and any modifications thereto.  Any use, reproduction, disclosure or
// distribution of this software and related documentation without an express
// license agreement from NVIDIA CORPORATION is strictly prohibited.

// @generated by protoc-gen-connect-es v1.4.0
// @generated from file ace_agent.proto (package nvidia.aceagent.chatcontroller.v1, syntax proto3)
/* eslint-disable */
// @ts-nocheck

import { APIStatusResponse, ChatRequest, ChatResponse, EventRequest, EventResponse, GetStatusRequest, GetStatusResponse, PipelineRequest, ReceiveAudioRequest, ReceiveAudioResponse, ReloadSpeechConfigsRequest, SendAudioRequest, SpeechRecognitionControlRequest, StreamingSpeechResultsRequest, StreamingSpeechResultsResponse, SynthesizeSpeechRequest, UserContext, UserContextRequest, UserParametersRequest } from "./ace_agent_pb.js";
import { MethodKind } from "@bufbuild/protobuf";

/**
 *
 * The AceAgentGrpc service provides apis to interact with chat engine and speech
 * components.
 *
 * @generated from service nvidia.aceagent.chatcontroller.v1.AceAgentGrpc
 */
export const AceAgentGrpc = {
  typeName: "nvidia.aceagent.chatcontroller.v1.AceAgentGrpc",
  methods: {
    /**
     * CreatePipeline API is used to create new pipeline with Chat controller,
     *  It acquires a Chat controller pipeline with a unique stream_id populated
     * by the client in PipelineRequest.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.CreatePipeline
     */
    createPipeline: {
      name: "CreatePipeline",
      I: PipelineRequest,
      O: APIStatusResponse,
      kind: MethodKind.Unary,
    },
    /**
     * FreePipeline API is used to free up a pipeline with Chat controller,
     * created by using CreatePipeline API. Client needs to pass same stream_id
     * in PipelineRequest as used in CreatePipeline.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.FreePipeline
     */
    freePipeline: {
      name: "FreePipeline",
      I: PipelineRequest,
      O: APIStatusResponse,
      kind: MethodKind.Unary,
    },
    /**
     * SendAudio API is used to stream audio content to ASR from Chat controller.
     * This is a client side streaming API.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.SendAudio
     */
    sendAudio: {
      name: "SendAudio",
      I: SendAudioRequest,
      O: APIStatusResponse,
      kind: MethodKind.ClientStreaming,
    },
    /**
     * ReceiveAudio API is used to receive synthesized audio from TTS through
     * Chat controller. This is a server side streaming API.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.ReceiveAudio
     */
    receiveAudio: {
      name: "ReceiveAudio",
      I: ReceiveAudioRequest,
      O: ReceiveAudioResponse,
      kind: MethodKind.ServerStreaming,
    },
    /**
     * StreamSpeechResults API is used to receive all the meta data from
     * Chat controller like  ASR transcripts, Chat engine responses, Pipeline
     * states etc. This is a broadcasting API i.e it can fan out responses to
     * multiple concurrent client instances using same stream_id.
     * This is a server side streaming API.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.StreamSpeechResults
     */
    streamSpeechResults: {
      name: "StreamSpeechResults",
      I: StreamingSpeechResultsRequest,
      O: StreamingSpeechResultsResponse,
      kind: MethodKind.ServerStreaming,
    },
    /**
     * StartRecognition API is used to start the ASR recognition in Chat
     * controller for the audio content streamed from SendAudio API.
     * This API also provides a flag to mark the ASR recognition as standalone,
     * i.e Chat Engine and TTS will not be invoked for the ASR transcript.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.StartRecognition
     */
    startRecognition: {
      name: "StartRecognition",
      I: SpeechRecognitionControlRequest,
      O: APIStatusResponse,
      kind: MethodKind.Unary,
    },
    /**
     * StopRecognition API is used to stop the ASR recognition for the audio
     * content streamed from SendAudio API.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.StopRecognition
     */
    stopRecognition: {
      name: "StopRecognition",
      I: SpeechRecognitionControlRequest,
      O: APIStatusResponse,
      kind: MethodKind.Unary,
    },
    /**
     * This API can be used to set the runtime user parameters like user_id
     * for Chat controller pipeline.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.SetUserParameters
     */
    setUserParameters: {
      name: "SetUserParameters",
      I: UserParametersRequest,
      O: APIStatusResponse,
      kind: MethodKind.Unary,
    },
    /**
     * GetStatus API can be used to get the latest state of Chat controller pipeline.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.GetStatus
     */
    getStatus: {
      name: "GetStatus",
      I: GetStatusRequest,
      O: GetStatusResponse,
      kind: MethodKind.Unary,
    },
    /**
     * ReloadSpeechConfigs API can be used to reload the ASR word boosting and
     * TTS Arpbet configs in Chat controller.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.ReloadSpeechConfigs
     */
    reloadSpeechConfigs: {
      name: "ReloadSpeechConfigs",
      I: ReloadSpeechConfigsRequest,
      O: APIStatusResponse,
      kind: MethodKind.Unary,
    },
    /**
     * SynthesizeSpeech API is used to send text transcript directly to the TTS
     * for standalone TTS audio synthesis.
     * The generated audio will be routed to the path specified in the pipeline
     * graph provided in Chat controller.
     * e.g. if the TTS audio is routed to A2F in the graph, the audio will be
     * sent to A2F server.
     * If the TTS audio is routed to Grpc client then it will be available
     * through the server side streaming ReceiveAudio API.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.SynthesizeSpeech
     */
    synthesizeSpeech: {
      name: "SynthesizeSpeech",
      I: SynthesizeSpeechRequest,
      O: APIStatusResponse,
      kind: MethodKind.Unary,
    },
    /**
     * GetUserContext API is used to get the current user context from Chat Engine.
     * The API returns a UserContext message containing the current conversation
     * history and any context attached to the active user_id.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.GetUserContext
     */
    getUserContext: {
      name: "GetUserContext",
      I: UserContextRequest,
      O: UserContext,
      kind: MethodKind.Unary,
    },
    /**
     * SetUserContext API is used to set the current user context in Chat Engine.
     * The API accepts a UserContext message containing the conversation
     * history and any context to be attached to the active user_id.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.SetUserContext
     */
    setUserContext: {
      name: "SetUserContext",
      I: UserContext,
      O: APIStatusResponse,
      kind: MethodKind.Unary,
    },
    /**
     * UpdateUserContext API is used to update the current user context from
     * Chat Engine. The API accepts a UserContext message containing any context
     * to be attached to the active user_id.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.UpdateUserContext
     */
    updateUserContext: {
      name: "UpdateUserContext",
      I: UserContext,
      O: APIStatusResponse,
      kind: MethodKind.Unary,
    },
    /**
     * DeleteUserContext API is used to delete the current user context attached
     * to a user_id in Chat Engine.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.DeleteUserContext
     */
    deleteUserContext: {
      name: "DeleteUserContext",
      I: UserContextRequest,
      O: APIStatusResponse,
      kind: MethodKind.Unary,
    },
    /**
     * Chat API is used to send text queries to Chat Engine via Chat controller.
     * This API also provides a flag to disable TTS synthesis for the response
     * generated by Chat Engine.
     * This can be used for a text in and text out type of scenario.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.Chat
     */
    chat: {
      name: "Chat",
      I: ChatRequest,
      O: ChatResponse,
      kind: MethodKind.ServerStreaming,
    },
    /**
     * Event API is used to send events to Chat Engine via Chat controller.
     *
     * @generated from rpc nvidia.aceagent.chatcontroller.v1.AceAgentGrpc.Event
     */
    event: {
      name: "Event",
      I: EventRequest,
      O: EventResponse,
      kind: MethodKind.ServerStreaming,
    },
  }
};
````

## File: microservices/ace_agent/4.1/webui/server/grpc/gen/ace_agent_pb.d.ts
````typescript
//
// Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
//
// NVIDIA CORPORATION and its licensors retain all intellectual property
// and proprietary rights in and to this software, related documentation
// and any modifications thereto.  Any use, reproduction, disclosure or
// distribution of this software and related documentation without an express
// license agreement from NVIDIA CORPORATION is strictly prohibited.

// @generated by protoc-gen-es v1.8.0
// @generated from file ace_agent.proto (package nvidia.aceagent.chatcontroller.v1, syntax proto3)
/* eslint-disable */
// @ts-nocheck

import type { BinaryReadOptions, FieldList, JsonReadOptions, JsonValue, PartialMessage, PlainMessage } from "@bufbuild/protobuf";
import { Message, proto3 } from "@bufbuild/protobuf";

/**
 *
 * Message type field for Chat controller metadata streaming
 *
 * @generated from enum nvidia.aceagent.chatcontroller.v1.MessageType
 */
export declare enum MessageType {
  /**
   * @generated from enum value: UNKNOWN_RESPONSE = 0;
   */
  UNKNOWN_RESPONSE = 0,

  /**
   * @generated from enum value: ASR_RESPONSE = 1;
   */
  ASR_RESPONSE = 1,

  /**
   * @generated from enum value: CHAT_ENGINE_RESPONSE = 2;
   */
  CHAT_ENGINE_RESPONSE = 2,

  /**
   * @generated from enum value: TTS_RESPONSE = 3;
   */
  TTS_RESPONSE = 3,

  /**
   * @generated from enum value: PIPELINE_STATE_RESPONSE = 4;
   */
  PIPELINE_STATE_RESPONSE = 4,

  /**
   * @generated from enum value: DISPLAY_TEXT = 5;
   */
  DISPLAY_TEXT = 5,
}

/**
 *
 * Generic Chat controller API status
 *
 * @generated from enum nvidia.aceagent.chatcontroller.v1.APIStatus
 */
export declare enum APIStatus {
  /**
   * @generated from enum value: UNKNOWN_STATUS = 0;
   */
  UNKNOWN_STATUS = 0,

  /**
   * @generated from enum value: SUCCESS = 1;
   */
  SUCCESS = 1,

  /**
   * @generated from enum value: PIPELINE_AVAILABLE = 2;
   */
  PIPELINE_AVAILABLE = 2,

  /**
   * @generated from enum value: PIPELINE_NOT_AVAILABLE = 3;
   */
  PIPELINE_NOT_AVAILABLE = 3,

  /**
   * @generated from enum value: BUSY = 4;
   */
  BUSY = 4,

  /**
   * @generated from enum value: ERROR = 5;
   */
  ERROR = 5,

  /**
   * @generated from enum value: INFO = 6;
   */
  INFO = 6,
}

/**
 *
 * Chat controller Pipeline States
 *
 * @generated from enum nvidia.aceagent.chatcontroller.v1.PipelineState
 */
export declare enum PipelineState {
  /**
   * @generated from enum value: INIT = 0;
   */
  INIT = 0,

  /**
   * @generated from enum value: IDLE = 1;
   */
  IDLE = 1,

  /**
   * @generated from enum value: WAIT_FOR_TRIGGER = 2;
   */
  WAIT_FOR_TRIGGER = 2,

  /**
   * @generated from enum value: ASR_ACTIVE = 3;
   */
  ASR_ACTIVE = 3,

  /**
   * @generated from enum value: DM_ACTIVE = 4;
   */
  DM_ACTIVE = 4,

  /**
   * @generated from enum value: TTS_ACTIVE = 5;
   */
  TTS_ACTIVE = 5,
}

/**
 *
 * AudioEncoding specifies the encoding of the audio bytes in the encapsulating message.
 *
 * @generated from enum nvidia.aceagent.chatcontroller.v1.AudioEncoding
 */
export declare enum AudioEncoding {
  /**
   * Not specified.
   *
   * @generated from enum value: UNKNOWN = 0;
   */
  UNKNOWN = 0,

  /**
   * Uncompressed 16-bit signed little-endian samples (Linear PCM).
   *
   * @generated from enum value: LINEAR_PCM = 1;
   */
  LINEAR_PCM = 1,

  /**
   * `FLAC` (Free Lossless Audio
   * Codec) is the recommended encoding because it is
   * lossless--therefore recognition is not compromised--and
   * requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
   * encoding supports 16-bit and 24-bit samples, however, not all fields in
   * `STREAMINFO` are supported.
   *
   * @generated from enum value: FLAC = 2;
   */
  FLAC = 2,

  /**
   * 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
   *
   * @generated from enum value: MULAW = 3;
   */
  MULAW = 3,

  /**
   * 8-bit samples that compand 13-bit audio samples using G.711 PCMU/a-law.
   *
   * @generated from enum value: ALAW = 5;
   */
  ALAW = 5,
}

/**
 *
 * Used in storing conversation history for user and bot
 *
 * @generated from enum nvidia.aceagent.chatcontroller.v1.Role
 */
export declare enum Role {
  /**
   * @generated from enum value: UNDEFINED = 0;
   */
  UNDEFINED = 0,

  /**
   * @generated from enum value: USER = 1;
   */
  USER = 1,

  /**
   * @generated from enum value: BOT = 2;
   */
  BOT = 2,

  /**
   * @generated from enum value: SYSTEM = 3;
   */
  SYSTEM = 3,
}

/**
 *
 * The SendAudioRequest is used to send either StreamingRecognitionConfig message
 * or audio content. The first SendAudioRequest message must contain a
 * StreamingRecognitionConfig message, followed by the audio content messages.
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.SendAudioRequest
 */
export declare class SendAudioRequest extends Message<SendAudioRequest> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  /**
   * The streaming request, which is either a streaming config or audio content.
   *
   * @generated from oneof nvidia.aceagent.chatcontroller.v1.SendAudioRequest.streaming_request
   */
  streamingRequest: {
    /**
     * Provides information to the recognizer that specifies how to process the
     * request. The first `SendAudioRequest` message must contain a
     * `streaming_config`  message.
     *
     * @generated from field: nvidia.aceagent.chatcontroller.v1.StreamingRecognitionConfig streaming_config = 2;
     */
    value: StreamingRecognitionConfig;
    case: "streamingConfig";
  } | {
    /**
     * The audio data to be recognized. Sequential chunks of audio data are
     * streamed from client.
     *
     * @generated from field: bytes audio_content = 3;
     */
    value: Uint8Array;
    case: "audioContent";
  } | { case: undefined; value?: undefined };

  /**
   * source id of the audio data
   *
   * @generated from field: string source_id = 4;
   */
  sourceId: string;

  /**
   * audio buffer creation timestamp in ISO8601 format
   *
   * @generated from field: string create_time = 5;
   */
  createTime: string;

  constructor(data?: PartialMessage<SendAudioRequest>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.SendAudioRequest";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): SendAudioRequest;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): SendAudioRequest;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): SendAudioRequest;

  static equals(a: SendAudioRequest | PlainMessage<SendAudioRequest> | undefined, b: SendAudioRequest | PlainMessage<SendAudioRequest> | undefined): boolean;
}

/**
 * Provides information to the ASR recognizer about incoming audio data
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.StreamingRecognitionConfig
 */
export declare class StreamingRecognitionConfig extends Message<StreamingRecognitionConfig> {
  /**
   * The encoding of the audio data sent in the request.
   *
   * All encodings support only 1 channel (mono) audio.
   *
   * @generated from field: nvidia.aceagent.chatcontroller.v1.AudioEncoding encoding = 1;
   */
  encoding: AudioEncoding;

  /**
   * The sample rate in hertz (Hz) of the audio data sent in the
   * `SendAudioRequest` message.
   *
   * @generated from field: int32 sample_rate_hertz = 2;
   */
  sampleRateHertz: number;

  /**
   * The language of the supplied audio as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * Example: "en-US".
   * Default is en-US.
   *
   * @generated from field: string language_code = 3;
   */
  languageCode: string;

  /**
   * The number of channels in the input audio data.
   *
   * @generated from field: int32 audio_channel_count = 4;
   */
  audioChannelCount: number;

  /**
   * Which model to select for the given request.
   *
   * @generated from field: string model = 5;
   */
  model: string;

  constructor(data?: PartialMessage<StreamingRecognitionConfig>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.StreamingRecognitionConfig";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): StreamingRecognitionConfig;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): StreamingRecognitionConfig;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): StreamingRecognitionConfig;

  static equals(a: StreamingRecognitionConfig | PlainMessage<StreamingRecognitionConfig> | undefined, b: StreamingRecognitionConfig | PlainMessage<StreamingRecognitionConfig> | undefined): boolean;
}

/**
 *
 * ReceiveAudioRequest is used to request audio data for specified stream_id.
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.ReceiveAudioRequest
 */
export declare class ReceiveAudioRequest extends Message<ReceiveAudioRequest> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  constructor(data?: PartialMessage<ReceiveAudioRequest>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.ReceiveAudioRequest";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ReceiveAudioRequest;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ReceiveAudioRequest;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ReceiveAudioRequest;

  static equals(a: ReceiveAudioRequest | PlainMessage<ReceiveAudioRequest> | undefined, b: ReceiveAudioRequest | PlainMessage<ReceiveAudioRequest> | undefined): boolean;
}

/**
 *
 * StreamingSpeechResultsRequest is used to request various results from chat
 * controller.
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.StreamingSpeechResultsRequest
 */
export declare class StreamingSpeechResultsRequest extends Message<StreamingSpeechResultsRequest> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  /**
   * uuid to identify concurrent client request
   *
   * @generated from field: string request_id = 2;
   */
  requestId: string;

  constructor(data?: PartialMessage<StreamingSpeechResultsRequest>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.StreamingSpeechResultsRequest";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): StreamingSpeechResultsRequest;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): StreamingSpeechResultsRequest;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): StreamingSpeechResultsRequest;

  static equals(a: StreamingSpeechResultsRequest | PlainMessage<StreamingSpeechResultsRequest> | undefined, b: StreamingSpeechResultsRequest | PlainMessage<StreamingSpeechResultsRequest> | undefined): boolean;
}

/**
 *
 * PipelineRequest is used to create/free pipeline specified using stream_id
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.PipelineRequest
 */
export declare class PipelineRequest extends Message<PipelineRequest> {
  /**
   * A  unique id sent by the client to identify the client connection.
   * It is mapped to a unique pipeline on the Chat Controller server.
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  /**
   * user id
   *
   * @generated from field: string user_id = 2;
   */
  userId: string;

  constructor(data?: PartialMessage<PipelineRequest>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.PipelineRequest";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): PipelineRequest;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): PipelineRequest;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): PipelineRequest;

  static equals(a: PipelineRequest | PlainMessage<PipelineRequest> | undefined, b: PipelineRequest | PlainMessage<PipelineRequest> | undefined): boolean;
}

/**
 *
 * UserParametersRequest is used to set user parameters
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.UserParametersRequest
 */
export declare class UserParametersRequest extends Message<UserParametersRequest> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  /**
   * used id
   *
   * @generated from field: string user_id = 2;
   */
  userId: string;

  /**
   * bot name with version like {bot_name}_v{bot_version}, e.g chitchat_bot_v1.
   *
   * @generated from field: string bot_name = 3;
   */
  botName: string;

  constructor(data?: PartialMessage<UserParametersRequest>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.UserParametersRequest";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): UserParametersRequest;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): UserParametersRequest;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): UserParametersRequest;

  static equals(a: UserParametersRequest | PlainMessage<UserParametersRequest> | undefined, b: UserParametersRequest | PlainMessage<UserParametersRequest> | undefined): boolean;
}

/**
 *
 * GetStatusRequest used to get on demand Chat controller pipeline status
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.GetStatusRequest
 */
export declare class GetStatusRequest extends Message<GetStatusRequest> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  constructor(data?: PartialMessage<GetStatusRequest>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.GetStatusRequest";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): GetStatusRequest;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): GetStatusRequest;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): GetStatusRequest;

  static equals(a: GetStatusRequest | PlainMessage<GetStatusRequest> | undefined, b: GetStatusRequest | PlainMessage<GetStatusRequest> | undefined): boolean;
}

/**
 *
 * SpeechRecognitionControlRequest is used for controlling input to
 * ASR internally muting ASR.
 * It is also used to disable DM-TTS flow for the incoming ASR input
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.SpeechRecognitionControlRequest
 */
export declare class SpeechRecognitionControlRequest extends Message<SpeechRecognitionControlRequest> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  /**
   * Flag to mention whether asr transcripts to be passed to DM-TTS or get
   * only transcripts
   *
   * @generated from field: bool is_standalone = 2;
   */
  isStandalone: boolean;

  constructor(data?: PartialMessage<SpeechRecognitionControlRequest>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.SpeechRecognitionControlRequest";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): SpeechRecognitionControlRequest;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): SpeechRecognitionControlRequest;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): SpeechRecognitionControlRequest;

  static equals(a: SpeechRecognitionControlRequest | PlainMessage<SpeechRecognitionControlRequest> | undefined, b: SpeechRecognitionControlRequest | PlainMessage<SpeechRecognitionControlRequest> | undefined): boolean;
}

/**
 *
 * Reload Speech Configs Request
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.ReloadSpeechConfigsRequest
 */
export declare class ReloadSpeechConfigsRequest extends Message<ReloadSpeechConfigsRequest> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  constructor(data?: PartialMessage<ReloadSpeechConfigsRequest>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.ReloadSpeechConfigsRequest";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ReloadSpeechConfigsRequest;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ReloadSpeechConfigsRequest;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ReloadSpeechConfigsRequest;

  static equals(a: ReloadSpeechConfigsRequest | PlainMessage<ReloadSpeechConfigsRequest> | undefined, b: ReloadSpeechConfigsRequest | PlainMessage<ReloadSpeechConfigsRequest> | undefined): boolean;
}

/**
 *
 * UserContextRequest used to request user context
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.UserContextRequest
 */
export declare class UserContextRequest extends Message<UserContextRequest> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  /**
   * user id
   *
   * @generated from field: string user_id = 2;
   */
  userId: string;

  constructor(data?: PartialMessage<UserContextRequest>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.UserContextRequest";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): UserContextRequest;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): UserContextRequest;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): UserContextRequest;

  static equals(a: UserContextRequest | PlainMessage<UserContextRequest> | undefined, b: UserContextRequest | PlainMessage<UserContextRequest> | undefined): boolean;
}

/**
 *
 * UserContext data containing user specific information.
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.UserContext
 */
export declare class UserContext extends Message<UserContext> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  /**
   * user id
   *
   * @generated from field: string user_id = 2;
   */
  userId: string;

  /**
   * bot name with version like {bot_name}_v{bot_version}, e.g chitchat_bot_v1.
   *
   * @generated from field: string bot_name = 3;
   */
  botName: string;

  /**
   * conversation history of user
   *
   * @generated from field: repeated nvidia.aceagent.chatcontroller.v1.ConversationHistory conversation_history = 4;
   */
  conversationHistory: ConversationHistory[];

  /**
   * json formatted data of user context
   *
   * @generated from field: string context_json = 5;
   */
  contextJson: string;

  constructor(data?: PartialMessage<UserContext>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.UserContext";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): UserContext;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): UserContext;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): UserContext;

  static equals(a: UserContext | PlainMessage<UserContext> | undefined, b: UserContext | PlainMessage<UserContext> | undefined): boolean;
}

/**
 * @generated from message nvidia.aceagent.chatcontroller.v1.ConversationHistory
 */
export declare class ConversationHistory extends Message<ConversationHistory> {
  /**
   * bot name with version like {bot_name}_v{bot_version}, e.g chitchat_bot_v1.
   *
   * @generated from field: string bot_name = 1;
   */
  botName: string;

  /**
   * @generated from field: repeated nvidia.aceagent.chatcontroller.v1.ConversationInstance conversation = 2;
   */
  conversation: ConversationInstance[];

  constructor(data?: PartialMessage<ConversationHistory>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.ConversationHistory";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ConversationHistory;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ConversationHistory;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ConversationHistory;

  static equals(a: ConversationHistory | PlainMessage<ConversationHistory> | undefined, b: ConversationHistory | PlainMessage<ConversationHistory> | undefined): boolean;
}

/**
 * @generated from message nvidia.aceagent.chatcontroller.v1.ConversationInstance
 */
export declare class ConversationInstance extends Message<ConversationInstance> {
  /**
   * @generated from field: nvidia.aceagent.chatcontroller.v1.Role role = 1;
   */
  role: Role;

  /**
   * @generated from field: string content = 2;
   */
  content: string;

  constructor(data?: PartialMessage<ConversationInstance>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.ConversationInstance";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ConversationInstance;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ConversationInstance;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ConversationInstance;

  static equals(a: ConversationInstance | PlainMessage<ConversationInstance> | undefined, b: ConversationInstance | PlainMessage<ConversationInstance> | undefined): boolean;
}

/**
 * Chat controller pipeline status response
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.GetStatusResponse
 */
export declare class GetStatusResponse extends Message<GetStatusResponse> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  /**
   * @generated from field: nvidia.aceagent.chatcontroller.v1.PipelineStateResponse pipeline_state = 2;
   */
  pipelineState?: PipelineStateResponse;

  constructor(data?: PartialMessage<GetStatusResponse>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.GetStatusResponse";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): GetStatusResponse;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): GetStatusResponse;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): GetStatusResponse;

  static equals(a: GetStatusResponse | PlainMessage<GetStatusResponse> | undefined, b: GetStatusResponse | PlainMessage<GetStatusResponse> | undefined): boolean;
}

/**
 * Chat controller Metadata streaming response
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.StreamingSpeechResultsResponse
 */
export declare class StreamingSpeechResultsResponse extends Message<StreamingSpeechResultsResponse> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  /**
   * message type as defined in `MessageType`
   *
   * @generated from field: nvidia.aceagent.chatcontroller.v1.MessageType message_type = 2;
   */
  messageType: MessageType;

  /**
   * @generated from oneof nvidia.aceagent.chatcontroller.v1.StreamingSpeechResultsResponse.metadata
   */
  metadata: {
    /**
     * @generated from field: nvidia.aceagent.chatcontroller.v1.ASRResult asr_result = 3;
     */
    value: ASRResult;
    case: "asrResult";
  } | {
    /**
     * @generated from field: nvidia.aceagent.chatcontroller.v1.ChatEngineResponse chat_engine_response = 4;
     */
    value: ChatEngineResponse;
    case: "chatEngineResponse";
  } | {
    /**
     * @generated from field: nvidia.aceagent.chatcontroller.v1.TTSResult tts_result = 5;
     */
    value: TTSResult;
    case: "ttsResult";
  } | {
    /**
     * @generated from field: nvidia.aceagent.chatcontroller.v1.PipelineStateResponse pipeline_state = 6;
     */
    value: PipelineStateResponse;
    case: "pipelineState";
  } | {
    /**
     * @generated from field: string display_text = 7;
     */
    value: string;
    case: "displayText";
  } | { case: undefined; value?: undefined };

  constructor(data?: PartialMessage<StreamingSpeechResultsResponse>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.StreamingSpeechResultsResponse";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): StreamingSpeechResultsResponse;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): StreamingSpeechResultsResponse;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): StreamingSpeechResultsResponse;

  static equals(a: StreamingSpeechResultsResponse | PlainMessage<StreamingSpeechResultsResponse> | undefined, b: StreamingSpeechResultsResponse | PlainMessage<StreamingSpeechResultsResponse> | undefined): boolean;
}

/**
 * ASR Result
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.ASRResult
 */
export declare class ASRResult extends Message<ASRResult> {
  /**
   * Complete ASR Response in Riva Skills ASR result schema
   *
   * @generated from field: nvidia.aceagent.chatcontroller.v1.StreamingRecognitionResult results = 1;
   */
  results?: StreamingRecognitionResult;

  /**
   * @generated from field: float latency_ms = 2;
   */
  latencyMs: number;

  /**
   * start time in ISO8601 format, e.g. 2024-03-08T13:33:30.736Z
   *
   * @generated from field: string start_time = 3;
   */
  startTime: string;

  /**
   * stop time in ISO8601 format
   *
   * @generated from field: string stop_time = 4;
   */
  stopTime: string;

  constructor(data?: PartialMessage<ASRResult>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.ASRResult";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ASRResult;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ASRResult;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ASRResult;

  static equals(a: ASRResult | PlainMessage<ASRResult> | undefined, b: ASRResult | PlainMessage<ASRResult> | undefined): boolean;
}

/**
 * A streaming speech recognition result corresponding to a portion of the audio
 * that is currently being processed.
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.StreamingRecognitionResult
 */
export declare class StreamingRecognitionResult extends Message<StreamingRecognitionResult> {
  /**
   * May contain one or more recognition hypotheses (up to the
   * maximum specified in `max_alternatives`).
   * These alternatives are ordered in terms of accuracy, with the top (first)
   * alternative being the most probable, as ranked by the recognizer.
   *
   * @generated from field: repeated nvidia.aceagent.chatcontroller.v1.SpeechRecognitionAlternative alternatives = 1;
   */
  alternatives: SpeechRecognitionAlternative[];

  /**
   * If `false`, this `StreamingRecognitionResult` represents an
   * interim result that may change. If `true`, this is the final time the
   * speech service will return this particular `StreamingRecognitionResult`,
   * the recognizer will not return any further hypotheses for this portion of
   * the transcript and corresponding audio.
   *
   * @generated from field: bool is_final = 2;
   */
  isFinal: boolean;

  /**
   * An estimate of the likelihood that the recognizer will not
   * change its guess about this interim result. Values range from 0.0
   * (completely unstable) to 1.0 (completely stable).
   * This field is only provided for interim results (`is_final=false`).
   * The default of 0.0 is a sentinel value indicating `stability` was not set.
   *
   * @generated from field: float stability = 3;
   */
  stability: number;

  /**
   * For multi-channel audio, this is the channel number corresponding to the
   * recognized result for the audio from that channel.
   * For audio_channel_count = N, its output values can range from '1' to 'N'.
   *
   * @generated from field: int32 channel_tag = 5;
   */
  channelTag: number;

  /**
   * Length of audio processed so far in seconds
   *
   * @generated from field: float audio_processed = 6;
   */
  audioProcessed: number;

  constructor(data?: PartialMessage<StreamingRecognitionResult>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.StreamingRecognitionResult";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): StreamingRecognitionResult;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): StreamingRecognitionResult;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): StreamingRecognitionResult;

  static equals(a: StreamingRecognitionResult | PlainMessage<StreamingRecognitionResult> | undefined, b: StreamingRecognitionResult | PlainMessage<StreamingRecognitionResult> | undefined): boolean;
}

/**
 * Alternative hypotheses (a.k.a. n-best list).
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.SpeechRecognitionAlternative
 */
export declare class SpeechRecognitionAlternative extends Message<SpeechRecognitionAlternative> {
  /**
   * Transcript text representing the words that the user spoke.
   *
   * @generated from field: string transcript = 1;
   */
  transcript: string;

  /**
   * The non-normalized confidence estimate. A higher number
   * indicates an estimated greater likelihood that the recognized words are
   * correct. This field is set only for a non-streaming
   * result or, of a streaming result where `is_final=true`.
   * This field is not guaranteed to be accurate and users should not rely on it
   * to be always provided.
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;

  /**
   * A list of word-specific information for each recognized word. Only populated
   * if is_final=true
   *
   * @generated from field: repeated nvidia.aceagent.chatcontroller.v1.WordInfo words = 3;
   */
  words: WordInfo[];

  constructor(data?: PartialMessage<SpeechRecognitionAlternative>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.SpeechRecognitionAlternative";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): SpeechRecognitionAlternative;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): SpeechRecognitionAlternative;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): SpeechRecognitionAlternative;

  static equals(a: SpeechRecognitionAlternative | PlainMessage<SpeechRecognitionAlternative> | undefined, b: SpeechRecognitionAlternative | PlainMessage<SpeechRecognitionAlternative> | undefined): boolean;
}

/**
 * Word-specific information for recognized words.
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.WordInfo
 */
export declare class WordInfo extends Message<WordInfo> {
  /**
   * Time offset relative to the beginning of the audio in ms
   * and corresponding to the start of the spoken word.
   * This field is only set if `enable_word_time_offsets=true` and only
   * in the top hypothesis.
   *
   * @generated from field: int32 start_time = 1;
   */
  startTime: number;

  /**
   * Time offset relative to the beginning of the audio in ms
   * and corresponding to the end of the spoken word.
   * This field is only set if `enable_word_time_offsets=true` and only
   * in the top hypothesis.
   *
   * @generated from field: int32 end_time = 2;
   */
  endTime: number;

  /**
   * The word corresponding to this set of information.
   *
   * @generated from field: string word = 3;
   */
  word: string;

  /**
   * The non-normalized confidence estimate. A higher number indicates an
   * estimated greater likelihood that the recognized words are correct. This
   * field is not guaranteed to be accurate and users should not rely on it to
   * be always provided. The default of 0.0 is a sentinel value indicating
   * confidence was not set.
   *
   * @generated from field: float confidence = 4;
   */
  confidence: number;

  constructor(data?: PartialMessage<WordInfo>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.WordInfo";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): WordInfo;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): WordInfo;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): WordInfo;

  static equals(a: WordInfo | PlainMessage<WordInfo> | undefined, b: WordInfo | PlainMessage<WordInfo> | undefined): boolean;
}

/**
 * Chat Engine Result json
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.ChatEngineResponse
 */
export declare class ChatEngineResponse extends Message<ChatEngineResponse> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  /**
   * chat engine result
   *
   * @generated from field: string result = 2;
   */
  result: string;

  /**
   * @generated from field: float latency_ms = 3;
   */
  latencyMs: number;

  constructor(data?: PartialMessage<ChatEngineResponse>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.ChatEngineResponse";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ChatEngineResponse;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ChatEngineResponse;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ChatEngineResponse;

  static equals(a: ChatEngineResponse | PlainMessage<ChatEngineResponse> | undefined, b: ChatEngineResponse | PlainMessage<ChatEngineResponse> | undefined): boolean;
}

/**
 * TTS result metadata
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.TTSResult
 */
export declare class TTSResult extends Message<TTSResult> {
  /**
   * TTS latency in milliseconds
   *
   * @generated from field: float latency_ms = 1;
   */
  latencyMs: number;

  constructor(data?: PartialMessage<TTSResult>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.TTSResult";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): TTSResult;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): TTSResult;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): TTSResult;

  static equals(a: TTSResult | PlainMessage<TTSResult> | undefined, b: TTSResult | PlainMessage<TTSResult> | undefined): boolean;
}

/**
 * Chat controller pipeline state response
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.PipelineStateResponse
 */
export declare class PipelineStateResponse extends Message<PipelineStateResponse> {
  /**
   * @generated from field: nvidia.aceagent.chatcontroller.v1.PipelineState state = 1;
   */
  state: PipelineState;

  constructor(data?: PartialMessage<PipelineStateResponse>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.PipelineStateResponse";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): PipelineStateResponse;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): PipelineStateResponse;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): PipelineStateResponse;

  static equals(a: PipelineStateResponse | PlainMessage<PipelineStateResponse> | undefined, b: PipelineStateResponse | PlainMessage<PipelineStateResponse> | undefined): boolean;
}

/**
 * Receive Audio API Response
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.ReceiveAudioResponse
 */
export declare class ReceiveAudioResponse extends Message<ReceiveAudioResponse> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  /**
   * synthesized audio data
   *
   * @generated from field: bytes audio_content = 2;
   */
  audioContent: Uint8Array;

  /**
   * The encoding of the audio data
   *
   * @generated from field: nvidia.aceagent.chatcontroller.v1.AudioEncoding encoding = 3;
   */
  encoding: AudioEncoding;

  /**
   * The sample rate in hertz (Hz) of the audio data
   *
   * @generated from field: int32 sample_rate_hertz = 4;
   */
  sampleRateHertz: number;

  /**
   * The number of channels in the audio data. Only mono is supported
   *
   * @generated from field: int32 audio_channel_count = 5;
   */
  audioChannelCount: number;

  /**
   * frame size of audio data
   *
   * @generated from field: int32 frame_size = 6;
   */
  frameSize: number;

  constructor(data?: PartialMessage<ReceiveAudioResponse>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.ReceiveAudioResponse";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ReceiveAudioResponse;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ReceiveAudioResponse;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ReceiveAudioResponse;

  static equals(a: ReceiveAudioResponse | PlainMessage<ReceiveAudioResponse> | undefined, b: ReceiveAudioResponse | PlainMessage<ReceiveAudioResponse> | undefined): boolean;
}

/**
 * Generic API status response message
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.APIStatusResponse
 */
export declare class APIStatusResponse extends Message<APIStatusResponse> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  /**
   * response message
   *
   * @generated from field: string response_msg = 2;
   */
  responseMsg: string;

  /**
   * API response status code as defined in `APIStatus`
   *
   * @generated from field: nvidia.aceagent.chatcontroller.v1.APIStatus status = 3;
   */
  status: APIStatus;

  constructor(data?: PartialMessage<APIStatusResponse>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.APIStatusResponse";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): APIStatusResponse;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): APIStatusResponse;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): APIStatusResponse;

  static equals(a: APIStatusResponse | PlainMessage<APIStatusResponse> | undefined, b: APIStatusResponse | PlainMessage<APIStatusResponse> | undefined): boolean;
}

/**
 *
 * Request message for standalone TTS synthesis of provided text transcript
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.SynthesizeSpeechRequest
 */
export declare class SynthesizeSpeechRequest extends Message<SynthesizeSpeechRequest> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  /**
   * transcript text to be synthesized
   *
   * @generated from field: string transcript = 2;
   */
  transcript: string;

  constructor(data?: PartialMessage<SynthesizeSpeechRequest>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.SynthesizeSpeechRequest";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): SynthesizeSpeechRequest;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): SynthesizeSpeechRequest;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): SynthesizeSpeechRequest;

  static equals(a: SynthesizeSpeechRequest | PlainMessage<SynthesizeSpeechRequest> | undefined, b: SynthesizeSpeechRequest | PlainMessage<SynthesizeSpeechRequest> | undefined): boolean;
}

/**
 *
 * Request message for Chat API which will be sent to chat engine
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.ChatRequest
 */
export declare class ChatRequest extends Message<ChatRequest> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  /**
   * bot name with version like {bot_name}_v{bot_version}, e.g chitchat_bot_v1.
   *
   * @generated from field: string bot_name = 2;
   */
  botName: string;

  /**
   * query
   *
   * @generated from field: string query = 3;
   */
  query: string;

  /**
   * unique id for identifying the query
   *
   * @generated from field: string query_id = 4;
   */
  queryId: string;

  /**
   * user id
   *
   * @generated from field: string user_id = 5;
   */
  userId: string;

  /**
   * The language of the supplied query string as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * Example: "en-US".
   *
   * @generated from field: string source_language = 6;
   */
  sourceLanguage: string;

  /**
   * The language of the response required from chat engine as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * Example: "en-US".
   *
   * @generated from field: string target_language = 7;
   */
  targetLanguage: string;

  /**
   * Flag to send standalone text requests, when set true reponse is not sent
   * to TTS when set to false reponse will be sent to TTS
   *
   * @generated from field: bool is_standalone = 8;
   */
  isStandalone: boolean;

  /**
   * key-value pair for user context to be sent to chat engine
   *
   * @generated from field: map<string, string> user_context = 9;
   */
  userContext: { [key: string]: string };

  /**
   * key-value pair for meta data to be sent to chat engine
   *
   * @generated from field: map<string, string> metadata = 10;
   */
  metadata: { [key: string]: string };

  constructor(data?: PartialMessage<ChatRequest>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.ChatRequest";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ChatRequest;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ChatRequest;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ChatRequest;

  static equals(a: ChatRequest | PlainMessage<ChatRequest> | undefined, b: ChatRequest | PlainMessage<ChatRequest> | undefined): boolean;
}

/**
 *
 * Response message from chat engine for Chat API invocation
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.ChatResponse
 */
export declare class ChatResponse extends Message<ChatResponse> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  /**
   * query
   *
   * @generated from field: string query = 2;
   */
  query: string;

  /**
   * unique id for identifying the query
   *
   * @generated from field: string query_id = 3;
   */
  queryId: string;

  /**
   * user id
   *
   * @generated from field: string user_id = 4;
   */
  userId: string;

  /**
   * session id if generated by chat engine
   *
   * @generated from field: string session_id = 5;
   */
  sessionId: string;

  /**
   * chat engine response for the query passed in `ChatRequest`
   *
   * @generated from field: string text = 6;
   */
  text: string;

  /**
   * chat engine cleaned up response text after markdown language tags removal
   *
   * @generated from field: string cleaned_text = 7;
   */
  cleanedText: string;

  /**
   * flag to indicate whether this is final response or intermediate response, when true
   * there will be no more responses for the requested `ChatRequest`
   *
   * @generated from field: bool is_final = 8;
   */
  isFinal: boolean;

  /**
   * chat engine response in json format
   *
   * @generated from field: string json_response = 9;
   */
  jsonResponse: string;

  constructor(data?: PartialMessage<ChatResponse>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.ChatResponse";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ChatResponse;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ChatResponse;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ChatResponse;

  static equals(a: ChatResponse | PlainMessage<ChatResponse> | undefined, b: ChatResponse | PlainMessage<ChatResponse> | undefined): boolean;
}

/**
 *
 * Request message for Event API which will be sent to chat engine
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.EventRequest
 */
export declare class EventRequest extends Message<EventRequest> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  /**
   * bot name with version like {bot_name}_v{bot_version}, e.g chitchat_bot_v1.
   *
   * @generated from field: string bot_name = 2;
   */
  botName: string;

  /**
   * event type
   *
   * @generated from field: string event_type = 3;
   */
  eventType: string;

  /**
   * unique event id
   *
   * @generated from field: string event_id = 4;
   */
  eventId: string;

  /**
   * user id
   *
   * @generated from field: string user_id = 5;
   */
  userId: string;

  /**
   * key-value pair for user context to be sent to chat engine
   *
   * @generated from field: map<string, string> user_context = 6;
   */
  userContext: { [key: string]: string };

  /**
   * key-value pair for meta data to be sent to chat engine
   *
   * @generated from field: map<string, string> metadata = 7;
   */
  metadata: { [key: string]: string };

  constructor(data?: PartialMessage<EventRequest>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.EventRequest";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): EventRequest;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): EventRequest;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): EventRequest;

  static equals(a: EventRequest | PlainMessage<EventRequest> | undefined, b: EventRequest | PlainMessage<EventRequest> | undefined): boolean;
}

/**
 * @generated from message nvidia.aceagent.chatcontroller.v1.EventResponse
 */
export declare class EventResponse extends Message<EventResponse> {
  /**
   * unique id to identify the client connection
   *
   * @generated from field: string stream_id = 1;
   */
  streamId: string;

  /**
   * event type
   *
   * @generated from field: string event_type = 2;
   */
  eventType: string;

  /**
   * unique event id
   *
   * @generated from field: string event_id = 3;
   */
  eventId: string;

  /**
   * user id
   *
   * @generated from field: string user_id = 4;
   */
  userId: string;

  /**
   * text response
   *
   * @generated from field: string text = 5;
   */
  text: string;

  /**
   * @generated from field: string cleaned_text = 6;
   */
  cleanedText: string;

  /**
   * @generated from field: bool is_final = 7;
   */
  isFinal: boolean;

  /**
   * @generated from field: string json_response = 8;
   */
  jsonResponse: string;

  /**
   * @generated from field: repeated string events = 9;
   */
  events: string[];

  constructor(data?: PartialMessage<EventResponse>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "nvidia.aceagent.chatcontroller.v1.EventResponse";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): EventResponse;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): EventResponse;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): EventResponse;

  static equals(a: EventResponse | PlainMessage<EventResponse> | undefined, b: EventResponse | PlainMessage<EventResponse> | undefined): boolean;
}
````

## File: microservices/ace_agent/4.1/webui/server/grpc/gen/ace_agent_pb.js
````javascript
//
// Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
//
// NVIDIA CORPORATION and its licensors retain all intellectual property
// and proprietary rights in and to this software, related documentation
// and any modifications thereto.  Any use, reproduction, disclosure or
// distribution of this software and related documentation without an express
// license agreement from NVIDIA CORPORATION is strictly prohibited.

// @generated by protoc-gen-es v1.8.0
// @generated from file ace_agent.proto (package nvidia.aceagent.chatcontroller.v1, syntax proto3)
/* eslint-disable */
// @ts-nocheck

import { proto3 } from "@bufbuild/protobuf";

/**
 *
 * Message type field for Chat controller metadata streaming
 *
 * @generated from enum nvidia.aceagent.chatcontroller.v1.MessageType
 */
export const MessageType = /*@__PURE__*/ proto3.makeEnum(
  "nvidia.aceagent.chatcontroller.v1.MessageType",
  [
    {no: 0, name: "UNKNOWN_RESPONSE"},
    {no: 1, name: "ASR_RESPONSE"},
    {no: 2, name: "CHAT_ENGINE_RESPONSE"},
    {no: 3, name: "TTS_RESPONSE"},
    {no: 4, name: "PIPELINE_STATE_RESPONSE"},
    {no: 5, name: "DISPLAY_TEXT"},
  ],
);

/**
 *
 * Generic Chat controller API status
 *
 * @generated from enum nvidia.aceagent.chatcontroller.v1.APIStatus
 */
export const APIStatus = /*@__PURE__*/ proto3.makeEnum(
  "nvidia.aceagent.chatcontroller.v1.APIStatus",
  [
    {no: 0, name: "UNKNOWN_STATUS"},
    {no: 1, name: "SUCCESS"},
    {no: 2, name: "PIPELINE_AVAILABLE"},
    {no: 3, name: "PIPELINE_NOT_AVAILABLE"},
    {no: 4, name: "BUSY"},
    {no: 5, name: "ERROR"},
    {no: 6, name: "INFO"},
  ],
);

/**
 *
 * Chat controller Pipeline States
 *
 * @generated from enum nvidia.aceagent.chatcontroller.v1.PipelineState
 */
export const PipelineState = /*@__PURE__*/ proto3.makeEnum(
  "nvidia.aceagent.chatcontroller.v1.PipelineState",
  [
    {no: 0, name: "INIT"},
    {no: 1, name: "IDLE"},
    {no: 2, name: "WAIT_FOR_TRIGGER"},
    {no: 3, name: "ASR_ACTIVE"},
    {no: 4, name: "DM_ACTIVE"},
    {no: 5, name: "TTS_ACTIVE"},
  ],
);

/**
 *
 * AudioEncoding specifies the encoding of the audio bytes in the encapsulating message.
 *
 * @generated from enum nvidia.aceagent.chatcontroller.v1.AudioEncoding
 */
export const AudioEncoding = /*@__PURE__*/ proto3.makeEnum(
  "nvidia.aceagent.chatcontroller.v1.AudioEncoding",
  [
    {no: 0, name: "UNKNOWN"},
    {no: 1, name: "LINEAR_PCM"},
    {no: 2, name: "FLAC"},
    {no: 3, name: "MULAW"},
    {no: 5, name: "ALAW"},
  ],
);

/**
 *
 * Used in storing conversation history for user and bot
 *
 * @generated from enum nvidia.aceagent.chatcontroller.v1.Role
 */
export const Role = /*@__PURE__*/ proto3.makeEnum(
  "nvidia.aceagent.chatcontroller.v1.Role",
  [
    {no: 0, name: "UNDEFINED"},
    {no: 1, name: "USER"},
    {no: 2, name: "BOT"},
    {no: 3, name: "SYSTEM"},
  ],
);

/**
 *
 * The SendAudioRequest is used to send either StreamingRecognitionConfig message
 * or audio content. The first SendAudioRequest message must contain a
 * StreamingRecognitionConfig message, followed by the audio content messages.
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.SendAudioRequest
 */
export const SendAudioRequest = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.SendAudioRequest",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "streaming_config", kind: "message", T: StreamingRecognitionConfig, oneof: "streaming_request" },
    { no: 3, name: "audio_content", kind: "scalar", T: 12 /* ScalarType.BYTES */, oneof: "streaming_request" },
    { no: 4, name: "source_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 5, name: "create_time", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ],
);

/**
 * Provides information to the ASR recognizer about incoming audio data
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.StreamingRecognitionConfig
 */
export const StreamingRecognitionConfig = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.StreamingRecognitionConfig",
  () => [
    { no: 1, name: "encoding", kind: "enum", T: proto3.getEnumType(AudioEncoding) },
    { no: 2, name: "sample_rate_hertz", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 3, name: "language_code", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 4, name: "audio_channel_count", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 5, name: "model", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ],
);

/**
 *
 * ReceiveAudioRequest is used to request audio data for specified stream_id.
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.ReceiveAudioRequest
 */
export const ReceiveAudioRequest = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.ReceiveAudioRequest",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ],
);

/**
 *
 * StreamingSpeechResultsRequest is used to request various results from chat
 * controller.
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.StreamingSpeechResultsRequest
 */
export const StreamingSpeechResultsRequest = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.StreamingSpeechResultsRequest",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "request_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ],
);

/**
 *
 * PipelineRequest is used to create/free pipeline specified using stream_id
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.PipelineRequest
 */
export const PipelineRequest = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.PipelineRequest",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "user_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ],
);

/**
 *
 * UserParametersRequest is used to set user parameters
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.UserParametersRequest
 */
export const UserParametersRequest = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.UserParametersRequest",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "user_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "bot_name", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ],
);

/**
 *
 * GetStatusRequest used to get on demand Chat controller pipeline status
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.GetStatusRequest
 */
export const GetStatusRequest = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.GetStatusRequest",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ],
);

/**
 *
 * SpeechRecognitionControlRequest is used for controlling input to
 * ASR internally muting ASR.
 * It is also used to disable DM-TTS flow for the incoming ASR input
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.SpeechRecognitionControlRequest
 */
export const SpeechRecognitionControlRequest = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.SpeechRecognitionControlRequest",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "is_standalone", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
  ],
);

/**
 *
 * Reload Speech Configs Request
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.ReloadSpeechConfigsRequest
 */
export const ReloadSpeechConfigsRequest = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.ReloadSpeechConfigsRequest",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ],
);

/**
 *
 * UserContextRequest used to request user context
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.UserContextRequest
 */
export const UserContextRequest = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.UserContextRequest",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "user_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ],
);

/**
 *
 * UserContext data containing user specific information.
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.UserContext
 */
export const UserContext = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.UserContext",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "user_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "bot_name", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 4, name: "conversation_history", kind: "message", T: ConversationHistory, repeated: true },
    { no: 5, name: "context_json", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ],
);

/**
 * @generated from message nvidia.aceagent.chatcontroller.v1.ConversationHistory
 */
export const ConversationHistory = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.ConversationHistory",
  () => [
    { no: 1, name: "bot_name", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "conversation", kind: "message", T: ConversationInstance, repeated: true },
  ],
);

/**
 * @generated from message nvidia.aceagent.chatcontroller.v1.ConversationInstance
 */
export const ConversationInstance = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.ConversationInstance",
  () => [
    { no: 1, name: "role", kind: "enum", T: proto3.getEnumType(Role) },
    { no: 2, name: "content", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ],
);

/**
 * Chat controller pipeline status response
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.GetStatusResponse
 */
export const GetStatusResponse = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.GetStatusResponse",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "pipeline_state", kind: "message", T: PipelineStateResponse },
  ],
);

/**
 * Chat controller Metadata streaming response
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.StreamingSpeechResultsResponse
 */
export const StreamingSpeechResultsResponse = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.StreamingSpeechResultsResponse",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "message_type", kind: "enum", T: proto3.getEnumType(MessageType) },
    { no: 3, name: "asr_result", kind: "message", T: ASRResult, oneof: "metadata" },
    { no: 4, name: "chat_engine_response", kind: "message", T: ChatEngineResponse, oneof: "metadata" },
    { no: 5, name: "tts_result", kind: "message", T: TTSResult, oneof: "metadata" },
    { no: 6, name: "pipeline_state", kind: "message", T: PipelineStateResponse, oneof: "metadata" },
    { no: 7, name: "display_text", kind: "scalar", T: 9 /* ScalarType.STRING */, oneof: "metadata" },
  ],
);

/**
 * ASR Result
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.ASRResult
 */
export const ASRResult = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.ASRResult",
  () => [
    { no: 1, name: "results", kind: "message", T: StreamingRecognitionResult },
    { no: 2, name: "latency_ms", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 3, name: "start_time", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 4, name: "stop_time", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ],
);

/**
 * A streaming speech recognition result corresponding to a portion of the audio
 * that is currently being processed.
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.StreamingRecognitionResult
 */
export const StreamingRecognitionResult = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.StreamingRecognitionResult",
  () => [
    { no: 1, name: "alternatives", kind: "message", T: SpeechRecognitionAlternative, repeated: true },
    { no: 2, name: "is_final", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
    { no: 3, name: "stability", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 5, name: "channel_tag", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 6, name: "audio_processed", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
  ],
);

/**
 * Alternative hypotheses (a.k.a. n-best list).
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.SpeechRecognitionAlternative
 */
export const SpeechRecognitionAlternative = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.SpeechRecognitionAlternative",
  () => [
    { no: 1, name: "transcript", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "confidence", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 3, name: "words", kind: "message", T: WordInfo, repeated: true },
  ],
);

/**
 * Word-specific information for recognized words.
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.WordInfo
 */
export const WordInfo = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.WordInfo",
  () => [
    { no: 1, name: "start_time", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 2, name: "end_time", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 3, name: "word", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 4, name: "confidence", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
  ],
);

/**
 * Chat Engine Result json
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.ChatEngineResponse
 */
export const ChatEngineResponse = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.ChatEngineResponse",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "result", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "latency_ms", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
  ],
);

/**
 * TTS result metadata
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.TTSResult
 */
export const TTSResult = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.TTSResult",
  () => [
    { no: 1, name: "latency_ms", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
  ],
);

/**
 * Chat controller pipeline state response
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.PipelineStateResponse
 */
export const PipelineStateResponse = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.PipelineStateResponse",
  () => [
    { no: 1, name: "state", kind: "enum", T: proto3.getEnumType(PipelineState) },
  ],
);

/**
 * Receive Audio API Response
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.ReceiveAudioResponse
 */
export const ReceiveAudioResponse = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.ReceiveAudioResponse",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "audio_content", kind: "scalar", T: 12 /* ScalarType.BYTES */ },
    { no: 3, name: "encoding", kind: "enum", T: proto3.getEnumType(AudioEncoding) },
    { no: 4, name: "sample_rate_hertz", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 5, name: "audio_channel_count", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 6, name: "frame_size", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
  ],
);

/**
 * Generic API status response message
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.APIStatusResponse
 */
export const APIStatusResponse = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.APIStatusResponse",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "response_msg", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "status", kind: "enum", T: proto3.getEnumType(APIStatus) },
  ],
);

/**
 *
 * Request message for standalone TTS synthesis of provided text transcript
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.SynthesizeSpeechRequest
 */
export const SynthesizeSpeechRequest = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.SynthesizeSpeechRequest",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "transcript", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ],
);

/**
 *
 * Request message for Chat API which will be sent to chat engine
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.ChatRequest
 */
export const ChatRequest = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.ChatRequest",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "bot_name", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "query", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 4, name: "query_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 5, name: "user_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 6, name: "source_language", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 7, name: "target_language", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 8, name: "is_standalone", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
    { no: 9, name: "user_context", kind: "map", K: 9 /* ScalarType.STRING */, V: {kind: "scalar", T: 9 /* ScalarType.STRING */} },
    { no: 10, name: "metadata", kind: "map", K: 9 /* ScalarType.STRING */, V: {kind: "scalar", T: 9 /* ScalarType.STRING */} },
  ],
);

/**
 *
 * Response message from chat engine for Chat API invocation
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.ChatResponse
 */
export const ChatResponse = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.ChatResponse",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "query", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "query_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 4, name: "user_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 5, name: "session_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 6, name: "text", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 7, name: "cleaned_text", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 8, name: "is_final", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
    { no: 9, name: "json_response", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ],
);

/**
 *
 * Request message for Event API which will be sent to chat engine
 *
 * @generated from message nvidia.aceagent.chatcontroller.v1.EventRequest
 */
export const EventRequest = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.EventRequest",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "bot_name", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "event_type", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 4, name: "event_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 5, name: "user_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 6, name: "user_context", kind: "map", K: 9 /* ScalarType.STRING */, V: {kind: "scalar", T: 9 /* ScalarType.STRING */} },
    { no: 7, name: "metadata", kind: "map", K: 9 /* ScalarType.STRING */, V: {kind: "scalar", T: 9 /* ScalarType.STRING */} },
  ],
);

/**
 * @generated from message nvidia.aceagent.chatcontroller.v1.EventResponse
 */
export const EventResponse = /*@__PURE__*/ proto3.makeMessageType(
  "nvidia.aceagent.chatcontroller.v1.EventResponse",
  () => [
    { no: 1, name: "stream_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "event_type", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "event_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 4, name: "user_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 5, name: "text", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 6, name: "cleaned_text", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 7, name: "is_final", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
    { no: 8, name: "json_response", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 9, name: "events", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
  ],
);
````

## File: microservices/ace_agent/4.1/webui/server/grpc/ace_agent.proto
````protobuf
/*
* Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
*
* NVIDIA CORPORATION and its licensors retain all intellectual property
* and proprietary rights in and to this software, related documentation
* and any modifications thereto.  Any use, reproduction, disclosure or
* distribution of this software and related documentation without an express
* license agreement from NVIDIA CORPORATION is strictly prohibited.
*/

syntax = "proto3";

package nvidia.aceagent.chatcontroller.v1;

option cc_enable_arenas = true;

// Java package name
option java_package = "com.nvidia.aceagent.chatcontroller.v1";

/*
 * The AceAgentGrpc service provides apis to interact with chat engine and speech
 * components.
 */
service AceAgentGrpc {
    // CreatePipeline API is used to create new pipeline with Chat controller,
    //  It acquires a Chat controller pipeline with a unique stream_id populated
    // by the client in PipelineRequest.
    rpc CreatePipeline(PipelineRequest) returns (APIStatusResponse) {}

    // FreePipeline API is used to free up a pipeline with Chat controller,
    // created by using CreatePipeline API. Client needs to pass same stream_id
    // in PipelineRequest as used in CreatePipeline.
    rpc FreePipeline(PipelineRequest) returns (APIStatusResponse) {}

    // SendAudio API is used to stream audio content to ASR from Chat controller.
    // This is a client side streaming API.
    rpc SendAudio(stream SendAudioRequest) returns (APIStatusResponse) {}

    // ReceiveAudio API is used to receive synthesized audio from TTS through
    // Chat controller. This is a server side streaming API.
    rpc ReceiveAudio(ReceiveAudioRequest) returns (stream ReceiveAudioResponse) {}

    // StreamSpeechResults API is used to receive all the meta data from
    // Chat controller like  ASR transcripts, Chat engine responses, Pipeline
    // states etc. This is a broadcasting API i.e it can fan out responses to
    // multiple concurrent client instances using same stream_id.
    // This is a server side streaming API.
    rpc StreamSpeechResults(StreamingSpeechResultsRequest) returns (stream StreamingSpeechResultsResponse) {}

    // StartRecognition API is used to start the ASR recognition in Chat
    // controller for the audio content streamed from SendAudio API.
    // This API also provides a flag to mark the ASR recognition as standalone,
    // i.e Chat Engine and TTS will not be invoked for the ASR transcript.
    rpc StartRecognition (SpeechRecognitionControlRequest) returns (APIStatusResponse) {}

    // StopRecognition API is used to stop the ASR recognition for the audio
    // content streamed from SendAudio API.
    rpc StopRecognition (SpeechRecognitionControlRequest) returns (APIStatusResponse) {}

    // This API can be used to set the runtime user parameters like user_id
    // for Chat controller pipeline.
    rpc SetUserParameters (UserParametersRequest) returns (APIStatusResponse) {}

    // GetStatus API can be used to get the latest state of Chat controller pipeline.
    rpc GetStatus (GetStatusRequest) returns (GetStatusResponse) {}

    // ReloadSpeechConfigs API can be used to reload the ASR word boosting and
    // TTS Arpbet configs in Chat controller.
    rpc ReloadSpeechConfigs (ReloadSpeechConfigsRequest) returns (APIStatusResponse) {}

    // SynthesizeSpeech API is used to send text transcript directly to the TTS
    // for standalone TTS audio synthesis.
    // The generated audio will be routed to the path specified in the pipeline
    // graph provided in Chat controller.
    // e.g. if the TTS audio is routed to A2F in the graph, the audio will be
    // sent to A2F server.
    // If the TTS audio is routed to Grpc client then it will be available
    // through the server side streaming ReceiveAudio API.
    rpc SynthesizeSpeech (SynthesizeSpeechRequest) returns (APIStatusResponse) {}

    // GetUserContext API is used to get the current user context from Chat Engine.
    // The API returns a UserContext message containing the current conversation
    // history and any context attached to the active user_id.
    rpc GetUserContext (UserContextRequest) returns (UserContext) {}

    // SetUserContext API is used to set the current user context in Chat Engine.
    // The API accepts a UserContext message containing the conversation
    // history and any context to be attached to the active user_id.
    rpc SetUserContext (UserContext) returns (APIStatusResponse) {}

    // UpdateUserContext API is used to update the current user context from
    // Chat Engine. The API accepts a UserContext message containing any context
    // to be attached to the active user_id.
    rpc UpdateUserContext (UserContext) returns (APIStatusResponse) {}

    // DeleteUserContext API is used to delete the current user context attached
    // to a user_id in Chat Engine.
    rpc DeleteUserContext (UserContextRequest) returns (APIStatusResponse) {}

    // Chat API is used to send text queries to Chat Engine via Chat controller.
    // This API also provides a flag to disable TTS synthesis for the response
    // generated by Chat Engine.
    // This can be used for a text in and text out type of scenario.
    rpc Chat (ChatRequest) returns (stream ChatResponse) {}

    // Event API is used to send events to Chat Engine via Chat controller.
    rpc Event (EventRequest) returns (stream EventResponse) {}
}

/*
 * The SendAudioRequest is used to send either StreamingRecognitionConfig message
 * or audio content. The first SendAudioRequest message must contain a
 * StreamingRecognitionConfig message, followed by the audio content messages.
 */
message SendAudioRequest {
    // unique id to identify the client connection
    string stream_id = 1;

    // The streaming request, which is either a streaming config or audio content.
    oneof streaming_request {
        // Provides information to the recognizer that specifies how to process the
        // request. The first `SendAudioRequest` message must contain a
        // `streaming_config`  message.
        StreamingRecognitionConfig streaming_config = 2;

        // The audio data to be recognized. Sequential chunks of audio data are
        // streamed from client.
        bytes audio_content = 3;
    }

    // source id of the audio data
    string source_id = 4;

    // audio buffer creation timestamp in ISO8601 format
    string create_time = 5;
}

// Provides information to the ASR recognizer about incoming audio data
message StreamingRecognitionConfig {
    // The encoding of the audio data sent in the request.
    //
    // All encodings support only 1 channel (mono) audio.
    AudioEncoding encoding = 1;

    // The sample rate in hertz (Hz) of the audio data sent in the
    // `SendAudioRequest` message.
    int32 sample_rate_hertz = 2;

    // The language of the supplied audio as a
    // [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
    // Example: "en-US".
    // Default is en-US.
    string language_code = 3;

    // The number of channels in the input audio data.
    int32 audio_channel_count = 4;

    // Which model to select for the given request.
    string model = 5;
}

/*
 * ReceiveAudioRequest is used to request audio data for specified stream_id.
 */
message ReceiveAudioRequest {
    // unique id to identify the client connection
    string stream_id = 1;
}

/*
 * StreamingSpeechResultsRequest is used to request various results from chat
 * controller.
 */
message StreamingSpeechResultsRequest {
    // unique id to identify the client connection
    string stream_id = 1;

    // uuid to identify concurrent client request
    string request_id = 2;
}

/*
 * PipelineRequest is used to create/free pipeline specified using stream_id
 */
message PipelineRequest {
    // A  unique id sent by the client to identify the client connection.
    // It is mapped to a unique pipeline on the Chat Controller server.
    string stream_id = 1;

    // user id
    string user_id = 2;
}

/*
 * UserParametersRequest is used to set user parameters
 */
message UserParametersRequest {
    // unique id to identify the client connection
    string stream_id = 1;

    // used id
    string user_id = 2;

    // bot name with version like {bot_name}_v{bot_version}, e.g chitchat_bot_v1.
    string bot_name = 3;
}

/*
 * GetStatusRequest used to get on demand Chat controller pipeline status
 */
message GetStatusRequest {
    // unique id to identify the client connection
    string stream_id = 1;
}

/*
 * SpeechRecognitionControlRequest is used for controlling input to
 * ASR internally muting ASR.
 * It is also used to disable DM-TTS flow for the incoming ASR input
 */
message SpeechRecognitionControlRequest {
    // unique id to identify the client connection
    string stream_id = 1;

    // Flag to mention whether asr transcripts to be passed to DM-TTS or get
    // only transcripts
    bool is_standalone = 2;
}

/*
 * Reload Speech Configs Request
 */
message ReloadSpeechConfigsRequest {
    // unique id to identify the client connection
    string stream_id = 1;
}

/*
 * UserContextRequest used to request user context
 */
message UserContextRequest {
    // unique id to identify the client connection
    string stream_id = 1;

    // user id
    string user_id = 2;
}

/*
 * UserContext data containing user specific information.
 */
message UserContext {
    // unique id to identify the client connection
    string stream_id = 1;

    // user id
    string user_id = 2;

    // bot name with version like {bot_name}_v{bot_version}, e.g chitchat_bot_v1.
    string bot_name = 3;

    // conversation history of user
    repeated ConversationHistory conversation_history = 4;

    // json formatted data of user context
    string context_json = 5;
}

message ConversationHistory {
    // bot name with version like {bot_name}_v{bot_version}, e.g chitchat_bot_v1.
    string bot_name = 1;

    repeated ConversationInstance conversation = 2;
}

message ConversationInstance {
    Role role = 1;
    string content = 2;
}

// Chat controller pipeline status response
message GetStatusResponse {
    // unique id to identify the client connection
    string stream_id = 1;

    PipelineStateResponse pipeline_state = 2;
}

// Chat controller Metadata streaming response
message StreamingSpeechResultsResponse {
    // unique id to identify the client connection
    string stream_id = 1;

    // message type as defined in `MessageType`
    MessageType message_type = 2;

    oneof metadata {

        ASRResult asr_result = 3;

        ChatEngineResponse chat_engine_response = 4;

        TTSResult tts_result = 5;

        PipelineStateResponse pipeline_state = 6;

        string display_text = 7;
    }
}

// ASR Result
message ASRResult {
    // Complete ASR Response in Riva Skills ASR result schema
    StreamingRecognitionResult results = 1;

    float latency_ms = 2;

    // start time in ISO8601 format, e.g. 2024-03-08T13:33:30.736Z
    string start_time = 3;

    // stop time in ISO8601 format
    string stop_time = 4;
}

// A streaming speech recognition result corresponding to a portion of the audio
// that is currently being processed.
message StreamingRecognitionResult {
  // May contain one or more recognition hypotheses (up to the
  // maximum specified in `max_alternatives`).
  // These alternatives are ordered in terms of accuracy, with the top (first)
  // alternative being the most probable, as ranked by the recognizer.
  repeated SpeechRecognitionAlternative alternatives = 1;

  // If `false`, this `StreamingRecognitionResult` represents an
  // interim result that may change. If `true`, this is the final time the
  // speech service will return this particular `StreamingRecognitionResult`,
  // the recognizer will not return any further hypotheses for this portion of
  // the transcript and corresponding audio.
  bool is_final = 2;

  // An estimate of the likelihood that the recognizer will not
  // change its guess about this interim result. Values range from 0.0
  // (completely unstable) to 1.0 (completely stable).
  // This field is only provided for interim results (`is_final=false`).
  // The default of 0.0 is a sentinel value indicating `stability` was not set.
  float stability = 3;

  // For multi-channel audio, this is the channel number corresponding to the
  // recognized result for the audio from that channel.
  // For audio_channel_count = N, its output values can range from '1' to 'N'.
  int32 channel_tag = 5;

  // Length of audio processed so far in seconds
  float audio_processed = 6;
}

// Alternative hypotheses (a.k.a. n-best list).
message SpeechRecognitionAlternative {
  // Transcript text representing the words that the user spoke.
  string transcript = 1;

  // The non-normalized confidence estimate. A higher number
  // indicates an estimated greater likelihood that the recognized words are
  // correct. This field is set only for a non-streaming
  // result or, of a streaming result where `is_final=true`.
  // This field is not guaranteed to be accurate and users should not rely on it
  // to be always provided.
  float confidence = 2;

  // A list of word-specific information for each recognized word. Only populated
  // if is_final=true
  repeated WordInfo words = 3;
}

// Word-specific information for recognized words.
message WordInfo {
  // Time offset relative to the beginning of the audio in ms
  // and corresponding to the start of the spoken word.
  // This field is only set if `enable_word_time_offsets=true` and only
  // in the top hypothesis.
  int32 start_time = 1;

  // Time offset relative to the beginning of the audio in ms
  // and corresponding to the end of the spoken word.
  // This field is only set if `enable_word_time_offsets=true` and only
  // in the top hypothesis.
  int32 end_time = 2;

  // The word corresponding to this set of information.
  string word = 3;

  // The non-normalized confidence estimate. A higher number indicates an
  // estimated greater likelihood that the recognized words are correct. This
  // field is not guaranteed to be accurate and users should not rely on it to
  // be always provided. The default of 0.0 is a sentinel value indicating
  // confidence was not set.
  float confidence = 4;
}

// Chat Engine Result json
message ChatEngineResponse {
    // unique id to identify the client connection
    string stream_id = 1;

    // chat engine result
    string result = 2;

    float latency_ms = 3;
}

// TTS result metadata
message TTSResult {
    // TTS latency in milliseconds
    float latency_ms = 1;
}

// Chat controller pipeline state response
message PipelineStateResponse {
    PipelineState state = 1;
}

// Receive Audio API Response
message ReceiveAudioResponse {
    // unique id to identify the client connection
    string stream_id = 1;

    // synthesized audio data
    bytes audio_content = 2;

    // The encoding of the audio data
    AudioEncoding encoding = 3;

    // The sample rate in hertz (Hz) of the audio data
    int32 sample_rate_hertz = 4;

    // The number of channels in the audio data. Only mono is supported
    int32 audio_channel_count = 5;

    // frame size of audio data
    int32 frame_size = 6;
}

// Generic API status response message
message APIStatusResponse {
    // unique id to identify the client connection
    string stream_id = 1;

    // response message
    string response_msg = 2;

    // API response status code as defined in `APIStatus`
    APIStatus status = 3;
}

/**********************************************************************************************************/

/**********************************************************************************************************/
/*** Enum definitions ***/

/*
 * Message type field for Chat controller metadata streaming
 */
enum MessageType {
    UNKNOWN_RESPONSE = 0;
    ASR_RESPONSE = 1;
    CHAT_ENGINE_RESPONSE = 2;
    TTS_RESPONSE = 3;
    PIPELINE_STATE_RESPONSE = 4;
    DISPLAY_TEXT = 5;
}

/*
 * Generic Chat controller API status
 */
enum APIStatus {
    UNKNOWN_STATUS = 0;
    SUCCESS = 1;
    PIPELINE_AVAILABLE = 2;
    PIPELINE_NOT_AVAILABLE = 3;
    BUSY = 4;
    ERROR = 5;
    INFO = 6;
}

/*
 * Chat controller Pipeline States
 */
enum PipelineState {
    INIT = 0;
    IDLE = 1;
    WAIT_FOR_TRIGGER = 2;
    ASR_ACTIVE = 3;
    DM_ACTIVE = 4;
    TTS_ACTIVE = 5;
}

/*
 * AudioEncoding specifies the encoding of the audio bytes in the encapsulating message.
 */
enum AudioEncoding {
    // Not specified.
    UNKNOWN = 0;
    // Uncompressed 16-bit signed little-endian samples (Linear PCM).
    LINEAR_PCM = 1;
    // `FLAC` (Free Lossless Audio
    // Codec) is the recommended encoding because it is
    // lossless--therefore recognition is not compromised--and
    // requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
    // encoding supports 16-bit and 24-bit samples, however, not all fields in
    // `STREAMINFO` are supported.
    FLAC = 2;

    // 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
    MULAW = 3;

    // 8-bit samples that compand 13-bit audio samples using G.711 PCMU/a-law.
    ALAW = 5;
}

/*
 * Used in storing conversation history for user and bot
 */
enum Role {
    UNDEFINED = 0;
    USER = 1;
    BOT = 2;
    SYSTEM = 3;
}
/**********************************************************************************************************/

/**********************************************************************************************************/
/*** Standalone APIs messages ***/

/*
 * Request message for standalone TTS synthesis of provided text transcript
 */
message SynthesizeSpeechRequest {
    // unique id to identify the client connection
    string stream_id = 1;

    // transcript text to be synthesized
    string transcript = 2;
}

/*
 * Request message for Chat API which will be sent to chat engine
 */
message ChatRequest {
    // unique id to identify the client connection
    string stream_id = 1;

    // bot name with version like {bot_name}_v{bot_version}, e.g chitchat_bot_v1.
    string bot_name = 2;

    // query
    string query = 3;

    // unique id for identifying the query
    string query_id = 4;

    // user id
    string user_id = 5;

    // The language of the supplied query string as a
  	// [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
  	// Example: "en-US".
    string source_language = 6;

    // The language of the response required from chat engine as a
  	// [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
  	// Example: "en-US".
    string target_language = 7;

    // Flag to send standalone text requests, when set true reponse is not sent
    // to TTS when set to false reponse will be sent to TTS
    bool is_standalone = 8;

    // key-value pair for user context to be sent to chat engine
    map<string, string> user_context = 9;

    // key-value pair for meta data to be sent to chat engine
    map<string, string> metadata = 10;
}

/*
 * Response message from chat engine for Chat API invocation
 */
message ChatResponse {
    // unique id to identify the client connection
    string stream_id = 1;

    // query
    string query = 2;

    // unique id for identifying the query
    string query_id = 3;

    // user id
    string user_id = 4;

    // session id if generated by chat engine
    string session_id = 5;

    // chat engine response for the query passed in `ChatRequest`
    string text = 6;

    // chat engine cleaned up response text after markdown language tags removal
    string cleaned_text = 7;

    // flag to indicate whether this is final response or intermediate response, when true
    // there will be no more responses for the requested `ChatRequest`
    bool is_final = 8;

    // chat engine response in json format
    string json_response = 9;
}

/*
 * Request message for Event API which will be sent to chat engine
 */
message EventRequest {
    // unique id to identify the client connection
    string stream_id = 1;

    // bot name with version like {bot_name}_v{bot_version}, e.g chitchat_bot_v1.
    string bot_name = 2;

    // event type
    string event_type = 3;

    // unique event id
    string event_id = 4;

    // user id
    string user_id = 5;

    // key-value pair for user context to be sent to chat engine
    map<string, string> user_context = 6;

    // key-value pair for meta data to be sent to chat engine
    map<string, string> metadata = 7;
}

message EventResponse {
    // unique id to identify the client connection
    string stream_id = 1;

    // event type
    string event_type = 2;

    // unique event id
    string event_id = 3;

    // user id
    string user_id = 4;

    // text response
    string text = 5;
    string cleaned_text = 6;
    bool is_final = 7;
    string json_response = 8;
    repeated string events = 9;
}

/**********************************************************************************************************/
````

## File: microservices/ace_agent/4.1/webui/server/grpc/buf.gen.yaml
````yaml
# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

version: v1
managed:
  enabled: true
plugins:
  - plugin: buf.build/connectrpc/es:v1.4.0
    out: gen
  # dependencies
  - plugin: buf.build/bufbuild/es
    out: gen
````

## File: microservices/ace_agent/4.1/webui/server/umim/umim.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { randomUUID } from "crypto";

export enum ActionModality {
  BOT_SPEECH = "bot_speech",
  BOT_POSTURE = "bot_posture",
  BOT_GESTURE = "bot_gesture",
  USER_SPEECH = "user_speech",
  BOT_FACE = "bot_face",
  BOT_UPPER_BODY = "bot_upper_body",
  BOT_LOWER_BODY = "bot_lower_body",
  USER_FACE = "user_face",
  USER_UPPER_BODY = "user_upper_body",
  USER_LOWER_BODY = "user_lower_body",
  USER_ENGAGEMENT = "user_engagement",
  SOUND = "sound",
  ENVIRONMENT = "environment",
  CAMERA = "camera",
  INFORMATION = "information",
  VISUAL_EFFECT = "visual_effect",
  USER_PRESENCE = "user_presence",
  BOT_ACTIVE_WAITING = "bot_active_waiting",
  BOT_EXPECTATION = "bot_expectation",
  CUSTOM = "custom",
  TIME = "time",
  WEB_REQUEST = "web_request",
}

export enum ActionModalityPolicy {
  PARALLEL = "parallel",
  OVERRIDE = "override",
  REPLACE = "replace",
  SKIP = "skip",
}

export abstract class UMIM_Event {
  readonly event_created_at: string = now();
  readonly uid: string = randomUUID();
  abstract readonly type: string;

  constructor(
    readonly source_uid: string,
    readonly tags: { [id: string]: unknown } = ({} = {})
  ) {}

  toJSONString(): string {
    try {
      return JSON.stringify(this);
    } catch (e) {
      console.error(
        "Failed to convert UMIM Event to JSON. Using empty string instead",
        e
      );
      return "";
    }
  }
}

export class UMIM_PipelineAcquired extends UMIM_Event {
  readonly type = "PipelineAcquired";
  constructor(
    readonly source_uid: string,
    readonly stream_uid: string,
    readonly user_uid?: string | null | undefined,
    readonly tags: { [id: string]: unknown } = {},
    readonly session_uid?: string | null | undefined
  ) {
    super(source_uid, tags);
  }
}

export class UMIM_PipelineReleased extends UMIM_Event {
  readonly type = "PipelineReleased";
  constructor(
    readonly source_uid: string,
    readonly stream_uid: string,
    readonly user_uid?: string | null | undefined,
    readonly tags: { [id: string]: unknown } = {},
    readonly session_uid?: string | null | undefined
  ) {
    super(source_uid, tags);
  }
}

export abstract class UMIM_ActionEvent extends UMIM_Event {
  abstract readonly action_info_modality: ActionModality;
  abstract readonly action_info_modality_policy: ActionModalityPolicy;

  constructor(
    readonly source_uid: string,
    readonly tags: { [id: string]: unknown } = {},
    readonly action_uid: string
  ) {
    super(source_uid, tags);
  }
}

export abstract class UMIM_ActionFinished extends UMIM_ActionEvent {
  constructor(
    readonly source_uid: string,
    readonly tags: { [id: string]: unknown } = {},
    readonly action_uid: string = crypto.randomUUID(),
    readonly action_finished_at: string = now(),
    readonly is_success: boolean = true,
    readonly failure_reason?: string | null | undefined,
    readonly was_stopped?: boolean | null | undefined
  ) {
    super(source_uid, tags, action_uid);
  }
}

export abstract class UMIM_UserActionFinished extends UMIM_ActionFinished {
  constructor(
    readonly source_uid: string,
    readonly tags: { [id: string]: unknown } = {},
    readonly action_uid: string = crypto.randomUUID(),
    readonly action_finished_at: string = now(),
    readonly is_success: boolean = true,
    readonly failure_reason?: string | null | undefined,
    readonly was_stopped?: boolean | null | undefined,
    readonly user_uid?: string | null | undefined
  ) {
    super(
      source_uid,
      tags,
      action_uid,
      action_finished_at,
      is_success,
      failure_reason,
      was_stopped
    );
  }
}

export class UMIM_UtteranceUserActionFinished extends UMIM_UserActionFinished {
  readonly type = "UtteranceUserActionFinished";
  readonly action_info_modality = ActionModality.USER_SPEECH;
  readonly action_info_modality_policy = ActionModalityPolicy.REPLACE;

  constructor(
    readonly source_uid: string,
    readonly final_transcript: string,
    readonly action_uid: string = crypto.randomUUID(),
    readonly tags: { [id: string]: unknown } = {},
    readonly action_finished_at: string = now(),
    readonly is_success: boolean = true,
    readonly failure_reason?: string | null | undefined,
    readonly was_stopped?: boolean | null | undefined,
    readonly user_uid?: string | null | undefined
  ) {
    super(
      source_uid,
      tags,
      action_uid,
      action_finished_at,
      is_success,
      failure_reason,
      was_stopped,
      user_uid
    );
  }
}

abstract class UMIM_ActionStarted extends UMIM_ActionEvent {
  constructor(
    readonly source_uid: string,
    readonly tags: { [id: string]: unknown } = {},
    readonly action_uid: string = crypto.randomUUID(),
    readonly action_started_at: string = now()
  ) {
    super(source_uid, tags, action_uid);
  }
}

abstract class UMIM_UserActionStarted extends UMIM_ActionStarted {
  constructor(
    readonly source_uid: string,
    readonly tags: { [id: string]: unknown } = {},
    readonly action_uid: string = crypto.randomUUID(),
    readonly action_started_at: string = now(),
    readonly user_uid?: string | null | undefined
  ) {
    super(source_uid, tags, action_uid, action_started_at);
  }
}

export class UMIM_UtteranceUserActionStarted extends UMIM_UserActionStarted {
  readonly type = "UtteranceUserActionStarted";
  readonly action_info_modality = ActionModality.USER_SPEECH;
  readonly action_info_modality_policy = ActionModalityPolicy.REPLACE;

  constructor(
    readonly source_uid: string,
    readonly action_uid: string = crypto.randomUUID(),
    readonly tags: { [id: string]: unknown } = {},
    readonly action_started_at: string = now(),
    readonly user_uid?: string | null | undefined
  ) {
    super(source_uid, tags, action_uid, action_started_at, user_uid);
  }
}

export abstract class UMIM_BotActionStarted extends UMIM_ActionStarted {
  constructor(
    readonly source_uid: string,
    readonly tags: { [id: string]: unknown } = {},
    readonly action_uid: string = crypto.randomUUID(),
    readonly action_started_at: string = now(),
    readonly bot_id?: string | undefined | null
  ) {
    super(source_uid, tags, action_uid, action_started_at);
  }
}

export abstract class UMIM_BotActionFinished extends UMIM_ActionFinished {
  constructor(
    readonly source_uid: string,
    readonly tags: { [id: string]: unknown } = {},
    readonly action_uid: string = crypto.randomUUID(),
    readonly action_finished_at: string = now(),
    readonly is_success: boolean = true,
    readonly failure_reason?: string | null | undefined,
    readonly was_stopped?: boolean | null | undefined,
    readonly bot_id?: string | null | undefined
  ) {
    super(
      source_uid,
      tags,
      action_uid,
      action_finished_at,
      is_success,
      failure_reason,
      was_stopped
    );
  }
}

export class UMIM_UtteranceBotActionFinished extends UMIM_BotActionFinished {
  readonly type = "UtteranceBotActionFinished";
  readonly action_info_modality = ActionModality.BOT_SPEECH;
  readonly action_info_modality_policy = ActionModalityPolicy.REPLACE;

  constructor(
    readonly source_uid: string,
    readonly final_script: string,
    readonly action_uid: string = crypto.randomUUID(),
    readonly tags: { [id: string]: unknown } = {},
    readonly action_finished_at: string = now(),
    readonly is_success: boolean = true,
    readonly failure_reason?: string | null | undefined,
    readonly was_stopped?: boolean | null | undefined,
    readonly bot_id?: string | null | undefined
  ) {
    super(
      source_uid,
      tags,
      action_uid,
      action_finished_at,
      is_success,
      failure_reason,
      was_stopped,
      bot_id
    );
  }
}

export class UMIM_UtteranceBotActionStarted extends UMIM_BotActionStarted {
  readonly type = "UtteranceBotActionStarted";
  readonly action_info_modality = ActionModality.BOT_SPEECH;
  readonly action_info_modality_policy = ActionModalityPolicy.REPLACE;
  constructor(
    readonly source_uid: string,
    readonly action_uid: string = crypto.randomUUID(),
    readonly tags: { [id: string]: unknown } = {},
    readonly action_started_at: string = now(),
    readonly bot_id?: string | undefined | null
  ) {
    super(source_uid, tags, action_uid, action_started_at, bot_id);
  }
}

export abstract class UMIM_ActionUpdated extends UMIM_ActionEvent {
  constructor(
    readonly source_uid: string,
    readonly tags: { [id: string]: unknown } = {},
    readonly action_uid: string = crypto.randomUUID(),
    readonly action_updated_at: string = now()
  ) {
    super(source_uid, tags, action_uid);
  }
}

export abstract class UMIM_UserActionExtended extends UMIM_ActionUpdated {
  constructor(
    readonly source_uid: string,
    readonly tags: { [id: string]: unknown } = {},
    readonly action_uid: string = crypto.randomUUID(),
    readonly action_updated_at: string = now(),
    readonly user_id?: string | null | undefined
  ) {
    super(source_uid, tags, action_uid, action_updated_at);
  }
}

export class UMIM_UtteranceUserActionTranscriptUpdated extends UMIM_UserActionExtended {
  readonly type = "UtteranceUserActionTranscriptUpdated";
  readonly action_info_modality = ActionModality.USER_SPEECH;
  readonly action_info_modality_policy = ActionModalityPolicy.REPLACE;

  constructor(
    readonly source_uid: string,
    readonly action_uid: string = crypto.randomUUID(),
    readonly interim_transcript: string,
    readonly stability: number = 0.1,
    readonly tags: { [id: string]: unknown } = {},
    readonly action_updated_at: string = now(),
    readonly user_id?: string | null | undefined
  ) {
    super(source_uid, tags, action_uid, action_updated_at, user_id);
  }
}

export class UMIM_TimerBotActionStarted extends UMIM_BotActionStarted {
  readonly type = "TimerBotActionStarted";
  readonly action_info_modality = ActionModality.TIME;
  readonly action_info_modality_policy = ActionModalityPolicy.PARALLEL;

  constructor(
    readonly source_uid: string,
    readonly action_uid: string = crypto.randomUUID(),
    readonly tags: { [id: string]: unknown } = {},
    readonly action_started_at: string = now(),
    readonly bot_id?: string | undefined | null
  ) {
    super(source_uid, tags, action_uid, action_started_at, bot_id);
  }
}

export class UMIM_TimerBotActionFinished extends UMIM_BotActionFinished {
  readonly type = "TimerBotActionFinished";
  readonly action_info_modality = ActionModality.TIME;
  readonly action_info_modality_policy = ActionModalityPolicy.PARALLEL;

  constructor(
    readonly source_uid: string,
    readonly action_uid: string = crypto.randomUUID(),
    readonly tags: { [id: string]: unknown } = {},
    readonly action_finished_at: string = now(),
    readonly is_success: boolean = true,
    readonly failure_reason?: string | null | undefined,
    readonly was_stopped?: boolean | null | undefined,
    readonly bot_id?: string | null | undefined
  ) {
    super(
      source_uid,
      tags,
      action_uid,
      action_finished_at,
      is_success,
      failure_reason,
      was_stopped,
      bot_id
    );
  }
}

export class UMIM_PostureBotActionStarted extends UMIM_BotActionStarted {
  readonly type = "PostureBotActionStarted";
  readonly action_info_modality = ActionModality.BOT_POSTURE;
  readonly action_info_modality_policy = ActionModalityPolicy.OVERRIDE;

  constructor(
    readonly source_uid: string,
    readonly action_uid: string = crypto.randomUUID(),
    readonly tags: { [id: string]: unknown } = {},
    readonly action_started_at: string = now(),
    readonly bot_id?: string | undefined | null
  ) {
    super(source_uid, tags, action_uid, action_started_at, bot_id);
  }
}

export class UMIM_PostureBotActionFinished extends UMIM_BotActionFinished {
  readonly type = "PostureBotActionFinished";
  readonly action_info_modality = ActionModality.BOT_POSTURE;
  readonly action_info_modality_policy = ActionModalityPolicy.OVERRIDE;

  constructor(
    readonly source_uid: string,
    readonly action_uid: string = crypto.randomUUID(),
    readonly tags: { [id: string]: unknown } = {},
    readonly action_finished_at: string = now(),
    readonly is_success: boolean = true,
    readonly failure_reason?: string | null | undefined,
    readonly was_stopped?: boolean | null | undefined,
    readonly bot_id?: string | null | undefined
  ) {
    super(
      source_uid,
      tags,
      action_uid,
      action_finished_at,
      is_success,
      failure_reason,
      was_stopped,
      bot_id
    );
  }
}

export class UMIM_GestureBotActionStarted extends UMIM_BotActionStarted {
  readonly type = "GestureBotActionStarted";
  readonly action_info_modality = ActionModality.BOT_GESTURE;
  readonly action_info_modality_policy = ActionModalityPolicy.OVERRIDE;

  constructor(
    readonly source_uid: string,
    readonly action_uid: string = crypto.randomUUID(),
    readonly tags: { [id: string]: unknown } = {},
    readonly action_started_at: string = now(),
    readonly bot_id?: string | undefined | null
  ) {
    super(source_uid, tags, action_uid, action_started_at, bot_id);
  }
}

export class UMIM_GestureBotActionFinished extends UMIM_BotActionFinished {
  readonly type = "GestureBotActionFinished";
  readonly action_info_modality = ActionModality.BOT_GESTURE;
  readonly action_info_modality_policy = ActionModalityPolicy.OVERRIDE;

  constructor(
    readonly source_uid: string,
    readonly action_uid: string = crypto.randomUUID(),
    readonly tags: { [id: string]: unknown } = {},
    readonly action_finished_at: string = now(),
    readonly is_success: boolean = true,
    readonly failure_reason?: string | null | undefined,
    readonly was_stopped?: boolean | null | undefined,
    readonly bot_id?: string | null | undefined
  ) {
    super(
      source_uid,
      tags,
      action_uid,
      action_finished_at,
      is_success,
      failure_reason,
      was_stopped,
      bot_id
    );
  }
}

function now(): string {
  return new Date().toISOString();
}
````

## File: microservices/ace_agent/4.1/webui/server/utils/sleep.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

export default function sleep(ms: number): Promise<void> {
  return new Promise((resolve) => setTimeout(resolve, ms));
}
````

## File: microservices/ace_agent/4.1/webui/server/utils/waitAbortSignal.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Returns a promise that rejects as soon as the provided abortController is fired. This
 * is useful to stop the task from running after the tests are completed
 */
export default function waitAbortSignal(
  abortController: AbortController
): Promise<void> {
  return new Promise((_, reject) => {
    abortController.signal.addEventListener("abort", () => {
      const error: any = new Error("aborted");
      error.code = 1;
      reject(error);
    });
  });
}
````

## File: microservices/ace_agent/4.1/webui/server/.yarnrc.yml
````yaml
nodeLinker: node-modules
````

## File: microservices/ace_agent/4.1/webui/server/config.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

export const REDIS_URL = process.env.REDIS_URL ?? "redis://localhost:6379";
export const SERVER_PORT = parseInt(process.env.VITE_SERVER_PORT) || 7007;
export const EMOJI_FILE_PATH =
  process.env.EMOJI_FILE_PATH ?? "./data/emojis-all.json";
export const UMIM_SOURCE_NAME =
  process.env.UMIM_SOURCE_NAME ?? "ace-agent-bot-ui";
export const SYSTEM_EVENTS_STREAM =
  process.env.SYSTEM_EVENTS_STREAM ?? "ace_agent_system_events";
export const GRPC_URL = process.env.GRPC_URL ?? "http://localhost:50055";
export const HTTP_CHAT_URL =
  process.env.HTTP_CHAT_URL ?? "http://localhost:9000";

export const USER_SPEECH_SAMPLE_RATE =
  parseInt(process.env.USER_SPEECH_SAMPLE_RATE) || 16000;
````

## File: microservices/ace_agent/4.1/webui/server/index.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { WebSocketServer } from "ws";
import * as https from "node:https";
import * as fs from "fs";

import EmojiFinder from "./emoji-finder/index.js";
import ChatSessionTask from "./chat-session/tasks/ChatSessionTask.js";

import { EMOJI_FILE_PATH, SERVER_PORT } from "./config.js";
import getLogger from "./chat-session/logger.js";

const MAX_CLI_ARGS = 10;

const logger = getLogger("main");

async function initEmojiFinder(): Promise<EmojiFinder> {
  if (!EMOJI_FILE_PATH) {
    logger.info("No EMOJI_FILE_PATH found. Emojis will be disabled");
    return null;
  }
  logger.info("Loading emojis from file %s", EMOJI_FILE_PATH);
  const emojiFinder = EmojiFinder.fromEmojiFile(EMOJI_FILE_PATH);
  logger.info(
    "Initializing emoji finder... (can take several minutes on initial run)"
  );
  await emojiFinder.init();
  logger.info("Emoji finder ready");
  return emojiFinder;
}

function getCLIArgument(
  name: string,
  optional: boolean = true
): string | boolean {
  // Get command-line arguments, excluding the first two (node and script path)
  const args = process.argv.slice(2);

  if (args.length > MAX_CLI_ARGS) {
    // Ensure the user doesn't pass too many arguments. This is to address a vulnerability
    // flagged in checkmarkx
    throw new Error(
      `Too many arguments. Expected max ${MAX_CLI_ARGS} but got ${args.length}`
    );
  }

  for (let i = 0; i < MAX_CLI_ARGS; i++) {
    if (!args[i]) {
      break;
    }
    if (args[i] === `--${name}`) {
      if (args[i + 1]) {
        return args[i + 1]; // Return the next argument as the value
      } else {
        return true;
      }
    }
  }

  if (!optional) {
    throw new Error(
      `Expected argument --${name}. Run the command with --help for detailed usage`
    );
  }
}

function printHelp(): void {
  process.stdout.write("Runs the websocket server for the bot web UI. The\n");
  process.stdout.write("browser client communicates with the client through\n");
  process.stdout.write("websockets. The webserver interacts with ACE Agent\n");
  process.stdout.write("through its redis, gRPC or http APIs.\n\n");
  process.stdout.write("Arguments:\n");
  process.stdout.write("--ace-agent-text-chat-interface: Required. which\n");
  process.stdout.write("  interface of ACE Agent to use. Must be one of\n");
  process.stdout.write('  event (aka "redis", "umim"), grpc or server\n');
  process.stdout.write('  (aka "http"). The endpoints can be configured\n');
  process.stdout.write("  using environment variables: REDIS_URL, GRPC_URL\n");
  process.stdout.write("  and HTTP_CHAT_URL.\n");

  process.stdout.write("--speech: Optional. Whether the UI should allow\n");
  process.stdout.write("  speech conversations. Requires ACE Agent to run\n");
  process.stdout.write("  in speech mode. The endpoint defined in the\n");
  process.stdout.write("  GRPC_URL environment variable will be used to\n");
  process.stdout.write("  stream audio.\n\n");
}

function createSecureWebsocketServer(
  cert_path: string,
  key_path: string
): WebSocketServer {
  const server = https.createServer({
    cert: fs.readFileSync(cert_path),
    key: fs.readFileSync(key_path),
  }).listen(SERVER_PORT);

  return new WebSocketServer({ server });
}

function createInsecureWebsocketServer(): WebSocketServer {
  return new WebSocketServer({ port: SERVER_PORT });
}

function createWebsocketServer(): WebSocketServer {
  if (process.env.SSL_CERT_PATH || process.env.SSL_KEY_PATH) {
    if (!process.env.SSL_CERT_PATH || !process.env.SSL_KEY_PATH) {
      logger.warn(
        "Missing environment variable SSL_CERT_PATH or SSL_KEY_PATH. Creating insecure websocket server (ws://)"
      );
      return createInsecureWebsocketServer();
    }
    logger.info(
      "SSL_CERT_PATH and SSL_KEY_PATH environment variables were found. Creating a secure websocket server (wss://)"
    );
    return createSecureWebsocketServer(
      process.env.SSL_CERT_PATH,
      process.env.SSL_KEY_PATH
    );
  }

  logger.info('Did not find environment variables SSL_CERT_PATH or SSL_KEY_PATH. Creating insecure websocket server (ws://)')
  return createInsecureWebsocketServer();
}

async function run(): Promise<void> {
  const help = getCLIArgument("help");
  if (help) {
    printHelp();
    return;
  }
  const aceAgentTextChatInterface = getCLIArgument(
    "ace-agent-text-chat-interface",
    false
  );
  if (
    aceAgentTextChatInterface !== "server" &&
    aceAgentTextChatInterface !== "grpc" &&
    aceAgentTextChatInterface !== "event"
  ) {
    throw new Error(
      `--ace-agent-text-chat-interface argument must be one of server, grpc or event. Got ${aceAgentTextChatInterface} instead`
    );
  }

  const isSpeechEnabled = !!getCLIArgument("speech");
  const wss = createWebsocketServer();

  // Emoji finder is only used in event mode. Do not load emojis in other modes
  const emojiFinder =
    aceAgentTextChatInterface === "event" ? await initEmojiFinder() : null;

  logger.info(`Web server up and running on port ${SERVER_PORT}!`);

  wss.on("connection", async function connection(ws) {
    ws.binaryType = "arraybuffer";

    const session = new ChatSessionTask(
      ws,
      emojiFinder,
      aceAgentTextChatInterface as "server" | "grpc" | "event",
      isSpeechEnabled
    );
    logger.info(
      "New user connected! Created session with stream_id=%s",
      session.getStreamID()
    );
    session.start();
  });
}

run();
````

## File: microservices/ace_agent/4.1/webui/server/package.json
````json
{
  "name": "ace-agent-bot-ui-server",
  "version": "0.0.0",
  "main": "index.js",
  "type": "module",
  "author": "jnolan@nvidia.com",
  "devDependencies": {
    "@bufbuild/buf": "^1.30.1",
    "@types/bunyan": "^1",
    "@types/node": "^20.11.23",
    "@types/redis-mock": "^0",
    "@types/ws": "^8.5.10",
    "openapi-typescript-codegen": "^0.27.0",
    "redis-mock": "^0.56.3",
    "ts-node-dev": "^2.0.0",
    "tsx": "^4.7.1",
    "typescript": "^5.3.3"
  },
  "dependencies": {
    "@bufbuild/protobuf": "^1.8.0",
    "@connectrpc/connect": "^1.4.0",
    "@connectrpc/connect-node": "^1.4.0",
    "@tensorflow-models/universal-sentence-encoder": "^1.3.3",
    "@tensorflow/tfjs": "^4.17.0",
    "@tensorflow/tfjs-node": "^4.17.0",
    "bunyan": "^1.8.15",
    "redis": "^4.6.13",
    "ts-node": "^10.9.2",
    "ws": "^8.16.0"
  },
  "scripts": {
    "dev:server-mode": "tsx --watch index.ts --ace-agent-text-chat-interface server | npx bunyan -o short",
    "dev:event-mode": "tsx --watch index.ts --ace-agent-text-chat-interface event | npx bunyan -o short",
    "dev:speech-mode": "tsx --watch index.ts --ace-agent-text-chat-interface grpc --speech | npx bunyan -o short",
    "dev:event-speech-mode": "tsx --watch index.ts --ace-agent-text-chat-interface event --speech | npx bunyan -o short",
    "test": "NODE_ENV=test tsx --test **/*.test.ts",
    "gen-grpc": "cd grpc && buf generate",
    "build": "tsc"
  },
  "packageManager": "yarn@4.1.1"
}
````

## File: microservices/ace_agent/4.1/webui/server/tsconfig.json
````json
{
  "compilerOptions": {
    "resolveJsonModule": true,
    "moduleResolution": "Node",
    "lib": ["ES2023"],
    "module": "ES2022",
    "target": "ES2022",
    "skipLibCheck": true
  },
  "exclude": ["**/*.test.ts"]
}
````

## File: microservices/ace_agent/4.1/webui/shared/package.json
````json
{
  "type": "module"
}
````

## File: microservices/ace_agent/4.1/webui/shared/types.ts
````typescript
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

export type MessageID = string;

export enum ChatMessageContentType {
  TEXT = "TEXT",
  EMOJI = "EMOJI",
  TYPING = "TYPING",
  ASR = "ASR",
  TOGGLE_SPEECH = "TOGGLE_SPEECH",
  BOT_LIST = "BOT_LIST",
  USER_BARGE_IN = "USER_BARGE_IN",
}

export enum AuthorType {
  BOT = "BOT",
  USER = "USER",
  SYSTEM = "SYSTEM",
}

export interface ChatMessageTextContent {
  type: ChatMessageContentType.TEXT;
  messageID: MessageID;
  text: string;
  botName: string | null;
}

export interface ChatMessageEmojiContent {
  type: ChatMessageContentType.EMOJI;
  messageID: MessageID;
  emoji: string;
  title: string;
  botName: string | null;
}

interface ChatMessageTypingContent {
  type: ChatMessageContentType.TYPING;
  messageID: MessageID;
  text: string | null;
  isNewMessage: boolean;
}

interface ChatMessageUserBargeInContent {
  type: ChatMessageContentType.USER_BARGE_IN;
}

interface ASRContent {
  type: ChatMessageContentType.ASR;
  transcript: string;
  messageID: string;
}

interface BotList {
  type: ChatMessageContentType.BOT_LIST;
  botList: string[];
}

interface ToggleSpeechContent {
  type: ChatMessageContentType.TOGGLE_SPEECH;
  interactionMode: InteractionMode;
}

export interface BotChatMessage {
  author: AuthorType.BOT;
  content:
    | ChatMessageTextContent
    | ChatMessageEmojiContent
    | ChatMessageTypingContent
    | ChatMessageUserBargeInContent
    | ASRContent
    | BotList;
}

export enum SystemMessageContent {
  SHUTDOWN = "SHUTDOWN",
  CONFIG_CHANGE = "CONFIG_CHANGE",
}

export interface ServerConfig {
  type: SystemMessageContent.CONFIG_CHANGE;
  speechSupported: boolean;
}

export interface SystemShutdown {
  type: SystemMessageContent.SHUTDOWN;
  reason: string;
}

export interface SystemConfigMessage {
  author: AuthorType.SYSTEM;
  content: ServerConfig | SystemShutdown;
}

export interface SystemShutdownMessage {
  author: AuthorType.SYSTEM;
  content: SystemShutdown;
}

export type BotChatTextMessage = BotChatMessage & {
  content: ChatMessageTextContent;
};

export type BotChatEmojiMessage = BotChatMessage & {
  content: ChatMessageEmojiContent;
};

export type UserChatTextMessage = UserChatMessage & {
  content: ChatMessageTextContent;
};

export type UserChatToggleSpeechMessage = UserChatMessage & {
  content: ToggleSpeechContent;
};

export interface UserChatMessage {
  author: AuthorType.USER;
  content:
    | ChatMessageTextContent
    | ChatMessageTypingContent
    | ToggleSpeechContent;
}

export enum InteractionMode {
  TEXT = "Text",
  SPEECH = "Speech",
}
````

## File: microservices/ace_agent/4.1/webui/docker-compose.yml
````yaml
# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This docker compose file starts the bot web UI's client and server for local
# development, enabling hot-reloading and debugging features. It assumes ACE agent is
# running in "server", "gRPC" or "event" mode in the same network.
#
# This is not meant for production use-cases.
#
# Usage:
# - docker compose up server-mode
#   Runs the bot web UI server and client in text-only mode (no speech). The Bot Web UI
#   communicates with ACE Agent using its HTTP (aka "server") interface.
#
# - docker compose up event-mode
#   Runs the bot web UI server and client in text-only mode (no speech). The Bot Web UI
#   communicates with ACE Agent using its Redis (aka "event" or "umim") interface.
#
# - docker compose up speech-mode
#   Runs the bot web UI server and client in speech mode. The Bot Web UI communicates with
#   ACE Agent using its gRPC interface.
#
# - docker compose up event-speech-mode
#   Runs the bot web UI server and client in speech mode. The Bot Web UI communicates with
#   ACE Agent using its Redis (aka "event" or "umim") interface.
#
# Then: If you are running this on your workstation, allow port-forwarding on ports 7006
# and 7007 and reload chrome. Then, browse to http://localhost:7006 to view the UI.

services:
  x-base: &base
    image: node:21.2.0-slim
    network_mode: host
    volumes:
      - type: bind
        source: ./
        target: /app
    tty: true
    working_dir: /app

  bot-web-ui-dev-client:
    <<: *base
    container_name: bot-web-ui-dev-client
    command: sh -c "cd client && yarn install && yarn run dev"

  x-server-base: &server-base
    <<: *base
    depends_on:
      - bot-web-ui-dev-client

  server-mode:
    <<: *server-base
    container_name: bot-web-ui-dev-server-server
    command: sh -c "cd server && yarn install && yarn run dev:server-mode"

  event-mode:
    <<: *server-base
    container_name: bot-web-ui-dev-server-event
    command: sh -c "cd server && yarn install && yarn run dev:event-mode"

  speech-mode:
    <<: *server-base
    container_name: bot-web-ui-dev-server-speech
    command: sh -c "cd server && yarn install && yarn run dev:speech-mode"

  event-speech-mode:
    <<: *server-base
    container_name: bot-web-ui-dev-server-event-speech
    command: sh -c "cd server && yarn install && yarn run dev:event-speech-mode"
````

## File: microservices/ace_agent/4.1/webui/README.md
````markdown
<!--
/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->
# ACE Agent Bot Web UI

This is a web UI to interact with ACE Agent bots through text and speech.

## Features overview

ACE Agent UI is a web application to converse with ACE Agent bots using text or speech. This section describes the key features of the web application.

### Text conversations

The bot UI supports text-based conversations.

### Speech conversations

The Bot Web UI supports conversing with ACE Agent bots in `speech` mode. In this mode, the user can speak through their microphone, and hear back audio from the bot. The user can view their own audio transcript (ASR) in real time, as well as the bots speech transcript.

![Screenshot of a speech interaction in the bot web UI](./images/screenshot_speech.png)

### Emojis

In addition to speech and text, some bots can express themselves through gestures and emotions. To represent these non-verbal signals, the bot web UI translates gestures to emojis.

| ![Screenshot of emojis in speech mode](images/screenshot_emoji_speech.png) | ![Screenshot of emojis in text mode](images/screenshot_emoji_text.png) |
|---------------------------------------------------------------------|--------------------------------------------------------------|
| Emojis in speech mode                                                                   | Emojis in text mode                      |

## Run

If you follow the ACE Agent documentation to run or write your own bots, a bot web UI is automatically started and is available at `http://<YOUR_IP>:7006`. By default, the UI doesn't run with SSL, which blocks speech features (microphone). The workaround is to enable the "Insecure origins treated as secure" flag in `chrome://flags` or `edge://flags`, and add `http://<YOUR_IP>:7006`.

You can also run the UI separately. This is useful if you want to work on the UI code. Instructions on how to run the UI are available in the [docker-compose.yml](./docker-compose.yml) file.

## Developer guide

This section is for developers who wish to understand how the app is built and work on its code.

### Architecture

The web app is composed of two main components: a web client and a server. When the web client is initialized, it establishes a websocket connection with the server. The server interacts with ACE Agent through its gRPC, HTTP or event (Redis) APIs.

![High-level diagram of the architecture of the UI](./images/architecture_high_level.png)

#### Client

The web client is a React web application. It is built statically using ViteJS, and runs entirely in the browser. Its main responsibilities are described below.

##### Managing communications with the server

All interactions between the client and the server go through the same websocket. Whenever a message is received through the websocket (for example: the bot sends a text message), the client updates the applications state. Similarly, when the user does something (e.g. send a new message), this information is sent to the server through the websocket.

For more details, see:

1. The `useServerState()` custom React hook, which manages the websockets state, the messages going through it, and provides APIs for the client to interact with the server.
2. The `UserChatMessage` and BotChatMessage` interface definitions, which describe the payloads sent and received from the server.

##### Managing interaction modes (speech vs. text)

By default, the app is set in `text` mode, which allows the user to have written conversations with the bot. If the bot supports speech conversations, the UI shows the option to toggle between `text` and `speech` mode.

When the user toggles `speech` mode, the UI shows a simplified conversation history (last two messages), along with a button to start/stop the users microphone. It also shows a pulse animation when the bot or the user speaks.

Finally, it shows the real-time text transcription of what the user is speaking.

##### Managing multiple bots

In some cases, ACE Agent may run multiple bots at the same time. Currently, this is only supported when using the HTTP interface, which only supports the `text` interaction mode. When this happens, the UI shows a toggle allowing the user to select which bot to interact with. When the user toggles between bots, the conversation history is updated to show the conversation history with the current bot.

###### Recording and sending the users audio

![A diagram outlying the flow of audio data](./images/architecture_audio.png)

When `speech` mode is selected by the user, the UI requests access to the users microphone. When access is granted, it starts recording chunks of audio that are immediately sent to the server through the websocket connection. The user can choose to disable their microphone.

For more details, see:

1. The `useMicrophone()` custom React hook, which handles microphone access and recording APIs.
2. The `LinearPCMProcessor` class, an audio processing worklet that converts the users raw audio data to Linear PCM format, which is the format expected by the server.
3. The `<UserSpeechInput />` component, which displays the UI aspects of audio recording (button, pulse animation, loading states).

##### Playing the bots speech responses

![A diagram of the bot audio flow](./images/architecture_bot_audio.png)

In `speech` mode, the UI receives audio chunks from the bot. The chunks are immediately played in the browser as they come.

For more details, see the `useAudioPlayer()` custom React hook, which handles the audio buffer and converts the received audio into a browser-compatible one.

##### Render the conversation history

In `text` mode, this is the list of text messages typed and sent by the user, as well as the bots text responses. In `speech` mode, this is the list of audio transcripts (ASR) from the user.

For more details, see the `<ConversationHistory />` component.

#### Server

In essence, the server is a proxy between the web client and ACE agent. It manages the websocket connection with the client, forwards user actions (e.g. send a message) to ACE Agent, and forwards ACE agent actions (e.g. the bot sends a message) to the web client.

The main responsibility of the server is to manage API calls to ACE Agent.

ACE Agent supports multiple communication protocols (interfaces). These protocols do not all support the same features, and some protocols may be used simultaneously. For example, when running in `event` mode, ACE Agent sends and receives messages through a Redis stream, but also exposes a gRPC API to stream audio.

Furthermore, some ACE Agent APIs are only needed in certain interaction modes (`speech` or `text`). For example, there is no need to use ACE Agents `receiveAudio` gRPC API when the user doesnt use the `speech` interaction mode.

For these reasons, the server defines the concept of *tasks*, which allow the server to handle multiple protocols in parallel, and connect to their APIs only when needed.

##### Tasks

![A diagram explaining how tasks extend a base AbstractTask and how they interact with the client](images/tasks.png)

A *task* implements a specific aspect of the application, and manages the necessary API calls. A task defines a set of `interactionModes` (`text`, `speech`, or both) for which it must run, as well as the ACE Agent interface its designed to interact with. It implements `start()` and `stop()` methods.

When a client connects to the server, a new `ChatSession` object is created. The `ChatSession` checks which ACE Agent interfaces are available, and creates a list of all tasks that are compatible with these interfaces. It then immediately checks which tasks are meant to run in the users selected interaction mode (by default: `text` mode), and starts them.

If the user toggles `speech` mode, the `ChatSession` stops all `text` tasks, and starts all `speech` tasks.

Finally, when the user leaves the app, the `ChatSession` stops all tasks and cleans them up.

**Example task: GRPCSpeechTask**

![An interaction diagram describing how to GRPCSpeechTask works](./images/example_task.png)

For example, the `GRPCSpeechTask` is responsible for sending and receiving speech from ACE Agent. As its name suggests, it is only instantiated when ACE Agent exposes a gRPC interface (`speech` mode).

Because the task is only needed when the user uses `speech` mode, it defines its `interactionModes` as `[speech]`. As such, the `ChatSession` will only start this task when the user toggles `speech` mode from the UI.

When started, the tasks implementation creates two gRPC streams, sendAudio and receiveAudio. It continuously listens for audio chunks sent by the user, and sends them to ACE Agent via the sendAudio stream. Similarly, it continuously listens for audio chunks sent by ACE Agent through receiveAudio, and sends them to the user.

When the user toggles back to the text interaction mode, the ChatSession stops GRPCSpeechTask, which effectively interrupts the gRPC streams.

##### Communication between tasks

Tasks are designed to run independently, without knowledge of other tasks. This is to reduce coupling between tasks, which helps testing and maintainability.

To communicate, all tasks belonging to a specific `ChatSession` share a common `eventBus`, which is a simple pub-sub system. Tasks can emit events, and subscribe to events.

For example, consider the `WebsocketTask` and the `UMIMTask`. The `WebsocketTask` is responsible for handling communications with the client. The `UMIMTask` is responsible for handling communications with ACE Agents event interface. When the bot sends a new message, it is received by the `UMIMTask`, which in turn emits the messages content through the shared `eventBus`:

```js
// In UMIMTask
private handleRedisEvent(message) {
  switch (message.type) {
    case "StartUtteranceBotAction": {
this.eventBus.emit("botStartedUtterance", message.script);
      break;
      ...
    }
  }
}
```

The `WebsocketTask` can subscribe to this event and decide how to handle it:

```js
// In WebsocketTask
private async listenBotStartedUtterance() {
  while (this.isRunning()) {
    const [text] = await once(
      this.eventBus,
      "botStartedUtterance",
    );
    this.sendMessageToUser(text); // send to websocket
  }
}
```

The advantage of this event-based architecture is that tasks dont need to know which task emitted the event. For example, if ACE Agent was configured to use its `HTTP` interface instead of the Redis interface, the `HTTPChatTask` would run instead of the `UMIMTask`. As long as the `HTTPChatTask` emits the same `botStartedUtterance` event when the bot sends a message, the `WebsocketTask` will send the message to the user without requiring any changes.

##### Adding new tasks

If you need to implement a feature that is not supported by existing tasks, you can define a new task. Your task must extend the `AbstractTask` base class, and implement the following methods and fields:

1. `start()`: this method should run the core logic of the task (for example, continuously listen for new messages on a `gRPC` client).
2. `interactionModes`: which interaction modes are supported by the task (`text`, `speech` or both). The `ChatSession` will automatically `start()` and `stop()` tasks depending on the users chosen interaction mode.
3. `cleanup()`: (optional) if your task needs to run some clean up logic when the user closes the session, it can extend this method to run it. By default, this method does nothing.

Additionally, you should ensure that the tasks core logic is interrupted when the base class `abortController` is called. This ensures that the task doesnt keep running once its stopped. An `AbortController` is a native JavaScript API allowing to interrupt asynchronous work. It is compatible with many NodeJS APIs. For example, all gRPC calls accept an optional `signal` parameter:

```js
const metaDataResponse = this.gRPCClient.streamSpeechResults(request, {
  signal: this.abortController.signal,
});
```

Once implemented, the task can be added in `ChatSession`s `initTasks()` method. From there, the `ChatSession` will automatically `start()` and `stop()` the task based on the users chosen interaction mode (`speech` or `text`).

##### Clients

As of today, the server supports three protocols to communicate with ACE Agent:

1. `RedisClient` - to interact with ACE Agent through its event mode
2. `GRPCClient` - to interact with ACE Agent through its speech mode
3. `HTTPClient` - to interact with ACE Agent through its server mode

These clients are located in the `server/clients/` directory and implemented as singletons which can be used from anywhere in the server code:

```js

if (GRPCClient.isAvailable()) {
  const client = GRPCClient.get();
  const metaDataResponse = this.gRPCClient.streamSpeechResults(request);
}
```

The clients can be configured through environment variables. By default, these environment variables use ACE Agents default IP and ports:

```sh
# The Redis URL is used when ACE Agent is running in "event" (aka "umim") mode
REDIS_URL=redis://localhost:6379

# The gRPC URL is used when ACE Agent is running in gRPC mode and/or speech mode
GRPC_URL=http://localhost:50055

# The HTTP URL is used when ACE Agent is running in "server" (aka "http") mode
HTTP_CHAT_URL=http://localhost:9000
```

##### Adding new clients

If you would like to extend the Bot Web UI to interact with protocols that are not yet supported, you can create a new client. The client should be located in its own file, in the `server/clients` directory. It should provide a `isAvailable()` method, and a `get()` method.

##### Logging

The server uses `bunyan` for logging. The logger is defined in `server/logger.ts` and can be used from anywhere in the server code. It uses the common logging levels (`debug`, `info`, `warn`, etc) and `sprintf` syntaxes:

```js
import getLogger from "../logger";
const logger = getLogger(<file name>);
logger.info("Person %s said %s!", person.name, message);
```

By default, production and development logs go to `stdout`. In automated tests, only logs of level `ERROR` and above appear in the console.

### Automated tests

#### Running tests

Currently, unit and integration tests are only supported on the server. To run tests, install yarn and run:

```sh
cd ./server
yarn install
yarn test
```

Note: if running yarn install yields the following error:

```sh
YN0001:  Error: EACCES: permission denied, unlink '/home/<username>/src/bot-maker/bot-web-ui/server/node_modules/.bin/acorn'
```

Run sudo `rm -rf node_modules` and try again.

#### Writing tests

Tests uses node's native test runner, and tsx for typescript support. To write a test, create a file with extension `.test.ts`. As a convention, the test file should:

1. Be in the same directory as the file being tested
2. Have the same name as the file being tested

For example:

```sh
server/emoji-finder/index.ts
server/emoji-finder/index.test.ts
```
````

## File: microservices/ace_agent/4.1/README.md
````markdown
# ACE Agent

ACE Agent is a collection of microservices to help build LLM driven scalable and customizable Conversational AI Agents. It offers a complete workflow to build and deploy virtual agents that can support multi-turn and multi-user contextual conversation flow. It provides connectivity between AI skills like NVIDIA Riva Speech AI, NVIDIA ACE Avatar AI & Vision AI, usecase specific custom plugins, and user interfaces through efficient system integration and composable dialog management.

Some of the major benefits that ACE Agent provides are:

- **In-built LLM integration** - ACE Agent works with large language models (LLM) out-of-the-box and provides a hook to connect with the LLM model of your choice.

- **On-premise model deployment** - ACE Agent supports on premise deployment of both ACE Agent models as well as other community and custom models. NVIDIA NIM for LLMs brings state of the art GPU accelerated large language model serving. Using NIM, you can deploy an LLM of your choice on premise and use it with ACE Agent.

- **Highly customizable** - ACE Agent allows you to completely customize the behavior of the bot based on your usecase using Colang. It even allows you to integrate agents and bots built using LangChain or similar frameworks in the ACE Agent pipeline for building multi-model use cases.

- **RAG** - ACE Agent allows easy integration with Retrieval Augmented Generation (RAG) workflows to support building agents using existing knowledge documents with minimal efforts.

- **Low latency** - ACE Agent uses NVIDIA TensorRT optimized models, NVIDIA Triton Inference Server for model deployment, and optimized chat controller to ensure low latency and high throughput bot interactions.

For more details, check [the ACE Agent Documentation](https://docs.nvidia.com/ace/latest/modules/ace_agent/index.html).

## UCS Microservices

ACE Agent provides Kubernetes deployment using NVIDIA Unified Cloud Services (UCS) Tools. NVIDIA Unified Cloud Services Tools (UCS Tools) is a low-code framework for developing cloud-native, real-time, and multimodal AI applications. The NVIDIA ACE Agent releases includes the following UCS microservices:

| Microservice Name  | Version  | Description|
|---|---|---|
| ucf.svc.ace-agent.chat-controller  | 4.1.0  | The Chat Controller orchestrates the end-to-end pipeline for a speech IO based Conversational AI Agents. The Chat Controller creates a pipeline consisting of Automatic Speech Recognition (ASR), Chat Engine, Text-To-Speech (TTS), NVIDIA Omniverse Audio2Face Client, and manages the flow of audio or text data between these modules. |
| ucf.svc.ace-agent.chat-engine  | 4.1.0  | The Chat Engine is microservice built on top of the NVIDIA NeMo Guardrails and allow you to design conversational flow using Colang. |
| ucf.svc.ace-agent.nlp-server  | 4.1.0  | The ACE Agent NLP server exposes unified RESTful interfaces for integrating various NLP models and tasks in ACE Agent pipeline. |
| ucf.svc.ace-agent.plugin-server  | 4.1.0  | The Plugin server allows us to add use case/domain specific business logic such as getting weather data from weather APIs in the bots. The Plugin server can also allow you to integrate your own agent built using LangChain or LlamaIndex or any other framework in ACE Ecosystem. |
| ucf.svc.ace-agent.web-app | 4.1.0  | The sample frontend application for trying out bot deployed using ACE agent with voice capture and playback support as well as with text input-output support.  |


You can easily create your own custom application Helm chart using ACE Agent microservices with UCS applications. The ACE Agent Quick Start package comes with a number of UCS applications for sample bots which can be found in the [./deploy/ucs_apps/](./deploy/ucs_apps/) directory.

## Samples

The ACE Agent Quick Start package comes with a number of sample bots which can be found in the [./samples/](./samples/) directory. The sample bots are built for different use cases and industries and showcases various features of ACE Agent. 

For more details refer [Sample Bots section](https://docs.nvidia.com/ace/latest/modules/ace_agent/index.html#sample-bots) in the ACE Agent Documentation.
````

## File: tools/avatar_configurator/1.0/README.md
````markdown
# Avatar Configurator

See [ACE documentation](https://docs.nvidia.com/ace/latest/modules/avatar_customization/index.html) for more information.
````

## File: tools/ucs_tools/README.md
````markdown
# UCS Tools overview

NVIDIA Unified Cloud Services Tools (UCS Tools) is a low-code framework for developing cloud-native, real-time, & multimodal AI applications. It features low-code design tools for microservices & applications, as well as a collection of optimized microservices and sample applications. Adopting a Microservices Architecture approach, Unified Cloud Services enables developers to combine microservices into cloud-native applications or services, meeting the real-time requirements of interactive AI use cases.

Each microservice has a bounded domain context (Vision AI, Conversational AI, Animation AI & Rendering, Data Analytics, etc.) and can be independently deployed, managed, & scaled within the application. The abstraction of each domain from the application reduces the need for low-level domain and platform knowledge. With UCS Tools, developers can create complex AI applications in days rather than weeks and months. Moreover, application execution can be distributed on multiple devices and across the cloud to edge and embedded platforms.

Finally, a complete set of specifications and design guidelines will allow domain experts to create microservices using UCS-compatible NVIDIA SDKs.

Please visit the [UCS Tools documentation](https://docs.nvidia.com/ace/latest/modules/docs/docs/index.html) for additional details and examples.
````

## File: workflows/animation_pipeline/1.0/deploy/ucs_apps/animation_pipeline_params.yaml
````yaml
# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

audio2face:
  ucfVisibleGpus: [0]
  configs:
    a2f_config.yaml:
      streamNumber: "3"
      a2eEnabled: "True"
      a2eInferenceInterval: "5"
      faceParams: "{}"
      a2fModelName: "claire_v1.3"
      a2fDeviceId: "0"
      a2eEmotionContrast: "1.0"
      a2eLiveBlendCoef: "0.6"
      a2eEnablePreferredEmotion: "True"
      a2ePreferredEmotionStrength: "0.5"
      a2eEmotionStrength: "0.6"
      a2eMaxEmotions: "3"
      addSilencePaddingAfterAudio: "False"
      queueAfterStreammux: "1"
      queueAfterA2F: "3000"
      queueAfterA2E: "30"
      maxLenUUID: "100"
      maxSampleRate: "140000"
      minSampleRate: "16000"
      lowFps: "29"
      lowFpsMaxDurationSecond: "7"
      useFP16A2F: "True"
      useFP16A2E: "True"
animation-graph:
  ucfVisibleGpus: [0]
  replicas: 1
  resourceDownload:
    remoteResourcePath: "nvidia/ucs-ms/default-avatar-scene:1.0.0"
    secretName: ngc-api-key-secret
    image: nvcr.io/eevaigoeixww/animation/ngc-resource-downloader:1.0.1
avatar-renderer-a:
  ucfVisibleGpus: [0]
  replicas: 1
  resourceDownload:
    remoteResourcePath: "nvidia/ucs-ms/default-avatar-scene:1.0.0"
    secretName: ngc-api-key-secret
    image: nvcr.io/eevaigoeixww/animation/ngc-resource-downloader:1.0.1
  livestream:
    rtpNegotiationHostMockingEnabled: true
    host: "127.0.0.1"
    videoPort: 9020
    audioPort: 9021
avatar-renderer-b:
  ucfVisibleGpus: [1]
  replicas: 1
  resourceDownload:
    remoteResourcePath: "nvidia/ucs-ms/default-avatar-scene:1.0.0"
    secretName: ngc-api-key-secret
    image: nvcr.io/eevaigoeixww/animation/ngc-resource-downloader:1.0.1
  livestream:
    rtpNegotiationHostMockingEnabled: true
    host: "127.0.0.1"
    videoPort: 9030
    audioPort: 9031
# avatar-renderer-c:
#   ucfVisibleGpus: [2]
#   replicas: 1
#   resourceDownload:
#     remoteResourcePath: "nvidia/ucs-ms/default-avatar-scene:1.0.0"
#     secretName: ngc-api-key-secret
#     image: nvcr.io/eevaigoeixww/animation/ngc-resource-downloader:1.0.1
#   livestream:
#     rtpNegotiationHostMockingEnabled: true
#     host: "127.0.0.1"
#     videoPort: 9040
#     audioPort: 9041
````

## File: workflows/animation_pipeline/1.0/deploy/ucs_apps/animation_pipeline_values.yaml
````yaml
# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Quickfix for UCS bug to not link claims and configmaps correctly when using aliases
global: {}
ia-animation-graph-microservice: {}
avatar-renderer-a:
  applicationSpecs:
    deployment:
      containers:
        ms:
          command:
            - bash
            - /opt/scripts/startup.sh
            - --/exts/omni.services.transport.server.http/port=$(IAORMS_HTTP_SERVER_PORT)
            - --/app/printConfig=$(IAORMS_KIT_PRINT_SETTINGS)
            - --/app/window/width=$(IAORMS_WINDOW_WIDTH)
            - --/app/window/height=$(IAORMS_WINDOW_HEIGHT)
            - --enable
            - omni.kit.telemetry
            - --/crashreporter/enabled=true
            - --/crashreporter/data/serviceName=ia-omniverse-renderer-microservice-a-deployment
            - --/crashreporter/url=https://services.nvidia.com/submit
            - --/crashreporter/alwaysUpload=true
            - --/crashreporter/dumpDir=$(IAORMS_SCENE_DIRECTORY)
avatar-renderer-b:
  applicationSpecs:
    deployment:
      containers:
        ms:
          command:
            - bash
            - /opt/scripts/startup.sh
            - --/exts/omni.services.transport.server.http/port=$(IAORMS_HTTP_SERVER_PORT)
            - --/app/printConfig=$(IAORMS_KIT_PRINT_SETTINGS)
            - --/app/window/width=$(IAORMS_WINDOW_WIDTH)
            - --/app/window/height=$(IAORMS_WINDOW_HEIGHT)
            - --enable
            - omni.kit.telemetry
            - --/crashreporter/enabled=true
            - --/crashreporter/data/serviceName=ia-omniverse-renderer-microservice-a-deployment
            - --/crashreporter/url=https://services.nvidia.com/submit
            - --/crashreporter/alwaysUpload=true
            - --/crashreporter/dumpDir=$(IAORMS_SCENE_DIRECTORY)
# avatar-renderer-c:
#   applicationSpecs:
#     deployment:
#       containers:
#         ms:
#           command:
#             - bash
#             - /opt/scripts/startup.sh
#             - --/exts/omni.services.transport.server.http/port=$(IAORMS_HTTP_SERVER_PORT)
#             - --/app/printConfig=$(IAORMS_KIT_PRINT_SETTINGS)
#             - --/app/window/width=$(IAORMS_WINDOW_WIDTH)
#             - --/app/window/height=$(IAORMS_WINDOW_HEIGHT)
#             - --enable
#             - omni.kit.telemetry
#             - --/crashreporter/enabled=true
#             - --/crashreporter/data/serviceName=ia-omniverse-renderer-microservice-a-deployment
#             - --/crashreporter/url=https://services.nvidia.com/submit
#             - --/crashreporter/alwaysUpload=true
#             - --/crashreporter/dumpDir=$(IAORMS_SCENE_DIRECTORY)
#             - --/profiler/enabled=true
#             - --/app/profilerBackend=tracy
#             - --/app/profileFromStart=true
#             - --/profiler/gpu=true
#             - --/profiler/gpu/tracyinject/enabled=true
````

## File: workflows/animation_pipeline/1.0/deploy/ucs_apps/animation_pipeline.yaml
````yaml
# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

specVersion: 2.0.0

version: 1.0.1

name: animation-pipeline

description: A basic deployment configuration for NVIDIA's animation pipeline

dependencies:
  - ucf.svc.audio2face:1.0.14
  - ucf.svc.ia-animation-graph-microservice:1.0.1
  - ucf.svc.ia-omniverse-renderer-microservice:1.0.1
secrets:
  k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY:
    k8sSecret:
      secretName: ngc-api-key-secret
      key: NGC_CLI_API_KEY
components:
  - name: audio2face
    type: ucf.svc.audio2face
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: animation-graph
    type: ucf.svc.ia-animation-graph-microservice
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: avatar-renderer-a
    type: ucf.svc.ia-omniverse-renderer-microservice
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: avatar-renderer-b
    type: ucf.svc.ia-omniverse-renderer-microservice
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  # - name: avatar-renderer-c
  #   type: ucf.svc.ia-omniverse-renderer-microservice
  #   parameters:
  #     imagePullSecrets:
  #       - name: ngc-docker-reg-secret
  - name: fake-rtp-negotiation
    type: ucf.svc.external-endpoint
    parameters:
      service: 0.0.0.0
      port: 32667

connections:
  audio2face/a2f-grpc-client: animation-graph/anim-server
  avatar-renderer-a/anim-source: animation-graph/anim-server
  avatar-renderer-a/rtp-negot: fake-rtp-negotiation/endpoint
  avatar-renderer-b/anim-source: animation-graph/anim-server
  avatar-renderer-b/rtp-negot: fake-rtp-negotiation/endpoint
  # avatar-renderer-c/anim-source: animation-graph/anim-server
  # avatar-renderer-c/rtp-negot: fake-rtp-negotiation/endpoint
````

## File: workflows/animation_pipeline/1.0/README.md
````markdown
# ACE Animation Pipeline Workflow

See [ACE documentation](https://docs.nvidia.com/ace/latest/workflows/animation_pipeline/index.html) for more information.
````

## File: workflows/tokkio/4.1/3-stream/config/lp_grpc_in_udp_out_anim_tuning.json
````json
{
    "lp_config": {
      "animation_cropping_mode": "ANIMATION_CROPPING_MODE_BLEND",
      "model_selection": "MODEL_SELECTION_PERF",
      "eye_blink_config": {
        "blink_frequency": {
          "value": 15,
          "unit": "UNIT_TIMES_PER_MINUTE"
        },
        "blink_duration": {
          "value": 6,
          "unit": "UNIT_FRAME"
        }
      },
      "gaze_look_away_config": {
        "enable_gaze_look_away": false,
        "max_look_away_offset": {
          "value": 20,
          "unit": "UNIT_DEGREE_ANGLE"
        },
        "min_look_away_interval": {
          "value": 240,
          "unit": "UNIT_FRAME"
        },
        "look_away_interval_range": {
          "value": 60,
          "unit": "UNIT_FRAME"
        }
      },
      "mouth_expression_config": {
        "mouth_expression_multiplier": 1.0
      }
    },
    "endpoint_config": {
      "input_media_config": {
        "audio_input_config": {
          "stream_config": {
            "stream_type": "GRPC"
          },
          "channels": 1,
          "channel_index": 0,
          "layout": "AUDIO_LAYOUT_INTERLEAVED",
          "sample_rate_hz": 16000,
          "chunk_duration_ms": 33,
          "encoding": "AUDIO_ENCODING_RAW",
          "decoder_config": {
            "raw_dec_config": {
              "format": "AUDIO_FORMAT_S16LE"
            }
          }
        }
      },
      "output_media_config": {
        "audio_output_config": {
          "stream_config": {
            "stream_type": "UDP",
            "udp_params": {
              "host": "127.0.0.1",
              "port": "9017"
            }
          },
          "payloader_config": {
            "type": "PAYLOADER_RTP"
          },
          "sample_rate_hz": 16000,
          "chunk_duration_ms": 33,
          "encoding": "AUDIO_ENCODING_RAW",
          "encoder_config": {
            "raw_enc_config": {
              "format": "AUDIO_FORMAT_S16BE"
            }
          }
        },
        "video_output_config": {
          "stream_config": {
            "stream_type": "UDP",
            "udp_params": {
                "host": "127.0.0.1",
                "port": "9019"
              }
          },
          "payloader_config": {
            "type": "PAYLOADER_RTP"
          },
          "encoding": "H264",
          "encoder_config": {
            "h264_enc_config": {
              "idr_frame_interval": 30
            }
          }
        }
      }
    },
    "quality_profile": "SPEECH_LP_QUALITY_PROFILE_LOW_LATENCY"
  }
````

## File: workflows/tokkio/4.1/3-stream/tokkio-app-params.yaml
````yaml
---
chat-controller:
  ipaDictPath: cmudict_ipa.txt
  pipelineParams:
    grpc_server:
      nvidia::rrt::BotRuntimeGrpc:
        virtual_assistant_num_instances: 10
    riva_asr:
      RivaASR:
        enable_profanity_filter: false
    riva_logger:
      RivaLogger:
        enable_logging: true
    speech_pipeline_manager:
      SpeechPipelineManager:
        always_on: true
        initial_state: INIT
        tts_eos_delay_ms: 0
    live_portrait_grpc:
      LivePortraitGrpc:
        aux_file: /workspace/riva/lp_portrait.png
        config_file: /opt/ext-files/lp_config.json
        is_dump_audio: true
        lp_config_wait_us: 1000000
        rpc_timeout_ms: 3600000
        sample_rate: 16000
        silence_buffer_interval_ms: 33
        silence_buffer_polling_interval_ms: 1
        status_interval_ms: 2000
        streamId: ''
    riva_tts:
      RivaTTS:
        audio_start_threshold_ms: 0
        chunk_duration_ms: 33
        sample_rate: 16000
        send_audio_in_realtime: true
        voice_name: English-US.Female-1
  speechConfigPath: speech_config.yaml
  wordBoostFilePath: asr_words_to_boost_conformer.txt
  configNgcPath: nvidia/ace/tokkio_plugin_llm_rag:4.1.2
  pipeline: live_portrait_umim
chat-engine:
  enableUserAttention: "true"
  interface: event
  logLevel: INFO
  botConfigName: tokkio_rag_bot_config.yml
  configNgcPath: nvidia/ace/tokkio_plugin_llm_rag:4.1.2
ds-visionai:
  checkInterval: '1'
  jitterbufferLatency: 2000
  peerPadIdSameAsSourceId: 'true'
  redisCfg:
    payloadkey: message
    topic: visionai
  rtspReconnectInterval: 10
  streammuxResolution:
    height: 720
    width: 1280
  videoSink: none
  ucfVisibleGpus:
    - 0
mongodb:
  storageClaims:
    local-storage:
      spec:
        resources:
          requests:
            storage: 1Gi
occupancy-alerts:
  analytics:
    fov:
      metrics:
        storage:
          granularity15MinRetentionMsec: 604800000
          granularity15SecRetentionMsec: 86400000
          granularity1MinRetentionMsec: 604800000
          granularity1SecRetentionMsec: 86400000
    gazeROI:
      frameBuffer: 5
    lipActivity:
      frameBuffer: 0
    roi:
      frameBuffer: 35
      metrics:
        storage:
          granularity15MinRetentionMsec: 604800000
          granularity15SecRetentionMsec: 86400000
          granularity1MinRetentionMsec: 604800000
          granularity1SecRetentionMsec: 86400000
      pixelBuffer: 25
    stateManagement:
      classTargets:
        - Face
      sensor:
        maxIdleTimeSec: 60
  metadataStream: visionai
  plugins:
    - className: core.analytics.plugins.ue.ue.UserEngagement
      config:
        exponentialSmootheningAlpha: 0.4
        resultRetensionWindowMaxSize: 3600
      name: UserEngagement
  sensorTemplate:
    alert_rules:
      fov:
        rules:
          - count_threshold: 1
            id: dm_fov
            parameters:
              - name: time_interval_up
                value: 0.25
              - name: time_interval_down
                value: 2
            rule_id: fov_occupancy_threshold
            rule_type: occupancy_threshold_switch
            time_interval: 1
            type: fov
      roi:
        rules: []
      tripwire:
        rules: []
      user_engagement:
        rules:
          - id: UE_CONFIG_1
            name: ALERT_RULE_1
            rule_type: zone_change
            time_interval: 1
    gaze_rois: []
    rois: []
    tripwires: []
    user_engagement:
      - angleBuffer: 5
        frameBuffer: 2
        id: UE_CONFIG_1
        zones:
          - from_angle: 0
            id: ENGAGED
            name: Near to Camera
            to_angle: 35
          - from_angle: 35
            id: DISTRACTED
            name: Away from Camera
            to_angle: 100
          - from_angle: 100
            id: DISENGAGED
            name: Away from Camera
            to_angle: 180
  sensors:
    - alert_rules:
        fov:
          rules:
            - count_threshold: 1
              id: dm_fov
              parameters:
                - name: time_interval_up
                  value: 1
                - name: time_interval_down
                  value: 2
              rule_id: fov_occupancy_threshold_default
              rule_type: occupancy_threshold_switch
              time_interval: 1
              type: fov
        roi:
          rules: []
        tripwire:
          rules: []
      sensorId: drive-thru-0
  storageClaims:
    data:
      spec:
        resources:
          requests:
            storage: 1Gi
  vstEventsStream: vst_events
  vstEventsStreamIntegration: 0
occupancy-alerts-api:
  configs:
    cv_config.yaml:
      data:
        metadata:
          maxTimeRangeInMSec: 500000
          minTimeRangeInMSec: 0
        trajectory:
          maxTimeRangeInMSec: 500000
          minTimeRangeInMSec: 0
  storageClaims:
    data:
      spec:
        resources:
          requests:
            storage: 1Gi
plugin-server:
  pluginConfigPath: plugin_config.yaml
  configNgcPath: nvidia/ace/tokkio_plugin_llm_rag:4.1.2
  pluginConfig:
    plugins:
      rag:
        parameters:
          USE_RAG: false
          RAG_SERVER_URL: http://127.0.0.1:8081
          NIM_MODEL: "meta/llama3-8b-instruct"
          USE_OPENAI: false
          OPENAI_MODEL: "gpt-4"
redis:
  storageClaims:
    data:
      spec:
        resources:
          requests:
            storage: 100Mi
redis-timeseries:
  storageClaims:
    data:
      spec:
        resources:
          requests:
            storage: 100Mi
riva-speech:
  modelRepoGenerator:
    clearAllRMIRSAndModels: false
    ngcModelConfigs:
      triton0:
        models:
          - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
          - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
  persistentVolumeClaim:
    keepPVC: true
  riva:
    visibleGpus: '0'
tokkio-ingress-mgr:
  accessControlAllowOrigin: \*
  bypassTokenValidation: false
  enableCookie: 'true'
  enableRedisPubsubListener: 'true'
  enableRedisTsListener: 'true'
  enableSessionRefresh: 'true'
  enableSessionTrigger: 'false'
  enableStarFleetProd: false
  enableStarFleetStg: false
  enableTracing: 'false'
  maxNumSession: '3'
  otelExporterOtlpEndpoint: http://0.0.0.0:4317
  otelExporterOtlpProtocol: grpc
  tokenMaxAge: '40'
  tokenTTL: '30'
tokkio-ui-server:
  botMSMaxRetry: '120'
  botMSPartialTranscript: '1'
  botMSRetryInterval: '500'
  cartProtocol: http
  cartSuffix: ''
  consoleLogLevel: debug
  fileLogLevel: debug
  menuProtocol: http
  menuSuffix: /api
  redisAceAgentKey: ace_agent_system_events
  redisFovKey: emdat_alert_events
  redisMaxRetry: '60'
  redisMlopsKey: mlops_ui
  redisRatingKey: user_rating
  redisRetryInterval: '1000'
  redisUiActionServerKey: ui_events
  redisVstKey: vst_events
  redisWdmKeyPrefix: wdm_error_events
  vmsProtocol: http
  vmsSuffix: /api
vms:
  applicationSpecs:
    vms:
      containers:
        vms-container:
          env:
            - name: NODE_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
      securityContext:
        fsGroup: 1000
  configs:
    rtsp_streams.json:
      streams:
        - audio:
            bits_per_sample: 32
            codec: pcm
            enabled: true
            port: 30032
            sample_rate_Hz: 44100
          enabled: false
          name: Tokkio_Avatar
          stream_in: udp
          video:
            codec: h264
            framerate: 30
            port: 30031
    vst_config.json:
      data:
        always_recording: true
        enable_avsync_udp_input: true
        enable_silent_audio_in_udp_input: false
        enable_udp_input_dump: false
        gpu_indices: []
        use_software_path: false
        use_standalone_udp_input: false
        use_webrtc_hw_dec: true
        use_webrtc_inbuilt_encoder: ''
        webrtc_in_fixed_resolution: 1280x720
        webrtc_in_max_framerate: 30
        webrtc_out_default_resolution: 1920x1080
        webrtc_out_enc_preset: ultra_fast
        webrtc_out_enc_quality_tuning: ultra_low_latency
        webrtc_out_set_idr_interval: 30
        use_external_peerconnection: false
      debug:
        enable_network_bandwidth_notification: true
      network:
        coturn_turnurl_list_with_secret: null
        enable_grpc: true
        grpc_server_port: 50051
        max_webrtc_in_connections: 10
        max_webrtc_out_connections: 10
        ntp_servers: null
        reverse_proxy_server_address: 0.0.0.0:100
        rtsp_streaming_over_tcp: true
        static_turnurl_list:
          - username:password@0.0.0.0:3478
        stunurl_list:
          - stun.l.google.com:19302
          - stun1.l.google.com:19302
        twilio_account_sid: '0000000000000000000000000000000000'
        twilio_auth_token: '00000000000000000000000000000000'
        udp_drop_on_latency: false
        udp_latency_ms: 200
        use_coturn_auth_secret: false
        use_reverse_proxy: false
        use_twilio_stun_turn: false
        webrtc_in_audio_sender_max_bitrate: 128000
        webrtc_in_video_bitrate_thresold_percentage: 50
        webrtc_in_video_degradation_preference: detail
        webrtc_in_video_sender_max_framerate: 30
        webrtc_port_range:
          max: 30030
          min: 30001
        webrtc_video_quality_tunning:
          resolution_1080:
            bitrate_range:
              - 10000
              - 35000
            bitrate_start: 25000
            qp_range_I:
              - 10
              - 30
            qp_range_P:
              - 10
              - 30
          resolution_1440:
            bitrate_range:
              - 20000
              - 55000
            bitrate_start: 50000
            qp_range_I:
              - 0
              - 15
            qp_range_P:
              - 0
              - 15
          resolution_2160:
            bitrate_range:
              - 30000
              - 80000
            bitrate_start: 70000
            qp_range_I:
              - 0
              - 20
            qp_range_P:
              - 0
              - 20
          resolution_480:
            bitrate_range:
              - 800
              - 3000
            bitrate_start: 1000
            qp_range_I:
              - 10
              - 30
            qp_range_P:
              - 10
              - 30
          resolution_720:
            bitrate_range:
              - 3000
              - 10000
            bitrate_start: 5000
            qp_range_I:
              - 10
              - 30
            qp_range_P:
              - 10
              - 30
      notifications:
        enable_notification: true
        message_broker_topic: vst_events
        redis_server_env_var: REDIS_TIMESERIES_REDIS_TIMESERIES_SVC_SERVICE_HOST:6379
        use_message_broker: redis
      security:
        enable_user_cleanup: true
        multi_user_extra_options:
          - Secure
          - SameSite=none
        session_max_age_sec: 2592000
        use_http_digest_authentication: false
        use_https: false
        use_multi_user: false
    vst_storage.json:
      total_video_storage_size_MB: 100000
  storageClaims:
    local-storage:
      spec:
        resources:
          requests:
            storage: 10Gi
  ucfVisibleGpus:
    - 0
live-portrait:
  envOverride:
    CUDA_VISIBLE_DEVICES: $((${HOSTNAME##*-}))
    ENABLE_PORTRAIT_IMG_UPDATE: 'false'
    FLAGS_enforce_SRTP: false
    GLOG_logtostderr: '1'
    GLOG_v: '3'
    # GST_DEBUG: 3,nvdsaudio_speechlp:7,nvdsA2Vtemplate:7,nvdsvideotemplate*:4
    MAX_STREAMS: '1'
  replicas: 3
  ucfVisibleGpus:
    - 1
    - 2
    - 3
  applicationSpecs:
    default:
      apptype: statefull
      statefulSetServiceName: service
      containers:
        speech-live-portrait-container:
          resources:
            limits:
              nvidia.com/gpu: null
      services:
        headless-speech-live-portrait-service:
          extraSpecs:
            clusterIP: None
          fullNameOverride: true
          ports:
            - port: 8037
              protocol: TCP
              targetPort: 8037
          type: ClusterIP
internal-ingress-proxy:
  routeConfigurations:
    speech-live-portrait-service:
      clusters:
        - address: headless-speech-live-portrait-service
          name: speech-live-portrait-service-cluster
          port: 8037
      enabled: true
      routes:
        - match:
            prefix: /nvidia.maxine.speech_live_portrait.v1.SpeechLivePortraitService/StreamingAnimateBySpeech
          route:
            cluster: speech-live-portrait-service-cluster
signaling-proxy:
  envOverride:
    GLOG_v: '4'
````

## File: workflows/tokkio/4.1/3-stream/tokkio-app.yaml
````yaml
---
specVersion: 2.5.0
version: 4.1.4
name: ucs-tokkio-app-base-3-stream-llm-rag-2d
description: UCS Tokkio App
secrets:
  k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY:
    k8sSecret:
      secretName: ngc-api-key-secret
      key: NGC_CLI_API_KEY
  k8sSecret/nvidia-api-key-secret/NVIDIA_API_KEY:
    k8sSecret:
      secretName: nvidia-api-key-secret
      key: NVIDIA_API_KEY
  k8sSecret/openai-key-secret/OPENAI_API_KEY:
    k8sSecret:
      secretName: openai-key-secret
      key: OPENAI_API_KEY
dependencies:
  - ucf.svc.ace-agent.chat-controller:4.1.0
  - ucf.svc.ace-agent.chat-engine:4.1.0
  - ucf.svc.ace-agent.plugin-server:4.1.0
  - ucf.svc.core.mongodb:0.0.14
  - ucf.svc.core.redis-timeseries:0.0.20
  - ucf.svc.core.redis:0.0.20
  - ucf.svc.ds.visionai:0.4.43
  - ucf.svc.internal.ingress.proxy:0.1.10
  - ucf.svc.metropolis.occupancy-alerts-api:0.1.56
  - ucf.svc.metropolis.occupancy-alerts:0.1.55
  - ucf.svc.riva.speech-skills:2.17.0
  - ucf.svc.signaling.proxy:1.1.12
  - ucf.svc.speech-live-portrait:0.2.3
  - ucf.svc.tokkio.chat-controller-sdr-envoy:0.1.33
  - ucf.svc.tokkio.ds-sdr-envoy:0.1.33
  - ucf.svc.tokkio.ingress-mgr:0.2.32
  - ucf.svc.tokkio.ui-server:5.0.4
  - ucf.svc.tokkio.umim-action-server:1.0.3
  - ucf.svc.vms:1.2.41
components:
  - name: tokkio-ds-sdr
    type: ucf.svc.tokkio.ds-sdr-envoy
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: tokkio-chat-controller-sdr
    type: ucf.svc.tokkio.chat-controller-sdr-envoy
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: tokkio-ingress-mgr
    type: ucf.svc.tokkio.ingress-mgr
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: redis
    type: ucf.svc.core.redis
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: mongodb
    type: ucf.svc.core.mongodb
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: riva-speech
    type: ucf.svc.riva.speech-skills
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  - name: redis-timeseries
    type: ucf.svc.core.redis-timeseries
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: chat-engine
    type: ucf.svc.ace-agent.chat-engine
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
      openai-key-secret: k8sSecret/openai-key-secret/OPENAI_API_KEY
  - name: chat-controller
    type: ucf.svc.ace-agent.chat-controller
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
    files:
      lp_config.json: ./config/lp_grpc_in_udp_out_anim_tuning.json
  - name: ds-visionai
    type: ucf.svc.ds.visionai
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: vms
    type: ucf.svc.vms
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: occupancy-alerts-api
    type: ucf.svc.metropolis.occupancy-alerts-api
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: occupancy-alerts
    type: ucf.svc.metropolis.occupancy-alerts
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: plugin-server
    type: ucf.svc.ace-agent.plugin-server
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
      openai-key-secret: k8sSecret/openai-key-secret/OPENAI_API_KEY
      nvidia-api-key-secret: k8sSecret/nvidia-api-key-secret/NVIDIA_API_KEY
  - name: tokkio-ui-server
    type: ucf.svc.tokkio.ui-server
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: live-portrait
    type: ucf.svc.speech-live-portrait
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: signaling-proxy
    type: ucf.svc.signaling.proxy
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: internal-ingress-proxy
    type: ucf.svc.internal.ingress.proxy
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: tokkio-umim-action-server
    type: ucf.svc.tokkio.umim-action-server
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: renderer-adapter-endpoint
    type: ucf.svc.external-endpoint
    parameters:
      service: localhost
      port: 31111
connections:
  vms/redis: redis-timeseries/redis
  chat-engine/plugin-server: plugin-server/http-api
  chat-engine/redis: redis-timeseries/redis
  chat-controller/redis: redis-timeseries/redis
  chat-controller/riva: riva-speech/riva-speech-api
  chat-controller/chat-api: chat-engine/http-api
  chat-controller/lp-grpc: signaling-proxy/signaling-proxy-ingress-endpoint
  ds-visionai/redis: redis-timeseries/redis
  occupancy-alerts-api/redis: redis-timeseries/redis
  occupancy-alerts-api/mongodb: mongodb/mongo
  occupancy-alerts/redis: redis-timeseries/redis
  occupancy-alerts/occupancy-alerts-api: occupancy-alerts-api/http-api
  tokkio-ui-server/vms: vms/vms
  tokkio-ui-server/redis: redis-timeseries/redis
  tokkio-ui-server/chat-controller: chat-controller/grpc-api
  tokkio-ingress-mgr/redis: redis/redis
  tokkio-ingress-mgr/redis-ts: redis-timeseries/redis
  tokkio-ingress-mgr/ui-server: tokkio-ui-server/ui-server-http
  tokkio-ingress-mgr/vms: vms/vms
  tokkio-chat-controller-sdr/redis: redis-timeseries/redis
  tokkio-chat-controller-sdr/chat-controller: chat-controller/http-api
  tokkio-chat-controller-sdr/vms: vms/vms
  tokkio-ds-sdr/redis: redis-timeseries/redis
  tokkio-ds-sdr/httpds: ds-visionai/http-api
  tokkio-ds-sdr/vms: vms/vms
  tokkio-umim-action-server/redis: redis-timeseries/redis
  tokkio-umim-action-server/ui-server: tokkio-ui-server/ui-server-http
  signaling-proxy/signaling-proxy-vst-endpoint: vms/vms-grpc
  signaling-proxy/signaling-proxy-internal-endpoint: internal-ingress-proxy/internal-ingress-endpoint
  signaling-proxy/signaling-proxy-rproxy-endpoint: live-portrait/ucf-speech-live-portrait-endpoint
````

## File: workflows/tokkio/4.1/llm-rag-ov/1-stream/config/lp_grpc_in_udp_out_anim_tuning.json
````json
{
    "lp_config": {
      "animation_cropping_mode": "ANIMATION_CROPPING_MODE_BLEND",
      "model_selection": "MODEL_SELECTION_PERF",
      "eye_blink_config": {
        "blink_frequency": {
          "value": 15,
          "unit": "UNIT_TIMES_PER_MINUTE"
        },
        "blink_duration": {
          "value": 6,
          "unit": "UNIT_FRAME"
        }
      },
      "gaze_look_away_config": {
        "enable_gaze_look_away": false,
        "max_look_away_offset": {
          "value": 20,
          "unit": "UNIT_DEGREE_ANGLE"
        },
        "min_look_away_interval": {
          "value": 240,
          "unit": "UNIT_FRAME"
        },
        "look_away_interval_range": {
          "value": 60,
          "unit": "UNIT_FRAME"
        }
      },
      "mouth_expression_config": {
        "mouth_expression_multiplier": 1.0
      }
    },
    "endpoint_config": {
      "input_media_config": {
        "audio_input_config": {
          "stream_config": {
            "stream_type": "GRPC"
          },
          "channels": 1,
          "channel_index": 0,
          "layout": "AUDIO_LAYOUT_INTERLEAVED",
          "sample_rate_hz": 16000,
          "chunk_duration_ms": 33,
          "encoding": "AUDIO_ENCODING_RAW",
          "decoder_config": {
            "raw_dec_config": {
              "format": "AUDIO_FORMAT_S16LE"
            }
          }
        }
      },
      "output_media_config": {
        "audio_output_config": {
          "stream_config": {
            "stream_type": "UDP",
            "udp_params": {
              "host": "127.0.0.1",
              "port": "9017"
            }
          },
          "payloader_config": {
            "type": "PAYLOADER_RTP"
          },
          "sample_rate_hz": 16000,
          "chunk_duration_ms": 33,
          "encoding": "AUDIO_ENCODING_RAW",
          "encoder_config": {
            "raw_enc_config": {
              "format": "AUDIO_FORMAT_S16BE"
            }
          }
        },
        "video_output_config": {
          "stream_config": {
            "stream_type": "UDP",
            "udp_params": {
                "host": "127.0.0.1",
                "port": "9019"
              }
          },
          "payloader_config": {
            "type": "PAYLOADER_RTP"
          },
          "encoding": "H264",
          "encoder_config": {
            "h264_enc_config": {
              "idr_frame_interval": 30
            }
          }
        }
      }
    },
    "quality_profile": "SPEECH_LP_QUALITY_PROFILE_LOW_LATENCY"
  }
````

## File: workflows/tokkio/4.1/llm-rag-ov/1-stream/tokkio-app-params.yaml
````yaml
---
chat-controller:
  ipaDictPath: cmudict_ipa.txt
  pipelineParams:
    grpc_server:
      nvidia::rrt::BotRuntimeGrpc:
        virtual_assistant_num_instances: 10
    riva_asr:
      RivaASR:
        enable_profanity_filter: false
    riva_logger:
      RivaLogger:
        enable_logging: true
    speech_pipeline_manager:
      SpeechPipelineManager:
        always_on: true
        initial_state: INIT
        tts_eos_delay_ms: 0
    riva_tts:
      RivaTTS:
        audio_start_threshold_ms: 2000
        chunk_duration_ms: 600
        sample_rate: 16000
        send_audio_in_realtime: true
        voice_name: English-US.Male-1
  speechConfigPath: speech_config.yaml
  wordBoostFilePath: asr_words_to_boost_conformer.txt
  configNgcPath: nvidia/ace/tokkio_plugin_llm_rag:4.1.2
  pipeline: avatar_umim
chat-engine:
  enableUserAttention: "true"
  interface: event
  logLevel: INFO
  botConfigName: tokkio_rag_bot_config.yml
  configNgcPath: nvidia/ace/tokkio_plugin_llm_rag:4.1.2
ds-visionai:
  checkInterval: '1'
  jitterbufferLatency: 2000
  peerPadIdSameAsSourceId: 'true'
  redisCfg:
    payloadkey: message
    topic: visionai
  rtspReconnectInterval: 10
  streammuxResolution:
    height: 720
    width: 1280
  videoSink: none
  ucfVisibleGpus:
    - 0
mongodb:
  storageClaims:
    local-storage:
      spec:
        resources:
          requests:
            storage: 1Gi
occupancy-alerts:
  analytics:
    fov:
      metrics:
        storage:
          granularity15MinRetentionMsec: 604800000
          granularity15SecRetentionMsec: 86400000
          granularity1MinRetentionMsec: 604800000
          granularity1SecRetentionMsec: 86400000
    gazeROI:
      frameBuffer: 5
    lipActivity:
      frameBuffer: 0
    roi:
      frameBuffer: 35
      metrics:
        storage:
          granularity15MinRetentionMsec: 604800000
          granularity15SecRetentionMsec: 86400000
          granularity1MinRetentionMsec: 604800000
          granularity1SecRetentionMsec: 86400000
      pixelBuffer: 25
    stateManagement:
      classTargets:
        - Face
      sensor:
        maxIdleTimeSec: 60
  metadataStream: visionai
  plugins:
    - className: core.analytics.plugins.ue.ue.UserEngagement
      config:
        exponentialSmootheningAlpha: 0.4
        resultRetensionWindowMaxSize: 3600
      name: UserEngagement
  sensorTemplate:
    alert_rules:
      fov:
        rules:
          - count_threshold: 1
            id: dm_fov
            parameters:
              - name: time_interval_up
                value: 0.25
              - name: time_interval_down
                value: 2
            rule_id: fov_occupancy_threshold
            rule_type: occupancy_threshold_switch
            time_interval: 1
            type: fov
      roi:
        rules: []
      tripwire:
        rules: []
      user_engagement:
        rules:
          - id: UE_CONFIG_1
            name: ALERT_RULE_1
            rule_type: zone_change
            time_interval: 1
    gaze_rois: []
    rois: []
    tripwires: []
    user_engagement:
      - angleBuffer: 5
        frameBuffer: 2
        id: UE_CONFIG_1
        zones:
          - from_angle: 0
            id: ENGAGED
            name: Near to Camera
            to_angle: 35
          - from_angle: 35
            id: DISTRACTED
            name: Away from Camera
            to_angle: 100
          - from_angle: 100
            id: DISENGAGED
            name: Away from Camera
            to_angle: 180
  sensors:
    - alert_rules:
        fov:
          rules:
            - count_threshold: 1
              id: dm_fov
              parameters:
                - name: time_interval_up
                  value: 1
                - name: time_interval_down
                  value: 2
              rule_id: fov_occupancy_threshold_default
              rule_type: occupancy_threshold_switch
              time_interval: 1
              type: fov
        roi:
          rules: []
        tripwire:
          rules: []
      sensorId: drive-thru-0
  storageClaims:
    data:
      spec:
        resources:
          requests:
            storage: 1Gi
  vstEventsStream: vst_events
  vstEventsStreamIntegration: 0
occupancy-alerts-api:
  configs:
    cv_config.yaml:
      data:
        metadata:
          maxTimeRangeInMSec: 500000
          minTimeRangeInMSec: 0
        trajectory:
          maxTimeRangeInMSec: 500000
          minTimeRangeInMSec: 0
  storageClaims:
    data:
      spec:
        resources:
          requests:
            storage: 1Gi
plugin-server:
  pluginConfigPath: plugin_config.yaml
  configNgcPath: nvidia/ace/tokkio_plugin_llm_rag:4.1.2
  pluginConfig:
    plugins:
      rag:
        parameters:
          USE_RAG: false
          RAG_SERVER_URL: http://127.0.0.1:8081
          NIM_MODEL: "meta/llama3-8b-instruct"
          USE_OPENAI: false
          OPENAI_MODEL: "gpt-4"
redis:
  storageClaims:
    data:
      spec:
        resources:
          requests:
            storage: 100Mi
redis-timeseries:
  storageClaims:
    data:
      spec:
        resources:
          requests:
            storage: 100Mi
riva-speech:
  modelRepoGenerator:
    clearAllRMIRSAndModels: false
    ngcModelConfigs:
      triton0:
        models:
          - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
          - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
  persistentVolumeClaim:
    keepPVC: true
  riva:
    visibleGpus: '0'
tokkio-ingress-mgr:
  accessControlAllowOrigin: \*
  bypassTokenValidation: false
  enableCookie: 'true'
  enableRedisPubsubListener: 'true'
  enableRedisTsListener: 'true'
  enableSessionRefresh: 'true'
  enableSessionTrigger: 'false'
  enableStarFleetProd: false
  enableStarFleetStg: false
  enableTracing: 'false'
  maxNumSession: '1'
  otelExporterOtlpEndpoint: http://0.0.0.0:4317
  otelExporterOtlpProtocol: grpc
  tokenMaxAge: '40'
  tokenTTL: '30'
tokkio-ui-server:
  botMSMaxRetry: '120'
  botMSPartialTranscript: '1'
  botMSRetryInterval: '500'
  cartProtocol: http
  cartSuffix: ''
  consoleLogLevel: debug
  fileLogLevel: debug
  menuProtocol: http
  menuSuffix: /api
  redisAceAgentKey: ace_agent_system_events
  redisFovKey: emdat_alert_events
  redisMaxRetry: '60'
  redisMlopsKey: mlops_ui
  redisRatingKey: user_rating
  redisRetryInterval: '1000'
  redisUiActionServerKey: ui_events
  redisVstKey: vst_events
  redisWdmKeyPrefix: wdm_error_events
  vmsProtocol: http
  vmsSuffix: /api
vms:
  applicationSpecs:
    vms:
      containers:
        vms-container:
          env:
            - name: NODE_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
      securityContext:
        fsGroup: 1000
  configs:
    rtsp_streams.json:
      streams:
        - audio:
            bits_per_sample: 32
            codec: pcm
            enabled: true
            port: 30032
            sample_rate_Hz: 44100
          enabled: false
          name: Tokkio_Avatar
          stream_in: udp
          video:
            codec: h264
            framerate: 30
            port: 30031
    vst_config.json:
      data:
        always_recording: true
        enable_avsync_udp_input: true
        enable_silent_audio_in_udp_input: false
        enable_udp_input_dump: false
        gpu_indices: []
        use_software_path: false
        use_standalone_udp_input: false
        use_webrtc_hw_dec: true
        use_webrtc_inbuilt_encoder: ''
        webrtc_in_fixed_resolution: 1280x720
        webrtc_in_max_framerate: 30
        webrtc_out_default_resolution: 1920x1080
        webrtc_out_enc_preset: ultra_fast
        webrtc_out_enc_quality_tuning: ultra_low_latency
        webrtc_out_set_idr_interval: 30
        use_external_peerconnection: false
      debug:
        enable_network_bandwidth_notification: true
      network:
        coturn_turnurl_list_with_secret: null
        enable_grpc: true
        grpc_server_port: 50051
        max_webrtc_in_connections: 3
        max_webrtc_out_connections: 8
        ntp_servers: null
        reverse_proxy_server_address: 0.0.0.0:100
        rtsp_streaming_over_tcp: true
        static_turnurl_list:
          - username:password@0.0.0.0:3478
        stunurl_list:
          - stun.l.google.com:19302
          - stun1.l.google.com:19302
        twilio_account_sid: '0000000000000000000000000000000000'
        twilio_auth_token: '00000000000000000000000000000000'
        udp_drop_on_latency: false
        udp_latency_ms: 200
        use_coturn_auth_secret: false
        use_reverse_proxy: false
        use_twilio_stun_turn: false
        webrtc_in_audio_sender_max_bitrate: 128000
        webrtc_in_video_bitrate_thresold_percentage: 50
        webrtc_in_video_degradation_preference: detail
        webrtc_in_video_sender_max_framerate: 30
        webrtc_port_range:
          max: 30030
          min: 30001
        webrtc_video_quality_tunning:
          resolution_1080:
            bitrate_range:
              - 10000
              - 35000
            bitrate_start: 25000
            qp_range_I:
              - 10
              - 30
            qp_range_P:
              - 10
              - 30
          resolution_1440:
            bitrate_range:
              - 20000
              - 55000
            bitrate_start: 50000
            qp_range_I:
              - 0
              - 15
            qp_range_P:
              - 0
              - 15
          resolution_2160:
            bitrate_range:
              - 30000
              - 80000
            bitrate_start: 70000
            qp_range_I:
              - 0
              - 20
            qp_range_P:
              - 0
              - 20
          resolution_480:
            bitrate_range:
              - 800
              - 3000
            bitrate_start: 1000
            qp_range_I:
              - 10
              - 30
            qp_range_P:
              - 10
              - 30
          resolution_720:
            bitrate_range:
              - 3000
              - 10000
            bitrate_start: 5000
            qp_range_I:
              - 10
              - 30
            qp_range_P:
              - 10
              - 30
      notifications:
        enable_notification: true
        message_broker_topic: vst_events
        redis_server_env_var: REDIS_TIMESERIES_REDIS_TIMESERIES_SVC_SERVICE_HOST:6379
        use_message_broker: redis
      security:
        enable_user_cleanup: true
        multi_user_extra_options:
          - Secure
          - SameSite=none
        session_max_age_sec: 2592000
        use_http_digest_authentication: false
        use_https: false
        use_multi_user: false
    vst_storage.json:
      total_video_storage_size_MB: 100000
  storageClaims:
    local-storage:
      spec:
        resources:
          requests:
            storage: 10Gi
  ucfVisibleGpus:
    - 0
animation-graph:
  animationServer:
    maxStreamCapacity: 1
  ucfVisibleGpus:
    - 1
  resourceDownload:
    image: nvcr.io/nvidia/ace/ngc-resource-downloader:1.1.4
    remoteResourcePath: nvidia/ace/default-avatar-scene:1.0.0
    secretName: ngc-api-key-secret
audio2face-with-emotion:
  configs:
    a2f_config.yaml:
      streamNumber: '1'
      a2eEnabled: 'False'
      a2eInferenceInterval: '10'
      a2fModelName: claire_v1.3
  ucfVisibleGpus:
    - 0
avatar-renderer:
  deployment:
    gpuAllocLimit: 1
  replicas: 1
  ucfVisibleGpus:
    - 1
  livestream:
    audioDelay: 0.13
  resourceDownload:
    image: nvcr.io/nvidia/ace/ngc-resource-downloader:1.1.4
    remoteResourcePath: nvidia/ace/default-avatar-scene:1.0.0
    secretName: ngc-api-key-secret
renderer-sdr:
  sdrMaxReplicas: '1'
````

## File: workflows/tokkio/4.1/llm-rag-ov/1-stream/tokkio-app.yaml
````yaml
---
specVersion: 2.5.0
version: 4.1.4
name: ucs-tokkio-app-base-1-stream-llm-rag-3d-ov
description: UCS Tokkio App
secrets:
  k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY:
    k8sSecret:
      secretName: ngc-api-key-secret
      key: NGC_CLI_API_KEY
  k8sSecret/nvidia-api-key-secret/NVIDIA_API_KEY:
    k8sSecret:
      secretName: nvidia-api-key-secret
      key: NVIDIA_API_KEY
  k8sSecret/openai-key-secret/OPENAI_API_KEY:
    k8sSecret:
      secretName: openai-key-secret
      key: OPENAI_API_KEY
dependencies:
  - ucf.svc.ace-agent.chat-controller:4.1.0
  - ucf.svc.ace-agent.chat-engine:4.1.0
  - ucf.svc.ace-agent.plugin-server:4.1.0
  - ucf.svc.audio2face:1.0.41
  - ucf.svc.core.mongodb:0.0.14
  - ucf.svc.core.redis-timeseries:0.0.20
  - ucf.svc.core.redis:0.0.20
  - ucf.svc.ds.visionai:0.4.43
  - ucf.svc.ia-animation-graph-microservice:1.0.2
  - ucf.svc.ia-omniverse-renderer-microservice:1.0.6
  - ucf.svc.metropolis.occupancy-alerts-api:0.1.56
  - ucf.svc.metropolis.occupancy-alerts:0.1.55
  - ucf.svc.riva.speech-skills:2.17.0
  - ucf.svc.tokkio.anim-graph-sdr-envoy:0.1.33
  - ucf.svc.tokkio.chat-controller-sdr-envoy:0.1.33
  - ucf.svc.tokkio.ds-sdr-envoy:0.1.33
  - ucf.svc.tokkio.ingress-mgr:0.2.32
  - ucf.svc.tokkio.renderer-sdr-envoy:0.1.33
  - ucf.svc.tokkio.ui-server:5.0.4
  - ucf.svc.tokkio.umim-action-server:1.0.3
  - ucf.svc.vms:1.2.41
components:
  - name: tokkio-ds-sdr
    type: ucf.svc.tokkio.ds-sdr-envoy
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: tokkio-chat-controller-sdr
    type: ucf.svc.tokkio.chat-controller-sdr-envoy
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: tokkio-ingress-mgr
    type: ucf.svc.tokkio.ingress-mgr
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: redis
    type: ucf.svc.core.redis
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: mongodb
    type: ucf.svc.core.mongodb
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: riva-speech
    type: ucf.svc.riva.speech-skills
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  - name: redis-timeseries
    type: ucf.svc.core.redis-timeseries
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: chat-engine
    type: ucf.svc.ace-agent.chat-engine
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
      openai-key-secret: k8sSecret/openai-key-secret/OPENAI_API_KEY
  - name: chat-controller
    type: ucf.svc.ace-agent.chat-controller
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
    files:
      lp_config.json: ./config/lp_grpc_in_udp_out_anim_tuning.json
  - name: ds-visionai
    type: ucf.svc.ds.visionai
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: vms
    type: ucf.svc.vms
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: occupancy-alerts-api
    type: ucf.svc.metropolis.occupancy-alerts-api
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: occupancy-alerts
    type: ucf.svc.metropolis.occupancy-alerts
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: plugin-server
    type: ucf.svc.ace-agent.plugin-server
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
      openai-key-secret: k8sSecret/openai-key-secret/OPENAI_API_KEY
      nvidia-api-key-secret: k8sSecret/nvidia-api-key-secret/NVIDIA_API_KEY
  - name: tokkio-ui-server
    type: ucf.svc.tokkio.ui-server
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: animation-graph
    type: ucf.svc.ia-animation-graph-microservice
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  - name: avatar-renderer
    type: ucf.svc.ia-omniverse-renderer-microservice
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  - name: anim-graph-sdr
    type: ucf.svc.tokkio.anim-graph-sdr-envoy
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: renderer-sdr
    type: ucf.svc.tokkio.renderer-sdr-envoy
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: audio2face-with-emotion
    type: ucf.svc.audio2face
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: tokkio-umim-action-server
    type: ucf.svc.tokkio.umim-action-server
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: renderer-adapter-endpoint
    type: ucf.svc.external-endpoint
    parameters:
      service: localhost
      port: 31111
connections:
  vms/redis: redis-timeseries/redis
  chat-engine/plugin-server: plugin-server/http-api
  chat-engine/redis: redis-timeseries/redis
  chat-controller/redis: redis-timeseries/redis
  chat-controller/riva: riva-speech/riva-speech-api
  chat-controller/chat-api: chat-engine/http-api
  chat-controller/ov-a2f-grpc: audio2face-with-emotion/a2f-grpc-server
  ds-visionai/redis: redis-timeseries/redis
  occupancy-alerts-api/redis: redis-timeseries/redis
  occupancy-alerts-api/mongodb: mongodb/mongo
  occupancy-alerts/redis: redis-timeseries/redis
  occupancy-alerts/occupancy-alerts-api: occupancy-alerts-api/http-api
  tokkio-ui-server/vms: vms/vms
  tokkio-ui-server/redis: redis-timeseries/redis
  tokkio-ui-server/chat-controller: chat-controller/grpc-api
  tokkio-ingress-mgr/redis: redis/redis
  tokkio-ingress-mgr/redis-ts: redis-timeseries/redis
  tokkio-ingress-mgr/ui-server: tokkio-ui-server/ui-server-http
  tokkio-ingress-mgr/vms: vms/vms
  tokkio-chat-controller-sdr/redis: redis-timeseries/redis
  tokkio-chat-controller-sdr/chat-controller: chat-controller/http-api
  tokkio-chat-controller-sdr/vms: vms/vms
  tokkio-ds-sdr/redis: redis-timeseries/redis
  tokkio-ds-sdr/httpds: ds-visionai/http-api
  tokkio-ds-sdr/vms: vms/vms
  anim-graph-sdr/redis: redis-timeseries/redis
  anim-graph-sdr/anim-graph: animation-graph/http-api
  anim-graph-sdr/vms: vms/vms
  renderer-sdr/redis: redis-timeseries/redis
  renderer-sdr/vms: vms/vms
  renderer-sdr/renderer: avatar-renderer/http-api
  audio2face-with-emotion/a2f-grpc-client: anim-graph-sdr/grpc-envoy
  avatar-renderer/rtp-negot: vms/vms-grpc
  avatar-renderer/anim-source: anim-graph-sdr/grpc-envoy
  tokkio-umim-action-server/redis: redis-timeseries/redis
  tokkio-umim-action-server/anim-graph: animation-graph/http-api
  tokkio-umim-action-server/ui-server: tokkio-ui-server/ui-server-http
````

## File: workflows/tokkio/4.1/llm-rag-ov/3-stream/config/lp_grpc_in_udp_out_anim_tuning.json
````json
{
    "lp_config": {
      "animation_cropping_mode": "ANIMATION_CROPPING_MODE_BLEND",
      "model_selection": "MODEL_SELECTION_PERF",
      "eye_blink_config": {
        "blink_frequency": {
          "value": 15,
          "unit": "UNIT_TIMES_PER_MINUTE"
        },
        "blink_duration": {
          "value": 6,
          "unit": "UNIT_FRAME"
        }
      },
      "gaze_look_away_config": {
        "enable_gaze_look_away": false,
        "max_look_away_offset": {
          "value": 20,
          "unit": "UNIT_DEGREE_ANGLE"
        },
        "min_look_away_interval": {
          "value": 240,
          "unit": "UNIT_FRAME"
        },
        "look_away_interval_range": {
          "value": 60,
          "unit": "UNIT_FRAME"
        }
      },
      "mouth_expression_config": {
        "mouth_expression_multiplier": 1.0
      }
    },
    "endpoint_config": {
      "input_media_config": {
        "audio_input_config": {
          "stream_config": {
            "stream_type": "GRPC"
          },
          "channels": 1,
          "channel_index": 0,
          "layout": "AUDIO_LAYOUT_INTERLEAVED",
          "sample_rate_hz": 16000,
          "chunk_duration_ms": 33,
          "encoding": "AUDIO_ENCODING_RAW",
          "decoder_config": {
            "raw_dec_config": {
              "format": "AUDIO_FORMAT_S16LE"
            }
          }
        }
      },
      "output_media_config": {
        "audio_output_config": {
          "stream_config": {
            "stream_type": "UDP",
            "udp_params": {
              "host": "127.0.0.1",
              "port": "9017"
            }
          },
          "payloader_config": {
            "type": "PAYLOADER_RTP"
          },
          "sample_rate_hz": 16000,
          "chunk_duration_ms": 33,
          "encoding": "AUDIO_ENCODING_RAW",
          "encoder_config": {
            "raw_enc_config": {
              "format": "AUDIO_FORMAT_S16BE"
            }
          }
        },
        "video_output_config": {
          "stream_config": {
            "stream_type": "UDP",
            "udp_params": {
                "host": "127.0.0.1",
                "port": "9019"
              }
          },
          "payloader_config": {
            "type": "PAYLOADER_RTP"
          },
          "encoding": "H264",
          "encoder_config": {
            "h264_enc_config": {
              "idr_frame_interval": 30
            }
          }
        }
      }
    },
    "quality_profile": "SPEECH_LP_QUALITY_PROFILE_LOW_LATENCY"
  }
````

## File: workflows/tokkio/4.1/llm-rag-ov/3-stream/tokkio-app-params.yaml
````yaml
---
chat-controller:
  ipaDictPath: cmudict_ipa.txt
  pipelineParams:
    grpc_server:
      nvidia::rrt::BotRuntimeGrpc:
        virtual_assistant_num_instances: 10
    riva_asr:
      RivaASR:
        enable_profanity_filter: false
    riva_logger:
      RivaLogger:
        enable_logging: true
    speech_pipeline_manager:
      SpeechPipelineManager:
        always_on: true
        initial_state: INIT
        tts_eos_delay_ms: 0
    riva_tts:
      RivaTTS:
        audio_start_threshold_ms: 2000
        chunk_duration_ms: 600
        sample_rate: 16000
        send_audio_in_realtime: true
        voice_name: English-US.Male-1
  speechConfigPath: speech_config.yaml
  wordBoostFilePath: asr_words_to_boost_conformer.txt
  configNgcPath: nvidia/ace/tokkio_plugin_llm_rag:4.1.2
  pipeline: avatar_umim
chat-engine:
  enableUserAttention: "true"
  interface: event
  logLevel: INFO
  botConfigName: tokkio_rag_bot_config.yml
  configNgcPath: nvidia/ace/tokkio_plugin_llm_rag:4.1.2
ds-visionai:
  checkInterval: '1'
  jitterbufferLatency: 2000
  peerPadIdSameAsSourceId: 'true'
  redisCfg:
    payloadkey: message
    topic: visionai
  rtspReconnectInterval: 10
  streammuxResolution:
    height: 720
    width: 1280
  videoSink: none
  ucfVisibleGpus:
    - 0
mongodb:
  storageClaims:
    local-storage:
      spec:
        resources:
          requests:
            storage: 1Gi
occupancy-alerts:
  analytics:
    fov:
      metrics:
        storage:
          granularity15MinRetentionMsec: 604800000
          granularity15SecRetentionMsec: 86400000
          granularity1MinRetentionMsec: 604800000
          granularity1SecRetentionMsec: 86400000
    gazeROI:
      frameBuffer: 5
    lipActivity:
      frameBuffer: 0
    roi:
      frameBuffer: 35
      metrics:
        storage:
          granularity15MinRetentionMsec: 604800000
          granularity15SecRetentionMsec: 86400000
          granularity1MinRetentionMsec: 604800000
          granularity1SecRetentionMsec: 86400000
      pixelBuffer: 25
    stateManagement:
      classTargets:
        - Face
      sensor:
        maxIdleTimeSec: 60
  metadataStream: visionai
  plugins:
    - className: core.analytics.plugins.ue.ue.UserEngagement
      config:
        exponentialSmootheningAlpha: 0.4
        resultRetensionWindowMaxSize: 3600
      name: UserEngagement
  sensorTemplate:
    alert_rules:
      fov:
        rules:
          - count_threshold: 1
            id: dm_fov
            parameters:
              - name: time_interval_up
                value: 0.25
              - name: time_interval_down
                value: 2
            rule_id: fov_occupancy_threshold
            rule_type: occupancy_threshold_switch
            time_interval: 1
            type: fov
      roi:
        rules: []
      tripwire:
        rules: []
      user_engagement:
        rules:
          - id: UE_CONFIG_1
            name: ALERT_RULE_1
            rule_type: zone_change
            time_interval: 1
    gaze_rois: []
    rois: []
    tripwires: []
    user_engagement:
      - angleBuffer: 5
        frameBuffer: 2
        id: UE_CONFIG_1
        zones:
          - from_angle: 0
            id: ENGAGED
            name: Near to Camera
            to_angle: 35
          - from_angle: 35
            id: DISTRACTED
            name: Away from Camera
            to_angle: 100
          - from_angle: 100
            id: DISENGAGED
            name: Away from Camera
            to_angle: 180
  sensors:
    - alert_rules:
        fov:
          rules:
            - count_threshold: 1
              id: dm_fov
              parameters:
                - name: time_interval_up
                  value: 1
                - name: time_interval_down
                  value: 2
              rule_id: fov_occupancy_threshold_default
              rule_type: occupancy_threshold_switch
              time_interval: 1
              type: fov
        roi:
          rules: []
        tripwire:
          rules: []
      sensorId: drive-thru-0
  storageClaims:
    data:
      spec:
        resources:
          requests:
            storage: 1Gi
  vstEventsStream: vst_events
  vstEventsStreamIntegration: 0
occupancy-alerts-api:
  configs:
    cv_config.yaml:
      data:
        metadata:
          maxTimeRangeInMSec: 500000
          minTimeRangeInMSec: 0
        trajectory:
          maxTimeRangeInMSec: 500000
          minTimeRangeInMSec: 0
  storageClaims:
    data:
      spec:
        resources:
          requests:
            storage: 1Gi
plugin-server:
  pluginConfigPath: plugin_config.yaml
  configNgcPath: nvidia/ace/tokkio_plugin_llm_rag:4.1.2
  pluginConfig:
    plugins:
      rag:
        parameters:
          USE_RAG: false
          RAG_SERVER_URL: http://127.0.0.1:8081
          NIM_MODEL: "meta/llama3-8b-instruct"
          USE_OPENAI: false
          OPENAI_MODEL: "gpt-4"
redis:
  storageClaims:
    data:
      spec:
        resources:
          requests:
            storage: 100Mi
redis-timeseries:
  storageClaims:
    data:
      spec:
        resources:
          requests:
            storage: 100Mi
riva-speech:
  modelRepoGenerator:
    clearAllRMIRSAndModels: false
    ngcModelConfigs:
      triton0:
        models:
          - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
          - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
  persistentVolumeClaim:
    keepPVC: true
  riva:
    visibleGpus: '0'
tokkio-ingress-mgr:
  accessControlAllowOrigin: \*
  bypassTokenValidation: false
  enableCookie: 'true'
  enableRedisPubsubListener: 'true'
  enableRedisTsListener: 'true'
  enableSessionRefresh: 'true'
  enableSessionTrigger: 'false'
  enableStarFleetProd: false
  enableStarFleetStg: false
  enableTracing: 'false'
  maxNumSession: '3'
  otelExporterOtlpEndpoint: http://0.0.0.0:4317
  otelExporterOtlpProtocol: grpc
  tokenMaxAge: '40'
  tokenTTL: '30'
tokkio-ui-server:
  botMSMaxRetry: '120'
  botMSPartialTranscript: '1'
  botMSRetryInterval: '500'
  cartProtocol: http
  cartSuffix: ''
  consoleLogLevel: debug
  fileLogLevel: debug
  menuProtocol: http
  menuSuffix: /api
  redisAceAgentKey: ace_agent_system_events
  redisFovKey: emdat_alert_events
  redisMaxRetry: '60'
  redisMlopsKey: mlops_ui
  redisRatingKey: user_rating
  redisRetryInterval: '1000'
  redisUiActionServerKey: ui_events
  redisVstKey: vst_events
  redisWdmKeyPrefix: wdm_error_events
  vmsProtocol: http
  vmsSuffix: /api
vms:
  applicationSpecs:
    vms:
      containers:
        vms-container:
          env:
            - name: NODE_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
      securityContext:
        fsGroup: 1000
  configs:
    rtsp_streams.json:
      streams:
        - audio:
            bits_per_sample: 32
            codec: pcm
            enabled: true
            port: 30032
            sample_rate_Hz: 44100
          enabled: false
          name: Tokkio_Avatar
          stream_in: udp
          video:
            codec: h264
            framerate: 30
            port: 30031
    vst_config.json:
      data:
        always_recording: true
        enable_avsync_udp_input: true
        enable_silent_audio_in_udp_input: false
        enable_udp_input_dump: false
        gpu_indices: []
        use_software_path: false
        use_standalone_udp_input: false
        use_webrtc_hw_dec: true
        use_webrtc_inbuilt_encoder: ''
        webrtc_in_fixed_resolution: 1280x720
        webrtc_in_max_framerate: 30
        webrtc_out_default_resolution: 1920x1080
        webrtc_out_enc_preset: ultra_fast
        webrtc_out_enc_quality_tuning: ultra_low_latency
        webrtc_out_set_idr_interval: 30
        use_external_peerconnection: false
      debug:
        enable_network_bandwidth_notification: true
      network:
        coturn_turnurl_list_with_secret: null
        enable_grpc: true
        grpc_server_port: 50051
        max_webrtc_in_connections: 10
        max_webrtc_out_connections: 10
        ntp_servers: null
        reverse_proxy_server_address: 0.0.0.0:100
        rtsp_streaming_over_tcp: true
        static_turnurl_list:
          - username:password@0.0.0.0:3478
        stunurl_list:
          - stun.l.google.com:19302
          - stun1.l.google.com:19302
        twilio_account_sid: '0000000000000000000000000000000000'
        twilio_auth_token: '00000000000000000000000000000000'
        udp_drop_on_latency: false
        udp_latency_ms: 200
        use_coturn_auth_secret: false
        use_reverse_proxy: false
        use_twilio_stun_turn: false
        webrtc_in_audio_sender_max_bitrate: 128000
        webrtc_in_video_bitrate_thresold_percentage: 50
        webrtc_in_video_degradation_preference: detail
        webrtc_in_video_sender_max_framerate: 30
        webrtc_port_range:
          max: 30030
          min: 30001
        webrtc_video_quality_tunning:
          resolution_1080:
            bitrate_range:
              - 10000
              - 35000
            bitrate_start: 25000
            qp_range_I:
              - 10
              - 30
            qp_range_P:
              - 10
              - 30
          resolution_1440:
            bitrate_range:
              - 20000
              - 55000
            bitrate_start: 50000
            qp_range_I:
              - 0
              - 15
            qp_range_P:
              - 0
              - 15
          resolution_2160:
            bitrate_range:
              - 30000
              - 80000
            bitrate_start: 70000
            qp_range_I:
              - 0
              - 20
            qp_range_P:
              - 0
              - 20
          resolution_480:
            bitrate_range:
              - 800
              - 3000
            bitrate_start: 1000
            qp_range_I:
              - 10
              - 30
            qp_range_P:
              - 10
              - 30
          resolution_720:
            bitrate_range:
              - 3000
              - 10000
            bitrate_start: 5000
            qp_range_I:
              - 10
              - 30
            qp_range_P:
              - 10
              - 30
      notifications:
        enable_notification: true
        message_broker_topic: vst_events
        redis_server_env_var: REDIS_TIMESERIES_REDIS_TIMESERIES_SVC_SERVICE_HOST:6379
        use_message_broker: redis
      security:
        enable_user_cleanup: true
        multi_user_extra_options:
          - Secure
          - SameSite=none
        session_max_age_sec: 2592000
        use_http_digest_authentication: false
        use_https: false
        use_multi_user: false
    vst_storage.json:
      total_video_storage_size_MB: 100000
  storageClaims:
    local-storage:
      spec:
        resources:
          requests:
            storage: 10Gi
  ucfVisibleGpus:
    - 0
animation-graph:
  animationServer:
    maxStreamCapacity: 3
  ucfVisibleGpus:
    - 3
  resourceDownload:
    image: nvcr.io/nvidia/ace/ngc-resource-downloader:1.1.4
    remoteResourcePath: nvidia/ace/default-avatar-scene:1.0.0
    secretName: ngc-api-key-secret
audio2face-with-emotion:
  configs:
    a2f_config.yaml:
      streamNumber: '3'
      a2eEnabled: 'False'
      a2eInferenceInterval: '10'
      a2fModelName: claire_v1.3
  ucfVisibleGpus:
    - 0
avatar-renderer:
  deployment:
    gpuAllocLimit: 1
  replicas: 3
  ucfVisibleGpus:
    - 1
    - 2
    - 3
  livestream:
    audioDelay: 0.13
  resourceDownload:
    image: nvcr.io/nvidia/ace/ngc-resource-downloader:1.1.4
    remoteResourcePath: nvidia/ace/default-avatar-scene:1.0.0
    secretName: ngc-api-key-secret
renderer-sdr:
  sdrMaxReplicas: '3'
````

## File: workflows/tokkio/4.1/llm-rag-ov/3-stream/tokkio-app.yaml
````yaml
---
specVersion: 2.5.0
version: 4.1.4
name: ucs-tokkio-app-base-3-stream-llm-rag-3d-ov
description: UCS Tokkio App
secrets:
  k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY:
    k8sSecret:
      secretName: ngc-api-key-secret
      key: NGC_CLI_API_KEY
  k8sSecret/nvidia-api-key-secret/NVIDIA_API_KEY:
    k8sSecret:
      secretName: nvidia-api-key-secret
      key: NVIDIA_API_KEY
  k8sSecret/openai-key-secret/OPENAI_API_KEY:
    k8sSecret:
      secretName: openai-key-secret
      key: OPENAI_API_KEY
dependencies:
  - ucf.svc.ace-agent.chat-controller:4.1.0
  - ucf.svc.ace-agent.chat-engine:4.1.0
  - ucf.svc.ace-agent.plugin-server:4.1.0
  - ucf.svc.audio2face:1.0.41
  - ucf.svc.core.mongodb:0.0.14
  - ucf.svc.core.redis-timeseries:0.0.20
  - ucf.svc.core.redis:0.0.20
  - ucf.svc.ds.visionai:0.4.43
  - ucf.svc.ia-animation-graph-microservice:1.0.2
  - ucf.svc.ia-omniverse-renderer-microservice:1.0.6
  - ucf.svc.metropolis.occupancy-alerts-api:0.1.56
  - ucf.svc.metropolis.occupancy-alerts:0.1.55
  - ucf.svc.riva.speech-skills:2.17.0
  - ucf.svc.tokkio.anim-graph-sdr-envoy:0.1.33
  - ucf.svc.tokkio.chat-controller-sdr-envoy:0.1.33
  - ucf.svc.tokkio.ds-sdr-envoy:0.1.33
  - ucf.svc.tokkio.ingress-mgr:0.2.32
  - ucf.svc.tokkio.renderer-sdr-envoy:0.1.33
  - ucf.svc.tokkio.ui-server:5.0.4
  - ucf.svc.tokkio.umim-action-server:1.0.3
  - ucf.svc.vms:1.2.41
components:
  - name: tokkio-ds-sdr
    type: ucf.svc.tokkio.ds-sdr-envoy
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: tokkio-chat-controller-sdr
    type: ucf.svc.tokkio.chat-controller-sdr-envoy
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: tokkio-ingress-mgr
    type: ucf.svc.tokkio.ingress-mgr
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: redis
    type: ucf.svc.core.redis
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: mongodb
    type: ucf.svc.core.mongodb
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: riva-speech
    type: ucf.svc.riva.speech-skills
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  - name: redis-timeseries
    type: ucf.svc.core.redis-timeseries
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: chat-engine
    type: ucf.svc.ace-agent.chat-engine
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
      openai-key-secret: k8sSecret/openai-key-secret/OPENAI_API_KEY
  - name: chat-controller
    type: ucf.svc.ace-agent.chat-controller
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
    files:
      lp_config.json: ./config/lp_grpc_in_udp_out_anim_tuning.json
  - name: ds-visionai
    type: ucf.svc.ds.visionai
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: vms
    type: ucf.svc.vms
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: occupancy-alerts-api
    type: ucf.svc.metropolis.occupancy-alerts-api
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: occupancy-alerts
    type: ucf.svc.metropolis.occupancy-alerts
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: plugin-server
    type: ucf.svc.ace-agent.plugin-server
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
      openai-key-secret: k8sSecret/openai-key-secret/OPENAI_API_KEY
      nvidia-api-key-secret: k8sSecret/nvidia-api-key-secret/NVIDIA_API_KEY
  - name: tokkio-ui-server
    type: ucf.svc.tokkio.ui-server
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: animation-graph
    type: ucf.svc.ia-animation-graph-microservice
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  - name: avatar-renderer
    type: ucf.svc.ia-omniverse-renderer-microservice
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  - name: anim-graph-sdr
    type: ucf.svc.tokkio.anim-graph-sdr-envoy
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: renderer-sdr
    type: ucf.svc.tokkio.renderer-sdr-envoy
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: audio2face-with-emotion
    type: ucf.svc.audio2face
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: tokkio-umim-action-server
    type: ucf.svc.tokkio.umim-action-server
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: renderer-adapter-endpoint
    type: ucf.svc.external-endpoint
    parameters:
      service: localhost
      port: 31111
connections:
  vms/redis: redis-timeseries/redis
  chat-engine/plugin-server: plugin-server/http-api
  chat-engine/redis: redis-timeseries/redis
  chat-controller/redis: redis-timeseries/redis
  chat-controller/riva: riva-speech/riva-speech-api
  chat-controller/chat-api: chat-engine/http-api
  chat-controller/ov-a2f-grpc: audio2face-with-emotion/a2f-grpc-server
  ds-visionai/redis: redis-timeseries/redis
  occupancy-alerts-api/redis: redis-timeseries/redis
  occupancy-alerts-api/mongodb: mongodb/mongo
  occupancy-alerts/redis: redis-timeseries/redis
  occupancy-alerts/occupancy-alerts-api: occupancy-alerts-api/http-api
  tokkio-ui-server/vms: vms/vms
  tokkio-ui-server/redis: redis-timeseries/redis
  tokkio-ui-server/chat-controller: chat-controller/grpc-api
  tokkio-ingress-mgr/redis: redis/redis
  tokkio-ingress-mgr/redis-ts: redis-timeseries/redis
  tokkio-ingress-mgr/ui-server: tokkio-ui-server/ui-server-http
  tokkio-ingress-mgr/vms: vms/vms
  tokkio-chat-controller-sdr/redis: redis-timeseries/redis
  tokkio-chat-controller-sdr/chat-controller: chat-controller/http-api
  tokkio-chat-controller-sdr/vms: vms/vms
  tokkio-ds-sdr/redis: redis-timeseries/redis
  tokkio-ds-sdr/httpds: ds-visionai/http-api
  tokkio-ds-sdr/vms: vms/vms
  anim-graph-sdr/redis: redis-timeseries/redis
  anim-graph-sdr/anim-graph: animation-graph/http-api
  anim-graph-sdr/vms: vms/vms
  renderer-sdr/redis: redis-timeseries/redis
  renderer-sdr/vms: vms/vms
  renderer-sdr/renderer: avatar-renderer/http-api
  audio2face-with-emotion/a2f-grpc-client: anim-graph-sdr/grpc-envoy
  avatar-renderer/rtp-negot: vms/vms-grpc
  avatar-renderer/anim-source: anim-graph-sdr/grpc-envoy
  tokkio-umim-action-server/redis: redis-timeseries/redis
  tokkio-umim-action-server/anim-graph: animation-graph/http-api
  tokkio-umim-action-server/ui-server: tokkio-ui-server/ui-server-http
````

## File: workflows/tokkio/4.1/llm-rag-ov/6-stream/config/lp_grpc_in_udp_out_anim_tuning.json
````json
{
    "lp_config": {
      "animation_cropping_mode": "ANIMATION_CROPPING_MODE_BLEND",
      "model_selection": "MODEL_SELECTION_PERF",
      "eye_blink_config": {
        "blink_frequency": {
          "value": 15,
          "unit": "UNIT_TIMES_PER_MINUTE"
        },
        "blink_duration": {
          "value": 6,
          "unit": "UNIT_FRAME"
        }
      },
      "gaze_look_away_config": {
        "enable_gaze_look_away": false,
        "max_look_away_offset": {
          "value": 20,
          "unit": "UNIT_DEGREE_ANGLE"
        },
        "min_look_away_interval": {
          "value": 240,
          "unit": "UNIT_FRAME"
        },
        "look_away_interval_range": {
          "value": 60,
          "unit": "UNIT_FRAME"
        }
      },
      "mouth_expression_config": {
        "mouth_expression_multiplier": 1.0
      }
    },
    "endpoint_config": {
      "input_media_config": {
        "audio_input_config": {
          "stream_config": {
            "stream_type": "GRPC"
          },
          "channels": 1,
          "channel_index": 0,
          "layout": "AUDIO_LAYOUT_INTERLEAVED",
          "sample_rate_hz": 16000,
          "chunk_duration_ms": 33,
          "encoding": "AUDIO_ENCODING_RAW",
          "decoder_config": {
            "raw_dec_config": {
              "format": "AUDIO_FORMAT_S16LE"
            }
          }
        }
      },
      "output_media_config": {
        "audio_output_config": {
          "stream_config": {
            "stream_type": "UDP",
            "udp_params": {
              "host": "127.0.0.1",
              "port": "9017"
            }
          },
          "payloader_config": {
            "type": "PAYLOADER_RTP"
          },
          "sample_rate_hz": 16000,
          "chunk_duration_ms": 33,
          "encoding": "AUDIO_ENCODING_RAW",
          "encoder_config": {
            "raw_enc_config": {
              "format": "AUDIO_FORMAT_S16BE"
            }
          }
        },
        "video_output_config": {
          "stream_config": {
            "stream_type": "UDP",
            "udp_params": {
                "host": "127.0.0.1",
                "port": "9019"
              }
          },
          "payloader_config": {
            "type": "PAYLOADER_RTP"
          },
          "encoding": "H264",
          "encoder_config": {
            "h264_enc_config": {
              "idr_frame_interval": 30
            }
          }
        }
      }
    },
    "quality_profile": "SPEECH_LP_QUALITY_PROFILE_LOW_LATENCY"
  }
````

## File: workflows/tokkio/4.1/llm-rag-ov/6-stream/tokkio-app-params.yaml
````yaml
---
chat-controller:
  ipaDictPath: cmudict_ipa.txt
  pipelineParams:
    grpc_server:
      nvidia::rrt::BotRuntimeGrpc:
        virtual_assistant_num_instances: 10
    riva_asr:
      RivaASR:
        enable_profanity_filter: false
    riva_logger:
      RivaLogger:
        enable_logging: true
    speech_pipeline_manager:
      SpeechPipelineManager:
        always_on: true
        initial_state: INIT
        tts_eos_delay_ms: 0
    riva_tts:
      RivaTTS:
        audio_start_threshold_ms: 2000
        chunk_duration_ms: 600
        sample_rate: 16000
        send_audio_in_realtime: true
        voice_name: English-US.Male-1
  speechConfigPath: speech_config.yaml
  wordBoostFilePath: asr_words_to_boost_conformer.txt
  configNgcPath: nvidia/ace/tokkio_plugin_llm_rag:4.1.2
  pipeline: avatar_umim
chat-engine:
  enableUserAttention: "true"
  interface: event
  logLevel: INFO
  botConfigName: tokkio_rag_bot_config.yml
  configNgcPath: nvidia/ace/tokkio_plugin_llm_rag:4.1.2
ds-visionai:
  checkInterval: '1'
  jitterbufferLatency: 2000
  peerPadIdSameAsSourceId: 'true'
  redisCfg:
    payloadkey: message
    topic: visionai
  rtspReconnectInterval: 10
  streammuxResolution:
    height: 720
    width: 1280
  videoSink: none
  ucfVisibleGpus:
    - 0
mongodb:
  storageClaims:
    local-storage:
      spec:
        resources:
          requests:
            storage: 1Gi
occupancy-alerts:
  analytics:
    fov:
      metrics:
        storage:
          granularity15MinRetentionMsec: 604800000
          granularity15SecRetentionMsec: 86400000
          granularity1MinRetentionMsec: 604800000
          granularity1SecRetentionMsec: 86400000
    gazeROI:
      frameBuffer: 5
    lipActivity:
      frameBuffer: 0
    roi:
      frameBuffer: 35
      metrics:
        storage:
          granularity15MinRetentionMsec: 604800000
          granularity15SecRetentionMsec: 86400000
          granularity1MinRetentionMsec: 604800000
          granularity1SecRetentionMsec: 86400000
      pixelBuffer: 25
    stateManagement:
      classTargets:
        - Face
      sensor:
        maxIdleTimeSec: 60
  metadataStream: visionai
  plugins:
    - className: core.analytics.plugins.ue.ue.UserEngagement
      config:
        exponentialSmootheningAlpha: 0.4
        resultRetensionWindowMaxSize: 3600
      name: UserEngagement
  sensorTemplate:
    alert_rules:
      fov:
        rules:
          - count_threshold: 1
            id: dm_fov
            parameters:
              - name: time_interval_up
                value: 0.25
              - name: time_interval_down
                value: 2
            rule_id: fov_occupancy_threshold
            rule_type: occupancy_threshold_switch
            time_interval: 1
            type: fov
      roi:
        rules: []
      tripwire:
        rules: []
      user_engagement:
        rules:
          - id: UE_CONFIG_1
            name: ALERT_RULE_1
            rule_type: zone_change
            time_interval: 1
    gaze_rois: []
    rois: []
    tripwires: []
    user_engagement:
      - angleBuffer: 5
        frameBuffer: 2
        id: UE_CONFIG_1
        zones:
          - from_angle: 0
            id: ENGAGED
            name: Near to Camera
            to_angle: 35
          - from_angle: 35
            id: DISTRACTED
            name: Away from Camera
            to_angle: 100
          - from_angle: 100
            id: DISENGAGED
            name: Away from Camera
            to_angle: 180
  sensors:
    - alert_rules:
        fov:
          rules:
            - count_threshold: 1
              id: dm_fov
              parameters:
                - name: time_interval_up
                  value: 1
                - name: time_interval_down
                  value: 2
              rule_id: fov_occupancy_threshold_default
              rule_type: occupancy_threshold_switch
              time_interval: 1
              type: fov
        roi:
          rules: []
        tripwire:
          rules: []
      sensorId: drive-thru-0
  storageClaims:
    data:
      spec:
        resources:
          requests:
            storage: 1Gi
  vstEventsStream: vst_events
  vstEventsStreamIntegration: 0
occupancy-alerts-api:
  configs:
    cv_config.yaml:
      data:
        metadata:
          maxTimeRangeInMSec: 500000
          minTimeRangeInMSec: 0
        trajectory:
          maxTimeRangeInMSec: 500000
          minTimeRangeInMSec: 0
  storageClaims:
    data:
      spec:
        resources:
          requests:
            storage: 1Gi
plugin-server:
  pluginConfigPath: plugin_config.yaml
  configNgcPath: nvidia/ace/tokkio_plugin_llm_rag:4.1.2
  pluginConfig:
    plugins:
      rag:
        parameters:
          USE_RAG: false
          RAG_SERVER_URL: http://127.0.0.1:8081
          NIM_MODEL: "meta/llama3-8b-instruct"
          USE_OPENAI: false
          OPENAI_MODEL: "gpt-4"
redis:
  storageClaims:
    data:
      spec:
        resources:
          requests:
            storage: 100Mi
redis-timeseries:
  storageClaims:
    data:
      spec:
        resources:
          requests:
            storage: 100Mi
riva-speech:
  modelRepoGenerator:
    clearAllRMIRSAndModels: false
    ngcModelConfigs:
      triton0:
        models:
          - nvidia/ace/rmir_asr_parakeet_1-1b_en_us_str_vad:2.17.0
          - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.17.0
  persistentVolumeClaim:
    keepPVC: true
  riva:
    visibleGpus: '0'
tokkio-ingress-mgr:
  accessControlAllowOrigin: \*
  bypassTokenValidation: false
  enableCookie: 'true'
  enableRedisPubsubListener: 'true'
  enableRedisTsListener: 'true'
  enableSessionRefresh: 'true'
  enableSessionTrigger: 'false'
  enableStarFleetProd: false
  enableStarFleetStg: false
  enableTracing: 'false'
  maxNumSession: '6'
  otelExporterOtlpEndpoint: http://0.0.0.0:4317
  otelExporterOtlpProtocol: grpc
  tokenMaxAge: '40'
  tokenTTL: '30'
tokkio-ui-server:
  botMSMaxRetry: '120'
  botMSPartialTranscript: '1'
  botMSRetryInterval: '500'
  cartProtocol: http
  cartSuffix: ''
  consoleLogLevel: debug
  fileLogLevel: debug
  menuProtocol: http
  menuSuffix: /api
  redisAceAgentKey: ace_agent_system_events
  redisFovKey: emdat_alert_events
  redisMaxRetry: '60'
  redisMlopsKey: mlops_ui
  redisRatingKey: user_rating
  redisRetryInterval: '1000'
  redisUiActionServerKey: ui_events
  redisVstKey: vst_events
  redisWdmKeyPrefix: wdm_error_events
  vmsProtocol: http
  vmsSuffix: /api
vms:
  applicationSpecs:
    vms:
      containers:
        vms-container:
          env:
            - name: NODE_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
      securityContext:
        fsGroup: 1000
  configs:
    rtsp_streams.json:
      streams:
        - audio:
            bits_per_sample: 32
            codec: pcm
            enabled: true
            port: 30032
            sample_rate_Hz: 44100
          enabled: false
          name: Tokkio_Avatar
          stream_in: udp
          video:
            codec: h264
            framerate: 30
            port: 30031
    vst_config.json:
      data:
        always_recording: true
        enable_avsync_udp_input: true
        enable_silent_audio_in_udp_input: false
        enable_udp_input_dump: false
        gpu_indices: []
        use_software_path: false
        use_standalone_udp_input: false
        use_webrtc_hw_dec: true
        use_webrtc_inbuilt_encoder: ''
        webrtc_in_fixed_resolution: 1280x720
        webrtc_in_max_framerate: 30
        webrtc_out_default_resolution: 1920x1080
        webrtc_out_enc_preset: ultra_fast
        webrtc_out_enc_quality_tuning: ultra_low_latency
        webrtc_out_set_idr_interval: 30
        use_external_peerconnection: false
      debug:
        enable_network_bandwidth_notification: true
      network:
        coturn_turnurl_list_with_secret: null
        enable_grpc: true
        grpc_server_port: 50051
        max_webrtc_in_connections: 20
        max_webrtc_out_connections: 20
        ntp_servers: null
        reverse_proxy_server_address: 0.0.0.0:100
        rtsp_streaming_over_tcp: true
        static_turnurl_list:
          - username:password@0.0.0.0:3478
        stunurl_list:
          - stun.l.google.com:19302
          - stun1.l.google.com:19302
        twilio_account_sid: '0000000000000000000000000000000000'
        twilio_auth_token: '00000000000000000000000000000000'
        udp_drop_on_latency: false
        udp_latency_ms: 200
        use_coturn_auth_secret: false
        use_reverse_proxy: false
        use_twilio_stun_turn: false
        webrtc_in_audio_sender_max_bitrate: 128000
        webrtc_in_video_bitrate_thresold_percentage: 50
        webrtc_in_video_degradation_preference: detail
        webrtc_in_video_sender_max_framerate: 30
        webrtc_port_range:
          max: 30030
          min: 30001
        webrtc_video_quality_tunning:
          resolution_1080:
            bitrate_range:
              - 10000
              - 35000
            bitrate_start: 25000
            qp_range_I:
              - 10
              - 30
            qp_range_P:
              - 10
              - 30
          resolution_1440:
            bitrate_range:
              - 20000
              - 55000
            bitrate_start: 50000
            qp_range_I:
              - 0
              - 15
            qp_range_P:
              - 0
              - 15
          resolution_2160:
            bitrate_range:
              - 30000
              - 80000
            bitrate_start: 70000
            qp_range_I:
              - 0
              - 20
            qp_range_P:
              - 0
              - 20
          resolution_480:
            bitrate_range:
              - 800
              - 3000
            bitrate_start: 1000
            qp_range_I:
              - 10
              - 30
            qp_range_P:
              - 10
              - 30
          resolution_720:
            bitrate_range:
              - 3000
              - 10000
            bitrate_start: 5000
            qp_range_I:
              - 10
              - 30
            qp_range_P:
              - 10
              - 30
      notifications:
        enable_notification: true
        message_broker_topic: vst_events
        redis_server_env_var: REDIS_TIMESERIES_REDIS_TIMESERIES_SVC_SERVICE_HOST:6379
        use_message_broker: redis
      security:
        enable_user_cleanup: true
        multi_user_extra_options:
          - Secure
          - SameSite=none
        session_max_age_sec: 2592000
        use_http_digest_authentication: false
        use_https: false
        use_multi_user: false
    vst_storage.json:
      total_video_storage_size_MB: 100000
  storageClaims:
    local-storage:
      spec:
        resources:
          requests:
            storage: 10Gi
  ucfVisibleGpus:
    - 0
animation-graph:
  animationServer:
    maxStreamCapacity: 6
  ucfVisibleGpus:
    - 3
  resourceDownload:
    image: nvcr.io/nvidia/ace/ngc-resource-downloader:1.1.4
    remoteResourcePath: nvidia/ace/default-avatar-scene:1.0.0
    secretName: ngc-api-key-secret
audio2face-with-emotion:
  configs:
    a2f_config.yaml:
      streamNumber: '6'
      a2eEnabled: 'False'
      a2eInferenceInterval: '10'
      a2fModelName: claire_v1.3
  ucfVisibleGpus:
    - 0
avatar-renderer:
  deployment:
    gpuAllocLimit: 2
  replicas: 6
  ucfVisibleGpus:
    - 1
    - 2
    - 3
  livestream:
    audioDelay: 0.13
  resourceDownload:
    image: nvcr.io/nvidia/ace/ngc-resource-downloader:1.1.4
    remoteResourcePath: nvidia/ace/default-avatar-scene:1.0.0
    secretName: ngc-api-key-secret
renderer-sdr:
  sdrMaxReplicas: '6'
````

## File: workflows/tokkio/4.1/llm-rag-ov/6-stream/tokkio-app.yaml
````yaml
---
specVersion: 2.5.0
version: 4.1.4
name: ucs-tokkio-app-base-6-stream-llm-rag-3d-ov
description: UCS Tokkio App
secrets:
  k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY:
    k8sSecret:
      secretName: ngc-api-key-secret
      key: NGC_CLI_API_KEY
  k8sSecret/nvidia-api-key-secret/NVIDIA_API_KEY:
    k8sSecret:
      secretName: nvidia-api-key-secret
      key: NVIDIA_API_KEY
  k8sSecret/openai-key-secret/OPENAI_API_KEY:
    k8sSecret:
      secretName: openai-key-secret
      key: OPENAI_API_KEY
dependencies:
  - ucf.svc.ace-agent.chat-controller:4.1.0
  - ucf.svc.ace-agent.chat-engine:4.1.0
  - ucf.svc.ace-agent.plugin-server:4.1.0
  - ucf.svc.audio2face:1.0.41
  - ucf.svc.core.mongodb:0.0.14
  - ucf.svc.core.redis-timeseries:0.0.20
  - ucf.svc.core.redis:0.0.20
  - ucf.svc.ds.visionai:0.4.43
  - ucf.svc.ia-animation-graph-microservice:1.0.2
  - ucf.svc.ia-omniverse-renderer-microservice:1.0.6
  - ucf.svc.metropolis.occupancy-alerts-api:0.1.56
  - ucf.svc.metropolis.occupancy-alerts:0.1.55
  - ucf.svc.riva.speech-skills:2.17.0
  - ucf.svc.tokkio.anim-graph-sdr-envoy:0.1.33
  - ucf.svc.tokkio.chat-controller-sdr-envoy:0.1.33
  - ucf.svc.tokkio.ds-sdr-envoy:0.1.33
  - ucf.svc.tokkio.ingress-mgr:0.2.32
  - ucf.svc.tokkio.renderer-sdr-envoy:0.1.33
  - ucf.svc.tokkio.ui-server:5.0.4
  - ucf.svc.tokkio.umim-action-server:1.0.3
  - ucf.svc.vms:1.2.41
components:
  - name: tokkio-ds-sdr
    type: ucf.svc.tokkio.ds-sdr-envoy
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: tokkio-chat-controller-sdr
    type: ucf.svc.tokkio.chat-controller-sdr-envoy
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: tokkio-ingress-mgr
    type: ucf.svc.tokkio.ingress-mgr
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: redis
    type: ucf.svc.core.redis
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: mongodb
    type: ucf.svc.core.mongodb
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: riva-speech
    type: ucf.svc.riva.speech-skills
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  - name: redis-timeseries
    type: ucf.svc.core.redis-timeseries
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: chat-engine
    type: ucf.svc.ace-agent.chat-engine
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
      openai-key-secret: k8sSecret/openai-key-secret/OPENAI_API_KEY
  - name: chat-controller
    type: ucf.svc.ace-agent.chat-controller
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
    files:
      lp_config.json: ./config/lp_grpc_in_udp_out_anim_tuning.json
  - name: ds-visionai
    type: ucf.svc.ds.visionai
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: vms
    type: ucf.svc.vms
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: occupancy-alerts-api
    type: ucf.svc.metropolis.occupancy-alerts-api
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: occupancy-alerts
    type: ucf.svc.metropolis.occupancy-alerts
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: plugin-server
    type: ucf.svc.ace-agent.plugin-server
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
      openai-key-secret: k8sSecret/openai-key-secret/OPENAI_API_KEY
      nvidia-api-key-secret: k8sSecret/nvidia-api-key-secret/NVIDIA_API_KEY
  - name: tokkio-ui-server
    type: ucf.svc.tokkio.ui-server
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: animation-graph
    type: ucf.svc.ia-animation-graph-microservice
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  - name: avatar-renderer
    type: ucf.svc.ia-omniverse-renderer-microservice
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
    secrets:
      ngc-api-key-secret: k8sSecret/ngc-api-key-secret/NGC_CLI_API_KEY
  - name: anim-graph-sdr
    type: ucf.svc.tokkio.anim-graph-sdr-envoy
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: renderer-sdr
    type: ucf.svc.tokkio.renderer-sdr-envoy
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: audio2face-with-emotion
    type: ucf.svc.audio2face
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: tokkio-umim-action-server
    type: ucf.svc.tokkio.umim-action-server
    parameters:
      imagePullSecrets:
        - name: ngc-docker-reg-secret
  - name: renderer-adapter-endpoint
    type: ucf.svc.external-endpoint
    parameters:
      service: localhost
      port: 31111
connections:
  vms/redis: redis-timeseries/redis
  chat-engine/plugin-server: plugin-server/http-api
  chat-engine/redis: redis-timeseries/redis
  chat-controller/redis: redis-timeseries/redis
  chat-controller/riva: riva-speech/riva-speech-api
  chat-controller/chat-api: chat-engine/http-api
  chat-controller/ov-a2f-grpc: audio2face-with-emotion/a2f-grpc-server
  ds-visionai/redis: redis-timeseries/redis
  occupancy-alerts-api/redis: redis-timeseries/redis
  occupancy-alerts-api/mongodb: mongodb/mongo
  occupancy-alerts/redis: redis-timeseries/redis
  occupancy-alerts/occupancy-alerts-api: occupancy-alerts-api/http-api
  tokkio-ui-server/vms: vms/vms
  tokkio-ui-server/redis: redis-timeseries/redis
  tokkio-ui-server/chat-controller: chat-controller/grpc-api
  tokkio-ingress-mgr/redis: redis/redis
  tokkio-ingress-mgr/redis-ts: redis-timeseries/redis
  tokkio-ingress-mgr/ui-server: tokkio-ui-server/ui-server-http
  tokkio-ingress-mgr/vms: vms/vms
  tokkio-chat-controller-sdr/redis: redis-timeseries/redis
  tokkio-chat-controller-sdr/chat-controller: chat-controller/http-api
  tokkio-chat-controller-sdr/vms: vms/vms
  tokkio-ds-sdr/redis: redis-timeseries/redis
  tokkio-ds-sdr/httpds: ds-visionai/http-api
  tokkio-ds-sdr/vms: vms/vms
  anim-graph-sdr/redis: redis-timeseries/redis
  anim-graph-sdr/anim-graph: animation-graph/http-api
  anim-graph-sdr/vms: vms/vms
  renderer-sdr/redis: redis-timeseries/redis
  renderer-sdr/vms: vms/vms
  renderer-sdr/renderer: avatar-renderer/http-api
  audio2face-with-emotion/a2f-grpc-client: anim-graph-sdr/grpc-envoy
  avatar-renderer/rtp-negot: vms/vms-grpc
  avatar-renderer/anim-source: anim-graph-sdr/grpc-envoy
  tokkio-umim-action-server/redis: redis-timeseries/redis
  tokkio-umim-action-server/anim-graph: animation-graph/http-api
  tokkio-umim-action-server/ui-server: tokkio-ui-server/ui-server-http
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/config-files/helm-release/rproxy-override-values.yml
````yaml
rProxySpec:
  publicInterfaceName: "{{ task_vars.network_interface_name }}"
  privateInterfaceName: "{{ task_vars.network_interface_name }}"
  imagePullSecrets:
  - name: ngc-docker-reg-secret
  checkIPUri: "http://checkip.amazonaws.com"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/config-files/helm-release/tokkio-app-audio-video-app.yml
````yaml
vms:
  configs:
    vst_config.json:
      network:
        twilio_account_sid: "{{ task_vars.twilio_account_sid }}"
        twilio_auth_token: "{{ task_vars.twilio_auth_token }}"
        use_twilio_stun_turn: {{ task_vars.use_twilio_stun_turn | bool | lower }}
        static_turnurl_list:
        - "{{ task_vars.turnurl_conn_string }}"
        reverse_proxy_server_address: "{{ task_vars.reverse_proxy_server_address }}:100"
        use_reverse_proxy: {{ task_vars.use_reverse_proxy | bool | lower }}
tokkio-ingress-mgr:
  enableStarFleetStg: {{ task_vars.enable_idp_auth | bool | lower }}

ia-unreal-renderer-microservice:
  configs:
    IAUEMS_SIGNALLING_SERVER_PEER_CONNECTION_OPTIONS: "{\"iceServers\": [{\"urls\": [\"turn:{{ task_vars.turn_server_conn }}\"], \"username\": \"{{ task_vars.turn_username }}\", \"credential\": \"{{ task_vars.turn_password }}\"}], \"iceTransportPolicy\": \"relay\"}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/config-files/helm-release/tokkio-fluent-bit-override-values.yml
````yaml
fluent-bit:
  enable: true
  config:
    service: |
      [SERVICE]
          Daemon Off
          Flush {{ '{{' }} .Values.flush {{ '}}' }}
          Log_Level {{ '{{' }} .Values.logLevel {{ '}}' }}
          Parsers_File parsers.conf
          Parsers_File custom_parsers.conf
          HTTP_Server On
          HTTP_Listen 0.0.0.0
          HTTP_Port {{ '{{' }} .Values.metricsPort {{ '}}' }}
          Health_Check On

    ## https://docs.fluentbit.io/manual/pipeline/inputs
    inputs: |
      [INPUT]
          Name tail
          Path /var/log/containers/*.log
          multiline.parser docker, cri
          Tag kube.*
          Mem_Buf_Limit 5MB
          Skip_Long_Lines On

      [INPUT]
          Name systemd
          Tag host.*
          Systemd_Filter _SYSTEMD_UNIT=kubelet.service
          Read_From_Tail On

    ## https://docs.fluentbit.io/manual/pipeline/filters
    filters: |
      [FILTER]
          Name kubernetes
          Match kube.*
          Merge_Log On
          Keep_Log Off
          K8S-Logging.Parser On
          K8S-Logging.Exclude On

      [FILTER]
          Name grep
          Match kube.*
          Exclude kubernetes_container_name fluent-bit

    ## https://docs.fluentbit.io/manual/pipeline/outputs
    outputs: |
      [OUTPUT]
          Name es
          Match kube.*
          Host tokkio-logging-es-cluster-master-headless
          # Logstash_Format On
          # Logstash_DateFormat %Y-%m-%d
          Index k8s-logs
          # Logstash_Prefix k8s
          Replace_Dots On
          # Type _doc
          Retry_Limit False
          Trace_Error On

      [OUTPUT]
          Name es
          Match host.*
          Host tokkio-logging-es-cluster-master-headless
          Logstash_Format On
          Logstash_Prefix node
          Replace_Dots On
          # Type _doc
          Retry_Limit False
          # Trace_Error On

      # [OUTPUT]
      #     Name stdout
      #     Match kube.*
      #     Format json
      #     Json_date_key timestamp
      #     Json_date_format iso8601

    ## https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/classic-mode/upstream-servers
    ## This configuration is deprecated, please use `extraFiles` instead.
    upstream: {}

    ## https://docs.fluentbit.io/manual/pipeline/parsers
    customParsers: |
      [PARSER]
          Name docker_no_time
          Format json
          Time_Keep Off
          Time_Key time
          Time_Format %Y-%m-%dT%H:%M:%S.%L
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/config-files/helm-release/tokkio-logging-es-override-values.yml
````yaml
elasticsearch:
  clusterName: "{{ task_vars.ops_es_cluster_name }}"
  volumeClaimTemplate:
    storageClassName: mdx-local-path
kibana:
  elasticsearchHosts: "http://{{ task_vars.ops_es_cluster_name }}-master-headless:9200"
  service:
    nodePort: "31565"
ingress:
  enabled: false
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/config-files/k8s-manifest/es-replicas-update-deployment.yml
````yaml
kubeconfig: "{{ task_vars.kubeconfig | default(omit) }}"
definition:
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: es-replicas-update-deployment
    labels:
      app: es-replicas-update
    namespace: "{{ task_vars.namespace }}"
  spec:
      replicas: 1
      selector:
        matchLabels:
          app: es-replicas-update
      template:
        metadata:
          labels:
            app: es-replicas-update
        spec:
          containers:
          - name: curl-commands
            image: giantswarm/tiny-tools
            command: ["/bin/sh", "-c"]
            args: ["/scripts/update_es_replicas.sh"]
            resources:
              requests:
                memory: "100Mi"
                cpu: "200m"
              limits:
                memory: "200Mi"
                cpu: "1"
            volumeMounts:
              - name: script-volume
                mountPath: /scripts
          volumes:
            - name: script-volume
              configMap:
                name: script-config
                defaultMode: 493
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/config-files/k8s-manifest/logging-es-replicas-update-configmap.yml
````yaml
kubeconfig: "{{ task_vars.kubeconfig | default(omit) }}"
definition:
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: script-config
    namespace: "{{ task_vars.namespace }}"
  data:
    update_es_replicas.sh: |
      set -x
      while true; do
        date
        if [ "\$(curl -s -o /dev/null -w '%{http_code}' http://tokkio-logging-es-cluster-master:9200/_cluster/health)" -ne 200 ]; then
          echo "setting index.number_of_replicas to 0"
          curl "tokkio-logging-es-cluster-master-headless:9200/_settings/index.number_of_replicas?pretty";
          curl -X PUT "tokkio-logging-es-cluster-master-headless:9200/_settings" -H "Content-Type: application/json" -d '{"index":{"number_of_replicas":0}}'
          echo "showing updated index.number_of_replicas"
          curl "tokkio-logging-es-cluster-master-headless:9200/_settings/index.number_of_replicas?pretty"
        else
          echo "response is \$(curl -s -o /dev/null -w '%{http_code}' http://tokkio-logging-es-cluster-master:9200/_cluster/health)"
        fi
        sleep 10
      done
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/config-files/k8s-manifest/namespace.yml
````yaml
kubeconfig: "{{ task_vars.kubeconfig | default(omit) }}"
definition:
  apiVersion: v1
  kind: Namespace
  metadata:
    name: "{{ task_vars.name }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/config-files/k8s-secret/docker-config-json.yml
````yaml
kubeconfig: "{{ task_vars.kubeconfig | default(omit) }}"
name: "{{ task_vars.name }}"
namespace: "{{ task_vars.namespace }}"
type: "kubernetes.io/dockerconfigjson"
data:
  .dockerconfigjson: "{{ task_vars.docker_config_json | to_json | b64encode }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/config-files/k8s-secret/ngc-api-key-secret.yml
````yaml
kubeconfig: "{{ task_vars.kubeconfig | default(omit) }}"
name: "{{ task_vars.name }}"
namespace: "{{ task_vars.namespace }}"
type: Opaque
data:
  NGC_CLI_API_KEY: "{{ task_vars.ngc_cli_api_key | b64encode }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/config-files/k8s-secret/nvidia-api-key-secret.yml
````yaml
kubeconfig: "{{ task_vars.kubeconfig | default(omit) }}"
name: "{{ task_vars.name }}"
namespace: "{{ task_vars.namespace }}"
type: Opaque
data:
  NVIDIA_API_KEY: "{{ task_vars.nvidia_api_key | b64encode }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/config-files/k8s-secret/openai-key-secret.yml
````yaml
name: "{{ task_vars.name }}"
namespace: "{{ task_vars.namespace }}"
type: Opaque
data:
  OPENAI_API_KEY: "{{ task_vars.openai_api_key | b64encode }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/config-template-examples/llm-ov-3d-coturn-1x-stream/config-template.yml
````yaml
schema_version: '0.0.7'
name: '<replace-with-unique-deployment-name>'
spec:
  infra:
    csp: 'bm'
    backend: {}
    provider: {}
    configs:
      cns:
        override_values:
          cns_nvidia_driver: yes
      ssh_public_key: "{{ lookup('file', lookup('env', 'HOME') + '<replace-with-relative-path-of-public-ssh-key-w.r.t.-user-home-directory>') }}"
      ssh_private_key_path: "{{ lookup('env', 'HOME') + '<replace-with-relative-path-of-private-ssh-key-w.r.t.-user-home-directory>' }}"
      clusters:
        app:
          master:
            user: "{{ lookup('env', 'APP_HOST_SSH_USER') }}"
            host: "{{ lookup('env', 'APP_HOST_IPV4_ADDR') }}"
          ports:
            app:
              port: 80
            grafana:
              port: 32300
              path: login
            prometheus:
              port: 30090
              path: graph
            kibana:
              port: 31565
              path: 'app/kibana'
          features:
            cns: true
            app: true
        turn:
          master:
            user: "{{ lookup('env', 'COTURN_HOST_SSH_USER') }}"
            host: "{{ lookup('env', 'COTURN_HOST_IPV4_ADDR') }}"
          features:
            coturn: true
  platform:
    configs:
      k8s_secrets:
        - name: 'ngc-api-key-secret'
          type: 'Opaque'
          entries:
            - key: NGC_CLI_API_KEY
              value: "{{ lookup('env', 'NGC_CLI_API_KEY') }}"
        - name: 'ngc-docker-reg-secret'
          type: 'dockerconfigjson'
          registry_name: "nvcr.io"
          username: '$oauthtoken'
          password: "{{ lookup('env', 'NGC_CLI_API_KEY') }}"
    secrets:
      ngc_cli_api_key: "{{ lookup('env', 'NGC_CLI_API_KEY') }}"
  app:
    configs:
      app_settings:
        helm_chart:
          repo:
            chart_name: 'ucs-tokkio-app-base-1-stream-llm-rag-3d-ov'
        k8s_secrets:
          - name: 'ngc-api-key-secret'
            type: 'Opaque'
            entries:
              - key: NGC_CLI_API_KEY
                value: "{{ lookup('env', 'NGC_CLI_API_KEY') }}"
          - name: 'openai-key-secret'
            type: 'Opaque'
            entries:
              - key: OPENAI_API_KEY
                value: "{{ lookup('env', 'OPENAI_API_KEY') }}"
          - name: 'nvidia-api-key-secret'
            type: 'Opaque'
            entries:
              - key: NVIDIA_API_KEY
                value: "{{ lookup('env', 'NVIDIA_API_KEY') }}"
          - name: 'ngc-docker-reg-secret'
            type: 'dockerconfigjson'
            registry_name: "nvcr.io"
            username: '$oauthtoken'
            password: "{{ lookup('env', 'NGC_CLI_API_KEY') }}"
      ui_settings:
        user_env_vars:
            APP_TITLE: "NVIDIA LLM APP"
            APPLICATION_TYPE: "custom"
    secrets:
      ngc_cli_api_key: "{{ lookup('env', 'NGC_CLI_API_KEY') }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/config-template-examples/llm-ov-3d-coturn-1x-stream/my-config.env
````
export OPENAI_API_KEY="<replace-with-actual-value>"
export NGC_CLI_API_KEY="<replace-with-actual-value>"
export NVIDIA_API_KEY="<replace-with-actual-value>"
export APP_HOST_IPV4_ADDR="<replace-with-actual-value>"
export APP_HOST_SSH_USER="<replace-with-actual-value>"
export COTURN_HOST_IPV4_ADDR="<replace-with-actual-value>"
export COTURN_HOST_SSH_USER="<replace-with-actual-value>"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/iac-ref/templates/ansible-hosts.tpl
````
%{~ if length(jump_hosts) > 0 ~}
jump_hosts:
  hosts:
    %{~ for jump_host in jump_hosts ~}
    ${jump_host.name}:
      ansible_user: ${jump_host.user}
      ansible_host: ${jump_host.host}
      ansible_ssh_private_key_file: ${jump_host.private_key_file}
      ansible_ssh_extra_args: '${ansible_ssh_extra_args}'
    %{~ endfor ~}
%{~ endif ~}
%{~ if length(cns_clusters) > 0 ~}
cns_clusters:
  children:
    master:
      hosts:
        %{~ for cns_cluster in cns_clusters ~}
        ${cns_cluster.name}-${cns_cluster.master.name}:
          ansible_user: ${cns_cluster.master.user}
          ansible_host: ${cns_cluster.master.host}
          ansible_ssh_private_key_file: ${cns_cluster.master.private_key_file}
          %{~ if cns_cluster.bastion != null ~}
          ansible_ssh_extra_args: '${ansible_ssh_extra_args} -o ProxyCommand="ssh -i ${cns_cluster.bastion.private_key_file} -W %h:%p ${cns_cluster.bastion.user}@${cns_cluster.bastion.host} ${ansible_ssh_extra_args}"'
          %{~ else ~}
          ansible_ssh_extra_args: '${ansible_ssh_extra_args}'
          %{~ endif ~}
        %{~ endfor ~}
    %{~ if length(flatten(cns_clusters[*].nodes)) > 0 ~}
    nodes:
      hosts:
        %{~ for cns_cluster in cns_clusters ~}
        %{~ for node in cns_cluster.nodes ~}
        ${cns_cluster.name}-${node.name}:
          ansible_user: ${node.user}
          ansible_host: ${node.host}
          ansible_ssh_private_key_file: ${node.private_key_file}
          %{~ if cns_cluster.bastion != null ~}
          ansible_ssh_extra_args: '${ansible_ssh_extra_args} -o ProxyCommand="ssh -i ${cns_cluster.bastion.private_key_file} -W %h:%p ${cns_cluster.bastion.user}@${cns_cluster.bastion.host} ${ansible_ssh_extra_args}"'
          %{~ else ~}
          ansible_ssh_extra_args: '${ansible_ssh_extra_args}'
          %{~ endif ~}
        %{~ endfor ~}
        %{~ endfor ~}
    %{~ endif ~}
%{~ endif ~}
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/iac-ref/backend.tf
````hcl
terraform {
  backend "local" {
  }
}
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/iac-ref/locals.tf
````hcl
locals {
  cluster_names          = sort(keys(var.clusters))
  bastion_inventory_name = "bastion"
  master_inventory_name  = "master"
  ansible_ssh_extra_args = "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
  turn_server_provider = var.turn_server_provider
  use_reverse_proxy = local.turn_server_provider == "rp" ? true : false
  use_twilio_stun_turn = local.turn_server_provider == "twilio" ? true : false
}
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/iac-ref/outputs.tf
````hcl
output "info" {
  value = {
    access_urls = {
      for cluster in sort(keys(var.clusters)) :
      cluster => {
        for port_name in sort(keys(var.clusters[cluster].ports)) :
        port_name => format(
          "%s://%s:%s/%s",
          var.clusters[cluster].ports[port_name].protocol,
          var.clusters[cluster].master.host,
          var.clusters[cluster].ports[port_name].port,
          trimprefix(var.clusters[cluster].ports[port_name].path, "/")
        )
      }
    }
    ssh_command : {
      for cluster in sort(keys(var.clusters)) :
      cluster => merge(
        {
          bastion = var.clusters[cluster].bastion != null ? format(
            "ssh -i %s %s %s@%s",
            var.ssh_private_key_path,
            local.ansible_ssh_extra_args,
            var.clusters[cluster].bastion.user,
            var.clusters[cluster].bastion.host
          ) : null
          master = format(
            "ssh -i %s %s%s %s@%s",
            var.ssh_private_key_path,
            local.ansible_ssh_extra_args,
            var.clusters[cluster].bastion != null ? format(" -o ProxyCommand=\"%s\"",
              format("ssh -i %s %s -W %s %s@%s",
                var.ssh_private_key_path,
                local.ansible_ssh_extra_args,
                "%h:%p",
                var.clusters[cluster].bastion.user,
                var.clusters[cluster].bastion.host
              )
            ) : "",
            var.clusters[cluster].master.user,
            var.clusters[cluster].master.host
          )
        },
        {
          for node in keys(var.clusters[cluster].nodes) :
          node =>
          format(
            "ssh -i %s %s%s %s@%s",
            var.ssh_private_key_path,
            local.ansible_ssh_extra_args,
            var.clusters[cluster].bastion != null ? format(" -o ProxyCommand=\"%s\"",
              format("ssh -i %s %s -W %s %s@%s",
                var.ssh_private_key_path,
                local.ansible_ssh_extra_args,
                "%h:%p",
                var.clusters[cluster].bastion.user,
                var.clusters[cluster].bastion.host
              )
            ) : "",
            var.clusters[cluster].nodes[node].user,
            var.clusters[cluster].nodes[node].host
          )
        }
      )
    }
  }
}

output "hosts" {
  value = templatefile("${path.module}/templates/ansible-hosts.tpl", {
    jump_hosts = [for cluster in sort(keys(var.clusters)) : {
      name             = format("%s-%s", cluster, local.bastion_inventory_name)
      user             = var.clusters[cluster].bastion.user
      host             = var.clusters[cluster].bastion.host
      private_key_file = var.ssh_private_key_path
    } if var.clusters[cluster].bastion != null]
    cns_clusters = [for cluster in sort(keys(var.clusters)) :
      {
        name = cluster
        bastion = var.clusters[cluster].bastion != null ? {
          user             = var.clusters[cluster].bastion.user
          host             = var.clusters[cluster].bastion.host
          private_key_file = var.ssh_private_key_path
        } : null
        master = {
          name             = local.master_inventory_name
          user             = var.clusters[cluster].master.user
          host             = var.clusters[cluster].master.host
          private_key_file = var.ssh_private_key_path
        }
        nodes = [for node in keys(var.clusters[cluster].nodes) : {
          name             = node
          user             = var.clusters[cluster].nodes[node].user
          host             = var.clusters[cluster].nodes[node].host
          private_key_file = var.ssh_private_key_path
        }]
      }
    ]
    ansible_ssh_extra_args = local.ansible_ssh_extra_args
  })
}

output "cns_clusters" {
  value = {
    for cluster in sort(keys(var.clusters)) :
    cluster => {
      master_name = format("%s-%s", cluster, local.master_inventory_name)
      ssh_command = var.clusters[cluster].bastion != null ? format("ssh -i %s %s -o ProxyCommand=\"%s\" %s@%s", var.ssh_private_key_path, local.ansible_ssh_extra_args, format("ssh -i %s %s -W %s %s@%s", var.ssh_private_key_path, local.ansible_ssh_extra_args, "%h:%p", var.clusters[cluster].bastion.user, var.clusters[cluster].bastion.host), var.clusters[cluster].master.user, var.clusters[cluster].master.host) : format("ssh -i %s %s %s@%s", var.ssh_private_key_path, local.ansible_ssh_extra_args, var.clusters[cluster].master.user, var.clusters[cluster].master.host)
    }
  }
}

output "playbook_configs" {
  value = {
    jump_hosts = {
      for cluster in sort(keys(var.clusters)) :
      format("%s-bastion", cluster) => {
        target                     = format("%s-%s", cluster, local.bastion_inventory_name)
        additional_ssh_public_keys = var.additional_ssh_public_keys
      } if var.clusters[cluster].bastion != null
    }
    clusters = {
      for cluster in sort(keys(var.clusters)) :
      cluster => {
        targets = {
          all = join(":", [
            for node_suffix in concat(
              [local.master_inventory_name],
              [for node in keys(var.clusters[cluster].nodes) : node]
            ) : format("%s-%s", cluster, node_suffix)
          ])
          master = format("%s-%s", cluster, local.master_inventory_name)
        }
        ports = {
          for port_name in sort(keys(var.clusters[cluster].ports)) :
          port_name => var.clusters[cluster].ports[port_name].port
        }
        labels = {
          for l in concat(
            [
              {
                inventory = format("%s-%s", cluster, local.master_inventory_name)
                labels    = var.clusters[cluster].master["labels"]
              }
            ],
            [
              for node in keys(var.clusters[cluster].nodes) :
              {
                inventory = format("%s-%s", cluster, node)
                labels    = var.clusters[cluster].nodes[node]["labels"]
              }
            ]
          ) :
          l.inventory => l.labels
        }
        taints = {
          for t in concat(
            [
              {
                inventory = format("%s-%s", cluster, local.master_inventory_name)
                taints    = var.clusters[cluster].master["taints"]
              }
            ],
            [
              for node in keys(var.clusters[cluster].nodes) :
              {
                inventory = format("%s-%s", cluster, node)
                taints    = var.clusters[cluster].nodes[node]["taints"]
              }
            ]
          ) :
          t.inventory => t.taints
        }
        features                   = sort([for feature, enabled in var.clusters[cluster].features : feature if enabled])
        additional_ssh_public_keys = var.additional_ssh_public_keys
        ip_address = var.clusters[cluster].master.host
        use_reverse_proxy = local.use_reverse_proxy
        use_twilio_stun_turn = local.use_twilio_stun_turn        
      }
    }
  }
}
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/iac-ref/variables.tf
````hcl
variable "name" {
  type = string
}

variable "controller_ip" {
  type = string
}

variable "ssh_private_key_path" {
  type = string
}

variable "additional_ssh_public_keys" {
  type    = list(string)
  default = []
}

variable "clusters" {
  type = map(object({
    bastion = optional(object({
      user = string
      host = string
    }))
    master = object({
      user   = string
      host   = string
      labels = optional(map(string), {})
      taints = optional(list(object({
        key      = optional(string)
        operator = optional(string)
        value    = optional(string)
        effect   = optional(string)
      })), [])
    })
    nodes = optional(map(object({
      user   = string
      host   = string
      labels = optional(map(string), {})
      taints = optional(list(object({
        key      = optional(string)
        operator = optional(string)
        value    = optional(string)
        effect   = optional(string)
      })), [])
    })), {})
    ports = optional(map(object({
      port     = number
      protocol = optional(string, "http")
      path     = optional(string, "/")
    })), {})
    features = optional(map(bool), {})
  }))
}

variable "turn_server_provider" {
  type = string
  validation {
    condition     = contains(["rp", "coturn", "twilio"], var.turn_server_provider)
    error_message = "The turn_server_provider must be either 'rp' or 'coturn' or 'twilio"
  }
  default = "coturn"
}
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/iac-ref/versions.tf
````hcl
terraform {
  required_version = "= 1.5.7"

  required_providers {}
}
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/scripts/coturn-setup-bm.sh
````bash
#!/bin/bash

# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
#
# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
# property and proprietary rights in and to this material, related
# documentation and any modifications thereto. Any use, reproduction,
# disclosure or distribution of this material and related documentation
# without an express license agreement from NVIDIA CORPORATION or
# its affiliates is strictly prohibited.
if [ $1 == 'install' ]; then
  echo "Setup coturn server -- Start"
  DEBIAN_FRONTEND=noninteractive
  apt-get update -y
  apt-get install -y coturn

  if [ -n "$TURNSERVER_PUBLIC_IP" ] && [ -n "${TURNSERVER_PRIVATE_IP}" ]; then
    EXTERNAL_IP="${TURNSERVER_PUBLIC_IP}/${TURNSERVER_PRIVATE_IP}"
  elif [ -z "$TURNSERVER_PUBLIC_IP" ] && [ -n "${TURNSERVER_PRIVATE_IP}" ]; then
    EXTERNAL_IP="${TURNSERVER_PRIVATE_IP}"
  else
    echo "TURNSERVER_PRIVATE_IP variable ${TURNSERVER_PRIVATE_IP} is not set"
  fi

  sed \
    -e "s|^#TURNSERVER_ENABLED=1|TURNSERVER_ENABLED=1|g" \
    -i /etc/default/coturn

  sed \
    -e "/^#\?listening-port=/d" \
    -e "/^#\?listening-ip=/d" \
    -e "/^#\?external-ip=/d" \
    -e "/^#\?relay-ip=/d" \
    -e "/^#\?min-port=/d" \
    -e "/^#\?max-port=/d" \
    -e "/^#\?fingerprint/d" \
    -e "/^#\?realm=/d" \
    -e "/^#\?user=/d" \
    -e "/^#\?log-file=/d" /etc/turnserver.conf \
    -i /etc/turnserver.conf

  cat <<EOF >> /etc/turnserver.conf
listening-port=3478
listening-ip=${TURNSERVER_PRIVATE_IP}
external-ip=${EXTERNAL_IP}
relay-ip=${TURNSERVER_PRIVATE_IP}
min-port=49152
max-port=65535
fingerprint
realm=${TURNSERVER_REALM}
user=${TURNSERVER_USERNAME}:${TURNSERVER_PASSWORD}
log-file=/var/tmp/turn.log
EOF

  systemctl restart coturn

  echo "Setup coturn server -- End"

else
  systemctl stop coturn
  DEBIAN_FRONTEND=noninteractive
  apt-get purge coturn -y
  rm -rf /etc/turnserver.conf
fi
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/scripts/coturn-setup.sh
````bash
#!/bin/bash

# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
#
# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
# property and proprietary rights in and to this material, related
# documentation and any modifications thereto. Any use, reproduction,
# disclosure or distribution of this material and related documentation
# without an express license agreement from NVIDIA CORPORATION or
# its affiliates is strictly prohibited.
if [ $1 == 'install' ]; then
  echo "Setup coturn server -- Start"
  DEBIAN_FRONTEND=noninteractive
  apt-get update -y
  apt-get install -y coturn

  if [ -n "$TURNSERVER_PUBLIC_IP" ] && [ -n "${TURNSERVER_PRIVATE_IP}" ]; then
    EXTERNAL_IP="${TURNSERVER_PUBLIC_IP}/${TURNSERVER_PRIVATE_IP}"
  elif [ -z "$TURNSERVER_PUBLIC_IP" ] && [ -n "${TURNSERVER_PRIVATE_IP}" ]; then
    EXTERNAL_IP="${TURNSERVER_PRIVATE_IP}"
  else
    echo "TURNSERVER_PRIVATE_IP variable ${TURNSERVER_PRIVATE_IP} is not set"
  fi

  sed \
    -e "s|^#TURNSERVER_ENABLED=1|TURNSERVER_ENABLED=1|g" \
    -i /etc/default/coturn

  sed \
    -e "/^#\?listening-device=/d" \
    -e "/^#\?listening-port=/d" \
    -e "/^#\?listening-ip=/d" \
    -e "/^#\?external-ip=/d" \
    -e "/^#\?relay-ip=/d" \
    -e "/^#\?min-port=/d" \
    -e "/^#\?max-port=/d" \
    -e "/^#\?fingerprint/d" \
    -e "/^#\?realm=/d" \
    -e "/^#\?user=/d" \
    -e "/^#\?log-file=/d" /etc/turnserver.conf \
    -i /etc/turnserver.conf

  cat <<EOF >> /etc/turnserver.conf
listening-device=${LISTENING_DEVICE}
listening-port=3478
listening-ip=${TURNSERVER_PRIVATE_IP}
external-ip=${EXTERNAL_IP}
relay-ip=${TURNSERVER_PRIVATE_IP}
min-port=49152
max-port=65535
fingerprint
realm=${TURNSERVER_REALM}
user=${TURNSERVER_USERNAME}:${TURNSERVER_PASSWORD}
log-file=/var/tmp/turn.log
EOF

  systemctl restart coturn

  echo "Setup coturn server -- End"

else
  systemctl stop coturn
  DEBIAN_FRONTEND=noninteractive
  apt-get purge coturn -y
  rm -rf /etc/turnserver.conf
fi
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/scripts/install-tokkio-ui-aws.sh
````bash
#!/bin/bash
cd "${ansible_user_dir}"
export _resource_name="${resource_name}"
export _resource_team="${resource_team}"
export _org_name="${resource_org}"
export _resource_version="${resource_version}"
export _resource_file="${resource_file}"
export _tar_artifact_dir="${_resource_name}"_v"${_resource_version}"
export _tar_artifact_name="$_tar_artifact_dir/$_resource_file"
export NGC_CLI_API_KEY="${NGC_CLI_API_KEY}"
export PATH=$PATH:/usr/local/bin/ngc-cli

function install_awscli() {
  if ! hash aws 2>/dev/null; then
    {
      DEBIAN_FRONTEND=noninteractive
      apt-get update -y
      apt-get install -y awscli 
    } > /dev/null
  fi
}

function install_ngc() {
  if ! hash ngc 2>/dev/null; then
    {
      if ! hash unzip 2>/dev/null; then
        DEBIAN_FRONTEND=noninteractive
        apt-get -y update
        apt-get -y install unzip
      fi
      wget \
        --quiet \
        --content-disposition \
        -O /tmp/ngccli_linux.zip \
        https://ngc.nvidia.com/downloads/ngccli_linux.zip
      mkdir -p /usr/local/lib
      unzip -q /tmp/ngccli_linux.zip -d /usr/local/lib
      rm /tmp/ngccli_linux.zip
      ln -s /usr/local/lib/ngc-cli/ngc /usr/local/bin/ngc
    } > /dev/null
  fi
}

function prepare_ui_code() {
  {
    sudo rm -rf tokkio-ui
    sudo rm -rf "${_tar_artifact_dir}"
    sudo mkdir -p tokkio-ui
    ngc user who --org "${_org_name}"
    export TOKKIO_UI_RESOURCE_URL="$_org_name/$_resource_team/$_resource_name:$_resource_version"
    ngc registry resource download-version "${TOKKIO_UI_RESOURCE_URL}" --org "${_org_name}"
    sudo tar -C tokkio-ui -xf "${_tar_artifact_name}"
    cd tokkio-ui
    mkdir -p tokkio-ui
    ngc user who --org "${_org_name}"
    ngc registry resource download-version "${TOKKIO_UI_RESOURCE_URL}" --org "${_org_name}"
    tar -C tokkio-ui -xf "${_tar_artifact_name}"
    cd tokkio-ui
    # bash init.sh
    python3 init.py
    cp *.js build/
    sudo aws s3 rm "s3://${WEB_ASSETS_BUCKET_ID}" --recursive
    sudo aws s3 sync ./build "s3://${WEB_ASSETS_BUCKET_ID}" --delete
  } > /dev/null
}

function remove_ui_code() {
  {
    sudo aws s3 rm "s3://${WEB_ASSETS_BUCKET_ID}" --recursive
  } > /dev/null
}

if [ $1 == 'install' ]; then 
echo "Install Tokkio UI -- Start"
install_awscli
install_ngc
prepare_ui_code
echo "Install Tokkio UI -- End"
else 
echo "Stopping Tokkio UI"
remove_ui_code
echo "Stopped Tokkio UI"
fi
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/scripts/install-tokkio-ui-bm.sh
````bash
#!/bin/bash
if [ $1 == 'install' ]; then 
cd "${ansible_user_dir}"
export _resource_name="${resource_name}"
export _resource_team="${resource_team}"
export _org_name="${resource_org}"
export _resource_version="${resource_version}"
export _resource_file="${resource_file}"
export _tar_artifact_dir="${_resource_name}"_v"${_resource_version}"
export _tar_artifact_name="$_tar_artifact_dir/$_resource_file"
export NGC_CLI_API_KEY="${NGC_CLI_API_KEY}"
export PATH=$PATH:/usr/local/bin/ngc-cli
sudo rm -rf tokkio-ui
sudo rm -rf "${_tar_artifact_dir}"
sudo mkdir -p tokkio-ui
ngc user who --org "${_org_name}"
export TOKKIO_UI_RESOURCE_URL="$_org_name/$_resource_team/$_resource_name:$_resource_version"
ngc registry resource download-version "${TOKKIO_UI_RESOURCE_URL}" --org "${_org_name}"
sudo tar -C tokkio-ui -xf "${_tar_artifact_name}"
cd tokkio-ui
python3 init.py
cp *.js build/
# configure nginx 
DEBIAN_FRONTEND=noninteractive
sudo apt-get update -y
sudo apt-get install nginx -y
sudo mkdir -p /etc/nginx/sites-available/tokkio-ui && sudo mkdir -p /var/www/tokkio-ui
sudo cp -rp build/*  /var/www/tokkio-ui/
sudo rm -rf /etc/nginx/sites-enabled/default
sudo cat << 'EOF' | sudo tee /etc/nginx/sites-enabled/tokkio-ui
server {
    listen 80;
    server_name _;
    root /var/www/tokkio-ui;
    index index.html;

    location / {
        try_files $uri $uri/ =404;
    }
}
EOF
sudo systemctl restart nginx
sudo systemctl status nginx
else
sudo systemctl stop nginx
sudo apt-get purge nginx* -y 
sudo rm -rf /var/www/tokkio-ui/
echo "stopped tokkio-ui"
fi
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/scripts/install-tokkio-ui-gcp.sh
````bash
#!/bin/bash
cd "${ansible_user_dir}"
export _resource_name="${resource_name}"
export _resource_team="${resource_team}"
export _org_name="${resource_org}"
export _resource_version="${resource_version}"
export _resource_file="${resource_file}"
export _tar_artifact_dir="${_resource_name}"_v"${_resource_version}"
export _tar_artifact_name="$_tar_artifact_dir/$_resource_file"
export NGC_CLI_API_KEY="${NGC_CLI_API_KEY}"
export PATH=$PATH:/usr/local/bin/ngc-cli

function install_gsutil() {
  if ! hash gsutil 2>/dev/null; then
    {
      DEBIAN_FRONTEND=noninteractive
      echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
      curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -
      apt update
      apt install google-cloud-cli -y
    } > /dev/null
  fi
}

function install_ngc() {
  if ! hash ngc 2>/dev/null; then
    {
      if ! hash unzip 2>/dev/null; then
        DEBIAN_FRONTEND=noninteractive
        apt-get -y update
        apt-get -y install unzip
      fi
      wget \
        --quiet \
        --content-disposition \
        -O /tmp/ngccli_linux.zip \
        https://ngc.nvidia.com/downloads/ngccli_linux.zip
      mkdir -p /usr/local/lib
      unzip -q /tmp/ngccli_linux.zip -d /usr/local/lib
      rm /tmp/ngccli_linux.zip
      ln -s /usr/local/lib/ngc-cli/ngc /usr/local/bin/ngc
    } > /dev/null
  fi
}

function prepare_ui_code() {
  {
    sudo rm -rf tokkio-ui
    sudo rm -rf "${_tar_artifact_dir}"
    sudo mkdir -p tokkio-ui
    ngc user who --org "${_org_name}"
    export TOKKIO_UI_RESOURCE_URL="$_org_name/$_resource_team/$_resource_name:$_resource_version"
    ngc registry resource download-version "${TOKKIO_UI_RESOURCE_URL}" --org "${_org_name}"
    sudo tar -C tokkio-ui -xf "${_tar_artifact_name}"
    cd tokkio-ui
    mkdir -p tokkio-ui
    ngc user who --org "${_org_name}"
    ngc registry resource download-version "${TOKKIO_UI_RESOURCE_URL}" --org "${_org_name}"
    tar -C tokkio-ui -xf "${_tar_artifact_name}"
    cd tokkio-ui
    python3 init.py
    cp *.js build/
    sudo gsutil rm -r -f "gs://${WEB_ASSETS_BUCKET_ID}/**"
    sudo gsutil rsync -r ./build "gs://${WEB_ASSETS_BUCKET_ID}"
  } > /dev/null
}

function remove_ui_code() {
  {
    sudo gsutil rm -r -f "gs://${WEB_ASSETS_BUCKET_ID}/**"
  } > /dev/null
}

if [ $1 == 'install' ]; then 
echo "Install Tokkio UI -- Start"
install_gsutil
install_ngc
prepare_ui_code
echo "Install Tokkio UI -- End"
else 
echo "Stopping Tokkio UI"
remove_ui_code
echo "Stopped Tokkio UI"
fi
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/scripts/install-tokkio-ui.sh
````bash
#!/bin/bash

export UI_STORAGE_ACCOUNT_NAME="${UI_STORAGE_ACCOUNT_NAME}"
export UI_STORAGE_ACCESS_CLIENT_ID="${UI_STORAGE_ACCESS_CLIENT_ID}"
export _resource_name="${resource_name}"
export _resource_team="${resource_team}"
export _org_name="${resource_org}"
export _resource_version="${resource_version}"
export TOKKIO_UI_RESOURCE_URL="$_org_name/$_resource_team/$_resource_name:$_resource_version"
export _resource_file="${resource_file}"

function install_azcopy() {
  if ! hash azcopy 2>/dev/null; then
    {
      wget \
        --quiet \
        --content-disposition \
        -O azcopy_v10.tar.gz \
        https://aka.ms/downloadazcopy-v10-linux
      tar -xf azcopy_v10.tar.gz --strip-components=1
      rm azcopy_v10.tar.gz
      rm NOTICE.txt
      mv azcopy /usr/local/bin/azcopy
    } > /dev/null
  fi
}

function install_ngc() {
  if ! hash ngc 2>/dev/null; then
    {
      if ! hash unzip 2>/dev/null; then
        apt-get -y update
        apt-get -y install unzip
      fi
      wget \
        --quiet \
        --content-disposition \
        -O /tmp/ngccli_linux.zip \
        https://ngc.nvidia.com/downloads/ngccli_linux.zip
      mkdir -p /usr/local/lib
      unzip -q /tmp/ngccli_linux.zip -d /usr/local/lib
      rm /tmp/ngccli_linux.zip
      ln -s /usr/local/lib/ngc-cli/ngc /usr/local/bin/ngc
    } > /dev/null
  fi
}

function prepare_ui_code() {
  {
    _resource_name="$( echo -n "${TOKKIO_UI_RESOURCE_URL}" | awk -F ':' '{print $1}' | awk -F '/' '{print $3}')"
    _org_name="$( echo -n "${TOKKIO_UI_RESOURCE_URL}" | awk -F ':' '{print $1}' | awk -F '/' '{print $1}')"
    _resource_version="$( echo -n "${TOKKIO_UI_RESOURCE_URL}" | awk -F ':' '{print $2}')"
    _tar_artifact_dir="${_resource_name}_v${_resource_version}"
    _tar_artifact_name="${_tar_artifact_dir}/${_resource_file}"
    rm -rf tokkio-ui
    rm -rf tokkio-ui-prepared
    rm -rf "${_tar_artifact_dir}"
    mkdir -p tokkio-ui
    ngc user who --org "${_org_name}"
    ngc registry resource download-version "${TOKKIO_UI_RESOURCE_URL}" --org "${_org_name}"
    tar -C tokkio-ui -xf "${_tar_artifact_name}"
    cd tokkio-ui
    python3 init.py
    cp *.js build/
    cd ..
    mv tokkio-ui/build tokkio-ui-prepared
  } > /dev/null
}

function upload_ui_code() {
  {
    azcopy login --identity --identity-client-id "${UI_STORAGE_ACCESS_CLIENT_ID}"
    azcopy remove "https://${UI_STORAGE_ACCOUNT_NAME}.blob.core.windows.net/\$web/*" --recursive=true
    azcopy cp \
      "tokkio-ui-prepared/*" \
      "https://${UI_STORAGE_ACCOUNT_NAME}.blob.core.windows.net/\$web" \
      --recursive=true
    rm -rf tokkio-ui-prepared
  } > /dev/null
}

function remove_ui_code() {
  {
    azcopy login --identity --identity-client-id "${UI_STORAGE_ACCESS_CLIENT_ID}"
    azcopy remove "https://${UI_STORAGE_ACCOUNT_NAME}.blob.core.windows.net/\$web/*" --recursive=true
  } > /dev/null
}

if [ $1 == 'install' ]; then 
echo "Install Tokkio UI -- Start"
install_azcopy
install_ngc
prepare_ui_code
upload_ui_code
echo "Install Tokkio UI -- End"
else 
remove_ui_code
fi
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/scripts/mount-data-disk-aws.sh
````bash
#!/bin/bash


echo "Mount Data Disk -- Start"

apt-get update
apt-get install jq -y
DEVNAME="/dev/$(lsblk -bdnJI 259 -o NAME,SIZE,SERIAL | jq -r '.blockdevices | sort_by(.size) | reverse | map(select(.serial | startswith("vol"))) | map(.name) | .[0]')"
pvcreate "${DEVNAME}"
lvm_group_name=tokkio-volume-group

vgcreate ${lvm_group_name} "${DEVNAME}"

lvcreate -L 400G -n containerd ${lvm_group_name}
VDEVNAME="/dev/${lvm_group_name}/containerd"
mkfs -t ext4 ${VDEVNAME}
VDEVUUID=$(blkid ${VDEVNAME} -o export | grep '^UUID' | awk -F '=' '{print $2}')
mkdir -p /var/lib/containerd
echo "UUID=${VDEVUUID}  /var/lib/containerd  ext4  defaults,nofail  0  2" | tee -a /etc/fstab

lvcreate -l 100%FREE -n opt ${lvm_group_name}
VDEVNAME="/dev/${lvm_group_name}/opt"
mkfs -t ext4 ${VDEVNAME}
VDEVUUID=$(blkid ${VDEVNAME} -o export | grep '^UUID' | awk -F '=' '{print $2}')
mkdir -p /opt
echo "UUID=${VDEVUUID}  /opt  ext4  defaults,nofail  0  2" | tee -a /etc/fstab

mount -a

echo "Mount Data Disk -- End"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/scripts/mount-data-disk-gcp.sh
````bash
#!/bin/bash


echo "Mount Data Disk -- Start"

apt-get update
apt-get install jq -y
DEVNAME="/dev/$(lsblk -bdnJI 8,259 -o NAME,SIZE,SERIAL | jq -r '.blockdevices | map(select(.serial == "data")) | map(.name) | .[0]')"
pvcreate "${DEVNAME}"
lvm_group_name=tokkio-volume-group

vgcreate ${lvm_group_name} "${DEVNAME}"

lvcreate -L 400G -n containerd ${lvm_group_name}
VDEVNAME="/dev/${lvm_group_name}/containerd"
mkfs -t ext4 ${VDEVNAME}
VDEVUUID=$(blkid ${VDEVNAME} -o export | grep '^UUID' | awk -F '=' '{print $2}')
mkdir -p /var/lib/containerd
echo "UUID=${VDEVUUID}  /var/lib/containerd  ext4  defaults,nofail  0  2" | tee -a /etc/fstab

lvcreate -l 100%FREE -n opt ${lvm_group_name}
VDEVNAME="/dev/${lvm_group_name}/opt"
mkfs -t ext4 ${VDEVNAME}
VDEVUUID=$(blkid ${VDEVNAME} -o export | grep '^UUID' | awk -F '=' '{print $2}')
mkdir -p /opt
echo "UUID=${VDEVUUID}  /opt  ext4  defaults,nofail  0  2" | tee -a /etc/fstab

mount -a

echo "Mount Data Disk -- End"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/scripts/mount-data-disk.sh
````bash
#!/bin/bash


echo "Mount Data Disk -- Start"

apt-get update
apt-get install jq -y
DEVNAME="/dev/$(lsblk -bdnJI 8 -o NAME,SIZE | jq -r '.blockdevices | sort_by(.size) | map(.name) | .[1]')"
pvcreate "${DEVNAME}"
lvm_group_name=tokkio-volume-group

vgcreate ${lvm_group_name} "${DEVNAME}"

lvcreate -L 400G -n containerd ${lvm_group_name}
VDEVNAME="/dev/${lvm_group_name}/containerd"
mkfs -t ext4 ${VDEVNAME}
VDEVUUID=$(blkid ${VDEVNAME} -o export | grep '^UUID' | awk -F '=' '{print $2}')
mkdir -p /var/lib/containerd
echo "UUID=${VDEVUUID}  /var/lib/containerd  ext4  defaults,nofail  0  2" | tee -a /etc/fstab

lvcreate -l 100%FREE -n opt ${lvm_group_name}
VDEVNAME="/dev/${lvm_group_name}/opt"
mkfs -t ext4 ${VDEVNAME}
VDEVUUID=$(blkid ${VDEVNAME} -o export | grep '^UUID' | awk -F '=' '{print $2}')
mkdir -p /opt
echo "UUID=${VDEVUUID}  /opt  ext4  defaults,nofail  0  2" | tee -a /etc/fstab

mount -a

echo "Mount Data Disk -- End"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/tasks/helm-release-prep-values.yml
````yaml
---
- name: read file content
  set_fact:
    item_content: "{{ lookup('template', item) }}"
- name: output content to file
  copy:
    content: "{{ item_content }}"
    dest: "{{ ansible_user_dir }}/helm-values/ns-{{ task_config.namespace }}/release-{{ task_config.release_name }}/{{ item | basename }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/add-sysctl-config.yml
````yaml
- name: Configure and apply sysctl settings
  hosts: master
  become: yes
  tasks:
    - name: Add "{{ task_vars.parameter }}" to sysctl.conf
      ansible.builtin.lineinfile:
        path: /etc/sysctl.conf
        line: "{{ task_vars.parameter }}=${{ task_vars.value }}"
        state: present

    - name: Apply sysctl changes
      ansible.posix.sysctl:
        name: "{{ task_vars.parameter }}"
        value: "{{ task_vars.value }}"
        state: present
        sysctl_set: yes
        reload: yes
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/app-pre-requisites.yml
````yaml
---
- hosts: all
  gather_facts: no
  tasks:
  - name: install required packages
    become: yes
    become_user: root
    apt:
      state: present
      update_cache: yes
      name: 
        - unzip
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/authorize-ssh-keys.yml
````yaml
---
- hosts: all
  gather_facts: yes
  become: false
  tasks:
  - name: authorize key
    ansible.posix.authorized_key:
      state: "{{ state | default('present') }}"
      user: "{{ task_config.user }}"
      key: "{{ item }}"
    loop: "{{ task_config['keys'] }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/bootstrap-cns-kubeconfig.yml
````yaml
---
- hosts: master
  gather_facts: no
  become: false
  tasks:
  - name: copy kubeconfig
    fetch:
      src: "/etc/kubernetes/admin.conf"
      dest: "/tmp/kubeconfig"
      flat: true
- hosts: localhost
  connection: local
  gather_facts: no
  become: false
  tasks:
  - name: transform kubeconfig
    shell: |
      sed 's/kubernetes/{{ cluster_name }}/g' -i /tmp/kubeconfig
      yq eval /tmp/kubeconfig -o json | \
        jq '.clusters[0].cluster += {"proxy-url": "socks5://localhost:{{ tunnel_port }}"}' | \
        yq -P > /tmp/kubeconfig-with-proxy
      export _current_kubeconfig="${KUBECONFIG:-${HOME}/.kube/config}"
      mkdir -p "$(dirname "${_current_kubeconfig}")"
      touch "${_current_kubeconfig}"
      KUBECONFIG="/tmp/kubeconfig-with-proxy:${_current_kubeconfig}" kubectl config view --flatten > /tmp/kubeconfig-all-in-one
      mv /tmp/kubeconfig-all-in-one "${_current_kubeconfig}"
      chmod 600 "${_current_kubeconfig}"
      rm /tmp/kubeconfig || true
      rm /tmp/kubeconfig-with-proxy || true
      rm /tmp/kubeconfig-all-in-one || true
  - name: setup tunnel
    shell: |
      if (! sudo lsof -t -i @127.0.0.1:{{ tunnel_port }}) || (kill -9 "$(sudo lsof -t -i @127.0.0.1:{{ tunnel_port }})"); then
        eval "$(sed -e 's/^ssh/ssh -D {{ tunnel_port }} -q -N/g' -e 's/$/ \&/g' < {{ ssh_command }})"
      else
        echo "cannot kill process running on port {{ tunnel_port }}"
        exit 1
      fi
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/check-all-pods-in-namespace-up.yml
````yaml
---
- hosts: all
  gather_facts: yes
  become: false
  tasks:
  - name: fail if no pods
    kubernetes.core.k8s_info:
      kubeconfig: "{{ task_config.kubeconfig | default(omit) }}"
      namespace: "{{ task_config.namespace | default(omit) }}"
      kind: Pod
    retries: "{{ task_config.retries | default(20) }}"
    delay: "{{ task_config.delay | default(30) }}"
    register: namespace_pods_list
    until: namespace_pods_list.resources is defined and namespace_pods_list.resources | length > 0
    when: state == "present" and not ansible_check_mode
  - name: fail if pending pods
    kubernetes.core.k8s_info:
      kubeconfig: "{{ task_config.kubeconfig | default(omit) }}"
      namespace: "{{ task_config.namespace | default(omit) }}"
      kind: Pod
      field_selectors:
      - status.phase!=Succeeded
      - status.phase!=Running
    retries: "{{ task_config.retries | default(20) }}"
    delay: "{{ task_config.delay | default(30) }}"
    register: namespace_pending_pods_list
    until: namespace_pods_list.resources is defined and namespace_pending_pods_list.resources | length == 0
    when: state == "present" and not ansible_check_mode
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/check-inventory.yml
````yaml
---
- hosts: all
  gather_facts: no
  become: false
  tasks:
  - name: check sudo privileges
    become: true
    stat:
      path: /root
    no_log: true
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/cns-install.yml
````yaml
---
- hosts: master
  gather_facts: no
  become: false
  tasks:
  - name: check if cluster is ready
    shell: kubectl get --raw='/readyz'
    ignore_errors: true
    no_log: true
    register: cluster_ready_result
    when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
  - name: publish cluster readiness
    delegate_to: localhost
    copy:
      content: "{{ cluster_ready_result.rc == 0 }}"
      dest: "{{ tmp_task_dir }}/cluster-ready"
    when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
  - name: check if cluster is live
    shell: kubectl get --raw='/livez'
    ignore_errors: true
    no_log: true
    register: cluster_live_result
    when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
  - name: publish cluster liveness
    delegate_to: localhost
    copy:
      content: "{{ cluster_live_result.rc == 0 }}"
      dest: "{{ tmp_task_dir }}/cluster-live"
    when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
- hosts: all
  gather_facts: no
  become: false
  tasks:
  - name: register cluster readiness
    set_fact:
      cluster_ready: "{{ lookup('file', tmp_task_dir + '/cluster-ready') }}"
    when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
  - name: register cluster liveness
    set_fact:
      cluster_live: "{{ lookup('file', tmp_task_dir + '/cluster-live') }}"
    when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
- name: install cns
  when: state == "present" and not ansible_check_mode and dry_run_mode != "true" and not (cluster_ready and cluster_live)
  import_playbook: "../cns/playbooks/cns-installation.yaml"
- name: uninstall cns
  when: state != "present" and not ansible_check_mode and dry_run_mode != "true"
  import_playbook: "../cns/playbooks/cns-uninstall.yaml"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/cns-prepare.yml
````yaml
---
- hosts: localhost
  connection: local
  gather_facts: no
  become: false
  tasks:
  - name: install git
    become: true
    apt:
      name: git
      state: present
  - name: clone cns git repo
    block:
    - name: remove any existing cns git source directory
      file:
        path: "{{ dist_dir }}/cns"
        state: absent
    - name: clone cns git repo
      git:
        repo: https://github.com/NVIDIA/cloud-native-stack.git
        version: "{{ configs.cns.git_ref | default(omit) }}"
        dest: "{{ dist_dir }}/cns"
  - name: capture cns version
    set_fact:
      cns_version: "{{ configs.cns.version | default('12.2') }}"
  - name: populate cns version
    copy:
      content: "{{ cns_version }}"
      dest: "{{ dist_dir }}/cns/playbooks/cns_version.yaml"
  - name: populate cns override values
    copy:
      content: "{{ configs.cns.override_values | default('{}') | to_nice_yaml }}"
      dest: "{{ dist_dir }}/cns/playbooks/cns_override_values.yaml"
  - name: check values for cns version exists
    stat:
      path: "{{ dist_dir }}/cns/playbooks/cns_values_{{ cns_version }}.yaml"
    register: cns_values_for_version
  - name: populate cns version values when not exists
    copy:
      content: "{}"
      dest: "{{ dist_dir }}/cns/playbooks/cns_values_{{ cns_version }}.yaml"
    when: not cns_values_for_version.stat.exists
  - name: prepare cns values
    shell: |
      yq eval-all '. as $item ireduce ({}; . * $item )' {{ dist_dir }}/cns/playbooks/cns_values_{{ cns_version }}.yaml {{ dist_dir }}/cns/playbooks/cns_override_values.yaml
    register: cns_values
  - name: populate cns values
    copy:
      content: "{{ cns_values.stdout }}"
      dest: "{{ dist_dir }}/cns/playbooks/cns_values.yaml"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/cns-validate.yml
````yaml
---
- name: validate cns
  when: state == "present" and not ansible_check_mode and dry_run_mode == "true"
  import_playbook: "../cns/playbooks/cns-validation.yaml"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/copy-task-config.yml
````yaml
---
- hosts: localhost
  connection: local
  gather_facts: no
  become: false
  tasks:
  - name: copy task config
    shell: "yq eval '.' {{ config_source }} -o json | jq '{task_config: .}' | yq -P > {{ config_destination }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/docker-compose.yml
````yaml
---
- hosts: all
  gather_facts: yes
  become: false
  tasks:
  - name: create directory for compose file
    file:
      path: "{{ ansible_user_dir }}/{{ task_config.name }}"
      state: directory
      mode: '0755'
  - name: prepare docker-compose.yml
    copy:
      content: "{{ task_config.content }}"
      dest: "{{ ansible_user_dir }}/{{ task_config.name }}/docker-compose.yml"
  - name: create and start services
    community.docker.docker_compose_v2:
      project_src: "{{ ansible_user_dir }}/{{ task_config.name }}"
    when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
    register: start_output
  - name: verify services were started successfully
    assert:
      that:
      - "not start_output.failed"
    when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
  - name: stop all services
    community.docker.docker_compose_v2:
      project_src: "{{ ansible_user_dir }}/{{ task_config.name }}"
      state: stopped
    when: state != "present" and not ansible_check_mode and dry_run_mode != "true"
    register: stop_output
  - name: verify services were stopped successfully
    assert:
      that:
      - "not stop_output.failed"
    when: state != "present" and not ansible_check_mode and dry_run_mode != "true"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/docker-login.yml
````yaml
---
- hosts: all
  gather_facts: no
  become: false
  tasks:
  - name: login to docker registry
    docker_login:
      registry: "{{ task_config.registry }}"
      username: "{{ task_config.username }}"
      password: "{{ task_config.password }}"
    when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
  - name: logout of docker registry
    docker_login:
      registry: "{{ task_config.registry }}"
      state: absent
    when: state != "present" and not ansible_check_mode and dry_run_mode != "true"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/empty_var_check.yml
````yaml
- hosts: all
  gather_facts: yes
  become: false
  tasks:
    - name: Fail if variable is empty
      fail:
        msg: "The variable 'item.key' is empty. Please provide a valid value."
      when: my_var | length == 0
      loop: "{{ task_vars }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/get-network-interface.yml
````yaml
---
- hosts: all
  gather_facts: yes
  become: false
  tasks:
    - name: get network_interface_name
      shell: ip route | grep default | awk '{print $5}'
      register: network_interface_name
    - name: Ensure destination directory exists
      file:
        path: "{{ dist_dir }}/{{ ansible_hostname }}"
        state: directory
        mode: '0755'
      delegate_to: localhost
    - name: publish network_interface_name
      delegate_to: localhost
      copy:
        content: "{{ network_interface_name.stdout }}"
        dest: "{{ dist_dir }}/{{ ansible_hostname }}/nic_name"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/helm-release.yml
````yaml
---
- hosts: all
  gather_facts: yes
  tasks:
  - name: check chart repo details
    fail:
      msg: repo_name is required when repo_url is provided and vice-versa
    when: (task_config.repo_name is defined) != (task_config.repo_url is defined)
  - name: check local chart details
    fail:
      msg: chart_ref is not supported for local charts
    when: (task_config.local_chart is defined) == (task_config.chart_ref is defined)
  - name: check chart source
    fail:
      msg: multiple chart sources cannot be provided
    when: "[(task_config.repo_url is defined | string), (task_config.git_repo is defined | string), (task_config.local_chart is defined | string)] | select('match', 'True') | length > 1"
  - name: add repo
    kubernetes.core.helm_repository:
      repo_name: "{{ task_config.repo_name }}"
      repo_url: "{{ task_config.repo_url }}"
      repo_username: "{{ task_config.repo_username | default(None) }}"
      repo_password: "{{ task_config.repo_password | default(None) }}"
      force_update: true
    when: task_config.repo_name is defined and task_config.repo_url is defined
  - name: clone git repo
    when: task_config.git_repo is defined
    block:
    - name: create git sources directories
      file:
        path: "{{ ansible_user_dir }}/helm-git-sources/ns-{{ task_config.namespace }}/release-{{ task_config.release_name }}"
        state: directory
        mode: '0755'
    - name: clone git repo
      git:
        repo: "{{ task_config.git_repo }}"
        version: "{{ task_config.git_repo_version | default(omit) }}"
        dest: "{{ ansible_user_dir }}/helm-git-sources/ns-{{ task_config.namespace }}/release-{{ task_config.release_name }}"
  - name: copy local chart
    when: task_config.local_chart is defined
    block:
    - name: remove any existing local chart directory
      file:
        path: "{{ ansible_user_dir }}/helm-local-charts/ns-{{ task_config.namespace }}/release-{{ task_config.release_name }}"
        state: absent
    - name: create local chart directory
      file:
        path: "{{ ansible_user_dir }}/helm-local-charts/ns-{{ task_config.namespace }}/release-{{ task_config.release_name }}"
        state: directory
        mode: '0755'
    - name: copy local chart
      copy:
        src: "{{ task_config.local_chart }}"
        dest: "{{ ansible_user_dir }}/helm-local-charts/ns-{{ task_config.namespace }}/release-{{ task_config.release_name }}"
  - name: create values files directories
    file:
      path: "{{ ansible_user_dir }}/helm-values/ns-{{ task_config.namespace }}/release-{{ task_config.release_name }}"
      state: directory
      mode: '0755'
    when: task_config.values_files is defined
  - name: prep values files
    include_tasks: tasks/helm-release-prep-values.yml
    loop: "{{ task_config.values_files }}"
    when: task_config.values_files is defined
  - name: install release
    kubernetes.core.helm:
      release_name: "{{ task_config.release_name }}"
      chart_ref: "{{ task_config.repo_name }}/{{ task_config.chart_ref }}"
      chart_version: "{{ task_config.chart_version | default(omit)  }}"
      release_namespace: "{{ task_config.namespace }}"
      create_namespace: true
      release_values: "{{ task_config.release_values | default(omit) }}"
      values_files: "{{ [ansible_user_dir + '/helm-values/ns-' + task_config.namespace + '/release-' + task_config.release_name + '/'] | product(task_config.values_files | default([]) | map('basename')) | map('join') | list }}"
      release_state: "{{ task_config.state | default('present') }}"
      kubeconfig: "{{ task_config.kubeconfig | default(omit) }}"
    when: task_config.repo_name is defined and task_config.repo_url is defined and task_config.git_repo is not defined
  - name: install release
    kubernetes.core.helm:
      release_name: "{{ task_config.release_name }}"
      chart_ref: "{{ ansible_user_dir }}/helm-git-sources/ns-{{ task_config.namespace }}/release-{{ task_config.release_name }}/{{ task_config.chart_ref }}"
      chart_version: "{{ task_config.chart_version | default(omit)  }}"
      release_namespace: "{{ task_config.namespace }}"
      create_namespace: true
      release_values: "{{ task_config.release_values | default(omit) }}"
      values_files: "{{ [ansible_user_dir + '/helm-values/ns-' + task_config.namespace + '/release-' + task_config.release_name + '/'] | product(task_config.values_files | default([]) | map('basename')) | map('join') | list }}"
      release_state: "{{ task_config.state | default('present') }}"
      kubeconfig: "{{ task_config.kubeconfig | default(omit) }}"
    when: task_config.git_repo is defined
  - name: install release
    kubernetes.core.helm:
      release_name: "{{ task_config.release_name }}"
      chart_ref: "{{ ansible_user_dir }}/helm-local-charts/ns-{{ task_config.namespace }}/release-{{ task_config.release_name }}/{{ task_config.local_chart | basename }}"
      chart_version: "{{ task_config.chart_version | default(omit)  }}"
      release_namespace: "{{ task_config.namespace }}"
      create_namespace: true
      release_values: "{{ task_config.release_values | default(omit) }}"
      values_files: "{{ [ansible_user_dir + '/helm-values/ns-' + task_config.namespace + '/release-' + task_config.release_name + '/'] | product(task_config.values_files | default([]) | map('basename')) | map('join') | list }}"
      release_state: "{{ task_config.state | default('present') }}"
      kubeconfig: "{{ task_config.kubeconfig | default(omit) }}"
    when: task_config.local_chart is defined
  - name: install release
    kubernetes.core.helm:
      release_name: "{{ task_config.release_name }}"
      chart_ref: "{{ task_config.chart_ref }}"
      chart_version: "{{ task_config.chart_version | default(omit)  }}"
      release_namespace: "{{ task_config.namespace }}"
      create_namespace: true
      release_values: "{{ task_config.release_values | default(omit) }}"
      values_files: "{{ [ansible_user_dir + '/helm-values/ns-' + task_config.namespace + '/release-' + task_config.release_name + '/'] | product(task_config.values_files | default([]) | map('basename')) | map('join') | list }}"
      release_state: "{{ task_config.state | default('present') }}"
      kubeconfig: "{{ task_config.kubeconfig | default(omit) }}"
    when: not (task_config.repo_name is defined or task_config.repo_url is defined or task_config.git_repo is defined or task_config.local_chart is defined)
  - name: remove repo
    kubernetes.core.helm_repository:
      repo_name: "{{ task_config.repo_name }}"
      state: absent
    when: task_config.repo_name is defined and task_config.repo_url is defined
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/helm-task-pre-requisites.yml
````yaml
---
- hosts: all
  gather_facts: no
  tasks:
  - name: install helm diff plugin
    kubernetes.core.helm_plugin:
      plugin_path: https://github.com/databus23/helm-diff
      state: present
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/install-tokkio-ui-aws.yml
````yaml
---
- name: Setup tokkio UI
  hosts: all
  become: yes
  gather_facts: yes
  vars:
    environment: "{{ environment }}"
  tasks:
    # - name: NGC CLI Setup
    #   become: true
    #   block:
    #     - name: Download CLI
    #       get_url:
    #         url: https://ngc.nvidia.com/downloads/ngccli_linux.zip
    #         dest: /tmp/ngccli_linux.zip
    #         mode: 0664

    #     - name: Install NGC CLI
    #       unarchive:
    #         src: /tmp/ngccli_linux.zip
    #         dest: /usr/local/bin/
    #         remote_src: yes

    # - name: Debug script variable
    #   debug:
    #     msg: "Script: {{ task_config.script }}"
    #   when: state == "present" and not ansible_check_mode and dry_run_mode != "true"


    # - name: Debug environment variables
    #   debug:
    #     msg: "Environment: {{ task_config.environment }}"
    #   when: state == "present" and not ansible_check_mode and dry_run_mode != "true"

    - name: Create script to export environment variables
      copy:
        dest: "/mnt/tokkio-ui-env.sh"
        content: |
          #!/bin/bash

    - name: Add system_env_vars variable to the tokkio-ui-env.sh script
      lineinfile:
        path: "/mnt/tokkio-ui-env.sh"
        line: "export {{ item.key  }}=\"{{ item.value }}\""
      with_dict: "{{ task_config.environment.system_env_vars }}"

    - name: Add user_env_vars variable to the tokkio-ui-env.sh script
      lineinfile:
        path: "/mnt/tokkio-ui-env.sh"
        line: "export {{ item.key  }}=\"{{ item.value }}\""
      with_dict: "{{ task_config.environment.user_env_vars }}"

    - name: Add resource_env_vars variable to the tokkio-ui-env.sh script
      lineinfile:
        path: "/mnt/tokkio-ui-env.sh"
        line: "export {{ item.key  }}=\"{{ item.value }}\""
      with_dict: "{{ task_config.environment.resource_env_vars }}"

    - name: Ensure the export script is executable
      file:
        path: "/mnt/tokkio-ui-env.sh"
        mode: '0755'

    - name: Copy tokkio-ui setup script to target
      copy:
        src: "{{ task_config.script }}"
        dest: "/mnt/install-tokkio-ui-aws.sh"
        mode: '0755'

    - name: Setting up Tokkio-UI
      shell: |
         . /mnt/tokkio-ui-env.sh
         /mnt/install-tokkio-ui-aws.sh install
      #environment: "{{ task_config.environment }}"
      register: script_out
      when: state == "present" and not ansible_check_mode and dry_run_mode != "true"

    # - name: Check script output
    #   debug:
    #     msg: "{{ script_out }}"

    - name: Uninstall Tokkio-UI
      shell: |
         . /mnt/tokkio-ui-env.sh
         /mnt/install-tokkio-ui-aws.sh uninstall
      #environment: "{{ task_config.environment }}"
      when: state != "present" and not ansible_check_mode and dry_run_mode != "true"
      args:
        executable: /bin/bash
      register: script_out

    # - name: Check script output
    #   debug:
    #     msg: "{{ script_out }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/install-tokkio-ui-bm.yml
````yaml
---
- name: Setup Tokkio-UI
  hosts: all
  become: yes
  gather_facts: yes
  vars:
    environment: "{{ environment }}"
  tasks:
    - name: NGC CLI Setup
      become: true
      block:
        - name: Download CLI
          get_url:
            url: https://ngc.nvidia.com/downloads/ngccli_linux.zip
            dest: /tmp/ngccli_linux.zip
            mode: 0664

        - name: Install NGC CLI
          unarchive:
            src: /tmp/ngccli_linux.zip
            dest: /usr/local/bin/
            remote_src: yes

    # - name: Debug script variable
    #   debug:
    #     msg: "Script: {{ task_config.script }}"


    # - name: Debug environment variables
    #   debug:
    #     msg: "Environment: {{ task_config.environment }}"

    - name: Create script to export environment variables
      copy:
        dest: "/mnt/tokkio-ui-env.sh"
        content: |
          #!/bin/bash

    - name: Add system_env_vars variable to the tokkio-ui-env.sh script
      lineinfile:
        path: "/mnt/tokkio-ui-env.sh"
        line: "export {{ item.key  }}=\"{{ item.value }}\""
      with_dict: "{{ task_config.environment.system_env_vars }}"

    - name: Add user_env_vars variable to the tokkio-ui-env.sh script
      lineinfile:
        path: "/mnt/tokkio-ui-env.sh"
        line: "export {{ item.key  }}=\"{{ item.value }}\""
      with_dict: "{{ task_config.environment.user_env_vars }}"

    - name: Add resource_env_vars variable to the tokkio-ui-env.sh script
      lineinfile:
        path: "/mnt/tokkio-ui-env.sh"
        line: "export {{ item.key  }}=\"{{ item.value }}\""
      with_dict: "{{ task_config.environment.resource_env_vars }}"

    - name: Ensure the export script is executable
      file:
        path: "/mnt/tokkio-ui-env.sh"
        mode: '0755'

    - name: Copy Tokkio-UI setup script to target
      copy:
        src: "{{ task_config.script }}"
        dest: "/mnt/install-tokkio-ui-bm.sh"
        mode: '0755'


    - name:  Setting up Tokkio-UI
      shell: |
        . /mnt/tokkio-ui-env.sh
        /mnt/install-tokkio-ui-bm.sh install
      #environment: "{{ task_config.environment }}"
      when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
      register: script_out

    # - name: Check script output
    #   debug:
    #     msg: "{{ script_out }}"

    - name: Uninstall Tokkio-UI
      shell: | 
        . /mnt/tokkio-ui-env.sh
        /mnt/install-tokkio-ui-bm.sh uninstall
      #environment: "{{ task_config.environment }}"
      when: state != "present" and not ansible_check_mode and dry_run_mode != "true"
      args:
        executable: /bin/bash
      register: script_out
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/install-tokkio-ui-gcp.yml
````yaml
---
- name: Setup tokkio UI
  hosts: all
  become: yes
  gather_facts: yes
  vars:
    environment: "{{ environment }}"
  tasks:
    # - name: NGC CLI Setup
    #   become: true
    #   block:
    #     - name: Download CLI
    #       get_url:
    #         url: https://ngc.nvidia.com/downloads/ngccli_linux.zip
    #         dest: /tmp/ngccli_linux.zip
    #         mode: 0664

    #     - name: Install NGC CLI
    #       unarchive:
    #         src: /tmp/ngccli_linux.zip
    #         dest: /usr/local/bin/
    #         remote_src: yes

    # - name: Debug script variable
    #   debug:
    #     msg: "Script: {{ task_config.script }}"
    #   when: state == "present" and not ansible_check_mode and dry_run_mode != "true"


    # - name: Debug environment variables
    #   debug:
    #     msg: "Environment: {{ task_config.environment }}"
    #   when: state == "present" and not ansible_check_mode and dry_run_mode != "true"

    - name: Create script to export environment variables
      copy:
        dest: "/mnt/tokkio-ui-env.sh"
        content: |
          #!/bin/bash

    - name: Add system_env_vars variable to the tokkio-ui-env.sh script
      lineinfile:
        path: "/mnt/tokkio-ui-env.sh"
        line: "export {{ item.key  }}=\"{{ item.value }}\""
      with_dict: "{{ task_config.environment.system_env_vars }}"

    - name: Add user_env_vars variable to the tokkio-ui-env.sh script
      lineinfile:
        path: "/mnt/tokkio-ui-env.sh"
        line: "export {{ item.key  }}=\"{{ item.value }}\""
      with_dict: "{{ task_config.environment.user_env_vars }}"

    - name: Add resource_env_vars variable to the tokkio-ui-env.sh script
      lineinfile:
        path: "/mnt/tokkio-ui-env.sh"
        line: "export {{ item.key  }}=\"{{ item.value }}\""
      with_dict: "{{ task_config.environment.resource_env_vars }}"

    - name: Ensure the export script is executable
      file:
        path: "/mnt/tokkio-ui-env.sh"
        mode: '0755'

    - name: Copy tokkio-ui setup script to target
      copy:
        src: "{{ task_config.script }}"
        dest: "/mnt/install-tokkio-ui-aws.sh"
        mode: '0755'

    - name: Setting up Tokkio-UI
      shell: |
         . /mnt/tokkio-ui-env.sh
         /mnt/install-tokkio-ui-aws.sh install
      #environment: "{{ task_config.environment }}"
      register: script_out
      when: state == "present" and not ansible_check_mode and dry_run_mode != "true"

    # - name: Check script output
    #   debug:
    #     msg: "{{ script_out }}"

    - name: Uninstall Tokkio-UI
      shell: |
        . /mnt/tokkio-ui-env.sh
        /mnt/install-tokkio-ui-aws.sh uninstall
      #environment: "{{ task_config.environment }}"
      when: state != "present" and not ansible_check_mode and dry_run_mode != "true"
      args:
        executable: /bin/bash
      register: script_out

    # - name: Check script output
    #   debug:
    #     msg: "{{ script_out }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/install-tokkio-ui.yml
````yaml
---
- name: Setup tokkio UI
  hosts: all
  become: yes
  gather_facts: yes
  vars:
    environment: "{{ environment }}"
  tasks:
    # - name: NGC CLI Setup
    #   become: true
    #   block:
    #     - name: Download CLI
    #       get_url:
    #         url: https://ngc.nvidia.com/downloads/ngccli_linux.zip
    #         dest: /tmp/ngccli_linux.zip
    #         mode: 0664

    #     - name: Install NGC CLI
    #       unarchive:
    #         src: /tmp/ngccli_linux.zip
    #         dest: /usr/local/bin/
    #         remote_src: yes

    # - name: Debug script variable
    #   debug:
    #     msg: "Script: {{ task_config.script }}"
    #   when: state == "present" and not ansible_check_mode and dry_run_mode != "true"


    # - name: Debug environment variables
    #   debug:
    #     msg: "Environment: {{ task_config.environment }}"
    #   when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
    - name: Create script to export environment variables
      copy:
        dest: "/mnt/tokkio-ui-env.sh"
        content: |
          #!/bin/bash

    - name: Add system_env_vars variable to the tokkio-ui-env.sh script
      lineinfile:
        path: "/mnt/tokkio-ui-env.sh"
        line: "export {{ item.key  }}=\"{{ item.value }}\""
      with_dict: "{{ task_config.environment.system_env_vars }}"

    - name: Add user_env_vars variable to the tokkio-ui-env.sh script
      lineinfile:
        path: "/mnt/tokkio-ui-env.sh"
        line: "export {{ item.key  }}=\"{{ item.value }}\""
      with_dict: "{{ task_config.environment.user_env_vars }}"

    - name: Add resource_env_vars variable to the tokkio-ui-env.sh script
      lineinfile:
        path: "/mnt/tokkio-ui-env.sh"
        line: "export {{ item.key  }}=\"{{ item.value }}\""
      with_dict: "{{ task_config.environment.resource_env_vars }}"

    - name: Ensure the export script is executable
      file:
        path: "/mnt/tokkio-ui-env.sh"
        mode: '0755'

    - name: Copy tokkio-ui setup script to target
      copy:
        src: "{{ task_config.script }}"
        dest: "/mnt/install-tokkio-ui.sh"
        mode: '0755'

    - name: Setting up Tokkio-UI
      shell: |
        . /mnt/tokkio-ui-env.sh
        /mnt/install-tokkio-ui.sh install
      #environment: "{{ task_config.environment }}"
      register: script_out
      when: state == "present" and not ansible_check_mode and dry_run_mode != "true"

    - name: Uninstall Tokkio-UI
      shell: |
        . /mnt/tokkio-ui-env.sh
        /mnt/install-tokkio-ui.sh uninstall
      #environment: "{{ task_config.environment }}"
      when: state != "present" and not ansible_check_mode and dry_run_mode != "true"
      args:
        executable: /bin/bash
      register: script_out

    # - name: Check script output
    #   debug:
    #     msg: "{{ script_out }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/k8s-label.yml
````yaml
---
- hosts: all
  gather_facts: yes
  tasks: []
- hosts: master
  gather_facts: no
  tasks:
  - name: prepare node labels
    set_fact:
      node_labels: "{{ node_labels | default({}) | combine( {hostvars[item.key].ansible_facts.hostname: item.value} ) }}"
    with_dict: "{{ task_config.node_labels }}"
  - name: apply node labels
    kubernetes.core.k8s:
      kubeconfig: "{{ task_config.kubeconfig | default(omit) }}"
      state: "{{ state | default('present') }}"
      definition:
        apiVersion: v1
        kind: Node
        metadata:
          name: "{{ item.key }}"
          labels: "{{ item.value if item.value is defined else {} }}"
    with_dict: "{{ node_labels }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/k8s-manifest.yml
````yaml
---
- hosts: all
  gather_facts: yes
  tasks:
  - name: download manifest from url
    set_fact:
      manifest_from_url_path: "{{ ansible_user_dir }}/{{ task_config.url | basename }}"
    when: task_config.url is defined
  - name: download manifest from url
    ignore_errors: "{{ task_config.ignore_errors | default(false) }}"
    get_url:
      url: "{{ task_config.url }}"
      dest: "{{ manifest_from_url_path }}"
      mode: '0664'
    when: task_config.url is defined
  - name: install manifest
    ignore_errors: "{{ task_config.ignore_errors | default(false) }}"
    kubernetes.core.k8s:
      kubeconfig: "{{ task_config.kubeconfig | default(omit) }}"
      state: "{{ state | default('present') }}"
      namespace: "{{ task_config.namespace | default(omit) }}"
      kind: "{{ task_config.kind | default(omit) }}"
      src: "{{ manifest_from_url_path | default(omit) }}"
      definition:
        "{{ task_config.definition | default(omit) }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/k8s-patch.yml
````yaml
---
- hosts: all
  gather_facts: yes
  tasks:
  - name: apply patch
    ignore_errors: "{{ task_config.ignore_errors | default(false) }}"
    kubernetes.core.k8s_json_patch:
      kubeconfig: "{{ task_config.kubeconfig | default(omit) }}"
      api_version: "{{ task_config.api_version | default('v1') }}"
      kind: "{{ task_config.kind }}"
      namespace: "{{ task_config.namespace }}"
      name: "{{ task_config.name }}"
      patch:
        "{{ task_config.patch }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/k8s-secret-check-empty-dockerconfig.yml
````yaml
- name: Fail if variable is empty
    fail:
      msg: "The password for {{ item.username }} is empty. Please provide a valid value."
    when: item.password | length == 0
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/k8s-secret-check-empty-opaque.yml
````yaml
- name: Fail if variable is empty
    fail:
      msg: "The variable {{ item.key }} is empty. Please provide a valid value."
    when: item.value | length == 0
    with_items: "{{ item.entries }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/k8s-secret-check-empty.yml
````yaml
---
- hosts: all
  gather_facts: yes
  tasks:
    - name: check if k8s secrets are empty of type Opaque
      loop: "{{ task_config.k8s_secrets }}"
      when: item.type == "Opaque"
      include_tasks: k8s-secret-check-empty-opaque.yml

    - name: check if k8s secrets are empty of type dockerconfigjson
      loop: "{{ task_config.k8s_secrets }}"
      when: item.type == "dockerconfigjson"
      include_tasks: k8s-secret-check-empty-dockerconfig.yml
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/k8s-secret-dockerconfig.yml
````yaml
- name: Concatenate username and password
    set_fact:
      combined_string: "{{ item.username }}:{{ item.password }}"
  - name: Encode concatenated string to Base64
    set_fact:
      encoded_credentials: "{{ combined_string | b64encode }}"
  - name: create dockerconfigjson
    set_fact:
      dockerconfigjson: |
          {
            "auths": {
              "{{ item.registry_name }}": {
                "auth": "{{ encoded_credentials }}"
              }
            }
          }
  - name: encode dockerconfigjson
    set_fact:
      encode_dockerconfigjson: "{{ dockerconfigjson | to_json | b64encode }}"
  # - name: display
  #   debug:
  #     msg: "{{ encode_dockerconfigjson }}"
  - name: install k8s secret of type dockerconfigjson
    kubernetes.core.k8s:
      kubeconfig: "{{ item.kubeconfig | default(omit) }}"
      state: "{{ state | default('present') }}"
      definition:
        apiVersion: v1
        kind: Secret
        type: "kubernetes.io/{{ item.type }}"
        metadata:
          name: "{{ item.name }}"
          namespace: "{{ task_config.namespace | default('default') }}"
        data:
          .dockerconfigjson: "{{ encode_dockerconfigjson }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/k8s-secret-old.yml
````yaml
---
- hosts: all
  gather_facts: yes
  tasks:
  - name: install secret
    kubernetes.core.k8s:
      kubeconfig: "{{ task_config.kubeconfig | default(omit) }}"
      state: "{{ state | default('present') }}"
      definition:
        apiVersion: v1
        kind: Secret
        type: "{{ task_config.type }}"
        metadata:
          name: "{{ task_config.name }}"
          namespace: "{{ task_config.namespace | default('default') }}"
        data:
          "{{ task_config.data }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/k8s-secret-opaque.yml
````yaml
- name: create secret_data variable 
      set_fact:
        secret_data: |
          {% set result = {} %}
          {%- for element in item.entries -%}
          {% set _ = result.update({element.key: element.value | b64encode}) %}
          {%- endfor -%}
          {{ result }}

    # - name: Debug secret data
    #   debug:
    #     msg: "{{ secret_data }}"

    - name: Create Kubernetes secrets of type Opaque
      kubernetes.core.k8s:
        kubeconfig: "{{ item.kubeconfig | default(omit) }}"
        state: "{{ state | default('present') }}"
        definition:
          api_version: v1
          kind: Secret
          type: "{{ item.type }}"
          metadata:
            name: "{{ item.name }}"
            namespace: "{{ task_config.namespace | default('default') }}"
          data: "{{ secret_data }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/k8s-secret.yml
````yaml
---
- hosts: all
  gather_facts: yes
  tasks:
  - name: install k8s secret of type Opaque
    loop: "{{ task_config.k8s_secrets }}"
    when: item.type == "Opaque"
    include_tasks: k8s-secret-opaque.yml


  - name: Install k8s secret of type dockerconfigjson
    loop: "{{ task_config.k8s_secrets }}"
    when: item.type == "dockerconfigjson"
    include_tasks: k8s-secret-dockerconfig.yml
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/k8s-taint.yml
````yaml
---
- hosts: all
  gather_facts: yes
  tasks: []
- hosts: master
  gather_facts: no
  tasks:
  - name: prepare node taints
    set_fact:
      node_taints: "{{ node_taints | default({}) | combine( {hostvars[item.key].ansible_facts.hostname: item.value} ) }}"
    with_dict: "{{ task_config.node_taints }}"
  - name: apply node taints
    kubernetes.core.k8s_taint:
      kubeconfig: "{{ task_config.kubeconfig | default(omit) }}"
      state: "{{ state | default('present') }}"
      name: "{{ item.key }}"
      taints: "{{ item.value if item.value is defined else [] }}"
    with_dict: "{{ node_taints }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/k8s-task-pre-requisites.yml
````yaml
---
- hosts: all
  gather_facts: no
  tasks:
  - name: install pip3
    become: yes
    become_user: root
    apt:
      name: python3-pip
      state: present
  - name: install pre-requisites
    pip:
      name:
      - jsonpatch
      - kubernetes>=12.0.0
      - PyYAML>3.11
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/mount-data-disk.yml
````yaml
---
- name: Mount data disk on /opt
  hosts: all
  become: yes
  gather_facts: yes
  # vars:
  #   environment: "{{ environment }}"
  tasks:
    - name: Debug script variable
      debug:
        msg: "Script: {{ task_config.script }}"

    # - name: Debug environment variables
    #   debug:
    #     msg: "Environment: {{ task_config.environment }}"

    - name: Copy mount-data-disk.sh setup script to target
      copy:
        src: "{{ task_config.script }}"
        dest: "/mnt/mount-data-disk.sh"
        mode: '0755'

    - name: Run mount-data-disk.sh setup script
      shell: "/mnt/mount-data-disk.sh"
      # environment: "{{ task_config.environment }}"
      register: script_out

    # - name: Check script output
    #   debug:
    #     msg: "{{ script_out }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/nvidia-container-toolkit-install.yml
````yaml
---
- hosts: all
  become: true
  become_method: sudo
  tasks:
  - name: gather the package facts
    package_facts:
      manager: auto
  - name: add apt signing key
    when: "'nvidia-container-toolkit' not in ansible_facts.packages"
    apt_key:
      url: https://nvidia.github.io/libnvidia-container/gpgkey
      keyring: /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
      state: present
  - name: add repos
    when: "'nvidia-container-toolkit' not in ansible_facts.packages"
    block:
    - name: fetch repo urls
      uri:
        url: "https://nvidia.github.io/libnvidia-container/{{ ansible_distribution | lower }}{{ ansible_distribution_version }}/libnvidia-container.list"
        return_content: true
      register: repos_file
    - name: prepare repo urls
      shell: "echo '{{ repos_file.content }}' | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | grep -v '^[[:space:]]*#'"
      register: repos
    - name: add repos to sources
      apt_repository:
        repo: "{{ item }}"
        state: present
        filename: nvidia-container-toolkit
      with_items: "{{ repos.stdout_lines }}"
  - name: force an apt update
    when: "'nvidia-container-toolkit' not in ansible_facts.packages"
    apt:
      update_cache: true
    changed_when: false
    register: update
    retries: 10
    until: update is success
  - name: install nvidia-container-toolkit
    when: "'nvidia-container-toolkit' not in ansible_facts.packages"
    apt:
      name: nvidia-container-toolkit
      state: present
    register: nvidia_container_toolkit_check
    retries: 10
    until: nvidia_container_toolkit_check is success
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/nvidia-docker-install.yml
````yaml
---
- hosts: all
  become: true
  become_method: sudo
  vars:
    docker_daemon_options:
      default-address-pools:
      - base: "192.168.2.0/16"
        size: 24
      default-runtime: nvidia
      runtimes:
        nvidia:
          path: /usr/bin/nvidia-container-runtime
          runtimeArgs: []
    docker_users:
    - "{{ ansible_user }}"
  tasks:
  - name: populate service facts
    service_facts:
  - name: capture running services
    set_fact:
      running_services: "{{ ansible_facts.services | dict2items | selectattr('value.state', 'equalto', 'running') | map(attribute='key') }}"
      all_services: "{{ ansible_facts.services | dict2items | map(attribute='key') }}"
  - name: cleanup containerd
    when: running_services | intersect(['docker', 'docker.service']) | length == 0
    block:
    - name: stop and disable containerd service
      systemd_service:
        name: containerd
        state: stopped
        enabled: false
      when: all_services | intersect(['containerd', 'containerd.service']) | length > 0
    - name: systemd daemon reload
      systemd_service:
        daemon_reload: true
    - name: remove apt packages for containerd
      apt:
        name: containerd.io
        state: absent
        purge: true
    - name: remove containerd configs
      file:
        path: "{{ item }}"
        state: absent
      loop:
      - /etc/systemd/system/containerd.service.d
      - /etc/systemd/system/containerd.service
  - name: install docker
    include_role:
      name: geerlingguy.docker
    when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
- import_playbook: nvidia-container-toolkit-install.yml
  when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
- hosts: all
  become: true
  become_method: sudo
  tasks:
  - name: restart docker
    systemd_service:
      state: restarted
      name: docker
    when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/nvidia-driver-install.yml
````yaml
---
- hosts: all
  become: true
  become_method: sudo
  vars:
    gpu_driver_version: "{{ task_config.gpu_driver_version | default('550.54.15') }}"
  tasks:
  - name: check nvidia driver modules are loaded
    shell: lsmod | grep -i nvidia
    register: nvidia_mod
    when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
    no_log: true
    failed_when: false
  - name: check nvidia-smi loaded
    shell: nvidia-smi
    register: nvidia_smi
    when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
    no_log: true
    failed_when: false
  - name: get current driver version
    shell: nvidia-smi --query-gpu=driver_version --format=csv,noheader --id=0
    register: current_driver_version
    when: state == "present" and not ansible_check_mode and dry_run_mode != "true" and nvidia_smi.rc == 0
    no_log: true
    failed_when: false
  - name: check if driver installation is required
    set_fact:
      install_driver: "{{ state == 'present' and not ansible_check_mode and dry_run_mode != 'true' and (nvidia_mod.rc >= 1 or nvidia_smi.rc >= 1 or current_driver_version.stdout != gpu_driver_version) }}"
  - name: unload nvidia
    when: (state != "present" and not ansible_check_mode and dry_run_mode != "true") or install_driver
    shell: /usr/bin/nvidia-uninstall --silent; kill -9 $(lsof /dev/nvidia* | awk '{print $2}' | grep -v PID | uniq); rmmod nvidia_uvm; rmmod nvidia_drm; rmmod nvidia_modeset; rmmod nvidia
    become: true
    failed_when: false
  - name: nvidia driver clean up
    when: (state != "present" and not ansible_check_mode and dry_run_mode != "true") or install_driver
    block:
    - name: remove ubuntu unattended upgrades to prevent apt lock
      ansible.builtin.apt:
        name: unattended-upgrades
        state: absent
        purge: yes
      register: apt_cleanup
      retries: 10
      until: apt_cleanup is success
    - name: remove old apt repository
      apt_repository:
        repo: ppa:graphics-drivers/ppa
        state: absent
      register: ppa_clean
      retries: 10
      until: ppa_clean is success
    - name: remove nvidia packages
      apt:
        name:
        - "*cuda*"
        - "libnvidia-cfg1-*"
        - "libnvidia-common-*"
        - "libnvidia-compute-*"
        - "libnvidia-decode-*"
        - "libnvidia-encode-*"
        - "libnvidia-extra-*"
        - "libnvidia-fbc1-*"
        - "libnvidia-gl-*"
        - "nvidia-compute-utils-*"
        - "nvidia-dkms-*"
        - "nvidia-driver-*"
        - "nvidia-kernel-common-*"
        - "nvidia-kernel-source-*"
        - "nvidia-modprobe"
        - "nvidia-prime"
        - "nvidia-settings"
        - "nvidia-utils-*"
        - "screen-resolution-extra"
        - "xserver-xorg-video-nvidia-*"
        - "gdm*"
        - "xserver-xorg-*"
        autoremove: yes
        purge: yes
        state: absent
      register: nvidia_cleanup
      retries: 10
      until: nvidia_cleanup is success
    - name: remove old keyring
      shell:
        cmd: "apt-key del 7fa2af80"
  - name: update ubuntu system
    become: true
    when: install_driver
    ignore_errors: true
    block:
    - name: force an apt update
      apt:
        update_cache: true
      changed_when: false
      register: update
      retries: 10
      until: update is success
    - name: ensure build-essential is installed
      apt:
        name: build-essential
        state: present
      register: build_essential_check
      retries: 10
      until: build_essential_check is success
    - name: ensure kmod is installed
      apt:
        name: kmod
        state: present
      register: kmod_check
      retries: 10
      until: kmod_check is success
  - name: install nvidia trd driver
    become: true
    when: install_driver
    ignore_errors: true
    block:
    - name: temporarily adjust account password policy to allow for successful nvidia driver install
      shell: chage -d 1 root
    - name: install driver packages
      shell: "BASE_URL=https://us.download.nvidia.com/tesla; curl -fSsl -O $BASE_URL/{{ gpu_driver_version }}/NVIDIA-Linux-{{ ansible_architecture }}-{{ gpu_driver_version }}.run; chmod +x ./NVIDIA-Linux-{{ ansible_architecture }}-{{ gpu_driver_version }}.run; sh ./NVIDIA-Linux-{{ ansible_architecture }}-{{ gpu_driver_version }}.run --silent"
  - name: enable nvidia persistence daemon
    become: true
    when: install_driver
    shell: nvidia-persistenced
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/output-jinja2-expression-to-file.yml
````yaml
---
- hosts: localhost
  connection: local
  gather_facts: no
  become: false
  tasks:
  - name: delete any existing output file
    file:
      path: "{{ output_file }}"
      state: absent
  - name: create output file
    file:
      path: "{{ output_file }}"
      state: touch
  - name: populate output file
    copy:
      content: "{{ expression | to_nice_yaml(indent=2) }}"
      dest: "{{ output_file }}"
    when: output_format is defined and output_format == "yaml"
  - name: populate output file
    copy:
      content: "{{ expression | to_nice_json(indent=2) }}"
      dest: "{{ output_file }}"
    when: output_format is defined and output_format == "json"
  - name: populate output file
    copy:
      content: "{{ expression }}"
      dest: "{{ output_file }}"
    when: output_format is not defined or output_format not in ["yaml", "json"]
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/process-user-config.yml
````yaml
---
- hosts: localhost
  connection: local
  gather_facts: yes
  become: false
  tasks:
  - name: populate output file
    copy:
      content: "{{ lookup('template', input_file) }}"
      dest: "{{ output_file }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/setup-coturn-bm.yml
````yaml
---
- name: Setup Coturn Server
  hosts: all
  become: yes
  gather_facts: yes
  vars:
    script: "{{ playbook_dir }}/scripts/coturn-setup-bm.sh"  # Ensure the script path is correct
    environment: "{{ environment }}"
  tasks:
    # - name: Debug script variable
    #   debug:
    #     msg: "Script: {{ task_config.script }}"
    #   when: state == "present" and not ansible_check_mode and dry_run_mode != "true"

    # - name: Debug environment variables
    #   debug:
    #     msg: "Environment: {{ task_config.environment }}"
    #   when: state == "present" and not ansible_check_mode and dry_run_mode != "true"

    - name: Copy coturn setup script to target
      copy:
        src: "{{ task_config.script }}"
        dest: "/mnt/coturn-setup-bm.sh"
        mode: '0755'
      when: state == "present" and not ansible_check_mode and dry_run_mode != "true"

    - name: Setting up Coturn Server
      shell: "/mnt/coturn-setup-bm.sh install"
      environment: "{{ task_config.environment }}"
      when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
      register: script_output
    
    - name: Uninstalling Coturn Server 
      shell: "/mnt/coturn-setup-bm.sh uninstall"
      when: state != "present" and not ansible_check_mode and dry_run_mode != "true"
      register: script_output
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/setup-coturn.yml
````yaml
---
- name: Setup Coturn Server
  hosts: all
  become: yes
  gather_facts: yes
  vars:
    script: "{{ playbook_dir }}/scripts/coturn-setup.sh"  # Ensure the script path is correct
    environment: "{{ environment }}"
  tasks:
    # - name: Debug script variable
    #   debug:
    #     msg: "Script: {{ task_config.script }}"
    #   when: state == "present" and not ansible_check_mode and dry_run_mode != "true"

    # - name: Debug environment variables
    #   debug:
    #     msg: "Environment: {{ task_config.environment }}"
    #   when: state == "present" and not ansible_check_mode and dry_run_mode != "true"

    - name: Copy coturn setup script to target
      copy:
        src: "{{ task_config.script }}"
        dest: "/mnt/coturn-setup.sh"
        mode: '0755'
      when: state == "present" and not ansible_check_mode and dry_run_mode != "true"

    - name: Setting up Coturn Server
      shell: "/mnt/coturn-setup.sh install"
      environment: "{{ task_config.environment }}"
      when: state == "present" and not ansible_check_mode and dry_run_mode != "true"
      register: script_output
    
    - name: Uninstalling Coturn Server 
      shell: "/mnt/coturn-setup.sh uninstall"
      when: state != "present" and not ansible_check_mode and dry_run_mode != "true"
      register: script_output
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/sleep.yml
````yaml
---
- hosts: all
  gather_facts: yes
  become: false
  tasks:
    - name: "sleep {{ task_vars.time }}"
      shell: |
        sleep "{{ task_vars.time }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/playbooks/write-to-file.yml
````yaml
---
- hosts: all
  gather_facts: yes
  become: false
  tasks:
  - name: ensure dir exists
    file:
      path: "{{ task_config.dest | dirname }}"
      state: directory
      mode: '0755'
  - name: write content to file
    copy:
      content: "{{ task_config.content }}"
      dest: "{{ task_config.dest }}"
      mode: "{{ task_config.mode | default(omit) }}"
      backup: "{{ task_config.backup | default(omit) }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/ansible-requirements.yml
````yaml
collections:
- name: ansible.posix
  version: 1.5.4
- name: community.general
  version: 8.3.0
- name: kubernetes.core
  version: 3.2.0
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/app-tasks.yml
````yaml
schema_version: '0.0.7'
tasks:
- name: app-pre-requisite
  play: app-pre-requisites
  targets:
  - "{{ task_each_vars.value.targets.master }}"  
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
- name: create-app-namespace
  play: k8s-manifest
  targets:
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  config_files:
  - k8s-manifest/namespace.yml
  vars:
    name: "{{ configs.app_settings.k8s_namespace | default('app') }}"
- name: validate-app-k8s-secrets-value
  play: k8s-secret-check-empty
  targets:
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  config:
    k8s_secrets: "{{ configs.app_settings.k8s_secrets }}"
- name: app-k8s-secrets
  play: k8s-secret
  targets:
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  config:
    k8s_secrets: "{{ configs.app_settings.k8s_secrets }}"
    namespace: "{{ configs.app_settings.k8s_namespace | default('app') }}"
- name: coturn-get-nic-name
  play: get-network-interface
  targets:
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['coturn']) | items2dict }}"
- name: coturn-setup-script
  play: setup-coturn-bm
  targets:
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['coturn']) | items2dict }}"
  config:
    script: "{{ playbook_dir }}/scripts/coturn-setup-bm.sh"
    environment:
      TURNSERVER_REALM: "{{ configs.turn_server_settings.coturn.realm | default('mydummyt.org')  }}"
      TURNSERVER_USERNAME: "{{ configs.turn_server_settings.coturn.username | default('foo')  }}"
      TURNSERVER_PASSWORD: "{{ configs.turn_server_settings.coturn.password | default('bar')  }}"
      #TURNSERVER_PUBLIC_IP: "{{ iac.clusters.turn.ip_address }}"
      TURNSERVER_PRIVATE_IP: "{{ iac.clusters.turn.ip_address }}"
      LISTENING_DEVICE: "{{ lookup('file', dist_dir + '/' + ansible_hostname + '/nic_name') }}"

#below plays are for rp server configuration 
# - name: create-namespace-for-rp
#   play: k8s-manifest
#   targets:
#   - "{{ task_each_vars.value.targets.master }}"
#   for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['rp']) | items2dict }}"
#   config_files:
#   - k8s-manifest/namespace.yml
#   vars:
#     name: "{{ configs.app_settings.k8s_namespace | default('app') }}"

# - name: ngc-docker-reg-secret-for-rp
#   play: k8s-secret
#   targets:
#   - "{{ task_each_vars.value.targets.master }}"
#   for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['rp']) | items2dict }}"
#   config_files:
#   - k8s-secret/docker-config-json.yml
#   vars:
#     name: ngc-docker-reg-secret
#     namespace: "{{ configs.app_settings.k8s_namespace | default('app') }}"
#     docker_config_json: >
#       {
#         "auths": {
#           "nvcr.io": {
#             "auth": "{{ ('$oauthtoken:' + secrets.ngc_cli_api_key) | b64encode }}"
#           }
#         }
#       }

# - name: rp-app-chart-install
#   play: helm-release
#   targets: 
#   - "{{ task_each_vars.value.targets.master }}"
#   for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['rp']) | items2dict }}"
#   vars:
#     network_interface_name: "{{ configs.rp_network_interface_name | default('eth0') }}"
#   config:
#     repo_name: "rp-repo"
#     repo_url: "{{ configs.rp_chart.repo_url | default('https://helm.ngc.nvidia.com/lypzw7yma4rr/tokkiodev') }}"
#     repo_username: "$oauthtoken"
#     repo_password: "{{ secrets.ngc_cli_api_key }}"
#     chart_ref: "{{ configs.rp_chart.chart_name | default('rproxy') }}"
#     chart_version: "{{ configs.rp_chart.chart_version | default('0.0.6') }}"
#     namespace: "{{ configs.app_settings.k8s_namespace | default('app') }}"
#     release_name: "{{ configs.rp_chart.release_name | default('rp') }}"
#     state: "{{ state | default('present') }}"
#     values_files:
#     - "{{ dist_dir }}/config-files/helm-release/rproxy-override-values.yml"

- name: tokkio-app
  play: helm-release
  targets:
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  condition: "{{ (configs.app_settings.helm_chart.repo.enable | default(true) == true) and (configs.app_settings.helm_chart.local.enable | default(false) == false)  }}"
  vars: 
    twilio_account_sid: "{{ configs.turn_server_settings.twilio.account_sid | default('') }}"
    twilio_auth_token: "{{ configs.turn_server_settings.twilio.auth_token | default('') }}"
    use_twilio_stun_turn: "{{ iac.clusters.app.use_twilio_stun_turn | lower | bool }}"
    turnurl_conn_string: "{{ configs.turn_server_settings.coturn.username | default('foo') }}:{{ configs.turn_server_settings.coturn.password | default('bar') }}@{{ iac.clusters.turn.ip_address | default('127.0.0.1') }}:3478"
    reverse_proxy_server_address : "{{ iac.clusters.turn.ip_address | default('') }}"
    use_reverse_proxy: "{{ iac.clusters.app.use_reverse_proxy | lower | bool }}"
    enable_idp_auth: "{{ configs.app_settings.enable_idp_auth | default(false) | lower | bool }}"
    turn_server_conn: "{{ iac.clusters.turn.ip_address | default('127.0.0.1') }}:3478"
    turn_username: "{{ configs.turn_server_settings.coturn.username | default('foo') }}"
    turn_password: "{{ configs.turn_server_settings.coturn.password | default('bar') }}"
  config:
    repo_name: "tokkio-app"
    repo_url: "{{ configs.app_settings.helm_chart.repo.repo_url | default('https://helm.ngc.nvidia.com/nvidia/ace') }}"
    repo_username: "$oauthtoken"
    repo_password: "{{ secrets.ngc_cli_api_key }}"
    chart_ref: "{{ configs.app_settings.helm_chart.repo.chart_name | default('ucs-tokkio-app-base-3-stream-llm-rag-3d-ov') }}"
    chart_version: "{{ configs.app_settings.helm_chart.repo.chart_version | default('4.1.4') }}"
    namespace: "{{ configs.app_settings.k8s_namespace | default('app') }}"
    release_name: "{{ configs.app_settings.helm_chart.repo.release_name | default('tokkio-app') }}"
    state: "{{ state | default('present') }}"
    values_files: "{{ [[dist_dir + '/config-files/helm-release/tokkio-app-audio-video-app.yml'], [ configs.app_settings.helm_chart.repo.user_value_override_files | default([])]] | flatten }}"

- name: tokkio-app-local-chart
  play: helm-release
  targets:
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  condition: "{{ configs.app_settings.helm_chart.local.enable | default(false) == true }}"
  vars:
    twilio_account_sid: "{{ configs.turn_server_settings.twilio.account_sid | default('') }}"
    twilio_auth_token: "{{ configs.turn_server_settings.twilio.auth_token | default('') }}"
    use_twilio_stun_turn: "{{ iac.clusters.app.use_twilio_stun_turn | lower | bool }}"
    turnurl_conn_string: "{{ configs.turn_server_settings.coturn.username | default('foo') }}:{{ configs.turn_server_settings.coturn.password | default('bar') }}@{{ iac.clusters.turn.ip_address | default('127.0.0.1') }}:3478"
    reverse_proxy_server_address : "{{ iac.clusters.turn.ip_address | default('') }}"
    use_reverse_proxy: "{{ iac.clusters.app.use_reverse_proxy | lower | bool }}"
    enable_idp_auth: "{{ configs.app_settings.enable_idp_auth | default(false) | lower | bool }}"
    turn_server_conn: "{{ iac.clusters.turn.ip_address | default('127.0.0.1') }}:3478"
    turn_username: "{{ configs.turn_server_settings.coturn.username | default('foo') }}"
    turn_password: "{{ configs.turn_server_settings.coturn.password | default('bar') }}"
  config:
    local_chart: "{{ configs.app_settings.helm_chart.local.path }}"
    namespace: "{{ configs.app_settings.k8s_namespace | default('app') }}"
    release_name: "{{ configs.app_settings.helm_chart.local.release_name | default('tokkio-app') }}"
    state: "{{ state | default('present') }}"
    values_files: "{{ [[dist_dir + '/config-files/helm-release/tokkio-app-audio-video-app.yml'], [ configs.app_settings.helm_chart.local.user_value_override_files | default([])]] | flatten }}"

- name: setup-tokkio-ui
  play: install-tokkio-ui-bm
  targets:
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  config:
    script: "{{ playbook_dir }}/scripts/install-tokkio-ui-bm.sh"
    environment:
      resource_env_vars: 
        resource_org: "{{ configs.ui_settings.resource.ngc.org | default('nvidia') }}"
        resource_team: "{{ configs.ui_settings.resource.ngc.team | default('ace') }}"
        resource_name: "{{ configs.ui_settings.resource.ngc.name | default('tokkio_ui') }}"
        resource_version: "{{ configs.ui_settings.resource.ngc.version | default('5.0.0') }}"
        resource_file: "{{ configs.ui_settings.resource.ngc.file | default('ui.tar.gz') }}"
      user_env_vars: "{{ configs.ui_settings.user_env_vars | default({}) }}"
      system_env_vars:
        NGC_CLI_API_KEY: "{{ secrets.ngc_cli_api_key }}"
        VST_ENDPOINT: "ws://{{ iac.clusters.app.ip_address }}:30888/vms/ws"
        UI_SERVER_ENDPOINT: "http://{{ iac.clusters.app.ip_address }}:30888"
        WEBSOCKET_ENDPOINT: "ws://{{ iac.clusters.app.ip_address }}:30888/ws"
        VST_WEBSOCKET_ENDPOINT: "ws://{{ iac.clusters.app.ip_address }}:30888/vms/ws"
        INGRESS_ENDPOINT: "http://{{ iac.clusters.app.ip_address }}:30888"
        ansible_user_dir: "{{ ansible_user_dir }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/config-template.yml
````yaml
schema_version: '0.0.7'
name: '<replace-with-unique-name>'
spec:
  infra:
    csp: 'bm'
    backend: {}
    provider: {}
    configs:
      cns:
        # version: <replace-with-desired-cns-version-defaults-to-12.2>
        # git_ref: <replace-with-git-commit-hash-defaults-to-latest-commit-hash-of-master-branch>
        override_values:
          cns_nvidia_driver: <replace-with-yes-or-no-yes-when-need-to-install-nvidia-driver-using-runfile>
          #gpu_driver_version: <replace-with-desired-gpu-driver-version>
      ssh_public_key: "{{ lookup('file', lookup('env', 'HOME') +'ssh-pub-key-path') }}"
      ssh_private_key_path: "{{ lookup('env', 'HOME') + 'ssh-pem-key-path' }}"
      additional_ssh_public_keys: []
      #turn_server_provider: '<one-of-allowed-implementation-coturn|twilio-defaults-to-coturn>'
      clusters:
        app:
          master:
            user: "{{ lookup('env', 'APP_HOST_SSH_USER') }}"
            host: "{{ lookup('env', 'APP_HOST_IPV4_ADDR') }}"
          ports:
            app:
              port: 80
            grafana:
              port: 32300
              path: login
            prometheus:
              port: 30090
              path: graph
            kibana:
              port: 31565
              path: 'app/kibana'
          features:
            cns: true
            app: true
        turn:
          master:
            user: "{{ lookup('env', 'COTURN_HOST_SSH_USER') }}"
            host: "{{ lookup('env', 'COTURN_HOST_IPV4_ADDR') }}"
          features:
            coturn: true
  platform:
    configs:
      # k8s_namespace: '<replace-with-k8s-namespace-for-foundational-chart-to-deploy-defaults-to-platform>'
      k8s_secrets:
        - name: 'ngc-api-key-secret'
          type: 'Opaque'
          entries:
            - key: NGC_CLI_API_KEY
              value: "{{ lookup('env', 'NGC_CLI_API_KEY') }}"
        - name: 'ngc-docker-reg-secret'
          type: 'dockerconfigjson'
          registry_name: "nvcr.io"
          username: '$oauthtoken'
          password: "{{ lookup('env', 'NGC_CLI_API_KEY') }}"
    secrets:
      ngc_cli_api_key: "{{ lookup('env', 'NGC_CLI_API_KEY') }}"
  app:
    configs:
      app_settings:
        # k8s_namespace: '<replace-with-k8s-namespace-for-app-chart-to-deploy-defaults-to-app>'
        # helm_chart:
        #   repo:
        #     enable: <true/false-defaults-to-true>
        #     repo_url: '<replace-with-app-chart-helm-repo-url-defaults-to-https://helm.ngc.nvidia.com/nvidia/ace>'
        #     chart_name: '<replace-with-app-chart-name-defaults-to-ucs-tokkio-app-base-3-stream-llm-rag-3d-ov>'
        #     chart_version: '<replace-with-chart-version-defaults-to-4.1.4>'
        #     release_name: '<replace-with-release-name-for-helm-deploy-defaults-to-tokkio-app>'
        #     user_value_override_files: ['list-of-absolute-path-of-value-override-files']
        #   local:
        #      enable: <true/false-defaults-to-false>
        #      path: '<absolute-path-of-helm-chart-present-locally>'
        #      release_name: '<replace-with-release-name-for-helm-deploy-defaults-to-tokkio-app>'
        #      user_value_override_files: ['list-of-absolute-path-of-value-override-files']
        k8s_secrets:
          - name: 'ngc-api-key-secret'
            type: 'Opaque'
            entries:
              - key: NGC_CLI_API_KEY
                value: "{{ lookup('env', 'NGC_CLI_API_KEY') }}"
          - name: 'openai-key-secret'
            type: 'Opaque'
            entries:
              - key: OPENAI_API_KEY
                value: "{{ lookup('env', 'OPENAI_API_KEY') }}"
          - name: 'nvidia-api-key-secret'
            type: 'Opaque'
            entries:
              - key: NVIDIA_API_KEY
                value: "{{ lookup('env', 'NVIDIA_API_KEY') }}"
          - name: 'ngc-docker-reg-secret'
            type: 'dockerconfigjson'
            registry_name: "nvcr.io"
            username: '$oauthtoken'
            password: "{{ lookup('env', 'NGC_CLI_API_KEY') }}"
          # #NOTE: Uncomment below secret when deploying UE chart - ucs-tokkio-app-base-3-stream-retail-3d-ue
          # - name: 'ghcr-docker-reg-secret'
          #   type: 'dockerconfigjson'
          #   registry_name: "ghcr.io"
          #   username: "{{ lookup('env', 'GITHUB_USERNAME') }}"
          #   password: "{{ lookup('env', 'GITHUB_ACCESS_TOKEN') }}"
      # turn_server_settings:
        # # NOTE: Uncomment and update below section - coturn_settings in case turn_server_provider = coturn
        # coturn:
        #   username: '<replace-with-a-username-to-use-for-the-turnserver-defaults-to-foo>'
        #   password: '<replace-with-a-password-to-use-for-the-turnserver-defaults-to-bar>'
        #   realm: '<replace-with-a-realm-to-use-for-the-turnserver-defaults-to-mydummyt.org>'
        # # NOTE: # Uncomment and pass correct value for account_sid & auth_token for account_sid & account_sid  when turn_server_provider = twilio.
        # twilio:
        #   account_sid: '<replace-with-correct-twilio-account_sid-value-defaults-to-empty-string>'
        #   auth_token: '<replace-with-correct-twilio-auth_token-value-defailts-to-empty-string>'
      # ui_settings:
      #   resource:
      #     ngc:
      #       org: '<replace-with-ngc-org-defaults-to-nvidia>'
      #       team: '<replace-with-ngc-team-defaults-to-ace>'
      #       name: '<replace-with-ngc-resource-name-defaults-to-tokkio_ui>'
      #       version: '<replace-with-resource-version-defaults-to-5.0.0>'
      #       file: '<replace-with-resource-file-name-defaults-to-ui.tar.gz>'
      #   # NOTE: Uncomment user_env_vars when you need to pass ui configuration. it should be key: value format and its case sensitive.
      #   user_env_vars:
    secrets:
      ngc_cli_api_key: "{{ lookup('env', 'NGC_CLI_API_KEY') }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/envbuild.sh
````bash
#!/bin/bash

schema_version="0.0.7"
script_name="${0}"
exec_dir="$(pwd)"
script_dir="$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
config_file="${script_dir}/config.yml"
stages=()
dry_run="FALSE"
tmp_dir="$(mktemp -d -p "${script_dir}")"
commands=()

function usage() {
  echo "Usage: ${script_name} (-v|--version)"
  echo "   or: ${script_name} (-h|--help)"
  echo "   or: ${script_name} (install/uninstall) (-c|--component <component>) [options]"
  echo "   or: ${script_name} (info) [options]"
  echo ""
  echo "install/uninstall components:"
  echo "-c, --component        one or more of all/infra/platform/app, pass arg multiple times for more than one"
  echo ""
  echo "install/uninstall options:"
  echo "-f, --config-file      path to file containing config overrides, defaults to config.yml"
  echo "-i, --skip-infra       skip install/uninstall of infra component"
  echo "-p, --skip-platform    skip install/uninstall of platform component"
  echo "-a, --skip-app         skip install/uninstall of app component"
  echo "-d, --dry-run          don't make any changes, instead, try to predict some of the changes that may occur"
  echo "-h, --help             provide usage information"
  echo ""
  echo "info options:"
  echo "-f, --config-file      path to file containing config overrides, defaults to config.yml"
  echo "-h, --help             provide usage information"
}

function execute() {
  validate_args "${@}"
  process_args "${@}"
}

function validate_args() {
  local _args _all_good _valid_args _options _short_options _config_files _components _sub_commands _valid_sub_commands _valid_components _component
  _args=("${@}")
  _all_good=0
  _valid_args=$(getopt -q -o c:f:ipadvh --long component:,config-file:,skip-infra,skip-platform,skip-app,dry-run,version,help -- "${_args[@]}")
  _all_good=$(( _all_good + $? ))
  if [[ _all_good -gt 0 ]]; then
    echo "Invalid usage: ${_args[*]}"
  else
    eval set -- "${_valid_args}"
    _options=()
    _short_options=()
    _config_files=()
    _components=()
    while true; do
      case "${1}" in
        -f | --config-file) _options+=("${1}"); _short_options+=("-f"); shift; _options+=("${1}"); _config_files+=("${exec_dir}/${1}"); shift; ;;
        -c | --component) _options+=("${1}"); _short_options+=("-c"); shift; _options+=("${1}"); _components+=("${1}"); shift; ;;
        -i | --skip-infra) _options+=("${1}"); _short_options+=("-i"); shift; ;;
        -p | --skip-platform) _options+=("${1}"); _short_options+=("-p"); shift; ;;
        -a | --skip-app) _options+=("${1}"); _short_options+=("-a"); shift; ;;
        -d | --dry-run) _options+=("${1}"); _short_options+=("-d"); shift; ;;
        -h | --help) _options+=("${1}"); _short_options+=("-h"); shift; ;;
        -v | --version) _options+=("${1}"); _short_options+=("-v"); shift; ;;
        --) shift; break ;;
      esac
    done
    IFS=" " read -r -a _short_options <<< "$(de_dupe_elements "${_short_options[@]}")"
    IFS=" " read -r -a _config_files <<< "$(de_dupe_elements "${_config_files[@]}")"
    IFS=" " read -r -a _components <<< "$(de_dupe_elements "${_components[@]}")"
    _sub_commands=()
    while [[ -n "${1}" ]]; do
      _sub_commands+=("${1}")
      shift
    done
    _valid_sub_commands=()
    _valid_sub_commands+=('info')
    _valid_sub_commands+=('install')
    _valid_sub_commands+=('uninstall')
    _install_uninstall_sub_commands=()
    _install_uninstall_sub_commands+=('install')
    _install_uninstall_sub_commands+=('uninstall')
    _valid_components=()
    _valid_components+=('all')
    _valid_components+=('infra')
    _valid_components+=('platform')
    _valid_components+=('app')
    if [[ "${#_sub_commands[@]}" -gt 1 ]]; then
      echo "Invalid usage: ${_args[*]}"
      ((_all_good++))
    elif [[ "${#_sub_commands[@]}" -eq 1 ]] && ! contains_element "${_sub_commands[0]}" "${_valid_sub_commands[@]}"; then
      echo "Invalid usage: ${_args[*]}"
      ((_all_good++))
    elif [[ "${#_sub_commands[@]}" -eq 1 ]] && [[ "${_sub_commands[0]}" == 'info' ]] && ! contains_element "-h" "${_short_options[@]}" && ! getopt -q -o f:h --long config-file:,help -- "${_args[@]}" &> /dev/null; then
      echo "Invalid usage: ${_args[*]}"
      ((_all_good++))
    elif [[ "${#_sub_commands[@]}" -eq 1 ]] && contains_element "${_sub_commands[0]}" "${_install_uninstall_sub_commands[@]}" && ! contains_element "-h" "${_short_options[@]}" && ! getopt -q -o c:f:ipadh --long component:,config-file:,skip-infra,skip-platform,skip-app,dry-run,help -- "${_args[@]}" &> /dev/null; then
      echo "Invalid usage: ${_args[*]}"
      ((_all_good++))
    elif [[ "${#_sub_commands[@]}" -eq 0 ]] && ! getopt -q -o vh --long version,help -- "${_args[@]}" &> /dev/null; then
      echo "Invalid usage: ${_args[*]}"
      ((_all_good++))
    elif [[ "${#_sub_commands[@]}" -eq 1 ]] && ! contains_element "-h" "${_short_options[@]}"; then
      if contains_element "${_sub_commands[0]}" "${_install_uninstall_sub_commands[@]}" && [[ "${#_components[@]}" -eq 0 ]] ; then
        echo "Please specify component from: all/infra/platform/app"
        ((_all_good++))
      else
        for _component in "${_components[@]}"
        do
          if ! contains_element "${_component}" "${_valid_components[@]}"; then
            echo "Invalid component provided: ${_component}"
            ((_all_good++))
          fi
        done
      fi
      if contains_element "-f" "${_short_options[@]}" && [[ "${#_config_files[@]}" -gt 1 ]]; then
        echo "Multiple config-files provided: ${_config_files[*]}"
        ((_all_good++))
      fi
    fi
  fi
  if [[ _all_good -gt 0 ]]; then
    echo ""
    usage
    exit 1
  fi
}

function process_args() {
  local _args _valid_args _short_options _config_files _components _sub_commands
  _args=("${@}")
  _valid_args=$(getopt -q -o c:f:ipadvh --long component:,config-file:,skip-infra,skip-platform,skip-app,dry-run,version,help -- "${_args[@]}")
  eval set -- "${_valid_args}"
  _short_options=()
  _config_files=()
  _components=()
  while true; do
    case "${1}" in
      -f | --config-file) _options+=("${1}"); _short_options+=("-f"); shift; _config_files+=("${exec_dir}/${1}"); shift; ;;
      -c | --component) _options+=("${1}"); _short_options+=("-c"); shift; _components+=("${1}"); shift; ;;
      -i | --skip-infra) _options+=("${1}"); _short_options+=("-i"); shift; ;;
      -p | --skip-platform) _options+=("${1}"); _short_options+=("-p"); shift; ;;
      -a | --skip-app) _options+=("${1}"); _short_options+=("-a"); shift; ;;
      -d | --dry-run) _options+=("${1}"); _short_options+=("-d"); dry_run="TRUE"; shift; ;;
      -h | --help) _options+=("${1}"); _short_options+=("-h"); shift; ;;
      -v | --version) _options+=("${1}"); _short_options+=("-v"); shift; ;;
      --) shift; break ;;
    esac
  done
  IFS=" " read -r -a _short_options <<< "$(de_dupe_elements "${_short_options[@]}")"
  IFS=" " read -r -a _config_files <<< "$(de_dupe_elements "${_config_files[@]}")"
  IFS=" " read -r -a _components <<< "$(de_dupe_elements "${_components[@]}")"
  _sub_commands=()
  while [[ -n "${1}" ]]; do
    _sub_commands+=("${1}")
    shift
  done
  if [[ "${#_short_options[@]}" -eq 0 ]]; then
    commands+=('usage')
  elif contains_element "-h" "${_short_options[@]}"; then
    commands+=('usage')
  elif [[ "${#_short_options[@]}" -eq 1 ]] && contains_element "-v" "${_short_options[@]}"; then
    commands+=('print_version')
  else
    if [[ "${#_config_files[@]}" -eq 1 ]]; then
      config_file="${_config_files[0]}"
    fi
    if (contains_element "all" "${_components[@]}" || contains_element "infra" "${_components[@]}") && ! contains_element "-i" "${_short_options[@]}" ; then
      stages+=('infra')
    fi
    if (contains_element "all" "${_components[@]}" || contains_element "platform" "${_components[@]}") && ! contains_element "-p" "${_short_options[@]}" ; then
      stages+=('platform')
    fi
    if (contains_element "all" "${_components[@]}" || contains_element "app" "${_components[@]}") && ! contains_element "-a" "${_short_options[@]}" ; then
      stages+=('app')
    fi
    case "${_sub_commands[0]}" in
      info)
        commands+=('verify_pre_requisites')
        commands+=('activate_venv')
        commands+=('init')
        commands+=('output')
        ;;
      install)
        commands+=('verify_pre_requisites')
        commands+=('activate_venv')
        commands+=('init')
        commands+=('install')
        commands+=('output')
        ;;
      uninstall)
        commands+=('verify_pre_requisites')
        commands+=('activate_venv')
        commands+=('init')
        commands+=('uninstall')
        ;;
    esac
  fi
}

function contains_element() {
  local _element _ref_array _array_element
  _element="${1}"
  _ref_array=("${@:2}")
  for _array_element in "${_ref_array[@]}"
  do
    if [[ "${_element}" == "${_array_element}" ]]; then
      return 0
    fi
  done
  return 1
}

function de_dupe_elements() {
  local _ref_array _de_duped_array _array_element
  _ref_array=("${@}")
  _de_duped_array=()
  for _array_element in "${_ref_array[@]}"
  do
    if [[ "${#_de_duped_array[@]}" -eq 0 ]] || ! contains_element "${_array_element}" "${_de_duped_array[@]}"; then
      _de_duped_array+=("${_array_element}")
    fi
  done
  echo "${_de_duped_array[@]}"
}

function print_version() {
  echo "Version: ${schema_version}"
}

function prompt_acceptance() {
  local _confirm
  read -rp  "Would you like to ${1}? (y/n): " _confirm
  is_yes "${_confirm}"
  return "${?}"
}

function verify_pre_requisites() {
  local _install_or_update_requirement _all_good _requirements _requirement
  _install_or_update_requirement="$(os_supports_install_or_update_of_requirements)"
  _all_good=0
  verify_config
  _all_good=$(( _all_good + $? ))
  verify_privilege_escalation
  _all_good=$(( _all_good + $? ))
  set_locale_utf8
  _all_good=$(( _all_good + $? ))
  _requirements=()
  _requirements+=('jq')
  _requirements+=('yq')
  _requirements+=('terraform')
  _requirements+=('python3')
  _requirements+=('python3-venv')
  _requirements+=('python3-setuptools')
  _requirements+=('python3-dev')
  _requirements+=('python3-pip')
  for _requirement in "${_requirements[@]}"
  do
    verify_requirement "${_requirement}" "${_install_or_update_requirement}"
    _all_good=$(( _all_good + $? ))
  done
  if [[ _all_good -gt 0 ]]; then
    echo "One or more pre-requisites were not met"
    exit 1
  fi
}

function verify_config() {
  if [[ ! -f "${config_file}" ]]; then
    echo "Config file (${config_file}) not found"
    echo "Please use ${script_dir}/config-template.yml to create the ${config_file}"
    return 1
  fi
}

function verify_privilege_escalation() {
  if [[ "${EUID}" -ne 0 ]] && ! sudo -n true &> /dev/null; then
    echo "Current user is neither root, nor has passwordless sudo ability"
    return 1
  else
    return 0
  fi
}

function set_locale_utf8() {
    sudo bash -c 'echo -e "LANG=en_US.UTF-8\nLC_ALL=en_US.UTF-8" > /etc/default/locale'
    sudo update-locale
}

function requirement_present() {
  local _requirement
  _requirement="${1}"
  case "${_requirement}" in
    jq | yq | terraform)
      hash "${_requirement}" 2> /dev/null
      return "${?}"
      ;;
    python3 | python3-venv | python3-setuptools | python3-dev | python3-pip)
      verify_apt_package_present "${_requirement}"
      return "${?}"
      ;;
    *)
      return 1
      ;;
  esac
}

function requirement_version() {
  local _requirement
  _requirement="${1}"
  case "${_requirement}" in
    jq)
      jq --version | cut -d '-' -f 2 2> /dev/null || echo ""
      ;;
    yq)
      yq --version | grep 'mikefarah' | awk '{print $4}' | tr -d 'v' 2> /dev/null || echo ""
      ;;
    terraform)
      terraform --version | head -n 1 | awk '{print $2}' | tr -d 'v' 2> /dev/null || echo ""
      ;;
    python3 | python3-venv | python3-setuptools | python3-dev | python3-pip)
      apt_package_version "${_requirement}"
      ;;
    *)
      echo "0"
      ;;
  esac
}

function requirement_min_version() {
  local _requirement
  _requirement="${1}"
  case "${_requirement}" in
    jq)
      echo '1.6'
      ;;
    yq)
      echo '4.34.1'
      ;;
    terraform)
      echo '1.5.7'
      ;;
    python3 | python3-venv | python3-dev)
      echo '3.9.0'
      ;;
    python3-setuptools)
      echo '59.6.0'
      ;;
    python3-pip)
      echo '22.0.0'
      ;;
    *)
      echo "1"
      ;;
  esac
}

function requirement_max_version() {
  local _requirement
  _requirement="${1}"
  case "${_requirement}" in
    terraform)
      echo '1.5.7'
      ;;
    *)
      echo ""
      ;;
  esac
}

function requirement_install_or_update() {
  local _requirement
  _requirement="${1}"
  case "${_requirement}" in
    yq)
      $(privilege_escalation) wget "https://github.com/mikefarah/yq/releases/download/v$(requirement_min_version "${_requirement}")/yq_linux_amd64" -O /usr/bin/yq
      $(privilege_escalation) chmod +x /usr/bin/yq
      ;;
    terraform)
      curl --silent -L https://raw.githubusercontent.com/versus/terraform-switcher/release/install.sh | $(privilege_escalation) bash
      mkdir -p "${HOME}/bin/terraform"
      tfswitch -qb "${HOME}/bin/terraform" "$(requirement_min_version "${_requirement}")"
      $(privilege_escalation) cp "${HOME}/.terraform.versions/terraform_$(requirement_min_version "${_requirement}")" /usr/local/bin/terraform
      ;;
    jq | python3 | python3-venv | python3-setuptools | python3-dev | python3-pip)
      install_or_update_apt_package "${_requirement}"
      ;;
    *)
      echo "1"
      ;;
  esac
}

function verify_apt_package_present() {
  local _apt_package
  _apt_package="${1}"
  if [[ "$(apt -qq list --installed "${_apt_package}" 2> /dev/null | wc -l)" -eq 1 ]]; then
    return 0;
  else
    return 1;
  fi
}

function apt_package_version() {
  local _apt_package
  _apt_package="${1}"
  apt show "${_apt_package}" 2> /dev/null | grep '^Version' | awk '{print $NF}' | awk -F ':' '{print $NF}' | awk -F '[-+]' '{print $1}'
}

function install_or_update_apt_package() {
  local _apt_package
  _apt_package="${1}"
  $(privilege_escalation) apt update
  $(privilege_escalation) apt-get install "${_apt_package}" -y
}

function verify_min_requirement_version() {
  local _requirement _requirement_version _requirement_min_version
  _requirement="${1}"
  _requirement_version="$(requirement_version "${_requirement}")"
  _requirement_min_version="$(requirement_min_version "${_requirement}")"
  if [[ -n "${_requirement_min_version}" ]] && ! dpkg --compare-versions "${_requirement_version}" ge "${_requirement_min_version}"; then
    return 1
  else
    return "0"
  fi
}

function verify_max_requirement_version() {
  local _requirement _requirement_version _requirement_max_version
  _requirement="${1}"
  _requirement_version="$(requirement_version "${_requirement}")"
  _requirement_max_version="$(requirement_max_version "${_requirement}")"
  if [[ -n "${_requirement_max_version}" ]] && ! dpkg --compare-versions "${_requirement_version}" le "${_requirement_max_version}"; then
    return 1
  else
    return "0"
  fi
}

function install_or_update_requirement() {
  local _requirement
  _requirement="${1}"
  if prompt_acceptance "install/update ${_requirement}"; then
    echo "installing ${_requirement}"
    if ! requirement_install_or_update "${_requirement}" &> /dev/null; then
      echo "failed to install ${_requirement}"
    fi
  fi
}

function get_os() {
  grep -iw ID /etc/os-release | awk -F '=' '{print $2}'
}

function get_os_version() {
  grep -iw VERSION_ID /etc/os-release | awk -F '=' '{print $2}' | tr -d '"'
}

function os_supports_install_or_update_of_requirements() {
  case "$(get_os)-$(get_os_version)" in
    ubuntu-22.04 | ubuntu-24.04)
      echo "yes" ;;
    *)
      echo "no" ;;
  esac
}

function is_yes() {
  if [[ "${1}" == [yY] || "${1}" == [yY][eE][sS] ]]; then
    return 0
  else
    return 1
  fi
}

function verify_requirement() {
  local _requirement
  _requirement="${1}"
  _attempt_install_or_update="${2}"
  if is_yes "${_attempt_install_or_update}" && verify_privilege_escalation &> /dev/null; then
    if ! requirement_present "${_requirement}" || ! verify_min_requirement_version "${_requirement}" || ! verify_max_requirement_version "${_requirement}"; then
      install_or_update_requirement "${_requirement}"
      verify_requirement "${_requirement}" "no"
    fi
  elif ! requirement_present "${_requirement}"; then
    echo "${_requirement} is required"
    return 1
  elif ! verify_min_requirement_version "${_requirement}" && ! verify_max_requirement_version "${_requirement}"; then
    echo "${_requirement} version should be between $(requirement_min_version "${_requirement}") and $(requirement_max_version "${_requirement}")"
    return 1
  elif ! verify_min_requirement_version "${_requirement}"; then
    echo "${_requirement} version should be greater than $(requirement_min_version "${_requirement}")"
    return 1
  elif ! verify_max_requirement_version "${_requirement}"; then
    echo "${_requirement} version should be lesser than $(requirement_max_version "${_requirement}")"
    return 1
  else
    return 0
  fi
}

function privilege_escalation() {
  if [[ "${EUID}" -ne 0 ]]; then
    echo -n "sudo"
  else
    echo -n ""
  fi
}

function exit_if_not_ok() {
  if [[ "${1}" != "0" ]]; then
    exit "${1}"
  fi
}

function deactivate_venv() {
  deactivate 2> /dev/null || true
}

function activate_venv() {
  deactivate_venv
  {
    python3 -m venv --copies --clear "${script_dir}/ansible-venv"
    source "${script_dir}/ansible-venv/bin/activate"
  } > /dev/null
  trap deactivate_venv EXIT
  {
    pip install --upgrade pip
    pip install ansible==7.0.0
  } > /dev/null
  if ! hash deactivate 2> /dev/null; then
    echo "Failed to activate virtualenv"
    exit 1
  fi
}

function abort_option() {
  echo "CTRL-C to abort"
  sleep 10
}

function switch_to_script_dir() {
  cd "${script_dir}" || exit 1
}

function init() {
  local _tmp_init_dir _config_yml _infra_yml _tf_backend_json _tf_variables_json _uncommented_config_file _infra_vars_yml _platform_vars_yml _app_vars_yml _name
  echo "preparing artifacts"
  switch_to_script_dir

  _tmp_init_dir="${tmp_dir}/init"
  mkdir -p "${_tmp_init_dir}"
  _config_yml="${_tmp_init_dir}/config.yml"
  _infra_yml="${_tmp_init_dir}/infra.yml"
  _tf_backend_json="${_tmp_init_dir}/tf-backend.json"
  _tf_variables_json="${_tmp_init_dir}/tf-variables.json"
  _uncommented_config_file="${_tmp_init_dir}/uncommented_config.yml"
  _infra_vars_yml="${_tmp_init_dir}/infra_vars.yml"
  _platform_vars_yml="${_tmp_init_dir}/platform_vars.yml"
  _app_vars_yml="${_tmp_init_dir}/app_vars.yml"

  # prepare _config_yml
  sed '/^[[:space:]]*#/d' "${config_file}" > "${_uncommented_config_file}"
  process_user_config "${_uncommented_config_file}" "${_config_yml}"

  # get _name
  _name="$(yq eval '.name' "${_config_yml}")"

  # prepare _infra_yml
  yq eval '.spec.infra' "${_config_yml}" > "${_infra_yml}"

  # prepare _tf_backend_json
  case "$(yq eval '.csp' "${_infra_yml}")" in
    aws | azure)
      jq -n \
        --arg name "${_name}" \
        --argjson config "$(yq eval '.backend' "${_infra_yml}" -o=json)" \
        '$config * { key: ($name + "/terraform.tfstate") }' > "${_tf_backend_json}"
      ;;
    gcp)
      jq -n \
        --arg name "${_name}" \
        --argjson config "$(yq eval '.backend' "${_infra_yml}" -o=json)" \
        '$config * { prefix: ($name + "/terraform.tfstate") }' > "${_tf_backend_json}"
      ;;
    oci)
      jq -n \
        --arg name "${_name}" \
        --arg par "$(yq eval '.backend.pre_authenticated_request' "${_infra_yml}")" \
        '{ address: ($par + $name + "/terraform.tfstate"), update_method: "PUT" }' > "${_tf_backend_json}"
      ;;
    bm)
      _bm_state_dir="${HOME}/.nvoc/tf-state"
      mkdir -p "${_bm_state_dir}/${_name}"
      jq -n \
        --arg bm_state_dir "${_bm_state_dir}" \
        --arg name "${_name}" \
        '{ path: ($bm_state_dir + "/" + $name + "/terraform.tfstate") }' > "${_tf_backend_json}"
      ;;
    *)
      jq -n '{}' > "${_tf_backend_json}"
      ;;
  esac

  # prepare _tf_variables_json
  jq -n \
   --arg name "${_name}" \
   --argjson controller_ip "$(jq -n --arg ip "$(curl -s ifconfig.me)" '{ controller_ip: $ip }')" \
   --argjson provider "$(yq eval '.provider' "${_infra_yml}" -o=json | jq '{provider_config: .}')" \
   --argjson configs "$(yq eval '.configs' "${_infra_yml}" -o=json)" \
    '{ name: $name } * $controller_ip * $provider * $configs' >  "${_tf_variables_json}"

  # prepare _infra_vars_yml
  yq eval '.spec.infra | del(.backend) | del(.provider)' "${_config_yml}" > "${_infra_vars_yml}"

  # prepare _platform_vars_yml
  yq eval '.spec.platform' "${_config_yml}" > "${_platform_vars_yml}"

  # prepare _app_vars_yml
  yq eval '.spec.app' "${_config_yml}" > "${_app_vars_yml}"

  # install ansible requirements
  if [[ -f ansible-requirements.yml ]]; then
    ansible-galaxy install -r ansible-requirements.yml &> /dev/null
  fi

  cp -r iac-ref iac
  cp "${_tf_backend_json}" iac/tf-backend.json
  cp "${_tf_variables_json}" iac/tf-variables.json
  cp "${_infra_vars_yml}" playbooks/infra-vars.yml
  cp "${_platform_vars_yml}" playbooks/platform-vars.yml
  cp "${_app_vars_yml}" playbooks/app-vars.yml
}

function install() {
  if contains_element "infra" "${stages[@]}"; then
    apply_tf_shape
    if ! extract_inventory || ! check_inventory; then
      exit 1
    else
      install_plays "infra"
      exit_if_not_ok "${?}"
    fi
  elif contains_element "platform" "${stages[@]}" || contains_element "app" "${stages[@]}"; then
    if ! extract_inventory || ! check_inventory; then
      exit 1
    fi
  fi
  if contains_element "platform" "${stages[@]}"; then
    install_plays "platform"
    exit_if_not_ok "${?}"
  fi
  if contains_element "app" "${stages[@]}"; then
    install_plays "app"
    exit_if_not_ok "${?}"
  fi
}

function uninstall() {
  if contains_element "infra" "${stages[@]}"; then
    if extract_inventory && check_inventory; then
      uninstall_plays "infra"
    fi
    destroy_tf_shape
  else
    if extract_inventory && check_inventory; then
      if contains_element "app" "${stages[@]}"; then
        uninstall_plays "app"
        exit_if_not_ok "${?}"
      fi
      if contains_element "platform" "${stages[@]}"; then
        uninstall_plays "platform"
        exit_if_not_ok "${?}"
      fi
    else
      exit 1
    fi
  fi
}

function output() {
  switch_to_script_dir
  terraform -chdir="iac" init -reconfigure -backend-config=tf-backend.json > /dev/null
  echo ""
  echo "==========================================================================================="
  terraform -chdir="iac" output -json | jq -r '.info.value // {} | del(..|nulls)' | yq -P
  echo "==========================================================================================="
}

function apply_tf_shape() {
  echo "applying TF shape"
  abort_option
  switch_to_script_dir
  terraform -chdir="iac" init -reconfigure -backend-config=tf-backend.json > /dev/null
  terraform -chdir="iac" plan -compact-warnings -out=tf-plan -var-file=tf-variables.json -detailed-exitcode
  _exit_code=$?
  if [[ "${_exit_code}" == "1" ]]; then
    echo "failed to determine IaC changes"
    exit 1
  fi
  if [[ "${dry_run}" == "FALSE" ]] && [[ "${_exit_code}" == "2" ]] && [[ -f "iac/tf-plan" ]]; then
    terraform -chdir="iac" apply tf-plan || exit 1
  fi
}

function destroy_tf_shape() {
  echo "destroying TF shape"
  abort_option
  switch_to_script_dir
  terraform -chdir="iac" init -reconfigure -backend-config=tf-backend.json > /dev/null
  terraform -chdir="iac" plan -compact-warnings -out=tf-plan -var-file=tf-variables.json -destroy -detailed-exitcode
  _exit_code=$?
  if [[ "${_exit_code}" == "1" ]]; then
    echo "failed to determine IaC changes"
    exit 1
  fi
  if [[ "${dry_run}" == "FALSE" ]] && [[ "${_exit_code}" == "2" ]] && [[ -f "iac/tf-plan" ]]; then
    terraform -chdir="iac" apply tf-plan
  fi
}

function extract_inventory() {
  local _exit_code
  echo "extracting inventory"
  switch_to_script_dir
  terraform -chdir="iac" init -reconfigure -backend-config=tf-backend.json > /dev/null
  terraform -chdir="iac" plan -compact-warnings -out=tf-plan -var-file=tf-variables.json -detailed-exitcode 1> /dev/null 2> /dev/null
  _exit_code=$?
  if [[ "${_exit_code}" != "0" ]]; then
    echo "cannot extract inventory when there are IaC changes"
    rm inventory.yml &> /dev/null || true
    rm cns-host-groups.json &> /dev/null || true
    rm playbooks/iac-vars.yml &> /dev/null || true
    return 1
  else
    terraform -chdir="iac" output -json | jq -r '.hosts.value' > inventory.yml
    terraform -chdir="iac" output -json | jq -r '.cns_clusters.value // {} | keys' > cns-host-groups.json
    terraform -chdir="iac" output -json | jq -r '.playbook_configs.value // {} | {iac: .}' | yq -P > playbooks/iac-vars.yml
    return 0
  fi
}

function check_inventory() {
  local _retry_counter
  echo "checking inventory"
  switch_to_script_dir
  _retry_counter=0
  while ! ansible-playbook -i inventory.yml playbooks/check-inventory.yml > /dev/null 2> /dev/null; do
    if [[ "${_retry_counter}" -ge 15 ]]; then
      echo "exhausted attempts waiting for inventory to be ready"
      return 1
    else
      ((_retry_counter++))
      echo "waiting for inventory to be ready"
      sleep 20
    fi
  done
  return 0
}

function install_plays() {
  local _stage _task
  _stage="${1}"
  echo "installing ${_stage} plays"
  abort_option
  switch_to_script_dir
  for _task in $(yq eval ".tasks" -o json "${_stage}-tasks.yml" | jq -r '. | map(.name)[]'); do
    if ! run_play "${_stage}" "${_task}" "present"; then
      return 1
    fi
  done
}

function uninstall_plays() {
  local _stage _task
  _stage="${1}"
  echo "uninstalling ${_stage} plays"
  abort_option
  switch_to_script_dir
  for _task in $(yq eval ".tasks" -o json "${_stage}-tasks.yml" | jq -r '. | reverse | map(.name)[]'); do
    if ! run_play "${_stage}" "${_task}" "absent"; then
      return 1
    fi
  done
}

function run_play() {
  local _stage _task _state _tmp_task_dir _play _when_dry_run _task_vars_file
  local _expression_args _condition_expression _condition_file
  local _task_config_args _task_config_file _config_file_counter _task_config_source _task_config_destination
  local _for_each_expression _for_each_file _each_vars_file _expression_args_for_each
  local _task_targets _target_file _target
  _stage="${1}"
  _task="${2}"
  _state="${3}"
  _tmp_task_dir="${tmp_dir}/${_stage}-tasks/${_task}"
  mkdir -p "${_tmp_task_dir}"
  _play="$(yq eval ".tasks" -o json "${_stage}-tasks.yml" | jq --arg task "${_task}" -r '. | map(select(.name == $task))[0].play')"
  _when_dry_run="$(yq eval ".tasks" -o json "${_stage}-tasks.yml" | jq --arg task "${_task}" -r '. | map(select(.name == $task))[0].when_dry_run // "dry-run"')"
  _task_vars_file="${_tmp_task_dir}/vars.yml"
  yq eval ".tasks" -o json "${_stage}-tasks.yml" | \
    jq --arg task "${_task}" -r '. | map(select(.name == $task))[0].vars // {} | {task_vars: .}' | \
    yq -P > "${_task_vars_file}"
  if [[ "${_state}" == "present" ]]; then
    echo "applying task: ${_task}"
  fi
  if [[ "${_state}" == "absent" ]]; then
    echo "reverting task: ${_task}"
  fi
  echo ""
  _expression_args=()
  _expression_args+=("-e")
  _expression_args+=("@${_task_vars_file}")
  _expression_args+=("-e")
  _expression_args+=("@playbooks/${_stage}-vars.yml")
  _expression_args+=("-e")
  _expression_args+=("@playbooks/iac-vars.yml")
  _expression_args+=("-e")
  _expression_args+=("dist_dir=${script_dir}")
  _expression_args+=("-e")
  _expression_args+=("state=${_state}")
  _expression_args+=("-e")
  _expression_args+=("tmp_task_dir=${_tmp_task_dir}")
  _expression_args+=("-e")
  _expression_args+=("dry_run_mode=${dry_run,,}")
  _condition_expression="$(yq eval ".tasks" -o json "${_stage}-tasks.yml" | jq --arg task "${_task}" -r '. | map(select(.name == $task))[0].condition // ""')"
  _condition_file="${_tmp_task_dir}/condition.txt"
  if [[ -n "${_condition_expression}" ]]; then
    if ! output_jinja2_expression "${_condition_expression}" "${_condition_file}" "txt" "${_expression_args[@]}"; then
      return 1
    fi
    if [[ "$(tr '[:upper:]' '[:lower:]' < "${_condition_file}")" != "true" ]]; then
      echo "skipping task: ${_task} since condition: ${_condition_expression} did not evaluate to true"
      return 0
    fi
  fi
  _task_config_args=()
  _task_config_file="${_tmp_task_dir}/config.yml"
  yq eval ".tasks" -o json "${_stage}-tasks.yml" | \
    jq --arg task "${_task}" -r '. | map(select(.name == $task))[0].config // {} | {task_config: .}' | \
    yq -P > "${_task_config_file}"
  _task_config_args+=("-e")
  _task_config_args+=("@${_task_config_file}")
  mkdir -p "${_tmp_task_dir}/config-files"
  _config_file_counter=0
  while read -r _config_file; do
    if [[ -n "${_config_file}" ]]; then
      ((_config_file_counter++))
      _task_config_source="${script_dir}/config-files/${_config_file}"
      _task_config_destination="${_tmp_task_dir}/config-files/config-${_config_file_counter}.yml"
      if ! ANSIBLE_JINJA2_NATIVE=true ansible-playbook \
          -c local \
          -i localhost, \
          -e config_source="${_task_config_source}" \
          -e config_destination="${_task_config_destination}" \
          "${_expression_args[@]}" \
          playbooks/copy-task-config.yml &> /dev/null; then
        echo "could not copy config file: ${_config_file}"
        return 1
      fi
      _task_config_args+=("-e")
      _task_config_args+=("@${_task_config_destination}")
    fi
  done <<< "$(yq eval ".tasks" -o json "${_stage}-tasks.yml" | jq --arg task "${_task}" -r '. | map(select(.name == $task))[0].config_files // [] | .[]')"
  _for_each_expression="$(yq eval ".tasks" -o json "${_stage}-tasks.yml" | jq --arg task "${_task}" -r '. | map(select(.name == $task))[0].for_each // ""')"
  _for_each_file="${_tmp_task_dir}/each.yml"
  if [[ -n "${_for_each_expression}" ]]; then
    if ! output_jinja2_expression "${_for_each_expression}" "${_for_each_file}" "yaml" "${_expression_args[@]}"; then
      return 1
    fi
  else
    jq -n '{default: {}}' | yq -P > "${_for_each_file}"
  fi
  for _each_key in $(yq eval 'keys' "${_for_each_file}" -o json | jq -r '.[]'); do
    _each_vars_file="${_tmp_task_dir}/each-vars.yml"
    yq eval ".${_each_key}" "${_for_each_file}" -o json | jq --arg key "${_each_key}" -r '{task_each_vars: {key: $key, value: .}}' | yq -P > "${_each_vars_file}"
    _expression_args_for_each=("${_expression_args[@]}")
    _expression_args_for_each+=("-e")
    _expression_args_for_each+=("@${_each_vars_file}")
    _task_targets=()
    while read -r _target_expression; do
      if [[ -n "${_target_expression}" ]]; then
        _target_file="${_tmp_task_dir}/target"
        if ! output_jinja2_expression "${_target_expression}" "${_target_file}" "yaml" "${_expression_args_for_each[@]}"; then
          return 1
        else
          _target="$(cat "${_target_file}")"
          _task_targets+=("${_target}")
        fi
      fi
    done <<< "$(yq eval ".tasks" -o json "${_stage}-tasks.yml" | jq --arg task "${_task}" -r '. | map(select(.name == $task))[0].targets // [] | .[]')"
    if [[ "${#_task_targets[*]}" -eq 0 ]]; then
      _task_targets+=("all")
    fi
    for _task_target in "${_task_targets[@]}"; do
      if [[ "${dry_run}" == "TRUE" ]] && [[ "${_when_dry_run}" == "dry-run" ]]; then
        ANSIBLE_JINJA2_NATIVE=true ansible-playbook \
          --check \
          -i inventory.yml \
          -l "localhost:${_task_target}" \
          "${_task_config_args[@]}" \
          "${_expression_args_for_each[@]}" \
          "playbooks/${_play}.yml"
      elif [[ "${dry_run}" != "TRUE" ]] || [[ "${_when_dry_run}" == "run" ]]; then
        if ! ANSIBLE_JINJA2_NATIVE=true ansible-playbook \
            -i inventory.yml \
            -l "localhost:${_task_target}" \
            "${_task_config_args[@]}" \
            "${_expression_args_for_each[@]}" \
            "playbooks/${_play}.yml"; then
          return 1
        fi
      else
        echo "skipping task: ${_task} since dry-run: ${dry_run,,} and task is marked: ${_when_dry_run,,} when dry-run is true"
      fi
    done
  done
}

function output_jinja2_expression() {
  local _expression _output_file _output_format
  _expression="${1}"
  _output_file="${2}"
  _output_format="${3}"
  if ! ANSIBLE_JINJA2_NATIVE=true ansible-playbook \
      -c local \
      -i localhost, \
      -e expression="${_expression}" \
      -e output_file="${_output_file}" \
      -e output_format="${_output_format}" \
      "${@:4}" \
      playbooks/output-jinja2-expression-to-file.yml &> /dev/null; then
    echo "could not evaluate expression: ${_expression}"
    return 1
  fi
}

function process_user_config() {
  local _input_file _output_file
  _input_file="${1}"
  _output_file="${2}"
  if ! ANSIBLE_JINJA2_NATIVE=true ansible-playbook \
      -c local \
      -i localhost, \
      -e input_file="${_input_file}" \
      -e output_file="${_output_file}" \
      playbooks/process-user-config.yml > /dev/null 2> /dev/stderr; then
    echo "could not process config file: ${_input_file}"
    exit 1
  fi
}

function cleanup() {
  switch_to_script_dir
  rm -rf "${tmp_dir}" 2> /dev/null
}

function abort() {
  echo "Aborting as SIGTERM/SIGINT received"
  trap - SIGINT SIGTERM # clear the trap
  kill -- -$$ # Sends SIGTERM to child/sub processes
}

trap cleanup EXIT
trap abort SIGINT SIGTERM

execute "${@}"
for command in "${commands[@]}"
do
  "${command}"
  exit_if_not_ok "${?}"
done
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/infra-tasks.yml
````yaml
schema_version: '0.0.7'
tasks:
- name: authorize-ssh-keys-for-clusters
  play: authorize-ssh-keys
  when_dry_run: skip
  targets:
  - "{{ task_each_vars.value.targets.all }}"
  for_each: "{{ iac.clusters }}"
  config:
    user: "{{ ansible_user }}"
    keys: "{{ task_each_vars.value.additional_ssh_public_keys }}"
- name: authorize-ssh-keys-for-jump-hosts
  play: authorize-ssh-keys
  when_dry_run: skip
  targets:
  - "{{ task_each_vars.value.target }}"
  for_each: "{{ iac.jump_hosts }}"
  config:
    user: "{{ ansible_user }}"
    keys: "{{ task_each_vars.value.additional_ssh_public_keys }}"
- name: prepare-cns-for-install
  play: cns-prepare
  condition: "{{ state == 'present' }}"
  when_dry_run: skip
  targets:
  - "{{ task_each_vars.value.targets.all }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['cns']) | items2dict }}"
- name: validate-cns
  play: cns-validate
  when_dry_run: skip
  targets:
  - "{{ task_each_vars.value.targets.all }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['cns']) | items2dict }}"
- name: install-cns
  play: cns-install
  when_dry_run: skip
  targets:
  - "{{ task_each_vars.value.targets.all }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['cns']) | items2dict }}"
- name: prepare-cns-for-uninstall
  play: cns-prepare
  when_dry_run: skip
  condition: "{{ state == 'absent' }}"
  targets:
  - "{{ task_each_vars.value.targets.all }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['cns']) | items2dict }}"
- name: setup-pre-requisites-for-k8s-tasks
  play: k8s-task-pre-requisites
  targets:
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['cns']) | items2dict }}"
  config: {}
- name: setup-pre-requisites-for-helm-tasks
  play: helm-task-pre-requisites
  targets:
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['cns']) | items2dict }}"
  config: {}
- name: check-nvidia-gpu-operator-pods-up
  play: check-all-pods-in-namespace-up
  when_dry_run: skip
  targets:
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['cns']) | items2dict }}"
  config:
    namespace: "nvidia-gpu-operator"
- name: k8s-label
  play: k8s-label
  when_dry_run: skip
  targets:
  - "{{ task_each_vars.value.targets.all }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['cns']) | items2dict }}"
  config:
    node_labels: "{{ task_each_vars.value.labels | default({}) }}"
- name: k8s-taint
  play: k8s-taint
  when_dry_run: skip
  targets:
  - "{{ task_each_vars.value.targets.all }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['cns']) | items2dict }}"
  config:
    node_taints: "{{ task_each_vars.value.taints | default([]) }}"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/install-pre-requisites.sh
````bash
#!/bin/bash

schema_version="0.0.7"
script_name="${0}"
no_prompt="no"
commands=()

function usage() {
  echo "Usage: ${script_name} (-v|--version)"
  echo "   or: ${script_name} (-h|--help)"
  echo "   or: ${script_name} [options]"
  echo ""
  echo "options:"
  echo "-y, --no-prompt    install all dependencies without prompt"
  echo "-h, --help         provide usage information"
}

function execute() {
  validate_args "${@}"
  process_args "${@}"
}

function validate_args() {
  local _args _all_good _valid_args _options _short_options _sub_commands
  _args=("${@}")
  _all_good=0
  _valid_args=$(getopt -q -o yvh --long no-prompt,version,help -- "${_args[@]}")
  _all_good=$(( _all_good + $? ))
  if [[ _all_good -gt 0 ]]; then
    echo "Invalid usage: ${_args[*]}"
  else
    eval set -- "${_valid_args}"
    _options=()
    _short_options=()
    while true; do
      case "${1}" in
        -y | --no-prompt) _options+=("${1}"); _short_options+=("-y"); shift; ;;
        -h | --help) _options+=("${1}"); _short_options+=("-h"); shift; ;;
        -v | --version) _options+=("${1}"); _short_options+=("-v"); shift; ;;
        --) shift; break ;;
      esac
    done
    IFS=" " read -r -a _short_options <<< "$(de_dupe_elements "${_short_options[@]}")"
    _sub_commands=()
    while [[ -n "${1}" ]]; do
      _sub_commands+=("${1}")
      shift
    done
    if [[ "${#_sub_commands[@]}" -gt 0 ]]; then
      echo "Invalid usage: ${_args[*]}"
      ((_all_good++))
    elif [[ "${#_sub_commands[@]}" -eq 0 ]] && contains_element "-v" "${_short_options[@]}" && [[ "${#_short_options[@]}" -gt 2 ]]; then
      echo "Invalid usage: ${_args[*]}"
      ((_all_good++))
    elif [[ "${#_sub_commands[@]}" -eq 0 ]] && contains_element "-v" "${_short_options[@]}" && [[ "${#_short_options[@]}" -eq 2 ]] && ! contains_element "-h" "${_short_options[@]}"; then
      echo "Invalid usage: ${_args[*]}"
      ((_all_good++))
    fi
  fi
  if [[ _all_good -gt 0 ]]; then
    echo ""
    usage
    exit 1
  fi
}

function process_args() {
  local _args _valid_args _short_options
  _args=("${@}")
  _valid_args=$(getopt -q -o yvh --long no-prompt,version,help -- "${_args[@]}")
  eval set -- "${_valid_args}"
  _short_options=()
  while true; do
    case "${1}" in
      -y | --no-prompt) _short_options+=("-y"); shift; ;;
      -h | --help) _short_options+=("-h"); shift; ;;
      -v | --version) _short_options+=("-v"); shift; ;;
      --) shift; break ;;
    esac
  done
  IFS=" " read -r -a _short_options <<< "$(de_dupe_elements "${_short_options[@]}")"
  if [[ "${#_short_options[@]}" -eq 0 ]]; then
    commands+=('verify_pre_requisites')
  elif contains_element "-h" "${_short_options[@]}"; then
    commands+=('usage')
  elif [[ "${#_short_options[@]}" -eq 1 ]]; then
    case "${_short_options[0]}" in
      -y)
        no_prompt="yes"
        commands+=('verify_pre_requisites')
        ;;
      -v)
        commands+=('print_version')
        ;;
    esac
  fi
}

function contains_element() {
  local _element _ref_array _array_element
  _element="${1}"
  _ref_array=("${@:2}")
  for _array_element in "${_ref_array[@]}"
  do
    if [[ "${_element}" == "${_array_element}" ]]; then
      return 0
    fi
  done
  return 1
}

function de_dupe_elements() {
  local _ref_array _de_duped_array _array_element
  _ref_array=("${@}")
  _de_duped_array=()
  for _array_element in "${_ref_array[@]}"
  do
    if [[ "${#_de_duped_array[@]}" -eq 0 ]] || ! contains_element "${_array_element}" "${_de_duped_array[@]}"; then
      _de_duped_array+=("${_array_element}")
    fi
  done
  echo "${_de_duped_array[@]}"
}

function print_version() {
  echo "Version: ${schema_version}"
}

function prompt_acceptance() {
  local _confirm
  if is_yes "${no_prompt}"; then
    return 0
  else
    read -rp  "Would you like to ${1}? (y/n): " _confirm
    is_yes "${_confirm}"
    return "${?}"
  fi
}

function verify_pre_requisites() {
  local _install_or_update_requirement _all_good _requirements _requirement
  _install_or_update_requirement="$(os_supports_install_or_update_of_requirements)"
  _all_good=0
  verify_privilege_escalation
  _all_good=$(( _all_good + $? ))
  _requirements=()
  _requirements+=('make')
  _requirements+=('git')
  _requirements+=('jq')
  _requirements+=('yq')
  _requirements+=('terraform')
  _requirements+=('python3')
  _requirements+=('python3-venv')
  _requirements+=('python3-setuptools')
  _requirements+=('python3-dev')
  _requirements+=('python3-pip')
  for _requirement in "${_requirements[@]}"
  do
    verify_requirement "${_requirement}" "${_install_or_update_requirement}"
    _all_good=$(( _all_good + $? ))
  done
  if [[ _all_good -gt 0 ]]; then
    echo "One or more pre-requisites were not met"
    exit 1
  else
    echo "all pre-requisites are met"
  fi
}

function verify_privilege_escalation() {
  if [[ "${EUID}" -ne 0 ]] && ! sudo -n true &> /dev/null; then
    echo "Current user is neither root, nor has passwordless sudo ability"
    return 1
  else
    return 0
  fi
}

function requirement_present() {
  local _requirement
  _requirement="${1}"
  case "${_requirement}" in
    make | git | jq | yq | terraform)
      hash "${_requirement}" 2> /dev/null
      return "${?}"
      ;;
    python3 | python3-venv | python3-setuptools | python3-dev | python3-pip)
      verify_apt_package_present "${_requirement}"
      return "${?}"
      ;;
    *)
      return 1
      ;;
  esac
}

function requirement_version() {
  local _requirement
  _requirement="${1}"
  case "${_requirement}" in
    make)
      make -v | grep 'GNU Make' | awk '{print $3}' 2> /dev/null || echo ""
      ;;
    git)
      git --version | awk '{print $3}' 2> /dev/null || echo ""
      ;;
    jq)
      jq --version | cut -d '-' -f 2 2> /dev/null || echo ""
      ;;
    yq)
      yq --version | grep 'mikefarah' | awk '{print $4}' | tr -d 'v' 2> /dev/null || echo ""
      ;;
    terraform)
      terraform --version | head -n 1 | awk '{print $2}' | tr -d 'v' 2> /dev/null || echo ""
      ;;
    python3 | python3-venv | python3-setuptools | python3-dev | python3-pip)
      apt_package_version "${_requirement}"
      ;;
    *)
      echo "0"
      ;;
  esac
}

function requirement_min_version() {
  local _requirement
  _requirement="${1}"
  case "${_requirement}" in
    make)
      echo '4.3'
      ;;
    git)
      echo '2.34.1'
      ;;
    jq)
      echo '1.6'
      ;;
    yq)
      echo '4.34.1'
      ;;
    terraform)
      echo '1.5.7'
      ;;
    python3 | python3-venv | python3-dev)
      echo '3.9.0'
      ;;
    python3-setuptools)
      echo '59.6.0'
      ;;
    python3-pip)
      echo '22.0.0'
      ;;
    *)
      echo "1"
      ;;
  esac
}

function requirement_max_version() {
  local _requirement
  _requirement="${1}"
  case "${_requirement}" in
    terraform)
      echo '1.5.7'
      ;;
    *)
      echo ""
      ;;
  esac
}

function requirement_install_or_update() {
  local _requirement
  _requirement="${1}"
  case "${_requirement}" in
    yq)
      $(privilege_escalation) wget "https://github.com/mikefarah/yq/releases/download/v$(requirement_min_version "${_requirement}")/yq_linux_amd64" -O /usr/bin/yq
      $(privilege_escalation) chmod +x /usr/bin/yq
      ;;
    terraform)
      curl --silent -L https://raw.githubusercontent.com/versus/terraform-switcher/release/install.sh | $(privilege_escalation) bash
      mkdir -p "${HOME}/bin/terraform"
      tfswitch -qb "${HOME}/bin/terraform" "$(requirement_min_version "${_requirement}")"
      $(privilege_escalation) cp "${HOME}/.terraform.versions/terraform_$(requirement_min_version "${_requirement}")" /usr/local/bin/terraform
      ;;
    make | git | jq | python3 | python3-venv | python3-setuptools | python3-dev | python3-pip)
      install_or_update_apt_package "${_requirement}"
      ;;
    *)
      echo "1"
      ;;
  esac
}

function verify_apt_package_present() {
  local _apt_package
  _apt_package="${1}"
  if [[ "$(apt -qq list --installed "${_apt_package}" 2> /dev/null | wc -l)" -eq 1 ]]; then
    return 0;
  else
    return 1;
  fi
}

function apt_package_version() {
  local _apt_package
  _apt_package="${1}"
  apt show "${_apt_package}" 2> /dev/null | grep '^Version' | awk '{print $NF}' | awk -F ':' '{print $NF}' | awk -F '[-+]' '{print $1}'
}

function install_or_update_apt_package() {
  local _apt_package
  _apt_package="${1}"
  $(privilege_escalation) apt update
  $(privilege_escalation) apt-get install "${_apt_package}" -y
}

function verify_min_requirement_version() {
  local _requirement _requirement_version _requirement_min_version
  _requirement="${1}"
  _requirement_version="$(requirement_version "${_requirement}")"
  _requirement_min_version="$(requirement_min_version "${_requirement}")"
  if [[ -n "${_requirement_min_version}" ]] && ! dpkg --compare-versions "${_requirement_version}" ge "${_requirement_min_version}"; then
    return 1
  else
    return "0"
  fi
}

function verify_max_requirement_version() {
  local _requirement _requirement_version _requirement_max_version
  _requirement="${1}"
  _requirement_version="$(requirement_version "${_requirement}")"
  _requirement_max_version="$(requirement_max_version "${_requirement}")"
  if [[ -n "${_requirement_max_version}" ]] && ! dpkg --compare-versions "${_requirement_version}" le "${_requirement_max_version}"; then
    return 1
  else
    return "0"
  fi
}

function install_or_update_requirement() {
  local _requirement
  _requirement="${1}"
  if prompt_acceptance "install/update ${_requirement}"; then
    echo "installing ${_requirement}"
    if ! requirement_install_or_update "${_requirement}" &> /dev/null; then
      echo "failed to install ${_requirement}"
    fi
  fi
}

function get_os() {
  grep -iw ID /etc/os-release | awk -F '=' '{print $2}'
}

function get_os_version() {
  grep -iw VERSION_ID /etc/os-release | awk -F '=' '{print $2}' | tr -d '"'
}

function os_supports_install_or_update_of_requirements() {
  case "$(get_os)-$(get_os_version)" in
    ubuntu-22.04 | ubuntu-24.04)
      echo "yes" ;;
    *)
      echo "no" ;;
  esac
}

function is_yes() {
  if [[ "${1}" == [yY] || "${1}" == [yY][eE][sS] ]]; then
    return 0
  else
    return 1
  fi
}

function verify_requirement() {
  local _requirement
  _requirement="${1}"
  _attempt_install_or_update="${2}"
  if is_yes "${_attempt_install_or_update}" && verify_privilege_escalation &> /dev/null; then
    if ! requirement_present "${_requirement}" || ! verify_min_requirement_version "${_requirement}" || ! verify_max_requirement_version "${_requirement}"; then
      install_or_update_requirement "${_requirement}"
      verify_requirement "${_requirement}" "no"
    fi
  elif ! requirement_present "${_requirement}"; then
    echo "${_requirement} is required"
    return 1
  elif ! verify_min_requirement_version "${_requirement}" && ! verify_max_requirement_version "${_requirement}"; then
    echo "${_requirement} version should be between $(requirement_min_version "${_requirement}") and $(requirement_max_version "${_requirement}")"
    return 1
  elif ! verify_min_requirement_version "${_requirement}"; then
    echo "${_requirement} version should be greater than $(requirement_min_version "${_requirement}")"
    return 1
  elif ! verify_max_requirement_version "${_requirement}"; then
    echo "${_requirement} version should be lesser than $(requirement_max_version "${_requirement}")"
    return 1
  else
    return 0
  fi
}

function privilege_escalation() {
  if [[ "${EUID}" -ne 0 ]]; then
    echo -n "sudo"
  else
    echo -n ""
  fi
}

function exit_if_not_ok() {
  if [[ "${1}" != "0" ]]; then
    exit "${1}"
  fi
}

execute "${@}"
for command in "${commands[@]}"
do
  "${command}"
  exit_if_not_ok "${?}"
done
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/platform-tasks.yml
````yaml
schema_version: '0.0.7'
tasks:
- name: create-platform-namespace
  play: k8s-manifest
  targets:
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  config_files:
  - k8s-manifest/namespace.yml
  vars:
    name: "{{ configs.k8s_namespace | default('platform') }}"

- name: validate-platform-k8s-secrets-value
  play: k8s-secret-check-empty
  targets:
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  config:
    k8s_secrets: "{{ configs.k8s_secrets }}"

- name: k8s-secrets-platform
  play: k8s-secret
  targets:
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  config:
    k8s_secrets: "{{ configs.k8s_secrets }}"
    namespace: "{{ configs.k8s_namespace | default('platform') }}"

- name: mdx-local-path-provisioner
  play: helm-release
  targets:
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  config:
    repo_name: "nvidia-ace"
    repo_url: "https://helm.ngc.nvidia.com/nvidia/ace"
    repo_username: "$oauthtoken"
    repo_password: "{{ secrets.ngc_cli_api_key }}"
    chart_ref: "mdx-local-path-provisioner"
    chart_version: '0.3.0'
    namespace: "{{ configs.k8s_namespace | default('platform') }}"
    release_name: mdx-local-path-provisioner
    state: "{{ state | default('present') }}"

- name: add-sysctl-settings
  play: add-sysctl-config
  targets: 
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  vars:
    parameter: 'fs.inotify.max_user_instances'
    value: '8192'

- name: tokkio-ingress-controller
  play: helm-release
  targets: 
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  config:
    repo_name: "nvidia-ace"
    repo_url: "https://helm.ngc.nvidia.com/nvidia/ace"
    repo_username: "$oauthtoken"
    repo_password: "{{ secrets.ngc_cli_api_key }}"
    chart_ref: "mdx-nginx-ingress-controller"
    chart_version: '1.0.0'
    namespace: "{{ configs.k8s_namespace | default('platform') }}"
    release_name: tokkio-ingress-controller
    state: "{{ state | default('present') }}"


- name: tokkio-logging-es
  play: helm-release
  targets: 
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  vars:
    ops_es_cluster_name: 'tokkio-logging-es-cluster'
  config:
    repo_name: "nvidia-ace"
    repo_url: "https://helm.ngc.nvidia.com/nvidia/ace"
    repo_username: "$oauthtoken"
    repo_password: "{{ secrets.ngc_cli_api_key }}"
    chart_ref: "logging-stack-elastic-kibana"
    chart_version: '0.0.2'
    namespace: "{{ configs.k8s_namespace | default('platform') }}"
    release_name: tokkio-logging-es
    state: "{{ state | default('present') }}"
    values_files:
    - "{{ dist_dir }}/config-files/helm-release/tokkio-logging-es-override-values.yml"

# # As setting index.number_of_replicas = 0 on es cluster as we have only single pod otherwise cluster status fails on node reboot for es-master pod
- name: logging-es-replicas-update-configmap
  play: k8s-manifest
  targets:
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  config_files:
  - k8s-manifest/logging-es-replicas-update-configmap.yml
  vars:
    namespace: "{{ configs.k8s_namespace | default('platform') }}"

- name: logging-es-replicas-update-deployment
  play: k8s-manifest
  targets:
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  config_files:
  - k8s-manifest/es-replicas-update-deployment.yml
  vars:
    namespace: "{{ configs.k8s_namespace | default('platform') }}"

- name: mdx-kube-prometheus-stack
  play: helm-release
  targets: 
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  config:
    repo_name: "nvidia-ace"
    repo_url: "https://helm.ngc.nvidia.com/nvidia/ace"
    repo_username: "$oauthtoken"
    repo_password: "{{ secrets.ngc_cli_api_key }}"
    chart_ref: "mdx-kube-prometheus-stack"
    chart_version: '1.0.4'
    namespace: "{{ configs.k8s_namespace | default('platform') }}"
    release_name: mdx-kube-prometheus-stack
    state: "{{ state | default('present') }}"

# - name: ingress-rules
#   play: k8s-manifest
#   targets:
#   - "{{ task_each_vars.value.targets.master }}"
#   for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
#   config_files:
#   - k8s-manifest/platform-ingress-rule.yml
#   vars:
#     namespace: "{{ configs.k8s_namespace | default('platform') }}"
#     elastic_domain: "{{ iac.clusters.app.platform.elasticsearch_endpoint }}"
#     kibana_domain: "{{ iac.clusters.app.platform.kibana_endpoint }}"
#     grafana_domain: "{{ iac.clusters.app.platform.grafana_endpoint }}"
#     ops_es_cluster_name: 'tokkio-logging-es-cluster'

- name: pause
  play: sleep 
  targets: 
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  vars:
    time: 180
  condition: "{{ state == 'present' }}"

- name: tokkio-fluent-bit
  play: helm-release
  targets: 
  - "{{ task_each_vars.value.targets.master }}"
  for_each: "{{ iac.clusters | dict2items | selectattr('value.features', 'issuperset', ['app']) | items2dict }}"
  vars:
    ops_es_cluster_name: 'tokkio-logging-es-cluster'
    Flush:  "{{ .Values.flush }}"
    Log_Level: "{{ .Values.logLevel }}"
    HTTP_Port: "{{ .Values.metricsPort }}"
  config:
    repo_name: "nvidia-ace"
    repo_url: "https://helm.ngc.nvidia.com/nvidia/ace"
    repo_username: "$oauthtoken"
    repo_password: "{{ secrets.ngc_cli_api_key }}"
    chart_ref: "tokkio-fluent-bit-logging-service"
    chart_version: '0.0.3'
    namespace: "{{ configs.k8s_namespace | default('platform') }}"
    release_name: tokkio-fluent-bit
    state: "{{ state | default('present') }}"
    values_files:
    - "{{ dist_dir }}/config-files/helm-release/tokkio-fluent-bit-override-values.yml"
````

## File: workflows/tokkio/4.1/scripts/one-click/baremetal/setup-cns-access.sh
````bash
#!/bin/bash

schema_version="0.0.7"
script_name="${0}"
exec_dir="$(pwd)"
script_dir="$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
config_file="${script_dir}/config.yml"
tunnel_port="32768"
cluster=""
tmp_dir="$(mktemp -d -p "${script_dir}")"
commands=()

function usage() {
  echo "Usage: ${script_name} (-v|--version)"
  echo "   or: ${script_name} (-h|--help)"
  echo "   or: ${script_name} [options]"
  echo ""
  echo "options:"
  echo "-f, --config-file    path to file containing config overrides, defaults to config.yml"
  echo "-p, --tunnel-port    port between 3276861000 on which to establish tunnel to CNS cluster, defaults to 32768"
  echo "-c, --cluster        name of the cluster if more than one clusters exist, optional if single cluster exists"
  echo "-h, --help           provide usage information"
}

function execute() {
  validate_args "${@}"
  process_args "${@}"
}

function validate_args() {
  local _args _all_good _valid_args _options _short_options _config_files _tunnel_ports _clusters _sub_commands
  _args=("${@}")
  _all_good=0
  _valid_args=$(getopt -q -o f:p:c:vh --long config-file:tunnel-port:,cluster:,version,help -- "${_args[@]}")
  _all_good=$(( _all_good + $? ))
  if [[ _all_good -gt 0 ]]; then
    echo "Invalid usage: ${_args[*]}"
  else
    eval set -- "${_valid_args}"
    _options=()
    _short_options=()
    _config_files=()
    _tunnel_ports=()
    _clusters=()
    while true; do
      case "${1}" in
        -f | --config-file) _options+=("${1}"); _short_options+=("-f"); shift; _options+=("${1}"); _config_files+=("${exec_dir}/${1}"); shift; ;;
        -p | --tunnel-port) _options+=("${1}"); _short_options+=("-p"); shift; _options+=("${1}"); _tunnel_ports+=("${1}"); shift; ;;
        -c | --cluster) _options+=("${1}"); _short_options+=("-c"); shift; _options+=("${1}"); _clusters+=("${1}"); shift; ;;
        -h | --help) _options+=("${1}"); _short_options+=("-h"); shift; ;;
        -v | --version) _options+=("${1}"); _short_options+=("-v"); shift; ;;
        --) shift; break ;;
      esac
    done
    IFS=" " read -r -a _short_options <<< "$(de_dupe_elements "${_short_options[@]}")"
    IFS=" " read -r -a _config_files <<< "$(de_dupe_elements "${_config_files[@]}")"
    IFS=" " read -r -a _tunnel_ports <<< "$(de_dupe_elements "${_tunnel_ports[@]}")"
    IFS=" " read -r -a _clusters <<< "$(de_dupe_elements "${_clusters[@]}")"
    _sub_commands=()
    while [[ -n "${1}" ]]; do
      _sub_commands+=("${1}")
      shift
    done
    if [[ "${#_sub_commands[@]}" -gt 0 ]]; then
      echo "Invalid usage: ${_args[*]}"
      ((_all_good++))
    elif [[ "${#_sub_commands[@]}" -eq 0 ]] && contains_element "-v" "${_short_options[@]}" && [[ "${#_short_options[@]}" -gt 2 ]]; then
      echo "Invalid usage: ${_args[*]}"
      ((_all_good++))
    elif [[ "${#_sub_commands[@]}" -eq 0 ]] && contains_element "-v" "${_short_options[@]}" && [[ "${#_short_options[@]}" -eq 2 ]] && ! contains_element "-h" "${_short_options[@]}"; then
      echo "Invalid usage: ${_args[*]}"
      ((_all_good++))
    elif [[ "${#_sub_commands[@]}" -eq 0 ]] && ! contains_element "-h" "${_short_options[@]}"; then
      if contains_element "-f" "${_short_options[@]}" && [[ "${#_config_files[@]}" -gt 1 ]]; then
        echo "Multiple config-files provided: ${_config_files[*]}"
        ((_all_good++))
      fi
      if contains_element "-p" "${_short_options[@]}" && [[ "${#_tunnel_ports[@]}" -gt 1 ]]; then
        echo "Multiple tunnel-ports provided: ${_tunnel_ports[*]}"
        ((_all_good++))
      fi
      if contains_element "-c" "${_short_options[@]}" && [[ "${#_clusters[@]}" -gt 1 ]]; then
        echo "Multiple clusters provided: ${_clusters[*]}"
        ((_all_good++))
      fi
    fi
  fi
  if [[ _all_good -gt 0 ]]; then
    echo ""
    usage
    exit 1
  fi
}

function process_args() {
  local _args _valid_args _short_options _config_files _tunnel_ports _clusters
  _args=("${@}")
  _valid_args=$(getopt -q -o f:p:c:vh --long config-file:tunnel-port:,cluster:,version,help -- "${_args[@]}")
  eval set -- "${_valid_args}"
  _short_options=()
  _config_files=()
  _tunnel_ports=()
  _clusters=()
  while true; do
    case "${1}" in
      -f | --config-file) _short_options+=("-f"); shift; _config_files+=("${exec_dir}/${1}"); shift; ;;
      -p | --tunnel-port) _short_options+=("-p"); shift; _tunnel_ports+=("${1}"); shift; ;;
      -c | --cluster) _short_options+=("-c"); shift; _clusters+=("${1}"); shift; ;;
      -h | --help) _short_options+=("-h"); shift; ;;
      -v | --version) _short_options+=("-v"); shift; ;;
      --) shift; break ;;
    esac
  done
  IFS=" " read -r -a _short_options <<< "$(de_dupe_elements "${_short_options[@]}")"
  IFS=" " read -r -a _config_files <<< "$(de_dupe_elements "${_config_files[@]}")"
  IFS=" " read -r -a _tunnel_ports <<< "$(de_dupe_elements "${_tunnel_ports[@]}")"
  IFS=" " read -r -a _clusters <<< "$(de_dupe_elements "${_clusters[@]}")"
  if contains_element "-h" "${_short_options[@]}"; then
    commands+=('usage')
  elif [[ "${#_short_options[@]}" -eq 1 ]] && [[ "${_short_options[0]}" == "-v" ]]; then
    commands+=('print_version')
  else
    if [[ "${#_config_files[@]}" -eq 1 ]]; then
      config_file="${_config_files[0]}"
    fi
    if [[ "${#_tunnel_ports[@]}" -eq 1 ]]; then
      tunnel_port="${_tunnel_ports[0]}"
    fi
    if [[ "${#_clusters[@]}" -eq 1 ]]; then
      cluster="${_clusters[0]}"
    fi
    commands+=('verify_pre_requisites')
    commands+=('activate_venv')
    commands+=('init')
    commands+=('extract_inventory')
    commands+=('check_inventory')
    commands+=('check_cluster_valid')
    commands+=('bootstrap_cns_kubeconfig')
  fi
}

function contains_element() {
  local _element _ref_array _array_element
  _element="${1}"
  _ref_array=("${@:2}")
  for _array_element in "${_ref_array[@]}"
  do
    if [[ "${_element}" == "${_array_element}" ]]; then
      return 0
    fi
  done
  return 1
}

function de_dupe_elements() {
  local _ref_array _de_duped_array _array_element
  _ref_array=("${@}")
  _de_duped_array=()
  for _array_element in "${_ref_array[@]}"
  do
    if [[ "${#_de_duped_array[@]}" -eq 0 ]] || ! contains_element "${_array_element}" "${_de_duped_array[@]}"; then
      _de_duped_array+=("${_array_element}")
    fi
  done
  echo "${_de_duped_array[@]}"
}

function print_version() {
  echo "Version: ${schema_version}"
}

function prompt_acceptance() {
  local _confirm
  read -rp  "Would you like to ${1}? (y/n): " _confirm
  is_yes "${_confirm}"
  return "${?}"
}

function verify_pre_requisites() {
  local _install_or_update_requirement _all_good _requirements _requirement
  _install_or_update_requirement="$(os_supports_install_or_update_of_requirements)"
  _all_good=0
  verify_config
  _all_good=$(( _all_good + $? ))
  verify_tunnel_port
  _all_good=$(( _all_good + $? ))
  verify_privilege_escalation
  _all_good=$(( _all_good + $? ))
  _requirements=()
  _requirements+=('jq')
  _requirements+=('yq')
  _requirements+=('terraform')
  _requirements+=('python3')
  _requirements+=('python3-venv')
  _requirements+=('python3-setuptools')
  _requirements+=('python3-dev')
  _requirements+=('python3-pip')
  for _requirement in "${_requirements[@]}"
  do
    verify_requirement "${_requirement}" "${_install_or_update_requirement}"
    _all_good=$(( _all_good + $? ))
  done
  if [[ _all_good -gt 0 ]]; then
    echo "One or more pre-requisites were not met"
    exit 1
  fi
}

function verify_config() {
  if [[ ! -f "${config_file}" ]]; then
    echo "Config file (${config_file}) not found"
    echo "Please use ${script_dir}/config-template.yml to create the ${config_file}"
    return 1
  fi
}

function verify_tunnel_port() {
  if [[ -z "${tunnel_port}" ]]; then
    echo "Tunnel port is required"
    return 1
  elif [[ "${tunnel_port}" -lt 32768 ]] || [[ "${tunnel_port}" -gt 61000 ]]; then
    echo "Tunnel port should be a number between 32768 and 61000"
    return 1
  else
    return 0
  fi
}

function verify_privilege_escalation() {
  if [[ "${EUID}" -ne 0 ]] && ! sudo -n true &> /dev/null; then
    echo "Current user is neither root, nor has passwordless sudo ability"
    return 1
  else
    return 0
  fi
}

function requirement_present() {
  local _requirement
  _requirement="${1}"
  case "${_requirement}" in
    jq | yq | terraform)
      hash "${_requirement}" 2> /dev/null
      return "${?}"
      ;;
    python3 | python3-venv | python3-setuptools | python3-dev | python3-pip)
      verify_apt_package_present "${_requirement}"
      return "${?}"
      ;;
    *)
      return 1
      ;;
  esac
}

function requirement_version() {
  local _requirement
  _requirement="${1}"
  case "${_requirement}" in
    jq)
      jq --version | cut -d '-' -f 2 2> /dev/null || echo ""
      ;;
    yq)
      yq --version | grep 'mikefarah' | awk '{print $4}' | tr -d 'v' 2> /dev/null || echo ""
      ;;
    terraform)
      terraform --version | head -n 1 | awk '{print $2}' | tr -d 'v' 2> /dev/null || echo ""
      ;;
    python3 | python3-venv | python3-setuptools | python3-dev | python3-pip)
      apt_package_version "${_requirement}"
      ;;
    *)
      echo "0"
      ;;
  esac
}

function requirement_min_version() {
  local _requirement
  _requirement="${1}"
  case "${_requirement}" in
    jq)
      echo '1.6'
      ;;
    yq)
      echo '4.34.1'
      ;;
    terraform)
      echo '1.5.7'
      ;;
    python3 | python3-venv | python3-dev)
      echo '3.9.0'
      ;;
    python3-setuptools)
      echo '59.6.0'
      ;;
    python3-pip)
      echo '22.0.0'
      ;;
    *)
      echo "1"
      ;;
  esac
}

function requirement_max_version() {
  local _requirement
  _requirement="${1}"
  case "${_requirement}" in
    terraform)
      echo '1.5.7'
      ;;
    *)
      echo ""
      ;;
  esac
}

function requirement_install_or_update() {
  local _requirement
  _requirement="${1}"
  case "${_requirement}" in
    yq)
      $(privilege_escalation) wget "https://github.com/mikefarah/yq/releases/download/v$(requirement_min_version "${_requirement}")/yq_linux_amd64" -O /usr/bin/yq
      $(privilege_escalation) chmod +x /usr/bin/yq
      ;;
    terraform)
      curl --silent -L https://raw.githubusercontent.com/versus/terraform-switcher/release/install.sh | $(privilege_escalation) bash
      mkdir -p "${HOME}/bin/terraform"
      tfswitch -qb "${HOME}/bin/terraform" "$(requirement_min_version "${_requirement}")"
      $(privilege_escalation) cp "${HOME}/.terraform.versions/terraform_$(requirement_min_version "${_requirement}")" /usr/local/bin/terraform
      ;;
    jq | python3 | python3-venv | python3-setuptools | python3-dev | python3-pip)
      install_or_update_apt_package "${_requirement}"
      ;;
    *)
      echo "1"
      ;;
  esac
}

function verify_apt_package_present() {
  local _apt_package
  _apt_package="${1}"
  if [[ "$(apt -qq list --installed "${_apt_package}" 2> /dev/null | wc -l)" -eq 1 ]]; then
    return 0;
  else
    return 1;
  fi
}

function apt_package_version() {
  local _apt_package
  _apt_package="${1}"
  apt show "${_apt_package}" 2> /dev/null | grep '^Version' | awk '{print $NF}' | awk -F ':' '{print $NF}' | awk -F '[-+]' '{print $1}'
}

function install_or_update_apt_package() {
  local _apt_package
  _apt_package="${1}"
  $(privilege_escalation) apt update
  $(privilege_escalation) apt-get install "${_apt_package}" -y
}

function verify_min_requirement_version() {
  local _requirement _requirement_version _requirement_min_version
  _requirement="${1}"
  _requirement_version="$(requirement_version "${_requirement}")"
  _requirement_min_version="$(requirement_min_version "${_requirement}")"
  if [[ -n "${_requirement_min_version}" ]] && ! dpkg --compare-versions "${_requirement_version}" ge "${_requirement_min_version}"; then
    return 1
  else
    return "0"
  fi
}

function verify_max_requirement_version() {
  local _requirement _requirement_version _requirement_max_version
  _requirement="${1}"
  _requirement_version="$(requirement_version "${_requirement}")"
  _requirement_max_version="$(requirement_max_version "${_requirement}")"
  if [[ -n "${_requirement_max_version}" ]] && ! dpkg --compare-versions "${_requirement_version}" le "${_requirement_max_version}"; then
    return 1
  else
    return "0"
  fi
}

function install_or_update_requirement() {
  local _requirement
  _requirement="${1}"
  if prompt_acceptance "install/update ${_requirement}"; then
    echo "installing ${_requirement}"
    if ! requirement_install_or_update "${_requirement}" &> /dev/null; then
      echo "failed to install ${_requirement}"
    fi
  fi
}

function get_os() {
  grep -iw ID /etc/os-release | awk -F '=' '{print $2}'
}

function get_os_version() {
  grep -iw VERSION_ID /etc/os-release | awk -F '=' '{print $2}' | tr -d '"'
}

function os_supports_install_or_update_of_requirements() {
  case "$(get_os)-$(get_os_version)" in
    ubuntu-22.04 | ubuntu-24.04)
      echo "yes" ;;
    *)
      echo "no" ;;
  esac
}

function is_yes() {
  if [[ "${1}" == [yY] || "${1}" == [yY][eE][sS] ]]; then
    return 0
  else
    return 1
  fi
}

function verify_requirement() {
  local _requirement
  _requirement="${1}"
  _attempt_install_or_update="${2}"
  if is_yes "${_attempt_install_or_update}" && verify_privilege_escalation &> /dev/null; then
    if ! requirement_present "${_requirement}" || ! verify_min_requirement_version "${_requirement}" || ! verify_max_requirement_version "${_requirement}"; then
      install_or_update_requirement "${_requirement}"
      verify_requirement "${_requirement}" "no"
    fi
  elif ! requirement_present "${_requirement}"; then
    echo "${_requirement} is required"
    return 1
  elif ! verify_min_requirement_version "${_requirement}" && ! verify_max_requirement_version "${_requirement}"; then
    echo "${_requirement} version should be between $(requirement_min_version "${_requirement}") and $(requirement_max_version "${_requirement}")"
    return 1
  elif ! verify_min_requirement_version "${_requirement}"; then
    echo "${_requirement} version should be greater than $(requirement_min_version "${_requirement}")"
    return 1
  elif ! verify_max_requirement_version "${_requirement}"; then
    echo "${_requirement} version should be lesser than $(requirement_max_version "${_requirement}")"
    return 1
  else
    return 0
  fi
}

function privilege_escalation() {
  if [[ "${EUID}" -ne 0 ]]; then
    echo -n "sudo"
  else
    echo -n ""
  fi
}

function exit_if_not_ok() {
  if [[ "${1}" != "0" ]]; then
    exit "${1}"
  fi
}

function deactivate_venv() {
  deactivate 2> /dev/null || true
}

function activate_venv() {
  deactivate_venv
  {
    python3 -m venv --copies --clear "${script_dir}/ansible-venv"
    source "${script_dir}/ansible-venv/bin/activate"
  } > /dev/null
  trap deactivate_venv EXIT
  {
    pip install --upgrade pip
    pip install ansible==7.0.0
  } > /dev/null
  if ! hash deactivate 2> /dev/null; then
    echo "Failed to activate virtualenv"
    exit 1
  fi
}

function abort_option() {
  echo "CTRL-C to abort"
  sleep 10
}

function switch_to_script_dir() {
  cd "${script_dir}" || exit 1
}

function init() {
  local _tmp_init_dir _config_yml _infra_yml _tf_backend_json _tf_variables_json _uncommented_config_file _name
  echo "preparing artifacts"
  switch_to_script_dir

  _tmp_init_dir="${tmp_dir}/init"
  mkdir -p "${_tmp_init_dir}"
  _config_yml="${_tmp_init_dir}/config.yml"
  _infra_yml="${_tmp_init_dir}/infra.yml"
  _tf_backend_json="${_tmp_init_dir}/tf-backend.json"
  _tf_variables_json="${_tmp_init_dir}/tf-variables.json"
  _uncommented_config_file="${_tmp_init_dir}/uncommented_config.yml"

  # prepare _config_yml
  sed '/^[[:space:]]*#/d' "${config_file}" > "${_uncommented_config_file}"
  process_user_config "${_uncommented_config_file}" "${_config_yml}"

  # get _name
  _name="$(yq eval '.name' "${_config_yml}")"

  # prepare _infra_yml
  yq eval '.spec.infra' "${_config_yml}" > "${_infra_yml}"

  # prepare _tf_backend_json
  case "$(yq eval '.csp' "${_infra_yml}")" in
    aws | azure)
      jq -n \
        --arg name "${_name}" \
        --argjson config "$(yq eval '.backend' "${_infra_yml}" -o=json)" \
        '$config * { key: ($name + "/terraform.tfstate") }' > "${_tf_backend_json}"
      ;;
    gcp)
      jq -n \
        --arg name "${_name}" \
        --argjson config "$(yq eval '.backend' "${_infra_yml}" -o=json)" \
        '$config * { prefix: ($name + "/terraform.tfstate") }' > "${_tf_backend_json}"
      ;;
    oci)
      jq -n \
        --arg name "${_name}" \
        --arg par "$(yq eval '.backend.pre_authenticated_request' "${_infra_yml}")" \
        '{ address: ($par + $name + "/terraform.tfstate"), update_method: "PUT" }' > "${_tf_backend_json}"
      ;;
    bm)
      _bm_state_dir="${HOME}/.nvoc/tf-state"
      mkdir -p "${_bm_state_dir}/${_name}"
      jq -n \
        --arg bm_state_dir "${_bm_state_dir}" \
        --arg name "${_name}" \
        '{ path: ($bm_state_dir + "/" + $name + "/terraform.tfstate") }' > "${_tf_backend_json}"
      ;;
    *)
      jq -n '{}' > "${_tf_backend_json}"
      ;;
  esac

  # prepare _tf_variables_json
  jq -n \
   --arg name "${_name}" \
   --argjson controller_ip "$(jq -n --arg ip "$(curl -s ifconfig.me)" '{ controller_ip: $ip }')" \
   --argjson provider "$(yq eval '.provider' "${_infra_yml}" -o=json | jq '{provider_config: .}')" \
   --argjson configs "$(yq eval '.configs' "${_infra_yml}" -o=json)" \
    '{ name: $name } * $controller_ip * $provider * $configs' >  "${_tf_variables_json}"

  cp -r iac-ref iac
  cp "${_tf_backend_json}" iac/tf-backend.json
  cp "${_tf_variables_json}" iac/tf-variables.json
}

function extract_inventory() {
  local _exit_code
  echo "extracting inventory"
  switch_to_script_dir
  terraform -chdir="iac" init -reconfigure -backend-config=tf-backend.json > /dev/null
  terraform -chdir="iac" plan -compact-warnings -out=tf-plan -var-file=tf-variables.json -detailed-exitcode 1> /dev/null 2> /dev/null
  _exit_code=$?
  if [[ "${_exit_code}" != "0" ]]; then
    echo "cannot extract inventory when there are IaC changes"
    rm inventory.yml &> /dev/null || true
    rm cns-host-groups.json &> /dev/null || true
    rm ssh-commands.json &> /dev/null || true
    return 1
  else
    terraform -chdir="iac" output -json | jq -r '.hosts.value' > inventory.yml
    terraform -chdir="iac" output -json | jq -r '.cns_clusters.value // {}' > cns-clusters.json
    return 0
  fi
}

function check_inventory() {
  local _retry_counter
  echo "checking inventory"
  switch_to_script_dir
  _retry_counter=0
  while ! ansible-playbook -i inventory.yml playbooks/check-inventory.yml > /dev/null 2> /dev/null; do
    if [[ "${_retry_counter}" -ge 15 ]]; then
      echo "exhausted attempts waiting for inventory to be ready"
      return 1
    else
      ((_retry_counter++))
      echo "waiting for inventory to be ready"
      sleep 20
    fi
  done
  return 0
}

function check_cluster_valid() {
  echo "checking cluster valid"
  switch_to_script_dir
  if [[ -n "${cluster}" ]] && [[ "$(jq --arg cluster "${cluster}" 'has($cluster)' < cns-clusters.json)" == "true" ]]; then
    return 0
  elif [[ -z "${cluster}" ]] && [[ "$(jq '. | to_entries | length' < cns-clusters.json)" -eq 1 ]]; then
    cluster="$(jq -r 'keys[0]' < cns-clusters.json)"
    return 0
  else
    echo "Invalid or ambiguous cluster"
    return 1
  fi
}

function bootstrap_cns_kubeconfig() {
  local _cluster_master _cluster_name
  switch_to_script_dir
  _cluster_master="$(jq -r --arg cluster "${cluster}" '.[$cluster].master_name' < cns-clusters.json)"
  jq -r --arg cluster "${cluster}" '.[$cluster].ssh_command' < cns-clusters.json > ssh-command.sh
  _cluster_name="$(jq -r '.name' < iac/tf-variables.json)-${cluster}"
  ANSIBLE_JINJA2_NATIVE=true ansible-playbook \
    -l "localhost,${_cluster_master}" \
    -i inventory.yml \
    -e cluster_name="${_cluster_name}" \
    -e ssh_command="${script_dir}/ssh-command.sh" \
    -e tunnel_port="${tunnel_port}" \
    playbooks/bootstrap-cns-kubeconfig.yml
}

function process_user_config() {
  local _input_file _output_file
  _input_file="${1}"
  _output_file="${2}"
  if ! ANSIBLE_JINJA2_NATIVE=true ansible-playbook \
      -c local \
      -i localhost, \
      -e input_file="${_input_file}" \
      -e output_file="${_output_file}" \
      playbooks/process-user-config.yml &> /dev/null; then
    echo "could not process config file: ${_input_file}"
    exit 1
  fi
}

function cleanup() {
  switch_to_script_dir
  rm -rf "${tmp_dir}" 2> /dev/null
}

function abort() {
  echo "Aborting as SIGTERM/SIGINT received"
  trap - SIGINT SIGTERM # clear the trap
  kill -- -$$ # Sends SIGTERM to child/sub processes
}

trap cleanup EXIT
trap abort SIGINT SIGTERM

execute "${@}"
for command in "${commands[@]}"
do
  "${command}"
  exit_if_not_ok "${?}"
done
````

## File: workflows/tokkio/4.1/README.md
````markdown
<p align="center">
  <img src="avatar_collage.jpg" alt="ACE">
</p>

Tokkio Reference Workflow
--------

Tokkio is an interactive avatar virtual customer service assistant product SDK. It can be adopted for virtual assistant use case in any domain. The face of the product is an avatar. The avatar can see using camera and perceive the situation using vision AI

The [Tokkio reference workflow documentation](https://docs.nvidia.com/ace/latest/workflows/tokkio/index.html) is presented in the [ACE documentation](https://docs.nvidia.com/ace/latest/index.html). We invite the user to visit the documentation to get the full context about Tokkio on getting started, CSP deployment and customization.

Tokkio application has 4 different reference flavors:

- [Tokkio LLM-RAG with 3D Omniverse renderering](https://docs.nvidia.com/ace/latest/workflows/tokkio/text/reference-workflows/Tokkio_LLM_RAG.html)
- [Tokkio LLM-RAG with 3D Unreal engine rendering](https://docs.nvidia.com/ace/latest/workflows/tokkio/text/reference-workflows/Tokkio_UE.html)
- [Tokkio LLM-RAG with A2F-2D rendering](https://docs.nvidia.com/ace/latest/workflows/tokkio/text/reference-workflows/Tokkio_Live_Portrait.html)
- [Tokkio Retail with 3D Omniverse renderering](https://docs.nvidia.com/ace/latest/workflows/tokkio/text/reference-workflows/Tokkio_Retail.html)
````

## File: .gitattributes
````
# Archives
*.7z filter=lfs diff=lfs merge=lfs -text
*.br filter=lfs diff=lfs merge=lfs -text
*.gz filter=lfs diff=lfs merge=lfs -text
*.tar filter=lfs diff=lfs merge=lfs -text
*.zip filter=lfs diff=lfs merge=lfs -text

# Documents
*.pdf filter=lfs diff=lfs merge=lfs -text
# Images
*.gif filter=lfs diff=lfs merge=lfs -text
*.ico filter=lfs diff=lfs merge=lfs -text
*.jpg filter=lfs diff=lfs merge=lfs -text
*.png filter=lfs diff=lfs merge=lfs -text
*.psd filter=lfs diff=lfs merge=lfs -text
*.webp filter=lfs diff=lfs merge=lfs -text
# Fonts
*.woff2 filter=lfs diff=lfs merge=lfs -text
# Other
*.exe filter=lfs diff=lfs merge=lfs -text
*.bin filter=lfs diff=lfs merge=lfs -text
*.dll filter=lfs diff=lfs merge=lfs -text
*.lib filter=lfs diff=lfs merge=lfs -text
*.wav !text !filter !merge !diff
````

## File: .gitignore
````
**.terraform/
**terraform.tfstate
**terraform.tfstate.backup**
**.terraform.lock.hcl
**ignore.auto.tfvars
**tfplan
**gcp-credentials/
**logs/
.vscode
````

## File: LICENSE
````
SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
SPDX-License-Identifier: Apache-2.0

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
````

## File: README.md
````markdown
NVIDIA ACE
--------

NVIDIA ACE is a suite of technologies that help developers bring digital humans to life with generative AI. ACE NIMs are microservices designed to run in the cloud or on PC.

![](https://lh7-us.googleusercontent.com/FJKnZYOQX34lHQ_OccHOvSXFfsFg3RyY1LWgg9_s5NA1RrQr4XH8cA5T3CvuQmysig74EpxQFbOwN4OP-CpQgYNGbjIpC6ior7YlhYPdqMI95fP-_Kv5dkZB_RSegAQ-m6-yzN2n-uwFjDAZB1rlPKQ)

On this Git repo, you will find samples and reference applications using ACE NIMs and microservices. However, these microservices can be obtained through an evaluation license of NV AI Enterprise(NVAIE) through NGC.

1. [Try NIM For Digital Human](https://build.nvidia.com/explore/gaming)
2. [Get NVIDIA AI Enterprise](https://docs.nvidia.com/ai-enterprise/latest/quick-start-guide/index.html#getting-your-nvidia-grid-software)
3. [Download ACE Microservices](https://catalog.ngc.nvidia.com/?filters=&orderBy=scoreDESC&query=ace&page=&pageSize=)


ACE Technologies
------
|                    Technology                   |                                Description                              |          Software   Support        |     Cloud   Deployment    |     Windows   Deployment    |
|:-----------------------------------------------:|:-----------------------------------------------------------------------:|:----------------------------------:|:-------------------------:|:---------------------------:|
|     Riva      Automatic Speech   Recognition    |                            Speech   -&gt; Text                          |        NVIDIA   AI Enterprise      |              X            |         Coming   Soon       |
|      Riva      Neural Machine   Translation     |                            Text   Translation                           |        NVIDIA   AI Enterprise      |              X            |                             |
|             Riva      Text-to-Speech            |                            Text   -&gt; Speech                          |        NVIDIA   AI Enterprise      |              X            |         Coming   Soon       |
|                    Audio2Face                   |           Audio   -&gt; Blendshapes      for   Facial Lip-sync          |        NVIDIA   AI Enterprise      |              X            |         Coming   Soon       |
|                     AnimGraph                   |                          Animation   controller                         |        NVIDIA   AI Enterprise      |              X            |                             |
|             Omniverse RTX Rendering Microservice           |                     Omniverse   Based Pixel Streamer                    |        NVIDIA   AI Enterprise      |              X            |                             |
|                     ACE Agent                   |                Conversational   Controller, RAG Workflows               |        NVIDIA   AI Enterprise      |              X            |                             |
|           Maxine Speech   Live Portrait         |                    2D   Picture Lipsync and Animation                   |     Early   Access   Evaluation    |              X            |                             |
|                Nemotron-3 4.5B SLM              |                          Small   Language Model                         |      Early   Access Evaluation     |        Coming   Soon      |               X             |
|             Gaming Reference Workflow           |                    Audio2Face   Unreal Engine Examples                  |          Example   Workflow        |              X            |         Coming   Soon       |
|       Customer Service Reference   Workflow     |     Full   reference workflow of customer service and kiosk usecases    |       Example   Workflow           |              X            |                             |


The Key Benefits of ACE
--------

### State-of-the-Art Models and Microservices

NVIDIA pre-trained models provide industry-leading quality and real-time performance.

### Safe and Consistent Results

AI models trained on commercially safe, responsibly licensed data. Fine-tuning and guardrails enable accurate, appropriate, and on-topic results no matter the user's input.

### Flexible Deployment Options

Handle inference through any public or private cloud, Windows PC, or a mix of both.

## Digital Human Workflows

Developers can leverage ACE to build their own digital human solutions from the ground up, or use NVIDIA's suite of domain-specific AI workflows for next-generation non-playable game characters (NPCs), interactive digital assistants for customer service, and digital avatars for real-time communication.

### Gaming Characters

NVIDIA Kairos Sample showcases an easy to use Unreal Engine project using the Audio2Face microservice. This sample shows how to connect Audio2Face to Metahuman and configure the Audio2Face microserivce. 

[Learn More About ACE NIMs for Gaming](https://build.nvidia.com/explore/gaming)

### Customer Service

NVIDIA Tokkio is a digital assistant workflow built with ACE, bringing AI-powered customer service capabilities to healthcare, financial services, and retail. It comes to life using state-of-the-art real-time language, speech, and animation generative AI models alongside retrieval augmented generation (RAG) to convey specific and up-to-date information to customers.

[Learn More Tokkio Customer Service Workflow](https://developer.nvidia.com/nvidia-omniverse-platform/ace/tokkio-showcase)

Documentation and Tutorials
-------------
Full ACE [developer documenation](https://docs.nvidia.com/ace/latest/index.html)

| Component | Documentation | Video/Tutorial |
| ------ | ------ | ------ |
|      Getting Started  |        | [NVIDIA Docker Setup](https://youtu.be/2uWXeIol468), [Install Kubernetes](https://www.youtube.com/watch?v=ACIkyiWglW4) |
|     NVIDIA UCS   | [Documentation](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ucs-ms/resources/ucs_tools/version) ||
|NVIDIA Audio2Face| [Documentation](https://docs.nvidia.com/ace/latest/modules/a2f-docs/index.html) | Coming soon! |
|NVIDIA Riva ASR| [Documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-overview.html)|Coming soon! |
|NVIDIA Riva TTS| [Documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-overview.html)|Coming soon! |
|NVIDIA Riva NMT|[Documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/translation/translation-overview.html) |Coming soon! |
|NVIDIA ACE Agent Microservices|[Documentation](https://docs.nvidia.com/ace/latest/modules/ace_agent/index.html)|Coming soon! |
|NVIDIA Maxine Live Portrait|[Documentation](https://registry.ngc.nvidia.com/orgs/eevaigoeixww/teams/live-portrait-ms/resources/live_portrait_user_guide)|Coming soon! |
|NVIDIA Avatar Configurator & Avatar Customization|[Documentation](https://docs.nvidia.com/ace/latest/modules/avatar_customization/Avatar_Configurator.html)|Coming soon! |
|NVIDIA Animation Graph Microservice|[Documentation](https://docs.nvidia.com/ace/latest/modules/animation_graph_microservice/index.html)|Coming soon! |
|NVIDIA Omniverse Renderer Microservice|[Documentation](https://docs.nvidia.com/ace/latest/modules/omniverse_renderer_microservice/index.html)|Coming soon! |

### Example Workflows

| Example | Description | Video |
| ------ | ------ | ------ |
|     Text-to-Gesture   |   Text-to-Gesture using A2X & Animation Graph Microservices     | [Creation of Basic Sentiment Analysis Utility](https://www.youtube.com/watch?v=g3Vb7EhlEUA),  [Connecting all Microservices in UCF](https://www.youtube.com/watch?v=TP4RD-T0GOI),  [Deployment & App Execution](https://www.youtube.com/watch?v=EHyga9smaSA)|
|    Reallusion Character    |   Exporting Character in Reallusion Character Creator + Audio2Face     | [Exporting Character from Reallusion Character Creator & Preparing Character in Audio2Face](https://www.youtube.com/watch?v=_Vkiup06lYQ), [Setup, streaming through a Reference App & Fine Tuning](https://www.youtube.com/watch?v=3xBhOKHbrFU)|
|      Stylised Avatar  |    Building Stylised Avatar Pipeline with ACE Components    | [Making & Animating a Stylised 3D Avatar From Text Inputs](https://www.youtube.com/watch?v=cnyy0mlL8C0), [Make Vincent Rig Compatible for UE5 & A2X LiveLink](https://www.youtube.com/watch?v=2MgzVluShtc), [Make Vincent Blueprint Receive A2X Animation Data](https://www.youtube.com/watch?v=fpthK6WHjX8), [Create Python App to Generate Audio from Text & Animate Vincent](https://www.youtube.com/watch?v=g14c2gcbowM)|


License
-------

Github - [Apache 2](https://www.apache.org/licenses/LICENSE-2.0.txt)

ACE NIMs and NGC Microservices - [NVIDIA AI Product License](https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/)
````
