[ACE](https://docs.nvidia.com/ace/overview/latest/index.html)Workflows
  * [Animation Pipeline](https://docs.nvidia.com/ace/animation-pipeline/latest/index.html)
  * [Tokkio](https://docs.nvidia.com/ace/tokkio/latest/index.html)
  * [Gaming Avatar](https://docs.nvidia.com/ace/gaming-avatar/latest/index.html)

Microservices
  * [Animation Graph](https://docs.nvidia.com/ace/animation-graph-microservice/latest/index.html)
  * [Omniverse Renderer](https://docs.nvidia.com/ace/omniverse-renderer-microservice/latest/index.html)
  * [Audio2Face-3D](https://docs.nvidia.com/ace/audio2face-3d-microservice/latest/index.html)
  * [ACE Agent](https://docs.nvidia.com/ace/ace-agent/latest/index.html)
  * [Riva ASR](https://docs.nvidia.com/ace/riva-asr-microservice/latest/index.html)
  * [Riva TTS](https://docs.nvidia.com/ace/riva-tts-microservice/latest/index.html)
  * [Riva Translation](https://docs.nvidia.com/ace/riva-translation-microservice/latest/index.html)
  * [Audio2Face-2D](https://docs.nvidia.com/ace/audio2face-2d-microservice/latest/index.html)
  * [Voice Font](https://docs.nvidia.com/ace/voice-font-microservice/latest/index.html)

Microservices (Early Access)
  * [Audio2Face-3D Authoring](https://docs.nvidia.com/ace/audio2face-3d-authoring-microservice/latest/index.html)
  * [Unreal Renderer](https://docs.nvidia.com/ace/unreal-renderer-microservice/latest/index.html)

Interfaces
  * [Animation Data Format](https://docs.nvidia.com/ace/animation-data-format/latest/index.html)
  * [Colang](https://docs.nvidia.com/ace/colang-language/latest/index.html)
  * [UMIM](https://docs.nvidia.com/ace/umim/latest/index.html)

Containers
  * [Resource Downloader](https://docs.nvidia.com/ace/resource-downloader-container/latest/index.html)

Tools
  * [Avatar Customization](https://docs.nvidia.com/ace/avatar-customization/latest/index.html)
  * [Unified Cloud Services Tools (UCS Tools)](https://docs.nvidia.com/ucf/)
  * [Maya-ACE](https://docs.nvidia.com/ace/maya-ace/latest/index.html)
  * [ACE Unreal Plugin](https://docs.nvidia.com/ace/ace-unreal-plugin/latest/index.html)


[Skip to main content](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-langchain-bot.html#main-content)
Back to top
`Ctrl`+`K`
[ ![ACE Agent - Home](https://docs.nvidia.com/ace/ace-agent/latest/_static/nvidia-logo-horiz-rgb-blk-for-screen.svg) ![ACE Agent - Home](https://docs.nvidia.com/ace/ace-agent/latest/_static/nvidia-logo-horiz-rgb-wht-for-screen.svg) ACE Agent ](https://docs.nvidia.com/ace/ace-agent/latest/index.html)
Search `Ctrl`+`K`
Search `Ctrl`+`K`
[ ![ACE Agent - Home](https://docs.nvidia.com/ace/ace-agent/latest/_static/nvidia-logo-horiz-rgb-blk-for-screen.svg) ![ACE Agent - Home](https://docs.nvidia.com/ace/ace-agent/latest/_static/nvidia-logo-horiz-rgb-wht-for-screen.svg) ACE Agent ](https://docs.nvidia.com/ace/ace-agent/latest/index.html)
Table of Contents
  * [Getting Started](https://docs.nvidia.com/ace/ace-agent/latest/getting-started.html)
    * [Overview](https://docs.nvidia.com/ace/ace-agent/latest/overview.html)
    * [Quick Start Guide](https://docs.nvidia.com/ace/ace-agent/latest/quick-start-guide.html)
    * [Release Notes](https://docs.nvidia.com/ace/ace-agent/latest/release-notes.html)
  * [Architecture](https://docs.nvidia.com/ace/ace-agent/latest/architecture/index.html)
    * [Architecture Introduction](https://docs.nvidia.com/ace/ace-agent/latest/architecture/arch-intro.html)
  * [Deployment](https://docs.nvidia.com/ace/ace-agent/latest/deployment/index.html)
    * [Interface Introduction](https://docs.nvidia.com/ace/ace-agent/latest/deployment/interface-intro.html)
    * [Docker Environment](https://docs.nvidia.com/ace/ace-agent/latest/deployment/docker-environment.html)
    * [Kubernetes Environment](https://docs.nvidia.com/ace/ace-agent/latest/deployment/kubernetes-environment.html)
    * [Python Environment](https://docs.nvidia.com/ace/ace-agent/latest/deployment/python-environment.html)
    * [Sample Clients](https://docs.nvidia.com/ace/ace-agent/latest/deployment/sample-clients.html)
  * [Tutorials](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/index.html)
    * [Building a Bot using Colang 2.0 and Event Interface](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-colang.html)
    * [Building LangChain-Based Bots](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-langchain-bot.html)
    * [Building a Low Latency Speech-To-Speech RAG Bot](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html)
    * [Customizing a Bot](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/customize-a-bot.html)
  * [Sample Bots](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/index.html)
    * [Sample Bots Introduction](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/sample-bots-intro.html)
    * [Chitchat Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/chitchat-bot.html)
    * [LLM Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/llm-bot.html)
    * [RAG Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/rag-bot.html)
    * [Stock Market Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/stock-market-bot.html)
    * [Gaming Non-Playing Character (NPC) Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/gaming-npc-bot.html)
    * [DuckDuckGo LangChain Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/duckduckgo-langchain-bot.html)
    * [Spanish Weather Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/spanish-weather-bot.html)
    * [Food Ordering Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/food-ordering-bot.html)
    * [Plan and Execute (LangGraph) Bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/plan-execute-langgraph-bot.html)
    * [Colang 1.0 Sample Bots](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/colang1-sample-bots.html)
  * [Configuration Guide](https://docs.nvidia.com/ace/ace-agent/latest/config/index.html)
    * [Bot Configurations Introduction](https://docs.nvidia.com/ace/ace-agent/latest/config/bot-configurations.html)
    * [General Configurations](https://docs.nvidia.com/ace/ace-agent/latest/config/general-configurations.html)
    * [Slots Configurations](https://docs.nvidia.com/ace/ace-agent/latest/config/slots-configurations.html)
    * [Plugin Configurations](https://docs.nvidia.com/ace/ace-agent/latest/config/plugin-configurations.html)
    * [Model Configurations](https://docs.nvidia.com/ace/ace-agent/latest/config/model-configurations.html)
    * [Chat Engine Configurations](https://docs.nvidia.com/ace/ace-agent/latest/config/chat-engine-configurations.html)
    * [Speech Configurations](https://docs.nvidia.com/ace/ace-agent/latest/config/speech-configurations.html)
  * [User Guide](https://docs.nvidia.com/ace/ace-agent/latest/user/index.html)
    * [Using Colang](https://docs.nvidia.com/ace/ace-agent/latest/user/colang.html)
    * [Integration with LangChain and LlamaIndex](https://docs.nvidia.com/ace/ace-agent/latest/user/integrate-langchan-llmaindex.html)
    * [NLP Server](https://docs.nvidia.com/ace/ace-agent/latest/user/nlp-server.html)
    * [Speech AI](https://docs.nvidia.com/ace/ace-agent/latest/user/speech-ai.html)
    * [Training Models](https://docs.nvidia.com/ace/ace-agent/latest/user/training-models.html)
    * [Plugin Server](https://docs.nvidia.com/ace/ace-agent/latest/user/plugin-server.html)
    * [Streaming in ACE Agent](https://docs.nvidia.com/ace/ace-agent/latest/user/streaming.html)
  * [API Guide](https://docs.nvidia.com/ace/ace-agent/latest/api/index.html)
    * [API Introduction](https://docs.nvidia.com/ace/ace-agent/latest/api/api-intro.html)
    * [HTTP Interface](https://docs.nvidia.com/ace/ace-agent/latest/api/http-interface.html)
    * [gRPC Interface](https://docs.nvidia.com/ace/ace-agent/latest/api/grpc-interface.html)
    * [Plugin Server](https://docs.nvidia.com/ace/ace-agent/latest/api/plugin-server-api.html)
    * [NLP Server](https://docs.nvidia.com/ace/ace-agent/latest/api/nlp-server-api.html)
  * [Best Practices](https://docs.nvidia.com/ace/ace-agent/latest/best-practices/index.html)
    * [Best Practices Introduction](https://docs.nvidia.com/ace/ace-agent/latest/best-practices/best-practices-intro.html)
    * [Planning your Bot](https://docs.nvidia.com/ace/ace-agent/latest/best-practices/planning-bot.html)
    * [Logging and Debugging Issues](https://docs.nvidia.com/ace/ace-agent/latest/best-practices/log-debug-issues.html)
    * [Security Considerations](https://docs.nvidia.com/ace/ace-agent/latest/best-practices/security.html)
  * [Reference](https://docs.nvidia.com/ace/ace-agent/latest/reference/index.html)
    * [Basic Concepts](https://docs.nvidia.com/ace/ace-agent/latest/reference/basic-concepts.html)
    * [Support Matrix](https://docs.nvidia.com/ace/ace-agent/latest/reference/support-matrix.html)
    * [Migrating from ACE Agent 4.0.0 to ACE Agent 4.1.0](https://docs.nvidia.com/ace/ace-agent/latest/reference/migrating.html)
    * [Troubleshooting](https://docs.nvidia.com/ace/ace-agent/latest/reference/troubleshooting.html)
    * [End User License Agreement](https://docs.nvidia.com/ace/ace-agent/latest/reference/eula.html)
    * [Notices](https://docs.nvidia.com/ace/ace-agent/latest/reference/notices.html)


4.1
[4.1](https://docs.nvidia.com/ace/ace-agent/4.1/tutorials/build-langchain-bot.html)[4.0](https://docs.nvidia.com/ace/ace-agent/4.0/tutorials/build-langchain-bot.html)
  * [ ](https://docs.nvidia.com/ace/ace-agent/latest/index.html)
  * [Tutorials](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/index.html)
  * Building...


# Building LangChain-Based Bots[#](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-langchain-bot.html#building-langchain-based-bots "Link to this heading")
In this section, let’s build a bot that uses a [LangChain runnable](https://python.langchain.com/v0.1/docs/expression_language/interface/) to drive the conversation. We will build a LangChain agent that performs DuckDuckGo searches and answers questions in the context of the conversation. You can swap out this agent with their own chains/agents in this tutorial. You can learn about [LangChain Runnable Interface](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface), [LangServe](https://python.langchain.com/v0.2/docs/langserve/), [LangGraph](https://langchain-ai.github.io/langgraph/), and a few other terminologies mentioned by following [LangChain Documentation](https://python.langchain.com/v0.2/docs/concepts/). You can cross reference the [DuckDuckGo LangChain Sample bot](https://docs.nvidia.com/ace/ace-agent/latest/sample-bots/duckduckgo-langchain-bot.html#duckduckgo-langchain-bot) for code or any issues in the tutorial steps.
The interaction with the LangChain runnable will happen via a custom Plugin server. In this tutorial, we will define a LangChain runnable directly in the custom Plugin server. However, you can also choose to deploy your LangChain runnables via LangServe and have your custom Plugins interact with the LangServe APIs using remote runnables.
The minimal file structure of the LangChain bot looks like this:
> ```
samples
└── langchain_tutorial_bot
  └── plugin
    └── langchain_agent.py
    └── schemas.py
  └── plugin_config.yaml
  └── speech_config.yaml
  └── model_config.yaml

```

Create an empty directory called `samples/langchain_tutorial_bot` and perform the following steps for updating bot configurations.
For the LangChain or custom RAG pipelines that do not require processing or guardrailing with Colang, you can directly connect the Chat Controller to the Plugin server. Follow the [plugin server architecture](https://docs.nvidia.com/ace/ace-agent/latest/architecture/arch-intro.html#plugin-serv-arch) section for more information. We will demonstrate this approach below.
## Creating the LangChain Plugin[#](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-langchain-bot.html#creating-the-langchain-plugin "Link to this heading")
In this section, let’s build the custom LangChain Plugin and test it as a standalone component.
  1. Define the input and output API schemas used by ACE Agent for communication with the Plugin server and Chat Controller. Update `plugin/schemas.py` with the following contents.
> ```
from pydantic import BaseModel, Field
from typing import Optional, Dict, List, Any

class ChatRequest(BaseModel):
  Query: Optional[str] = Field(default="", description="The user query which needs to be processed.")
  UserId: str = Field(
    description="Mandatory unique identifier to recognize which user is interacting with the Chat Engine."
  )
Metadata: Optional[Dict[str, Any]] = Field(
  default={},
  description="Any additional information related to the request.",
)

class EventRequest(BaseModel):
  EventType: str = Field(default="", description="The event name which needs to be processed.")
  UserId: str = Field(
    description="Mandatory unique identifier to recognize which user is interacting with the Chat Engine."
  )

class ResponseField(BaseModel):
  Text: str = Field(
    default="",
    description="Text response to be sent out. This field will also be picked by a Text to Speech Synthesis module if enabled for speech based bots.",
  )
  CleanedText: str = Field(
    default="", description="Text response from the Chat Engine with all SSML/HTML tags removed."
  )
  NeedUserResponse: Optional[bool] = Field(
    default=True,
    description="This field can be used by end user applications to deduce if user response is needed or not for a dialog initiated query. This is set to true automatically if form filling is active and one or more slots are missing.",
  )
  IsFinal: bool = Field(
    default=False,
    description="This field to indicate the final response chunk when streaming. The chunk with IsFinal=true will contain the full Chat Engine response attributes.",
  )

class ChatResponse(BaseModel):
  UserId: str = Field(
    default="",
    description="Unique identifier to recognize which user is interacting with the Chat Engine. This is populated from the request JSON.",
  )
  QueryId: str = Field(
    default="",
    description="Unique identifier for the user query assigned automatically by the Chat Engine unless specified in request JSON.",
  )
  Response: ResponseField = Field(
    default=ResponseField(),
    description="Final response template from the Chat Engine. This field can be picked up from domain rule files or can be formulated directly from custom plugin modules.",
  )
  Metadata: Optional[Dict[str, Any]] = Field(
    default={"SessionId": "", "StreamId": ""},
    description="Any additional information related to the request.",
  )

class EventResponse(BaseModel):
  UserId: str = Field(
    default="",
    description="Unique identifier to recognize which user is interacting with the Chat Engine. This is populated from the request JSON.",
  )
  Events: List[Dict[str, Any]] = Field(
    default=[], description="The generated event list for the provided EventType from Chat Engine."
  )
  Response: ResponseField = Field(
    default=ResponseField(),
    description="Final response template from the Chat Engine. This field can be picked up from domain rule files or can be formulated directly from custom plugin modules.",
  )

```

> Let’s understand the APIs and their schemas a little. There are two APIs used by the Chat Controller microservice: `/chat` and `/event`.
>     1. Events such as pipeline creation and deletion are sent by the Chat Controller to the `/event` endpoint. In most cases, you will not need to modify the default `/event` endpoint that we will define in the next step.
>     2. User questions are sent by the Chat Controller to the `/chat` endpoint, along with the `UserId` and an optional `QueryId`. The response schema for this API contains a `Response` attribute, which contains the details of the response. In this tutorial, we only need to manage two sub-fields: `Response.Text` (which contains the chunk that we are streaming) and `Response.IsFinal` (which indicates whether the stream is complete).
  2. Create the actual custom Plugin with the LangChain agent and the `/chat` and `/event` APIs. Update `plugin/langchain_agent.py` with the following code:
> ```
from fastapi import APIRouter, status, Body, Response
from fastapi.responses import StreamingResponse
import logging
import os
import sys
from typing_extensions import Annotated
from typing import Union, Dict
import json
from langchain_community.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.memory import ChatMessageHistory
from langchain.tools.ddg_search import DuckDuckGoSearchRun
from langchain_core.runnables import (
  RunnableParallel,
  RunnablePassthrough,
)
from langchain.tools import tool
logger = logging.getLogger("plugin")
router = APIRouter()
sys.path.append(os.path.dirname(__file__))
from schemas import ChatRequest, EventRequest, EventResponse, ChatResponse
EVENTS_NOT_REQUIRING_RESPONSE = [
  "system.event_pipeline_acquired",
  "system.event_pipeline_released",
  "system.event_exit",
]
duckduckgo = DuckDuckGoSearchRun()

@tool
def ddg_search(query: str):
"""Performs a duckduck go search"""
  logger.info(f"Input to DDG: {query}")
  answer = duckduckgo.run(query)
  logger.info(f"Answer from DDG: {answer}")
  return answer

rephraser_prompt = ChatPromptTemplate.from_messages(
  [
    (
      "system",
      f"You are an assistant whose job is to rephrase the question into a standalone question, based on the conversation history."
      f"The rephrased question should be as short and simple as possible. Do not attempt to provide an answer of your own!",
    ),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{query}"),
  ]
)
wiki_prompt = ChatPromptTemplate.from_messages(
  [
    (
      "system",
      "Answer the given question from the provided context. Only use the context to form an answer.\nContext: {context}",
    ),
    ("user", "{query}"),
  ]
)
chat_history_map = {}
llm = ChatOpenAI(model="gpt-4-turbo")
output_parser = StrOutputParser()
chain = (
  rephraser_prompt
  | llm
  | output_parser
  | RunnableParallel({"context": ddg_search, "query": RunnablePassthrough()})
  | wiki_prompt
  | llm
  | output_parser
)
chain_with_history = RunnableWithMessageHistory(
  chain,
  lambda session_id: chat_history_map.get(session_id),
  input_messages_key="query",
  history_messages_key="history",
)

@router.post(
  "/chat",
  status_code=status.HTTP_200_OK,
)
async def chat(
  request: Annotated[
    ChatRequest,
    Body(
      description="Chat Engine Request JSON. All the fields populated as part of this JSON is also available as part of request JSON."
    ),
  ],
  response: Response,
) -> StreamingResponse:
"""
  This endpoint can be used to provide response to query driven user request.
  """
  req = request.dict(exclude_none=True)
  logger.info(f"Received request JSON at /chat endpoint: {json.dumps(req,indent=4)}")
  try:
    session_id = req["UserId"]
    question = req["Query"]
    if session_id not in chat_history_map:
      chat_history_map[session_id] = ChatMessageHistory(messages=[])
    def generator(question: str, session_id: str):
      full_response = ""
      if question:
        for chunk in chain_with_history.stream(
          {"query": question}, config={"configurable": {"session_id": session_id}}
        ):
          if not chunk:
            continue
          full_response += chunk
          json_chunk = ChatResponse()
          json_chunk.Response.Text = chunk
          json_chunk = json.dumps(json_chunk.dict())
          yield json_chunk
      json_chunk = ChatResponse()
      json_chunk.Response.IsFinal = True
      json_chunk.Response.CleanedText = full_response
      json_chunk = json.dumps(json_chunk.dict())
      yield json_chunk
    return StreamingResponse(generator(question, session_id), media_type="text/event-stream")
  except Exception as e:
    response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
    return {"StatusMessage": str(e)}

@router.post("/event", status_code=status.HTTP_200_OK)
async def event(
  request: Annotated[
    EventRequest,
    Body(
      description="Chat Engine Request JSON. All the fields populated as part of this JSON is also available as part of request JSON."
    ),
  ],
  response: Response,
) -> Union[EventResponse, Dict[str, str]]:
"""
  This endpoint can be used to provide response to an event driven user request.
  """
  req = request.dict(exclude_none=True)
  logger.info(f"Received request JSON at /event endpoint: {json.dumps(req,indent=4)}")
  try:
    resp = EventResponse()
    resp.UserId = req["UserId"]
    resp.Response.IsFinal = True
    if req["EventType"] in EVENTS_NOT_REQUIRING_RESPONSE:
      resp.Response.NeedUserResponse = False
    return resp
  except Exception as e:
    response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
    return {"StatusMessage": str(e)}

```

> Let’s understand the above code.
>     1. The `ddg_search` tool takes in the query, performs a DuckDuckGo search, and returns the top responses.
>     2. The `rephraser_prompt` is used to rephrase the current question into a single, simple question based on the conversation history, represented by the `history` placeholder.
>     3. The `chain` runnable performs rephrasing of the query, passes the rephrased to the `ddg_search` tool, and finally calls the LLM again to fetch the answer from the DuckDuckGo search results.
>     4. The `chain_with_history` is the actual runnable that we will be using to generate answers. It fetches the correct conversation history based on the `session_id` and populates the `rephraser_prompt` with the right inputs.
>     5. Finally, the `/chat` endpoint invokes the `chain_with_history` with the right inputs. In this case, we return a generator where we call the `stream` method of the runnable, format the response into the `ChatResponse` schema, and push the chunk into the response stream.
> If you are using a custom LangChain runnable or if you want to modify the way you call the runnable, you will only need to make changes in the `generator` method.
  3. Register this plugin. Add the following to `plugin_config.yaml`.
> ```
config:
workers:1
timeout:30
plugins:
-name:langchain
path:./plugin/langchain_agent.py

```

  4. Add the Python dependencies used by this runnable in `deploy/docker/dockerfiles/plugin_server.Dockerfile`. This will install the custom dependencies when the Plugin server is being built.
> ```
##############################
# Install custom dependencies
##############################
RUNpipinstalllangchain==0.1.1\
langchain-community==0.0.13\
langchain-core==0.1.12\
duckduckgo-search==5.3.1b1

```



Note
If you see a crash in the plugin server or an issue with fetching a response from DuckDuckGo, try using a more recent `duckduckgo-search` version.
  1. Deploy the Plugin server for testing.
>     1. Set the OpenAI API key if it is not already set.
>> ```
exportOPENAI_API_KEY=...

```

>     2. Run the bot using the [Docker Environment](https://docs.nvidia.com/ace/ace-agent/latest/deployment/docker-environment.html#docker-environment).
>> ```
exportBOT_PATH=./samples/langchain_tutorial_bot
sourcedeploy/docker/docker_init.sh
dockercompose-fdeploy/docker/docker-compose.ymlup--buildplugin-server-d

```

  2. We can test the endpoint by running the following CURL command:
> ```
curl -X 'POST' \
 'http://localhost:9002/langchain/chat' \
 -H 'accept: application/json' \
 -H 'Content-Type: application/json' \
 -d '{
 "Query": "Who is the president of the United States?",
 "UserId": "user1"
}'

```

  3. After you are done testing the plugin, to stop the server, run:
> ```
dockercompose-fdeploy/docker/docker-compose.ymldown

```



## Connecting the Plugin to the Chat Controller[#](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-langchain-bot.html#connecting-the-plugin-to-the-chat-controller "Link to this heading")
Now that the Plugin is functional, let’s create the configs to connect the Chat Controller to the Plugin server, as well as enable speech.
> [![LangChain UCS App](https://docs.nvidia.com/ace/ace-agent/latest/_images/langchain-ucs-app.png) ](https://docs.nvidia.com/ace/ace-agent/latest/_images/langchain-ucs-app.png)
  1. Copy the `model_config.yaml` and `speech_config.yaml` files from `samples/chitchat_bot`. They represent the common settings for a speech pipeline.
  2. Update the `server` URL in the `dialog_manager` component in the `speech_config.yaml` file to point to the plugin we created in the previous section.
> ```
dialog_manager:
DialogManager:
server:"http://localhost:9002/langchain"
use_streaming:true

```

> With this change, the Chat Controller will directly call the `/chat` and `/event` endpoints of the Plugin server.
  3. Deploy the bot using the Docker environment.
  4. Set the environment variables required for the `docker-compose.yml` file.
> ```
exportBOT_PATH=./samples/langchain_tutorial_bot/
sourcedeploy/docker/docker_init.sh

```

  5. For Plugin Server Architecture bots, we need to use the `speech_lite` pipeline configuration for the Chat Controller microservice. Update the `PIPELINE` variable in `deploy/docker/docker_init.sh` or override by setting the `PIPELINE` environment variable manually.
> ```
exportPIPELINE=speech_lite

```

  6. Deploy the Riva ASR and TTS speech models.
> ```
dockercompose-fdeploy/docker/docker-compose.ymlupmodel-utils-speech

```

  7. Deploy the Plugin server with the LangChain plugin.
> ```
dockercompose-fdeploy/docker/docker-compose.ymlup--buildplugin-server-d

```

  8. Deploy the Chat Controller microservice with the gRPC interface.
> ```
dockercompose-fdeploy/docker/docker-compose.ymlupchat-controller-d

```

  9. Deploy the speech sample frontend application.
> ```
dockercompose-fdeploy/docker/docker-compose.ymlupbot-web-ui-clientbot-web-ui-speech-d

```

> Notice that we are not deploying the Chat Engine microservice at all.
> You can interact with the bot using your browser at `http://<YOUR_IP_ADDRESS>:7006`.


Here’s an example dialog with the bot.
[![LangChain Bot](https://docs.nvidia.com/ace/ace-agent/latest/_images/langchain-bot.png) ](https://docs.nvidia.com/ace/ace-agent/latest/_images/langchain-bot.png)
Note
If you want to utilize Colang along with the LangChain Agent or you want to add more modalities, refer to the [Building a Bot using Colang 2.0 and Event Interface](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-colang.html#build-bot-colang) and [Building a Low Latency Speech-To-Speech RAG Bot](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html#build-bot-rag) tutorials.
[ previous Building a Bot using Colang 2.0 and Event Interface ](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-colang.html "previous page") [ next Building a Low Latency Speech-To-Speech RAG Bot ](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-bot-rag.html "next page")
On this page 
  * [Creating the LangChain Plugin](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-langchain-bot.html#creating-the-langchain-plugin)
  * [Connecting the Plugin to the Chat Controller](https://docs.nvidia.com/ace/ace-agent/latest/tutorials/build-langchain-bot.html#connecting-the-plugin-to-the-chat-controller)


[ ![NVIDIA](https://docs.nvidia.com/ace/ace-agent/latest/_static/nvidia-logo-horiz-rgb-1c-blk-for-screen.svg) ![NVIDIA](https://docs.nvidia.com/ace/ace-agent/latest/_static/nvidia-logo-horiz-rgb-1c-wht-for-screen.svg) ](https://www.nvidia.com)
[Privacy Policy](https://www.nvidia.com/en-us/about-nvidia/privacy-policy/) | [Manage My Privacy](https://www.nvidia.com/en-us/about-nvidia/privacy-center/) | [Do Not Sell or Share My Data](https://www.nvidia.com/en-us/preferences/start/) | [Terms of Service](https://www.nvidia.com/en-us/about-nvidia/terms-of-service/) | [Accessibility](https://www.nvidia.com/en-us/about-nvidia/accessibility/) | [Corporate Policies](https://www.nvidia.com/en-us/about-nvidia/company-policies/) | [Product Security](https://www.nvidia.com/en-us/product-security/) | [Contact](https://www.nvidia.com/en-us/contact/)
Copyright © 2023-2025, NVIDIA Corporation. 
Last updated on Feb 27, 2025. 
